{"2024-11-20T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2411.13534v1","updated":"2024-11-20T18:35:41Z","published":"2024-11-20T18:35:41Z","title":"Predictive Insights into LGBTQ+ Minority Stress: A Transductive\n  Exploration of Social Media Discourse","summary":"  Individuals who identify as sexual and gender minorities, including lesbian,\ngay, bisexual, transgender, queer, and others (LGBTQ+) are more likely to\nexperience poorer health than their heterosexual and cisgender counterparts.\nOne primary source that drives these health disparities is minority stress\n(i.e., chronic and social stressors unique to LGBTQ+ communities' experiences\nadapting to the dominant culture). This stress is frequently expressed in\nLGBTQ+ users' posts on social media platforms. However, these expressions are\nnot just straightforward manifestations of minority stress. They involve\nlinguistic complexity (e.g., idiom or lexical diversity), rendering them\nchallenging for many traditional natural language processing methods to detect.\nIn this work, we designed a hybrid model using Graph Neural Networks (GNN) and\nBidirectional Encoder Representations from Transformers (BERT), a pre-trained\ndeep language model to improve the classification performance of minority\nstress detection. We experimented with our model on a benchmark social media\ndataset for minority stress detection (LGBTQ+ MiSSoM+). The dataset is\ncomprised of 5,789 human-annotated Reddit posts from LGBTQ+ subreddits. Our\napproach enables the extraction of hidden linguistic nuances through\npretraining on a vast amount of raw data, while also engaging in transductive\nlearning to jointly develop representations for both labeled training data and\nunlabeled test data. The RoBERTa-GCN model achieved an accuracy of 0.86 and an\nF1 score of 0.86, surpassing the performance of other baseline models in\npredicting LGBTQ+ minority stress. Improved prediction of minority stress\nexpressions on social media could lead to digital health interventions to\nimprove the wellbeing of LGBTQ+ people-a community with high rates of\nstress-sensitive health problems.\n","authors":["S. Chapagain","Y. Zhao","T. K. Rohleen","S. M. Hamdi","S. F. Boubrahimi","R. E. Flinn","E. M. Lund","D. Klooster","J. R. Scheer","C. J. Cascalheira"],"pdf_url":"https://arxiv.org/pdf/2411.13534v1.pdf","comment":"This paper is accepted in 2024 IEEE 11th International Conference on\n  Data Science and Advanced Analytics (DSAA)"},{"id":"http://arxiv.org/abs/2305.01626v3","updated":"2024-11-20T18:30:49Z","published":"2023-05-02T17:38:21Z","title":"Basic syntax from speech: Spontaneous concatenation in unsupervised deep\n  neural networks","summary":"  Computational models of syntax are predominantly text-based. Here we propose\nthat the most basic first step in the evolution of syntax can be modeled\ndirectly from raw speech in a fully unsupervised way. We focus on one of the\nmost ubiquitous and elementary suboperation of syntax -- concatenation. We\nintroduce spontaneous concatenation: a phenomenon where convolutional neural\nnetworks (CNNs) trained on acoustic recordings of individual words start\ngenerating outputs with two or even three words concatenated without ever\naccessing data with multiple words in the input. We replicate this finding in\nseveral independently trained models with different hyperparameters and\ntraining data. Additionally, networks trained on two words learn to embed words\ninto novel unobserved word combinations. We also show that the concatenated\noutputs contain precursors to compositionality. To our knowledge, this is a\npreviously unreported property of CNNs trained in the ciwGAN/fiwGAN setting on\nraw speech and has implications both for our understanding of how these\narchitectures learn as well as for modeling syntax and its evolution in the\nbrain from raw acoustic inputs. We also propose a potential neural mechanism\ncalled disinhibition that outlines a possible neural pathway towards\nconcatenation and compositionality and suggests our modeling is useful for\ngenerating testable prediction for biological and artificial neural processing\nof speech.\n","authors":["Gašper Beguš","Thomas Lu","Zili Wang"],"pdf_url":"https://arxiv.org/pdf/2305.01626v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13518v1","updated":"2024-11-20T18:10:19Z","published":"2024-11-20T18:10:19Z","title":"Advancing Complex Medical Communication in Arabic with Sporo AraSum:\n  Surpassing Existing Large Language Models","summary":"  The increasing demand for multilingual capabilities in healthcare underscores\nthe need for AI models adept at processing diverse languages, particularly in\nclinical documentation and decision-making. Arabic, with its complex\nmorphology, syntax, and diglossia, poses unique challenges for natural language\nprocessing (NLP) in medical contexts. This case study evaluates Sporo AraSum, a\nlanguage model tailored for Arabic clinical documentation, against JAIS, the\nleading Arabic NLP model. Using synthetic datasets and modified PDQI-9 metrics\nmodified ourselves for the purposes of assessing model performances in a\ndifferent language. The study assessed the models' performance in summarizing\npatient-physician interactions, focusing on accuracy, comprehensiveness,\nclinical utility, and linguistic-cultural competence.\n  Results indicate that Sporo AraSum significantly outperforms JAIS in\nAI-centric quantitative metrics and all qualitative attributes measured in our\nmodified version of the PDQI-9. AraSum's architecture enables precise and\nculturally sensitive documentation, addressing the linguistic nuances of Arabic\nwhile mitigating risks of AI hallucinations. These findings suggest that Sporo\nAraSum is better suited to meet the demands of Arabic-speaking healthcare\nenvironments, offering a transformative solution for multilingual clinical\nworkflows. Future research should incorporate real-world data to further\nvalidate these findings and explore broader integration into healthcare\nsystems.\n","authors":["Chanseo Lee","Sonu Kumar","Kimon A. Vogt","Sam Meraj","Antonia Vogt"],"pdf_url":"https://arxiv.org/pdf/2411.13518v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2411.06713"},{"id":"http://arxiv.org/abs/2406.16838v2","updated":"2024-11-20T17:57:26Z","published":"2024-06-24T17:45:59Z","title":"From Decoding to Meta-Generation: Inference-time Algorithms for Large\n  Language Models","summary":"  One of the most striking findings in modern research on large language models\n(LLMs) is that scaling up compute during training leads to better results.\nHowever, less attention has been given to the benefits of scaling compute\nduring inference. This survey focuses on these inference-time approaches. We\nexplore three areas under a unified mathematical formalism: token-level\ngeneration algorithms, meta-generation algorithms, and efficient generation.\nToken-level generation algorithms, often called decoding algorithms, operate by\nsampling a single token at a time or constructing a token-level search space\nand then selecting an output. These methods typically assume access to a\nlanguage model's logits, next-token distributions, or probability scores.\nMeta-generation algorithms work on partial or full sequences, incorporating\ndomain knowledge, enabling backtracking, and integrating external information.\nEfficient generation methods aim to reduce token costs and improve the speed of\ngeneration. Our survey unifies perspectives from three research communities:\ntraditional natural language processing, modern LLMs, and machine learning\nsystems.\n","authors":["Sean Welleck","Amanda Bertsch","Matthew Finlayson","Hailey Schoelkopf","Alex Xie","Graham Neubig","Ilia Kulikov","Zaid Harchaoui"],"pdf_url":"https://arxiv.org/pdf/2406.16838v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13504v1","updated":"2024-11-20T17:55:38Z","published":"2024-11-20T17:55:38Z","title":"Disentangling Memory and Reasoning Ability in Large Language Models","summary":"  Large Language Models (LLMs) have demonstrated strong performance in handling\ncomplex tasks requiring both extensive knowledge and reasoning abilities.\nHowever, the existing LLM inference pipeline operates as an opaque process\nwithout explicit separation between knowledge retrieval and reasoning steps,\nmaking the model's decision-making process unclear and disorganized. This\nambiguity can lead to issues such as hallucinations and knowledge forgetting,\nwhich significantly impact the reliability of LLMs in high-stakes domains. In\nthis paper, we propose a new inference paradigm that decomposes the complex\ninference process into two distinct and clear actions: (1) memory recall: which\nretrieves relevant knowledge, and (2) reasoning: which performs logical steps\nbased on the recalled knowledge. To facilitate this decomposition, we introduce\ntwo special tokens memory and reason, guiding the model to distinguish between\nsteps that require knowledge retrieval and those that involve reasoning. Our\nexperiment results show that this decomposition not only improves model\nperformance but also enhances the interpretability of the inference process,\nenabling users to identify sources of error and refine model responses\neffectively. The code is available at\nhttps://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning.\n","authors":["Mingyu Jin","Weidi Luo","Sitao Cheng","Xinyi Wang","Wenyue Hua","Ruixiang Tang","William Yang Wang","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.13504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13485v1","updated":"2024-11-20T17:35:21Z","published":"2024-11-20T17:35:21Z","title":"Utilizing Large Language Models to Synthesize Product Desirability\n  Datasets","summary":"  This research explores the application of large language models (LLMs) to\ngenerate synthetic datasets for Product Desirability Toolkit (PDT) testing, a\nkey component in evaluating user sentiment and product experience. Utilizing\ngpt-4o-mini, a cost-effective alternative to larger commercial LLMs, three\nmethods, Word+Review, Review+Word, and Supply-Word, were each used to\nsynthesize 1000 product reviews. The generated datasets were assessed for\nsentiment alignment, textual diversity, and data generation cost. Results\ndemonstrated high sentiment alignment across all methods, with Pearson\ncorrelations ranging from 0.93 to 0.97. Supply-Word exhibited the highest\ndiversity and coverage of PDT terms, although with increased generation costs.\nDespite minor biases toward positive sentiments, in situations with limited\ntest data, LLM-generated synthetic data offers significant advantages,\nincluding scalability, cost savings, and flexibility in dataset production.\n","authors":["John D. Hastings","Sherri Weitl-Harms","Joseph Doty","Zachary L. Myers","Warren Thompson"],"pdf_url":"https://arxiv.org/pdf/2411.13485v1.pdf","comment":"9 pages, 2 figures, 6 tables"},{"id":"http://arxiv.org/abs/2411.13477v1","updated":"2024-11-20T17:23:40Z","published":"2024-11-20T17:23:40Z","title":"PatentEdits: Framing Patent Novelty as Textual Entailment","summary":"  A patent must be deemed novel and non-obvious in order to be granted by the\nUS Patent Office (USPTO). If it is not, a US patent examiner will cite the\nprior work, or prior art, that invalidates the novelty and issue a non-final\nrejection. Predicting what claims of the invention should change given the\nprior art is an essential and crucial step in securing invention rights, yet\nhas not been studied before as a learnable task. In this work we introduce the\nPatentEdits dataset, which contains 105K examples of successful revisions that\novercome objections to novelty. We design algorithms to label edits sentence by\nsentence, then establish how well these edits can be predicted with large\nlanguage models (LLMs). We demonstrate that evaluating textual entailment\nbetween cited references and draft sentences is especially effective in\npredicting which inventive claims remained unchanged or are novel in relation\nto prior art.\n","authors":["Ryan Lee","Alexander Spangher","Xuezhe Ma"],"pdf_url":"https://arxiv.org/pdf/2411.13477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13476v1","updated":"2024-11-20T17:22:31Z","published":"2024-11-20T17:22:31Z","title":"When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context\n  Training","summary":"  Extending context window sizes allows large language models (LLMs) to process\nlonger sequences and handle more complex tasks. Rotary Positional Embedding\n(RoPE) has become the de facto standard due to its relative positional encoding\nproperties that benefit long-context training. However, we observe that using\nRoPE with BFloat16 format results in numerical issues, causing it to deviate\nfrom its intended relative positional encoding, especially in long-context\nscenarios. This issue arises from BFloat16's limited precision and accumulates\nas context length increases, with the first token contributing significantly to\nthis problem. To address this, we develop AnchorAttention, a plug-and-play\nattention method that alleviates numerical issues caused by BFloat16, improves\nlong-context capabilities, and speeds up training. AnchorAttention reduces\nunnecessary attention computations, maintains semantic coherence, and boosts\ncomputational efficiency by treating the first token as a shared anchor with a\nconsistent position ID, making it visible to all documents within the training\ncontext. Experiments on three types of LLMs demonstrate that AnchorAttention\nsignificantly improves long-context performance and reduces training time by\nover 50\\% compared to standard full attention mechanisms, while preserving the\noriginal LLM's capabilities on general tasks. Our code is available at\nhttps://github.com/haonan3/AnchorContext.\n","authors":["Haonan Wang","Qian Liu","Chao Du","Tongyao Zhu","Cunxiao Du","Kenji Kawaguchi","Tianyu Pang"],"pdf_url":"https://arxiv.org/pdf/2411.13476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13453v1","updated":"2024-11-20T16:59:41Z","published":"2024-11-20T16:59:41Z","title":"LIMBA: An Open-Source Framework for the Preservation and Valorization of\n  Low-Resource Languages using Generative Models","summary":"  Minority languages are vital to preserving cultural heritage, yet they face\ngrowing risks of extinction due to limited digital resources and the dominance\nof artificial intelligence models trained on high-resource languages. This\nwhite paper proposes a framework to generate linguistic tools for low-resource\nlanguages, focusing on data creation to support the development of language\nmodels that can aid in preservation efforts. Sardinian, an endangered language,\nserves as the case study to demonstrate the framework's effectiveness. By\naddressing the data scarcity that hinders intelligent applications for such\nlanguages, we contribute to promoting linguistic diversity and support ongoing\nefforts in language standardization and revitalization through modern\ntechnologies.\n","authors":["Salvatore Mario Carta","Stefano Chessa","Giulia Contu","Andrea Corriga","Andrea Deidda","Gianni Fenu","Luca Frigau","Alessandro Giuliani","Luca Grassi","Marco Manolo Manca","Mirko Marras","Francesco Mola","Bastianino Mossa","Piergiorgio Mura","Marco Ortu","Leonardo Piano","Simone Pisano","Alessia Pisu","Alessandro Sebastian Podda","Livio Pompianu","Simone Seu","Sandro Gabriele Tiddia"],"pdf_url":"https://arxiv.org/pdf/2411.13453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13451v1","updated":"2024-11-20T16:54:15Z","published":"2024-11-20T16:54:15Z","title":"AdaptAgent: Adapting Multimodal Web Agents with Few-Shot Learning from\n  Human Demonstrations","summary":"  State-of-the-art multimodal web agents, powered by Multimodal Large Language\nModels (MLLMs), can autonomously execute many web tasks by processing user\ninstructions and interacting with graphical user interfaces (GUIs). Current\nstrategies for building web agents rely on (i) the generalizability of\nunderlying MLLMs and their steerability via prompting, and (ii) large-scale\nfine-tuning of MLLMs on web-related tasks. However, web agents still struggle\nto automate tasks on unseen websites and domains, limiting their applicability\nto enterprise-specific and proprietary platforms. Beyond generalization from\nlarge-scale pre-training and fine-tuning, we propose building agents for\nfew-shot adaptability using human demonstrations. We introduce the AdaptAgent\nframework that enables both proprietary and open-weights multimodal web agents\nto adapt to new websites and domains using few human demonstrations (up to 2).\nOur experiments on two popular benchmarks -- Mind2Web & VisualWebArena -- show\nthat using in-context demonstrations (for proprietary models) or\nmeta-adaptation demonstrations (for meta-learned open-weights models) boosts\ntask success rate by 3.36% to 7.21% over non-adapted state-of-the-art models,\ncorresponding to a relative increase of 21.03% to 65.75%. Furthermore, our\nadditional analyses (a) show the effectiveness of multimodal demonstrations\nover text-only ones, (b) shed light on the influence of different data\nselection strategies during meta-learning on the generalization of the agent,\nand (c) demonstrate the effect of number of few-shot examples on the web\nagent's success rate. Overall, our results unlock a complementary axis for\ndeveloping widely applicable multimodal web agents beyond large-scale\npre-training and fine-tuning, emphasizing few-shot adaptability.\n","authors":["Gaurav Verma","Rachneet Kaur","Nishan Srishankar","Zhen Zeng","Tucker Balch","Manuela Veloso"],"pdf_url":"https://arxiv.org/pdf/2411.13451v1.pdf","comment":"18 pages, 3 figures, an abridged version to appear in NeurIPS 2024\n  AFM Workshop"},{"id":"http://arxiv.org/abs/2411.13425v1","updated":"2024-11-20T16:09:22Z","published":"2024-11-20T16:09:22Z","title":"WaterPark: A Robustness Assessment of Language Model Watermarking","summary":"  To mitigate the misuse of large language models (LLMs), such as\ndisinformation, automated phishing, and academic cheating, there is a pressing\nneed for the capability of identifying LLM-generated texts. Watermarking\nemerges as one promising solution: it plants statistical signals into LLMs'\ngenerative processes and subsequently verifies whether LLMs produce given\ntexts. Various watermarking methods (``watermarkers'') have been proposed; yet,\ndue to the lack of unified evaluation platforms, many critical questions remain\nunder-explored: i) What are the strengths/limitations of various watermarkers,\nespecially their attack robustness? ii) How do various design choices impact\ntheir robustness? iii) How to optimally operate watermarkers in adversarial\nenvironments?\n  To fill this gap, we systematize existing LLM watermarkers and watermark\nremoval attacks, mapping out their design spaces. We then develop WaterPark, a\nunified platform that integrates 10 state-of-the-art watermarkers and 12\nrepresentative attacks. More importantly, leveraging WaterPark, we conduct a\ncomprehensive assessment of existing watermarkers, unveiling the impact of\nvarious design choices on their attack robustness. For instance, a\nwatermarker's resilience to increasingly intensive attacks hinges on its\ncontext dependency. We further explore the best practices to operate\nwatermarkers in adversarial environments. For instance, using a generic\ndetector alongside a watermark-specific detector improves the security of\nvulnerable watermarkers. We believe our study sheds light on current LLM\nwatermarking techniques while WaterPark serves as a valuable testbed to\nfacilitate future research.\n","authors":["Jiacheng Liang","Zian Wang","Lauren Hong","Shouling Ji","Ting Wang"],"pdf_url":"https://arxiv.org/pdf/2411.13425v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2411.13424v1","updated":"2024-11-20T16:09:16Z","published":"2024-11-20T16:09:16Z","title":"CAFE A Novel Code switching Dataset for Algerian Dialect French and\n  English","summary":"  The paper introduces and publicly releases (Data download link available\nafter acceptance) CAFE -- the first Code-switching dataset between Algerian\ndialect, French, and english languages. The CAFE speech data is unique for (a)\nits spontaneous speaking style in vivo human-human conversation capturing\nphenomena like code-switching and overlapping speech, (b) addresses distinct\nlinguistic challenges in North African Arabic dialect; (c) the CAFE captures\ndialectal variations from various parts of Algeria within different\nsociolinguistic contexts. CAFE data contains approximately 37 hours of speech,\nwith a subset, CAFE-small, of 2 hours and 36 minutes released with manual human\nannotation including speech segmentation, transcription, explicit annotation of\ncode-switching points, overlapping speech, and other events such as noises, and\nlaughter among others. The rest approximately 34.58 hours contain pseudo label\ntranscriptions. In addition to the data release, the paper also highlighted the\nchallenges of using state-of-the-art Automatic Speech Recognition (ASR) models\nsuch as Whisper large-v2,3 and PromptingWhisper to handle such content.\nFollowing, we benchmark CAFE data with the aforementioned Whisper models and\nshow how well-designed data processing pipelines and advanced decoding\ntechniques can improve the ASR performance in terms of Mixed Error Rate (MER)\nof 0.310, Character Error Rate (CER) of 0.329 and Word Error Rate (WER) of\n0.538.\n","authors":["Houssam Eddine-Othman Lachemat","Akli Abbas","Nourredine Oukas","Yassine El Kheir","Samia Haboussi","Absar Showdhury Shammur"],"pdf_url":"https://arxiv.org/pdf/2411.13424v1.pdf","comment":"24 pages, submitted to tallip"},{"id":"http://arxiv.org/abs/2411.13409v1","updated":"2024-11-20T15:48:21Z","published":"2024-11-20T15:48:21Z","title":"Unification of Balti and trans-border sister dialects in the essence of\n  LLMs and AI Technology","summary":"  The language called Balti belongs to the Sino-Tibetan, specifically the\nTibeto-Burman language family. It is understood with variations, across\npopulations in India, China, Pakistan, Nepal, Tibet, Burma, and Bhutan,\ninfluenced by local cultures and producing various dialects. Considering the\ndiverse cultural, socio-political, religious, and geographical impacts, it is\nimportant to step forward unifying the dialects, the basis of common root,\nlexica, and phonological perspectives, is vital. In the era of globalization\nand the increasingly frequent developments in AI technology, understanding the\ndiversity and the efforts of dialect unification is important to understanding\ncommonalities and shortening the gaps impacted by unavoidable circumstances.\nThis article analyzes and examines how artificial intelligence AI in the\nessence of Large Language Models LLMs, can assist in analyzing, documenting,\nand standardizing the endangered Balti Language, based on the efforts made in\ndifferent dialects so far.\n","authors":["Muhammad Sharif","Jiangyan Yi","Muhammad Shoaib"],"pdf_url":"https://arxiv.org/pdf/2411.13409v1.pdf","comment":"Accepted by IEEE conference ISCSLP 2024"},{"id":"http://arxiv.org/abs/2411.13407v1","updated":"2024-11-20T15:46:48Z","published":"2024-11-20T15:46:48Z","title":"Transformer-Based Contextualized Language Models Joint with Neural\n  Networks for Natural Language Inference in Vietnamese","summary":"  Natural Language Inference (NLI) is a task within Natural Language Processing\n(NLP) that holds value for various AI applications. However, there have been\nlimited studies on Natural Language Inference in Vietnamese that explore the\nconcept of joint models. Therefore, we conducted experiments using various\ncombinations of contextualized language models (CLM) and neural networks. We\nuse CLM to create contextualized work presentations and use Neural Networks for\nclassification. Furthermore, we have evaluated the strengths and weaknesses of\neach joint model and identified the model failure points in the Vietnamese\ncontext. The highest F1 score in this experiment, up to 82.78\\% in the\nbenchmark dataset (ViNLI). By conducting experiments with various models, the\nmost considerable size of the CLM is XLM-R (355M). That combination has\nconsistently demonstrated superior performance compared to fine-tuning strong\npre-trained language models like PhoBERT (+6.58\\%), mBERT (+19.08\\%), and XLM-R\n(+0.94\\%) in terms of F1-score. This article aims to introduce a novel approach\nor model that attains improved performance for Vietnamese NLI. Overall, we find\nthat the joint approach of CLM and neural networks is simple yet capable of\nachieving high-quality performance, which makes it suitable for applications\nthat require efficient resource utilization.\n","authors":["Dat Van-Thanh Nguyen","Tin Van Huynh","Kiet Van Nguyen","Ngan Luu-Thuy Nguyen"],"pdf_url":"https://arxiv.org/pdf/2411.13407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13405v1","updated":"2024-11-20T15:45:08Z","published":"2024-11-20T15:45:08Z","title":"On the Way to LLM Personalization: Learning to Remember User\n  Conversations","summary":"  Large Language Models (LLMs) have quickly become an invaluable assistant for\na variety of tasks. However, their effectiveness is constrained by their\nability to tailor responses to human preferences and behaviors via\npersonalization. Prior work in LLM personalization has largely focused on style\ntransfer or incorporating small factoids about the user, as knowledge injection\nremains an open challenge. In this paper, we explore injecting knowledge of\nprior conversations into LLMs to enable future work on less redundant,\npersonalized conversations. We identify two real-world constraints: (1)\nconversations are sequential in time and must be treated as such during\ntraining, and (2) per-user personalization is only viable in\nparameter-efficient settings. To this aim, we propose PLUM, a pipeline\nperforming data augmentation for up-sampling conversations as question-answer\npairs, that are then used to finetune a low-rank adaptation adapter with a\nweighted cross entropy loss. Even in this first exploration of the problem, we\nperform competitively with baselines such as RAG, attaining an accuracy of\n81.5% across 100 conversations.\n","authors":["Lucie Charlotte Magister","Katherine Metcalf","Yizhe Zhang","Maartje ter Hoeve"],"pdf_url":"https://arxiv.org/pdf/2411.13405v1.pdf","comment":"16 pages, 6 tables, 3 figures"},{"id":"http://arxiv.org/abs/2409.08435v3","updated":"2024-11-20T15:41:38Z","published":"2024-09-13T00:03:19Z","title":"When Context Leads but Parametric Memory Follows in Large Language\n  Models","summary":"  Large language models (LLMs) have demonstrated remarkable progress in\nleveraging diverse knowledge sources. This study investigates how nine widely\nused LLMs allocate knowledge between local context and global parameters when\nanswering open-ended questions in knowledge-consistent scenarios. We introduce\na novel dataset, WikiAtomic, and systematically vary context sizes to analyze\nhow LLMs prioritize and utilize the provided information and their parametric\nknowledge in knowledge-consistent scenarios. Additionally, we also study their\ntendency to hallucinate under varying context sizes. Our findings reveal\nconsistent patterns across models, including a consistent reliance on both\ncontextual (around 70%) and parametric (around 30%) knowledge, and a decrease\nin hallucinations with increasing context. These insights highlight the\nimportance of more effective context organization and developing models that\nuse input more deterministically for robust performance.\n","authors":["Yufei Tao","Adam Hiatt","Erik Haake","Antonie J. Jetter","Ameeta Agrawal"],"pdf_url":"https://arxiv.org/pdf/2409.08435v3.pdf","comment":"Accepted by EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2411.13400v1","updated":"2024-11-20T15:38:33Z","published":"2024-11-20T15:38:33Z","title":"Executable QR codes with Machine Learning for Industrial Applications","summary":"  Executable QR codes, also known as eQR codes or just sQRy, are a special kind\nof QR codes that embed programs conceived to run on mobile devices like\nsmartphones. Since the program is directly encoded in binary form within the QR\ncode, it can be executed even when the reading device is not provided with\nInternet access. The applications of this technology are manifold, and range\nfrom smart user guides to advisory systems. The first programming language made\navailable for eQR is QRtree, which enables the implementation of decision trees\naimed, for example, at guiding the user in operating/maintaining a complex\nmachinery or for reaching a specific location.\n  In this work, an additional language is proposed, we term QRind, which was\nspecifically devised for Industry. It permits to integrate distinct\ncomputational blocks into the QR code, e.g., machine learning models to enable\npredictive maintenance and algorithms to ease machinery usage. QRind permits\nthe Industry 4.0/5.0 paradigms to be implemented, in part, also in those cases\nwhere Internet is unavailable.\n","authors":["Stefano Scanzio","Francesco Velluto","Matteo Rosani","Lukasz Wisniewski","Gianluca Cena"],"pdf_url":"https://arxiv.org/pdf/2411.13400v1.pdf","comment":"preprint, 4 pages, 2024"},{"id":"http://arxiv.org/abs/2312.05356v5","updated":"2024-11-20T14:22:06Z","published":"2023-12-08T20:28:08Z","title":"Neuron Patching: Semantic-based Neuron-level Language Model Repair for\n  Code Generation","summary":"  Language Models (LMs) have become widely used in software engineering,\nespecially for tasks such as code generation, where they are referred to as\ncode LMs. These models have proven effective in generating code, making it\neasier for developers to automate coding activities. However, research has\nhighlighted a significant limitation: despite their effectiveness, LMs often\nproduce code that is incorrect, buggy, or not fully functional. Updating these\nmodels with limited data can be prohibitively challenging, yet it is essential\nto maximize their utility. This may require hot-fix techniques (updating models\nwith limited data) to resolve. In this paper, we propose \\ul{M}odel\n\\ul{I}mprovement via \\ul{N}euron \\ul{T}argeting (\\textsc{MINT}), a novel\napproach for repairing code LMs. MINT leverages the semantic property of\nlanguage models to perform neuron-level repairs in a novel way. Further, by\nanalyzing the relationships between the model's latent representations, the\nincorrect outputs, and the desired outputs, \\textsc{MINT} determines which\nneurons are worth updating. This approach ensures that only the neurons crucial\nto the model's failure are targeted, avoiding unnecessary changes and allowing\nfor a more efficient and precise repair process. \\textsc{MINT} is effective,\nefficient, and reliable, capable of correcting a neural model by patching a\nminimum number of neurons (usually one or two neurons). Our approach is\nevaluated on three coding tasks: line-level code generation, shellcode\ngeneration, and intent-to-bash translation. The experimental results\ndemonstrate that the proposed approach significantly outperforms the\nstate-of-the-art in both effectiveness and efficiency measures. In addition, we\nanalyze and discuss the side effects of model repair techniques, including the\nbalance between generalization and specificity, and the performance after\nmultiple repairs in succession.\n","authors":["Jian Gu","Aldeida Aleti","Chunyang Chen","Hongyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.05356v5.pdf","comment":"13 pages, 7 figures, 7 tables, under peer-review"},{"id":"http://arxiv.org/abs/2411.13343v1","updated":"2024-11-20T14:15:18Z","published":"2024-11-20T14:15:18Z","title":"Fact-Level Confidence Calibration and Self-Correction","summary":"  Confidence calibration in LLMs, i.e., aligning their self-assessed confidence\nwith the actual accuracy of their responses, enabling them to self-evaluate the\ncorrectness of their outputs. However, current calibration methods for LLMs\ntypically estimate two scalars to represent overall response confidence and\ncorrectness, which is inadequate for long-form generation where the response\nincludes multiple atomic facts and may be partially confident and correct.\nThese methods also overlook the relevance of each fact to the query. To address\nthese challenges, we propose a Fact-Level Calibration framework that operates\nat a finer granularity, calibrating confidence to relevance-weighted\ncorrectness at the fact level. Furthermore, comprehensive analysis under the\nframework inspired the development of Confidence-Guided Fact-level\nSelf-Correction ($\\textbf{ConFix}$), which uses high-confidence facts within a\nresponse as additional knowledge to improve low-confidence ones. Extensive\nexperiments across four datasets and six models demonstrate that ConFix\neffectively mitigates hallucinations without requiring external knowledge\nsources such as retrieval systems.\n","authors":["Yige Yuan","Bingbing Xu","Hexiang Tan","Fei Sun","Teng Xiao","Wei Li","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2411.13343v1.pdf","comment":"Code is available at https://github.com/yuanyige/fact-calibration"},{"id":"http://arxiv.org/abs/2411.12254v2","updated":"2024-11-20T13:24:11Z","published":"2024-11-19T05:58:22Z","title":"Predicting User Intents and Musical Attributes from Music Discovery\n  Conversations","summary":"  Intent classification is a text understanding task that identifies user needs\nfrom input text queries. While intent classification has been extensively\nstudied in various domains, it has not received much attention in the music\ndomain. In this paper, we investigate intent classification models for music\ndiscovery conversation, focusing on pre-trained language models. Rather than\nonly predicting functional needs: intent classification, we also include a task\nfor classifying musical needs: musical attribute classification. Additionally,\nwe propose a method of concatenating previous chat history with just\nsingle-turn user queries in the input text, allowing the model to understand\nthe overall conversation context better. Our proposed model significantly\nimproves the F1 score for both user intent and musical attribute\nclassification, and surpasses the zero-shot and few-shot performance of the\npretrained Llama 3 model.\n","authors":["Daeyong Kwon","SeungHeon Doh","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2411.12254v2.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2411.13282v1","updated":"2024-11-20T12:49:42Z","published":"2024-11-20T12:49:42Z","title":"Combining Autoregressive and Autoencoder Language Models for Text\n  Classification","summary":"  This paper presents CAALM-TC (Combining Autoregressive and Autoencoder\nLanguage Models for Text Classification), a novel method that enhances text\nclassification by integrating autoregressive and autoencoder language models.\nAutoregressive large language models such as Open AI's GPT, Meta's Llama or\nMicrosoft's Phi offer promising prospects for content analysis practitioners,\nbut they generally underperform supervised BERT based models for text\nclassification. CAALM leverages autoregressive models to generate contextual\ninformation based on input texts, which is then combined with the original text\nand fed into an autoencoder model for classification. This hybrid approach\ncapitalizes on the extensive contextual knowledge of autoregressive models and\nthe efficient classification capabilities of autoencoders. Experimental results\non four benchmark datasets demonstrate that CAALM consistently outperforms\nexisting methods, particularly in tasks with smaller datasets and more abstract\nclassification objectives. The findings indicate that CAALM offers a scalable\nand effective solution for automated content analysis in social science\nresearch that minimizes sample size requirements.\n","authors":["João Gonçalves"],"pdf_url":"https://arxiv.org/pdf/2411.13282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13281v1","updated":"2024-11-20T12:48:34Z","published":"2024-11-20T12:48:34Z","title":"VideoAutoArena: An Automated Arena for Evaluating Large Multimodal\n  Models in Video Analysis through User Simulation","summary":"  Large multimodal models (LMMs) with advanced video analysis capabilities have\nrecently garnered significant attention. However, most evaluations rely on\ntraditional methods like multiple-choice questions in benchmarks such as\nVideoMME and LongVideoBench, which are prone to lack the depth needed to\ncapture the complex demands of real-world users. To address this limitation-and\ndue to the prohibitive cost and slow pace of human annotation for video\ntasks-we introduce VideoAutoArena, an arena-style benchmark inspired by LMSYS\nChatbot Arena's framework, designed to automatically assess LMMs' video\nanalysis abilities. VideoAutoArena utilizes user simulation to generate\nopen-ended, adaptive questions that rigorously assess model performance in\nvideo understanding. The benchmark features an automated, scalable evaluation\nframework, incorporating a modified ELO Rating System for fair and continuous\ncomparisons across multiple LMMs. To validate our automated judging system, we\nconstruct a 'gold standard' using a carefully curated subset of human\nannotations, demonstrating that our arena strongly aligns with human judgment\nwhile maintaining scalability. Additionally, we introduce a fault-driven\nevolution strategy, progressively increasing question complexity to push models\ntoward handling more challenging video analysis scenarios. Experimental results\ndemonstrate that VideoAutoArena effectively differentiates among\nstate-of-the-art LMMs, providing insights into model strengths and areas for\nimprovement. To further streamline our evaluation, we introduce VideoAutoBench\nas an auxiliary benchmark, where human annotators label winners in a subset of\nVideoAutoArena battles. We use GPT-4o as a judge to compare responses against\nthese human-validated answers. Together, VideoAutoArena and VideoAutoBench\noffer a cost-effective, and scalable framework for evaluating LMMs in\nuser-centric video analysis.\n","authors":["Ziyang Luo","Haoning Wu","Dongxu Li","Jing Ma","Mohan Kankanhalli","Junnan Li"],"pdf_url":"https://arxiv.org/pdf/2411.13281v1.pdf","comment":"Project Page: https://videoautoarena.github.io/"},{"id":"http://arxiv.org/abs/2410.08202v2","updated":"2024-11-20T12:15:08Z","published":"2024-10-10T17:59:22Z","title":"Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large\n  Language Models with Endogenous Visual Pre-training","summary":"  In this paper, we focus on monolithic Multimodal Large Language Models\n(MLLMs) that integrate visual encoding and language decoding into a single LLM.\nIn particular, we identify that existing pre-training strategies for monolithic\nMLLMs often suffer from unstable optimization or catastrophic forgetting. To\naddress this issue, our core idea is to embed a new visual parameter space into\na pre-trained LLM, thereby stably learning visual knowledge from noisy data\nwhile freezing the LLM. Based on this principle, we present Mono-InternVL, a\nnovel monolithic MLLM that seamlessly integrates a set of visual experts via a\nmultimodal mixture-of-experts structure. Moreover, we propose an innovative\npre-training strategy to maximize the visual capability of Mono-InternVL,\nnamely Endogenous Visual Pre-training (EViP). In particular, EViP is designed\nas a progressive learning process for visual experts, which aims to fully\nexploit the visual knowledge from noisy data to high-quality data. To validate\nour approach, we conduct extensive experiments on 16 benchmarks. Experimental\nresults confirm the superior performance of Mono-InternVL than existing\nmonolithic MLLMs on 13 of 16 multimodal benchmarks, e.g., +80 points over Emu3\non OCRBench. Compared to the modular baseline, i.e., InternVL-1.5,\nMono-InternVL still retains comparable multimodal performance while reducing up\nto 67% first token latency. Code and model are released at\nhttps://huggingface.co/OpenGVLab/Mono-InternVL-2B.\n","authors":["Gen Luo","Xue Yang","Wenhan Dou","Zhaokai Wang","Jiawen Liu","Jifeng Dai","Yu Qiao","Xizhou Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.08202v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13244v1","updated":"2024-11-20T12:03:17Z","published":"2024-11-20T12:03:17Z","title":"Leveraging Prior Experience: An Expandable Auxiliary Knowledge Base for\n  Text-to-SQL","summary":"  Large Language Models (LLMs) exhibit impressive problem-solving skills across\nmany tasks, but they still underperform compared to humans in various\ndownstream applications, such as text-to-SQL. On the BIRD benchmark\nleaderboard, human performance achieves an accuracy of 92.96\\%, whereas the\ntop-performing method reaches only 72.39\\%. Notably, these state-of-the-art\n(SoTA) methods predominantly rely on in-context learning to simulate human-like\nreasoning. However, they overlook a critical human skill: continual learning.\nInspired by the educational practice of maintaining mistake notebooks during\nour formative years, we propose LPE-SQL (Leveraging Prior Experience: An\nExpandable Auxiliary Knowledge Base for Text-to-SQL), a novel framework\ndesigned to augment LLMs by enabling continual learning without requiring\nparameter fine-tuning. LPE-SQL consists of four modules that \\textbf{i)}\nretrieve relevant entries, \\textbf{ii)} efficient sql generation, \\textbf{iii)}\ngenerate the final result through a cross-consistency mechanism and\n\\textbf{iv)} log successful and failed tasks along with their reasoning\nprocesses or reflection-generated tips. Importantly, the core module of LPE-SQL\nis the fourth one, while the other modules employ foundational methods,\nallowing LPE-SQL to be easily integrated with SoTA technologies to further\nenhance performance. Our experimental results demonstrate that this continual\nlearning approach yields substantial performance gains, with the smaller\nLlama-3.1-70B model with surpassing the performance of the larger\nLlama-3.1-405B model using SoTA methods.\n","authors":["Zhibo Chu","Zichong Wang","Qitao Qin"],"pdf_url":"https://arxiv.org/pdf/2411.13244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13237v1","updated":"2024-11-20T11:56:56Z","published":"2024-11-20T11:56:56Z","title":"BIPro: Zero-shot Chinese Poem Generation via Block Inverse Prompting\n  Constrained Generation Framework","summary":"  Recently, generative pre-trained models have made significant strides,\nparticularly highlighted by the release of ChatGPT and GPT-4, which exhibit\nsuperior cross-domain capabilities. However, these models still face challenges\non constrained writing tasks like poem generation under open-domain titles. In\nresponse to this challenge, we introduce Block Inverse Prompting (BIPro)\nconstrained generation framework. BIPro leverages two block inverse prompting\nmethods, revise and rewrite, that mimic the process of human text writing using\nblock generative models. It significantly improves the zero-shot generation\nquality on the formidable constrained generation task of open-domain\ntraditional-form Chinese poem generation. Based on a less powerful block\ngenerative model GLM-10B-Chinese, poems composed via BIPro without priming or\nadditional training outperform both most advanced direct generative systems\nlike GPT-4 or GLM-4 and best domain-specific systems such as Yusheng,\nShisanbai, or Baidu Poetry Helper in human evaluation by proficient poets.\nFinally, BIPro considerably narrows the gap between AI-generated works and\nshort-listed human literary arts in another human evaluation, unveiling the\npromising potential of block generative models in improving the quality of\nconstrained generation.\n","authors":["Xu Zou"],"pdf_url":"https://arxiv.org/pdf/2411.13237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10310v2","updated":"2024-11-20T11:47:58Z","published":"2024-06-14T06:22:47Z","title":"TEG-DB: A Comprehensive Dataset and Benchmark of Textual-Edge Graphs","summary":"  Text-Attributed Graphs (TAGs) augment graph structures with natural language\ndescriptions, facilitating detailed depictions of data and their\ninterconnections across various real-world settings. However, existing TAG\ndatasets predominantly feature textual information only at the nodes, with\nedges typically represented by mere binary or categorical attributes. This lack\nof rich textual edge annotations significantly limits the exploration of\ncontextual relationships between entities, hindering deeper insights into\ngraph-structured data. To address this gap, we introduce Textual-Edge Graphs\nDatasets and Benchmark (TEG-DB), a comprehensive and diverse collection of\nbenchmark textual-edge datasets featuring rich textual descriptions on nodes\nand edges. The TEG-DB datasets are large-scale and encompass a wide range of\ndomains, from citation networks to social networks. In addition, we conduct\nextensive benchmark experiments on TEG-DB to assess the extent to which current\ntechniques, including pre-trained language models, graph neural networks, and\ntheir combinations, can utilize textual node and edge information. Our goal is\nto elicit advancements in textual-edge graph research, specifically in\ndeveloping methodologies that exploit rich textual node and edge descriptions\nto enhance graph analysis and provide deeper insights into complex real-world\nnetworks. The entire TEG-DB project is publicly accessible as an open-source\nrepository on Github, accessible at\nhttps://github.com/Zhuofeng-Li/TEG-Benchmark.\n","authors":["Zhuofeng Li","Zixing Gou","Xiangnan Zhang","Zhongyuan Liu","Sirui Li","Yuntong Hu","Chen Ling","Zheng Zhang","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.10310v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.13226v1","updated":"2024-11-20T11:41:08Z","published":"2024-11-20T11:41:08Z","title":"AIDBench: A benchmark for evaluating the authorship identification\n  capability of large language models","summary":"  As large language models (LLMs) rapidly advance and integrate into daily\nlife, the privacy risks they pose are attracting increasing attention. We focus\non a specific privacy risk where LLMs may help identify the authorship of\nanonymous texts, which challenges the effectiveness of anonymity in real-world\nsystems such as anonymous peer review systems. To investigate these risks, we\npresent AIDBench, a new benchmark that incorporates several author\nidentification datasets, including emails, blogs, reviews, articles, and\nresearch papers. AIDBench utilizes two evaluation methods: one-to-one\nauthorship identification, which determines whether two texts are from the same\nauthor; and one-to-many authorship identification, which, given a query text\nand a list of candidate texts, identifies the candidate most likely written by\nthe same author as the query text. We also introduce a Retrieval-Augmented\nGeneration (RAG)-based method to enhance the large-scale authorship\nidentification capabilities of LLMs, particularly when input lengths exceed the\nmodels' context windows, thereby establishing a new baseline for authorship\nidentification using LLMs. Our experiments with AIDBench demonstrate that LLMs\ncan correctly guess authorship at rates well above random chance, revealing new\nprivacy risks posed by these powerful models. The source code and data will be\nmade publicly available after acceptance.\n","authors":["Zichen Wen","Dadi Guo","Huishuai Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.13226v1.pdf","comment":"21 pages, 7 figures"},{"id":"http://arxiv.org/abs/2411.12449v2","updated":"2024-11-20T10:06:05Z","published":"2024-11-19T12:17:43Z","title":"Neon: News Entity-Interaction Extraction for Enhanced Question Answering","summary":"  Capturing fresh information in near real-time and using it to augment\nexisting large language models (LLMs) is essential to generate up-to-date,\ngrounded, and reliable output. This problem becomes particularly challenging\nwhen LLMs are used for informational tasks in rapidly evolving fields, such as\nWeb search related to recent or unfolding events involving entities, where\ngenerating temporally relevant responses requires access to up-to-the-hour news\nsources. However, the information modeled by the parametric memory of LLMs is\noften outdated, and Web results from prototypical retrieval systems may fail to\ncapture the latest relevant information and struggle to handle conflicting\nreports in evolving news. To address this challenge, we present the NEON\nframework, designed to extract emerging entity interactions -- such as events\nor activities -- as described in news articles. NEON constructs an\nentity-centric timestamped knowledge graph that captures such interactions,\nthereby facilitating enhanced QA capabilities related to news events. Our\nframework innovates by integrating open Information Extraction (openIE) style\ntuples into LLMs to enable in-context retrieval-augmented generation. This\nintegration demonstrates substantial improvements in QA performance when\ntackling temporal, entity-centric search queries. Through NEON, LLMs can\ndeliver more accurate, reliable, and up-to-date responses.\n","authors":["Sneha Singhania","Silviu Cucerzan","Allen Herring","Sujay Kumar Jauhar"],"pdf_url":"https://arxiv.org/pdf/2411.12449v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13159v1","updated":"2024-11-20T09:49:37Z","published":"2024-11-20T09:49:37Z","title":"Hard-Synth: Synthesizing Diverse Hard Samples for ASR using Zero-Shot\n  TTS and LLM","summary":"  Text-to-speech (TTS) models have been widely adopted to enhance automatic\nspeech recognition (ASR) systems using text-only corpora, thereby reducing the\ncost of labeling real speech data. Existing research primarily utilizes\nadditional text data and predefined speech styles supported by TTS models. In\nthis paper, we propose Hard-Synth, a novel ASR data augmentation method that\nleverages large language models (LLMs) and advanced zero-shot TTS. Our approach\nemploys LLMs to generate diverse in-domain text through rewriting, without\nrelying on additional text data. Rather than using predefined speech styles, we\nintroduce a hard prompt selection method with zero-shot TTS to clone speech\nstyles that the ASR model finds challenging to recognize. Experiments\ndemonstrate that Hard-Synth significantly enhances the Conformer model,\nachieving relative word error rate (WER) reductions of 6.5\\%/4.4\\% on\nLibriSpeech dev/test-other subsets. Additionally, we show that Hard-Synth is\ndata-efficient and capable of reducing bias in ASR.\n","authors":["Jiawei Yu","Yuang Li","Xiaosong Qiao","Huan Zhao","Xiaofeng Zhao","Wei Tang","Min Zhang","Hao Yang","Jinsong Su"],"pdf_url":"https://arxiv.org/pdf/2411.13159v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13157v1","updated":"2024-11-20T09:46:30Z","published":"2024-11-20T09:46:30Z","title":"Closer Look at Efficient Inference Methods: A Survey of Speculative\n  Decoding","summary":"  Efficient inference in large language models (LLMs) has become a critical\nfocus as their scale and complexity grow. Traditional autoregressive decoding,\nwhile effective, suffers from computational inefficiencies due to its\nsequential token generation process. Speculative decoding addresses this\nbottleneck by introducing a two-stage framework: drafting and verification. A\nsmaller, efficient model generates a preliminary draft, which is then refined\nby a larger, more sophisticated model. This paper provides a comprehensive\nsurvey of speculative decoding methods, categorizing them into draft-centric\nand model-centric approaches. We discuss key ideas associated with each method,\nhighlighting their potential for scaling LLM inference. This survey aims to\nguide future research in optimizing speculative decoding and its integration\ninto real-world LLM applications.\n","authors":["Hyun Ryu","Eric Kim"],"pdf_url":"https://arxiv.org/pdf/2411.13157v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13100v1","updated":"2024-11-20T07:57:58Z","published":"2024-11-20T07:57:58Z","title":"Song Form-aware Full-Song Text-to-Lyrics Generation with Multi-Level\n  Granularity Syllable Count Control","summary":"  Lyrics generation presents unique challenges, particularly in achieving\nprecise syllable control while adhering to song form structures such as verses\nand choruses. Conventional line-by-line approaches often lead to unnatural\nphrasing, underscoring the need for more granular syllable management. We\npropose a framework for lyrics generation that enables multi-level syllable\ncontrol at the word, phrase, line, and paragraph levels, aware of song form.\nOur approach generates complete lyrics conditioned on input text and song form,\nensuring alignment with specified syllable constraints. Generated lyrics\nsamples are available at: https://tinyurl.com/lyrics9999\n","authors":["Yunkee Chae","Eunsik Shin","Hwang Suntae","Seungryeol Paik","Kyogu Lee"],"pdf_url":"https://arxiv.org/pdf/2411.13100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08903v2","updated":"2024-11-20T07:42:38Z","published":"2024-06-13T07:57:27Z","title":"Delta-CoMe: Training-Free Delta-Compression with Mixed-Precision for\n  Large Language Models","summary":"  Fine-tuning is a crucial process for adapting large language models (LLMs) to\ndiverse applications. In certain scenarios, such as multi-tenant serving,\ndeploying multiple LLMs becomes necessary to meet complex demands. Recent\nstudies suggest decomposing a fine-tuned LLM into a base model and\ncorresponding delta weights, which are then compressed using low-rank or\nlow-bit approaches to reduce costs. In this work, we observe that existing\nlow-rank and low-bit compression methods can significantly harm the model\nperformance for task-specific fine-tuned LLMs (e.g., WizardMath for math\nproblems). Motivated by the long-tail distribution of singular values in the\ndelta weights, we propose a delta quantization approach using mixed-precision.\nThis method employs higher-bit representation for singular vectors\ncorresponding to larger singular values. We evaluate our approach on various\nfine-tuned LLMs, including math LLMs, code LLMs, chat LLMs, and even VLMs.\nExperimental results demonstrate that our approach performs comparably to full\nfine-tuned LLMs, surpassing both low-rank and low-bit baselines by a\nconsiderable margin. Additionally, we show that our method is compatible with\nvarious backbone LLMs, such as Llama-2, Llama-3, and Mistral, highlighting its\ngeneralizability.\n","authors":["Bowen Ping","Shuo Wang","Hanqing Wang","Xu Han","Yuzhuang Xu","Yukun Yan","Yun Chen","Baobao Chang","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2406.08903v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.11053v2","updated":"2024-11-20T07:34:47Z","published":"2024-11-17T12:31:04Z","title":"SRA-MCTS: Self-driven Reasoning Augmentation with Monte Carlo Tree\n  Search for Enhanced Code Generation","summary":"  Large language models demonstrate exceptional performance in simple code\ngeneration tasks but still face challenges in tackling complex problems. These\nchallenges may stem from insufficient reasoning and problem decomposition\ncapabilities. To address this issue, we propose a reasoning-augmented data\ngeneration process, SRA-MCTS, which guides the model to autonomously generate\nhigh-quality intermediate reasoning paths. This creates a positive feedback\nloop, enabling continuous improvement. Our method operates entirely through the\nmodel itself without requiring additional supervision. By synthesizing natural\nlanguage reasoning paths and translating them into executable code, the\napproach ensures analytical accuracy and enhances the success rate in solving\ncomplex tasks. Experimental results show that, even without additional\nsupervisory signals, our method achieves performance improvements across\ndifferent model scales, demonstrating the significant potential of\nself-improvement in small models. Furthermore, the method remains robust when\ntraditional Chain-of-Thought (CoT) approaches exhibit performance degradation,\nwith notable improvements observed in diversity metrics such as pass@10. We\nencourage further exploration of reasoning processes within training data to\nenhance the ability of language models to address complex problems.\n","authors":["Bin Xu","Yiguan Lin","Yinghao Li","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2411.11053v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13082v1","updated":"2024-11-20T07:20:48Z","published":"2024-11-20T07:20:48Z","title":"Patience Is The Key to Large Language Model Reasoning","summary":"  Recent advancements in the field of large language models, particularly\nthrough the Chain of Thought (CoT) approach, have demonstrated significant\nimprovements in solving complex problems. However, existing models either tend\nto sacrifice detailed reasoning for brevity due to user preferences, or require\nextensive and expensive training data to learn complicated reasoning ability,\nlimiting their potential in solving complex tasks. To bridge this gap,\nfollowing the concept of scaling test-time, we propose a simple method by\nencouraging models to adopt a more patient reasoning style without the need of\nintroducing new knowledge or skills. To employ a preference optimization\napproach, we generate detailed reasoning processes as positive examples and\nsimple answers as negative examples, thereby training the model to favor\nthoroughness in its responses. Our results demonstrate a performance increase\nof up to 6.7% on GSM8k with training just on a lightweight dataset.\n","authors":["Yijiong Yu"],"pdf_url":"https://arxiv.org/pdf/2411.13082v1.pdf","comment":"The dataset and model are available at\n  https://huggingface.co/datasets/yuyijiong/patient-math-cot"},{"id":"http://arxiv.org/abs/2409.00055v5","updated":"2024-11-20T07:08:22Z","published":"2024-08-21T04:47:26Z","title":"SORSA: Singular Values and Orthonormal Regularized Singular Vectors\n  Adaptation of Large Language Models","summary":"  In this paper, we propose Singular Values and Orthonormal Regularized\nSingular Vectors Adaptation, or SORSA, a novel PEFT method. Each SORSA adapter\nconsists of two main parts: trainable principal singular weights $W_p = U_p\n\\text{diag}(S_p) V^\\top_p$, and frozen residual weights $W_r = U_r\n\\text{diag}(S_r) V^\\top_r$. These parts are initialized by performing singular\nvalue decomposition (SVD) on pre-trained weights. Moreover, we implement and\nanalyze an orthonormal regularizer, which we prove could decrease the condition\nnumber of $W_p$ and make the optimization more efficient. SORSA adapters could\nbe merged during inference, thus eliminating any inference latency. We also\nintroduce a method to analyze the variation of the parameters by performing SVD\nand discuss and analyze SORSA's superiority in minimizing the alteration in the\nSVD aspect. After all, SORSA shows a faster convergence than LoRA and PiSSA in\nour experiments. On the GSM-8K benchmark, Llama 2 7B adapted using SORSA\nachieved 56.03% accuracy, surpassing LoRA (42.30%), AdaLoRA (47.30%), Full FT\n(49.05%), and PiSSA (53.07%). On the MATH benchmark, SORSA achieved 10.36%\naccuracy, outperforming LoRA (5.50%), AdaLoRA (6.48%), Full FT (7.22%), and\nPiSSA (7.44%). We conclude that SORSA offers a new perspective on\nparameter-efficient fine-tuning, demonstrating remarkable performance.\n","authors":["Yang Cao"],"pdf_url":"https://arxiv.org/pdf/2409.00055v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03022v3","updated":"2024-11-20T07:07:41Z","published":"2023-12-05T07:27:08Z","title":"Beyond Isolation: Multi-Agent Synergy for Improving Knowledge Graph\n  Construction","summary":"  This paper introduces CooperKGC, a novel framework challenging the\nconventional solitary approach of large language models (LLMs) in knowledge\ngraph construction (KGC). CooperKGC establishes a collaborative processing\nnetwork, assembling a team capable of concurrently addressing entity, relation,\nand event extraction tasks. Experimentation demonstrates that fostering\ncollaboration within CooperKGC enhances knowledge selection, correction, and\naggregation capabilities across multiple rounds of interactions.\n","authors":["Hongbin Ye","Honghao Gui","Aijia Zhang","Tong Liu","Weiqiang Jia"],"pdf_url":"https://arxiv.org/pdf/2312.03022v3.pdf","comment":"Accepted by CCKS 2024, best english candidate paper"},{"id":"http://arxiv.org/abs/2411.13045v1","updated":"2024-11-20T05:30:15Z","published":"2024-11-20T05:30:15Z","title":"Explainable LLM-driven Multi-dimensional Distillation for E-Commerce\n  Relevance Learning","summary":"  Effective query-item relevance modeling is pivotal for enhancing user\nexperience and safeguarding user satisfaction in e-commerce search systems.\nRecently, benefiting from the vast inherent knowledge, Large Language Model\n(LLM) approach demonstrates strong performance and long-tail generalization\nability compared with previous neural-based specialized relevance learning\nmethods. Though promising, current LLM-based methods encounter the following\ninadequacies in practice: First, the massive parameters and computational\ndemands make it difficult to be deployed online. Second, distilling LLM models\nto online models is a feasible direction, but the LLM relevance modeling is a\nblack box, and its rich intrinsic knowledge is difficult to extract and apply\nonline. To improve the interpretability of LLM and boost the performance of\nonline relevance models via LLM, we propose an Explainable LLM-driven\nMulti-dimensional Distillation framework for e-commerce relevance learning,\nwhich comprises two core components: (1) An Explainable LLM for relevance\nmodeling (ELLM-rele), which decomposes the relevance learning into intermediate\nsteps and models relevance learning as a Chain-of-Thought (CoT) reasoning,\nthereby enhancing both interpretability and performance of LLM. (2) A\nMulti-dimensional Knowledge Distillation (MKD) architecture that transfers the\nknowledge of ELLM-rele to current deployable interaction-based and\nrepresentation-based student models from both the relevance score distribution\nand CoT reasoning aspects. Through distilling the probabilistic and CoT\nreasoning knowledge, MKD improves both the semantic interaction and long-tail\ngeneralization abilities of student models. Extensive offline evaluations and\nonline experiments on Taobao search ad scene demonstrate that our proposed\nframework significantly enhances e-commerce relevance learning performance and\nuser experience.\n","authors":["Gang Zhao","Ximing Zhang","Chenji Lu","Hui Zhao","Tianshu Wu","Pengjie Wang","Jian Xu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2411.13045v1.pdf","comment":"Submitted to WWW 2025"},{"id":"http://arxiv.org/abs/2403.08492v3","updated":"2024-11-20T04:00:39Z","published":"2024-03-13T12:55:43Z","title":"Rich Semantic Knowledge Enhanced Large Language Models for Few-shot\n  Chinese Spell Checking","summary":"  Chinese Spell Checking (CSC) is a widely used technology, which plays a vital\nrole in speech to text (STT) and optical character recognition (OCR). Most of\nthe existing CSC approaches relying on BERT architecture achieve excellent\nperformance. However, limited by the scale of the foundation model, BERT-based\nmethod does not work well in few-shot scenarios, showing certain limitations in\npractical applications. In this paper, we explore using an in-context learning\nmethod named RS-LLM (Rich Semantic based LLMs) to introduce large language\nmodels (LLMs) as the foundation model. Besides, we study the impact of\nintroducing various Chinese rich semantic information in our framework. We\nfound that by introducing a small number of specific Chinese rich semantic\nstructures, LLMs achieve better performance than the BERT-based model on\nfew-shot CSC task. Furthermore, we conduct experiments on multiple datasets,\nand the experimental results verified the superiority of our proposed\nframework.\n","authors":["Ming Dong","Yujing Chen","Miao Zhang","Hao Sun","Tingting He"],"pdf_url":"https://arxiv.org/pdf/2403.08492v3.pdf","comment":"This paper is accepted by Findings of the Association for\n  Computational Linguistics: ACL 2024"},{"id":"http://arxiv.org/abs/2411.13017v1","updated":"2024-11-20T03:43:03Z","published":"2024-11-20T03:43:03Z","title":"Breaking the Cycle of Recurring Failures: Applying Generative AI to Root\n  Cause Analysis in Legacy Banking Systems","summary":"  Traditional banks face significant challenges in digital transformation,\nprimarily due to legacy system constraints and fragmented ownership. Recent\nincidents show that such fragmentation often results in superficial incident\nresolutions, leaving root causes unaddressed and causing recurring failures. We\nintroduce a novel approach to post-incident analysis, integrating\nknowledge-based GenAI agents with the \"Five Whys\" technique to examine problem\ndescriptions and change request data. This method uncovered that approximately\n70% of the incidents previously attributed to management or vendor failures\nwere due to underlying internal code issues. We present a case study to show\nthe impact of our method. By scanning over 5,000 projects, we identified over\n400 files with a similar root cause. Overall, we leverage the knowledge-based\nagents to automate and elevate root cause analysis, transforming it into a more\nproactive process. These agents can be applied across other phases of the\nsoftware development lifecycle, further improving development processes.\n","authors":["Siyuan Jin","Zhendong Bei","Bichao Chen","Yong Xia"],"pdf_url":"https://arxiv.org/pdf/2411.13017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13009v1","updated":"2024-11-20T03:17:51Z","published":"2024-11-20T03:17:51Z","title":"LLMSteer: Improving Long-Context LLM Inference by Steering Attention on\n  Reused Contexts","summary":"  As large language models (LLMs) show impressive performance on complex tasks,\nthey still struggle with longer contextual understanding and high computational\ncosts. To balance efficiency and quality, we introduce LLMSteer, a\nfine-tuning-free framework that enhances LLMs through query-independent\nattention steering. Tested on popular LLMs and datasets, LLMSteer narrows the\nperformance gap with baselines by 65.9% and reduces the runtime delay by up to\n4.8x compared to recent attention steering methods.\n","authors":["Zhuohan Gu","Jiayi Yao","Kuntai Du","Junchen Jiang"],"pdf_url":"https://arxiv.org/pdf/2411.13009v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12992v1","updated":"2024-11-20T02:41:53Z","published":"2024-11-20T02:41:53Z","title":"MemoryFormer: Minimize Transformer Computation by Removing\n  Fully-Connected Layers","summary":"  In order to reduce the computational complexity of large language models,\ngreat efforts have been made to to improve the efficiency of transformer models\nsuch as linear attention and flash-attention. However, the model size and\ncorresponding computational complexity are constantly scaled up in pursuit of\nhigher performance. In this work, we present MemoryFormer, a novel transformer\narchitecture which significantly reduces the computational complexity (FLOPs)\nfrom a new perspective. We eliminate nearly all the computations of the\ntransformer model except for the necessary computation required by the\nmulti-head attention operation. This is made possible by utilizing an\nalternative method for feature transformation to replace the linear projection\nof fully-connected layers. Specifically, we first construct a group of\nin-memory lookup tables that store a large amount of discrete vectors to\nreplace the weight matrix used in linear projection. We then use a hash\nalgorithm to retrieve a correlated subset of vectors dynamically based on the\ninput embedding. The retrieved vectors combined together will form the output\nembedding, which provides an estimation of the result of matrix multiplication\noperation in a fully-connected layer. Compared to conducting matrix\nmultiplication, retrieving data blocks from memory is a much cheaper operation\nwhich requires little computations. We train MemoryFormer from scratch and\nconduct extensive experiments on various benchmarks to demonstrate the\neffectiveness of the proposed model.\n","authors":["Ning Ding","Yehui Tang","Haochen Qin","Zhenli Zhou","Chao Xu","Lin Li","Kai Han","Heng Liao","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2411.12992v1.pdf","comment":"NeurIPS2024"},{"id":"http://arxiv.org/abs/2411.12986v1","updated":"2024-11-20T02:27:40Z","published":"2024-11-20T02:27:40Z","title":"Training Bilingual LMs with Data Constraints in the Targeted Language","summary":"  Large language models are trained on massive scrapes of the web, as required\nby current scaling laws. Most progress is made for English, given its abundance\nof high-quality pretraining data. For most other languages, however, such high\nquality pretraining data is unavailable. In this work, we study how to boost\npretrained model performance in a data constrained target language by enlisting\ndata from an auxiliary language for which high quality data is available. We\nstudy this by quantifying the performance gap between training with data in a\ndata-rich auxiliary language compared with training in the target language,\nexploring the benefits of translation systems, studying the limitations of\nmodel scaling for data constrained languages, and proposing new methods for\nupsampling data from the auxiliary language. Our results show that stronger\nauxiliary datasets result in performance gains without modification to the\nmodel or training objective for close languages, and, in particular, that\nperformance gains due to the development of more information-rich English\npretraining datasets can extend to targeted language settings with limited\ndata.\n","authors":["Skyler Seto","Maartje ter Hoeve","He Bai","Natalie Schluter","David Grangier"],"pdf_url":"https://arxiv.org/pdf/2411.12986v1.pdf","comment":"22 pages, 14 figures, 15 tables"},{"id":"http://arxiv.org/abs/2411.12103v2","updated":"2024-11-20T02:23:11Z","published":"2024-11-18T22:31:17Z","title":"Does Unlearning Truly Unlearn? A Black Box Evaluation of LLM Unlearning\n  Methods","summary":"  Large language model unlearning aims to remove harmful information that LLMs\nhave learnt to prevent their use for malicious purposes. LLMU and RMU have been\nproposed as two methods for LLM unlearning, achieving impressive results on\nunlearning benchmarks. We study in detail the efficacy of these methods by\nevaluating their impact on general model capabilities on the WMDP benchmark as\nwell as a biology benchmark we create. Our experiments show that RMU generally\nleads to better preservation of model capabilities, for similar or better\nunlearning. We further test the robustness of these methods and find that doing\n5-shot prompting or rephrasing the question in simple ways can lead to an over\nten-fold increase in accuracy on unlearning benchmarks. Finally, we show that\ntraining on unrelated data can almost completely recover pre-unlearning\nperformance, demonstrating that these methods fail at truly unlearning. The\ncode is available at: https://github.com/JaiDoshi/Knowledge-Erasure.\n","authors":["Jai Doshi","Asa Cooper Stickland"],"pdf_url":"https://arxiv.org/pdf/2411.12103v2.pdf","comment":"9 pages, 2 figures"},{"id":"http://arxiv.org/abs/2411.12977v1","updated":"2024-11-20T02:10:44Z","published":"2024-11-20T02:10:44Z","title":"MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong\n  Collaborative Learning","summary":"  Contemporary embodied agents, such as Voyager in Minecraft, have demonstrated\npromising capabilities in open-ended individual learning. However, when powered\nwith open large language models (LLMs), these agents often struggle with\nrudimentary tasks, even when fine-tuned on domain-specific knowledge. Inspired\nby human cultural learning, we present \\collabvoyager, a novel framework that\nenhances Voyager with lifelong collaborative learning through explicit\nperspective-taking. \\collabvoyager introduces three key innovations: (1) theory\nof mind representations linking percepts, beliefs, desires, and actions; (2)\nnatural language communication between agents; and (3) semantic memory of task\nand environment knowledge and episodic memory of collaboration episodes. These\nadvancements enable agents to reason about their and others' mental states,\nempirically addressing two prevalent failure modes: false beliefs and faulty\ntask executions. In mixed-expertise Minecraft experiments, \\collabvoyager\nagents outperform Voyager counterparts, significantly improving task completion\nrate by $66.6\\% (+39.4\\%)$ for collecting one block of dirt and $70.8\\%\n(+20.8\\%)$ for collecting one wood block. They exhibit emergent behaviors like\nknowledge transfer from expert to novice agents and collaborative code\ncorrection. \\collabvoyager agents also demonstrate the ability to adapt to\nout-of-distribution tasks by using their previous experiences and beliefs\nobtained through collaboration. In this open-ended social learning paradigm,\n\\collabvoyager paves the way for the democratic development of embodied AI,\nwhere agents learn in deployment from both peer and environmental feedback.\n","authors":["Mircea Lică","Ojas Shirekar","Baptiste Colle","Chirag Raman"],"pdf_url":"https://arxiv.org/pdf/2411.12977v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.20181v2","updated":"2024-11-20T02:10:16Z","published":"2024-09-30T10:48:20Z","title":"Reference Trustable Decoding: A Training-Free Augmentation Paradigm for\n  Large Language Models","summary":"  Large language models (LLMs) have rapidly advanced and demonstrated\nimpressive capabilities. In-Context Learning (ICL) and Parameter-Efficient\nFine-Tuning (PEFT) are currently two mainstream methods for augmenting LLMs to\ndownstream tasks. ICL typically constructs a few-shot learning scenario, either\nmanually or by setting up a Retrieval-Augmented Generation (RAG) system,\nhelping models quickly grasp domain knowledge or question-answering patterns\nwithout changing model parameters. However, this approach involves trade-offs,\nsuch as slower inference speed and increased space occupancy. PEFT assists the\nmodel in adapting to tasks through minimal parameter modifications, but the\ntraining process still demands high hardware requirements, even with a small\nnumber of parameters involved. To address these challenges, we propose\nReference Trustable Decoding (RTD), a paradigm that allows models to quickly\nadapt to new tasks without fine-tuning, maintaining low inference costs. RTD\nconstructs a reference datastore from the provided training examples and\noptimizes the LLM's final vocabulary distribution by flexibly selecting\nsuitable references based on the input, resulting in more trustable responses\nand enabling the model to adapt to downstream tasks at a low cost. Experimental\nevaluations on various LLMs using different benchmarks demonstrate that RTD\nestablishes a new paradigm for augmenting models to downstream tasks.\nFurthermore, our method exhibits strong orthogonality with traditional methods,\nallowing for concurrent usage. Our code can be found at\nhttps://github.com/ShiLuohe/ReferenceTrustableDecoding\n","authors":["Luohe Shi","Yao Yao","Zuchao Li","Lefei Zhang","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2409.20181v2.pdf","comment":"Accepted by the Thirty-Eighth Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2407.18003v4","updated":"2024-11-20T02:04:10Z","published":"2024-07-25T12:56:22Z","title":"Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption","summary":"  Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache.\n","authors":["Luohe Shi","Hongyi Zhang","Yao Yao","Zuchao Li","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.18003v4.pdf","comment":"Published on the First Conference on Language Modeling (COLM 2024)"},{"id":"http://arxiv.org/abs/2410.18856v3","updated":"2024-11-20T01:04:33Z","published":"2024-10-24T15:41:56Z","title":"Demystifying Large Language Models for Medicine: A Primer","summary":"  Large language models (LLMs) represent a transformative class of AI tools\ncapable of revolutionizing various aspects of healthcare by generating\nhuman-like responses across diverse contexts and adapting to novel tasks\nfollowing human instructions. Their potential application spans a broad range\nof medical tasks, such as clinical documentation, matching patients to clinical\ntrials, and answering medical questions. In this primer paper, we propose an\nactionable guideline to help healthcare professionals more efficiently utilize\nLLMs in their work, along with a set of best practices. This approach consists\nof several main phases, including formulating the task, choosing LLMs, prompt\nengineering, fine-tuning, and deployment. We start with the discussion of\ncritical considerations in identifying healthcare tasks that align with the\ncore capabilities of LLMs and selecting models based on the selected task and\ndata, performance requirements, and model interface. We then review the\nstrategies, such as prompt engineering and fine-tuning, to adapt standard LLMs\nto specialized medical tasks. Deployment considerations, including regulatory\ncompliance, ethical guidelines, and continuous monitoring for fairness and\nbias, are also discussed. By providing a structured step-by-step methodology,\nthis tutorial aims to equip healthcare professionals with the tools necessary\nto effectively integrate LLMs into clinical practice, ensuring that these\npowerful technologies are applied in a safe, reliable, and impactful manner.\n","authors":["Qiao Jin","Nicholas Wan","Robert Leaman","Shubo Tian","Zhizheng Wang","Yifan Yang","Zifeng Wang","Guangzhi Xiong","Po-Ting Lai","Qingqing Zhu","Benjamin Hou","Maame Sarfo-Gyamfi","Gongbo Zhang","Aidan Gilson","Balu Bhasuran","Zhe He","Aidong Zhang","Jimeng Sun","Chunhua Weng","Ronald M. Summers","Qingyu Chen","Yifan Peng","Zhiyong Lu"],"pdf_url":"https://arxiv.org/pdf/2410.18856v3.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2411.12946v1","updated":"2024-11-20T00:31:23Z","published":"2024-11-20T00:31:23Z","title":"A Flexible Large Language Models Guardrail Development Methodology\n  Applied to Off-Topic Prompt Detection","summary":"  Large Language Models are prone to off-topic misuse, where users may prompt\nthese models to perform tasks beyond their intended scope. Current guardrails,\nwhich often rely on curated examples or custom classifiers, suffer from high\nfalse-positive rates, limited adaptability, and the impracticality of requiring\nreal-world data that is not available in pre-production. In this paper, we\nintroduce a flexible, data-free guardrail development methodology that\naddresses these challenges. By thoroughly defining the problem space\nqualitatively and passing this to an LLM to generate diverse prompts, we\nconstruct a synthetic dataset to benchmark and train off-topic guardrails that\noutperform heuristic approaches. Additionally, by framing the task as\nclassifying whether the user prompt is relevant with respect to the system\nprompt, our guardrails effectively generalize to other misuse categories,\nincluding jailbreak and harmful prompts. Lastly, we further contribute to the\nfield by open-sourcing both the synthetic dataset and the off-topic guardrail\nmodels, providing valuable resources for developing guardrails in\npre-production environments and supporting future research and development in\nLLM safety.\n","authors":["Gabriel Chua","Shing Yee Chan","Shaun Khoo"],"pdf_url":"https://arxiv.org/pdf/2411.12946v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2405.09223v2","updated":"2024-11-20T23:06:56Z","published":"2024-05-15T10:04:19Z","title":"Word Alignment as Preference for Machine Translation","summary":"  The problem of hallucination and omission, a long-standing problem in machine\ntranslation (MT), is more pronounced when a large language model (LLM) is used\nin MT because an LLM itself is susceptible to these phenomena. In this work, we\nmitigate the problem in an LLM-based MT model by guiding it to better word\nalignment. We first study the correlation between word alignment and the\nphenomena of hallucination and omission in MT. Then we propose to utilize word\nalignment as preference to optimize the LLM-based MT model. The preference data\nare constructed by selecting chosen and rejected translations from multiple MT\ntools. Subsequently, direct preference optimization is used to optimize the\nLLM-based model towards the preference signal. Given the absence of evaluators\nspecifically designed for hallucination and omission in MT, we further propose\nselecting hard instances and utilizing GPT-4 to directly evaluate the\nperformance of the models in mitigating these issues. We verify the rationality\nof these designed evaluation methods by experiments, followed by extensive\nresults demonstrating the effectiveness of word alignment-based preference\noptimization to mitigate hallucination and omission. On the other hand,\nalthough it shows promise in mitigating hallucination and omission, the overall\nperformance of MT in different language directions remains mixed, with slight\nincreases in BLEU and decreases in COMET.\n","authors":["Qiyu Wu","Masaaki Nagata","Zhongtao Miao","Yoshimasa Tsuruoka"],"pdf_url":"https://arxiv.org/pdf/2405.09223v2.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2411.13738v1","updated":"2024-11-20T22:43:18Z","published":"2024-11-20T22:43:18Z","title":"Assessing Gender Bias in LLMs: Comparing LLM Outputs with Human\n  Perceptions and Official Statistics","summary":"  This study investigates gender bias in large language models (LLMs) by\ncomparing their gender perception to that of human respondents, U.S. Bureau of\nLabor Statistics data, and a 50% no-bias benchmark. We created a new evaluation\nset using occupational data and role-specific sentences. Unlike common\nbenchmarks included in LLM training data, our set is newly developed,\npreventing data leakage and test set contamination. Five LLMs were tested to\npredict the gender for each role using single-word answers. We used\nKullback-Leibler (KL) divergence to compare model outputs with human\nperceptions, statistical data, and the 50% neutrality benchmark. All LLMs\nshowed significant deviation from gender neutrality and aligned more with\nstatistical data, still reflecting inherent biases.\n","authors":["Tetiana Bas"],"pdf_url":"https://arxiv.org/pdf/2411.13738v1.pdf","comment":"under review for Coling conference"},{"id":"http://arxiv.org/abs/2312.02783v4","updated":"2024-11-20T21:24:10Z","published":"2023-12-05T14:14:27Z","title":"Large Language Models on Graphs: A Comprehensive Survey","summary":"  Large language models (LLMs), such as GPT4 and LLaMA, are creating\nsignificant advancements in natural language processing, due to their strong\ntext encoding/decoding ability and newly found emergent capability (e.g.,\nreasoning). While LLMs are mainly designed to process pure texts, there are\nmany real-world scenarios where text data is associated with rich structure\ninformation in the form of graphs (e.g., academic networks, and e-commerce\nnetworks) or scenarios where graph data is paired with rich textual information\n(e.g., molecules with descriptions). Besides, although LLMs have shown their\npure text-based reasoning ability, it is underexplored whether such ability can\nbe generalized to graphs (i.e., graph-based reasoning). In this paper, we\nprovide a systematic review of scenarios and techniques related to large\nlanguage models on graphs. We first summarize potential scenarios of adopting\nLLMs on graphs into three categories, namely pure graphs, text-attributed\ngraphs, and text-paired graphs. We then discuss detailed techniques for\nutilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM\nas Aligner, and compare the advantages and disadvantages of different schools\nof models. Furthermore, we discuss the real-world applications of such methods\nand summarize open-source codes and benchmark datasets. Finally, we conclude\nwith potential future research directions in this fast-growing field. The\nrelated source can be found at\nhttps://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs.\n","authors":["Bowen Jin","Gang Liu","Chi Han","Meng Jiang","Heng Ji","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2312.02783v4.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2406.12031v2","updated":"2024-11-20T21:20:08Z","published":"2024-06-17T18:58:20Z","title":"Large Scale Transfer Learning for Tabular Data via Language Modeling","summary":"  Tabular data -- structured, heterogeneous, spreadsheet-style data with rows\nand columns -- is widely used in practice across many domains. However, while\nrecent foundation models have reduced the need for developing task-specific\ndatasets and predictors in domains such as language modeling and computer\nvision, this transfer learning paradigm has not had similar impact in the\ntabular domain. In this work, we seek to narrow this gap and present TabuLa-8B,\na language model for tabular prediction. We define a process for extracting a\nlarge, high-quality training dataset from the TabLib corpus, proposing methods\nfor tabular data filtering and quality control. Using the resulting dataset,\nwhich comprises over 2.1B rows from over 4M unique tables, we fine-tune a Llama\n3-8B large language model (LLM) for tabular data prediction (classification and\nbinned regression) using a novel packing and attention scheme for tabular\nprediction. Through evaluation across a test suite of 329 datasets, we find\nthat TabuLa-8B has zero-shot accuracy on unseen tables that is over 15\npercentage points (pp) higher than random guessing, a feat that is not possible\nwith existing state-of-the-art tabular prediction models (e.g. XGBoost,\nTabPFN). In the few-shot setting (1-32 shots), without any fine-tuning on the\ntarget datasets, TabuLa-8B is 5-15 pp more accurate than XGBoost and TabPFN\nmodels that are explicitly trained on equal, or even up to 16x more data. We\nrelease our model, code, and data along with the publication of this paper.\n","authors":["Josh Gardner","Juan C. Perdomo","Ludwig Schmidt"],"pdf_url":"https://arxiv.org/pdf/2406.12031v2.pdf","comment":"NeurIPS 2024 camera-ready updates"},{"id":"http://arxiv.org/abs/2411.13699v1","updated":"2024-11-20T20:38:34Z","published":"2024-11-20T20:38:34Z","title":"Test Security in Remote Testing Age: Perspectives from Process Data\n  Analytics and AI","summary":"  The COVID-19 pandemic has accelerated the implementation and acceptance of\nremotely proctored high-stake assessments. While the flexible administration of\nthe tests brings forth many values, it raises test security-related concerns.\nMeanwhile, artificial intelligence (AI) has witnessed tremendous advances in\nthe last five years. Many AI tools (such as the very recent ChatGPT) can\ngenerate high-quality responses to test items. These new developments require\ntest security research beyond the statistical analysis of scores and response\ntime. Data analytics and AI methods based on clickstream process data can get\nus deeper insight into the test-taking process and hold great promise for\nsecuring remotely administered high-stakes tests. This chapter uses real-world\nexamples to show that this is indeed the case.\n","authors":["Jiangang Hao","Michael Fauss"],"pdf_url":"https://arxiv.org/pdf/2411.13699v1.pdf","comment":"23 pages, 8 figures"},{"id":"http://arxiv.org/abs/2305.14336v5","updated":"2024-11-20T20:13:31Z","published":"2023-05-23T17:58:10Z","title":"Schema-Driven Information Extraction from Heterogeneous Tables","summary":"  In this paper, we explore the question of whether large language models can\nsupport cost-efficient information extraction from tables. We introduce\nschema-driven information extraction, a new task that transforms tabular data\ninto structured records following a human-authored schema. To assess various\nLLM's capabilities on this task, we present a benchmark comprised of tables\nfrom four diverse domains: machine learning papers, chemistry literature,\nmaterial science journals, and webpages. We use this collection of annotated\ntables to evaluate the ability of open-source and API-based language models to\nextract information from tables covering diverse domains and data formats. Our\nexperiments demonstrate that surprisingly competitive performance can be\nachieved without requiring task-specific pipelines or labels, achieving F1\nscores ranging from 74.2 to 96.1, while maintaining cost efficiency. Moreover,\nthrough detailed ablation studies and analyses, we investigate the factors\ncontributing to model success and validate the practicality of distilling\ncompact models to reduce API reliance.\n","authors":["Fan Bai","Junmo Kang","Gabriel Stanovsky","Dayne Freitag","Mark Dredze","Alan Ritter"],"pdf_url":"https://arxiv.org/pdf/2305.14336v5.pdf","comment":"Accepted to EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2411.13691v1","updated":"2024-11-20T20:10:43Z","published":"2024-11-20T20:10:43Z","title":"Retrieval-Augmented Generation for Domain-Specific Question Answering: A\n  Case Study on Pittsburgh and CMU","summary":"  We designed a Retrieval-Augmented Generation (RAG) system to provide large\nlanguage models with relevant documents for answering domain-specific questions\nabout Pittsburgh and Carnegie Mellon University (CMU). We extracted over 1,800\nsubpages using a greedy scraping strategy and employed a hybrid annotation\nprocess, combining manual and Mistral-generated question-answer pairs,\nachieving an inter-annotator agreement (IAA) score of 0.7625. Our RAG framework\nintegrates BM25 and FAISS retrievers, enhanced with a reranker for improved\ndocument retrieval accuracy. Experimental results show that the RAG system\nsignificantly outperforms a non-RAG baseline, particularly in time-sensitive\nand complex queries, with an F1 score improvement from 5.45% to 42.21% and\nrecall of 56.18%. This study demonstrates the potential of RAG systems in\nenhancing answer precision and relevance, while identifying areas for further\noptimization in document retrieval and model training.\n","authors":["Haojia Sun","Yaqi Wang","Shuting Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.13691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13687v1","updated":"2024-11-20T20:07:25Z","published":"2024-11-20T20:07:25Z","title":"Hierarchical Text Classification (HTC) vs. eXtreme Multilabel\n  Classification (XML): Two Sides of the Same Medal","summary":"  Assigning a subset of labels from a fixed pool of labels to a given input\ntext is a text classification problem with many real-world applications, such\nas in recommender systems. Two separate research streams address this issue.\nHierarchical Text Classification (HTC) focuses on datasets with smaller label\npools of hundreds of entries, accompanied by a semantic label hierarchy. In\ncontrast, eXtreme Multi-Label Text Classification (XML) considers very large\nlabel pools with up to millions of entries, in which the labels are not\narranged in any particular manner. However, in XML, a common approach is to\nconstruct an artificial hierarchy without any semantic information before or\nduring the training process. Here, we investigate how state-of-the-art models\nfrom one domain perform when trained and tested on datasets from the other\ndomain. The HBGL and HGLCR models from the HTC domain are trained and tested on\nthe datasets Wiki10-31K, AmazonCat-13K, and Amazon-670K from the XML domain. On\nthe other side, the XML models CascadeXML and XR-Transformer are trained and\ntested on the datasets Web of Science, The New York Times Annotated Corpus, and\nRCV1-V2 from the HTC domain. HTC models, on the other hand, are not equipped to\nhandle the size of XML datasets and achieve poor transfer results. The code and\nnumerous files that are needed to reproduce our results can be obtained from\nhttps://github.com/FloHauss/XMC_HTC\n","authors":["Nerijus Bertalis","Paul Granse","Ferhat Gül","Florian Hauss","Leon Menkel","David Schüler","Tom Speier","Lukas Galke","Ansgar Scherp"],"pdf_url":"https://arxiv.org/pdf/2411.13687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13676v1","updated":"2024-11-20T19:51:25Z","published":"2024-11-20T19:51:25Z","title":"Hymba: A Hybrid-head Architecture for Small Language Models","summary":"  We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput.\n","authors":["Xin Dong","Yonggan Fu","Shizhe Diao","Wonmin Byeon","Zijia Chen","Ameya Sunil Mahabaleshwarkar","Shih-Yang Liu","Matthijs Van Keirsbilck","Min-Hung Chen","Yoshi Suhara","Yingyan Lin","Jan Kautz","Pavlo Molchanov"],"pdf_url":"https://arxiv.org/pdf/2411.13676v1.pdf","comment":"20 pages, models are available on huggingface"},{"id":"http://arxiv.org/abs/2404.04254v3","updated":"2024-11-20T19:17:53Z","published":"2024-04-05T17:58:52Z","title":"Watermark-based Attribution of AI-Generated Content","summary":"  Several companies have deployed watermark-based detection to identify\nAI-generated content. However, attribution--the ability to trace back to the\nuser of a generative AI (GenAI) service who created a given piece of\nAI-generated content--remains largely unexplored despite its growing\nimportance. In this work, we aim to bridge this gap by conducting the first\nsystematic study on watermark-based, user-level attribution of AI-generated\ncontent. Our key idea is to assign a unique watermark to each user of the GenAI\nservice and embed this watermark into the AI-generated content created by that\nuser. Attribution is then performed by identifying the user whose watermark\nbest matches the one extracted from the given content. This approach, however,\nfaces a key challenge: How should watermarks be selected for users to maximize\nattribution performance? To address the challenge, we first theoretically\nderive lower bounds on detection and attribution performance through rigorous\nprobabilistic analysis for any given set of user watermarks. Then, we select\nwatermarks for users to maximize these lower bounds, thereby optimizing\ndetection and attribution performance. Our theoretical and empirical results\nshow that watermark-based attribution inherits both the accuracy and\n(non-)robustness properties of the underlying watermark. Specifically,\nattribution remains highly accurate when the watermarked AI-generated content\nis either not post-processed or subjected to common post-processing such as\nJPEG compression, as well as black-box adversarial post-processing with limited\nquery budgets.\n","authors":["Zhengyuan Jiang","Moyang Guo","Yuepeng Hu","Neil Zhenqiang Gong"],"pdf_url":"https://arxiv.org/pdf/2404.04254v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24160v2","updated":"2024-11-20T10:22:59Z","published":"2024-10-31T17:19:03Z","title":"Redefining <Creative> in Dictionary: Towards an Enhanced Semantic\n  Understanding of Creative Generation","summary":"  ``Creative'' remains an inherently abstract concept for both humans and\ndiffusion models. While text-to-image (T2I) diffusion models can easily\ngenerate out-of-domain concepts like ``a blue banana'', they struggle with\ngenerating combinatorial objects such as ``a creative mixture that resembles a\nlettuce and a mantis'', due to difficulties in understanding the semantic depth\nof ``creative''. Current methods rely heavily on synthesizing reference prompts\nor images to achieve a creative effect, typically requiring retraining for each\nunique creative output -- a process that is computationally intensive and\nlimits practical applications. To address this, we introduce CreTok, which\nbrings meta-creativity to diffusion models by redefining ``creative'' as a new\ntoken, \\texttt{<CreTok>}, thus enhancing models' semantic understanding for\ncombinatorial creativity. CreTok achieves such redefinition by iteratively\nsampling diverse text pairs from our proposed CangJie dataset to form adaptive\nprompts and restrictive prompts, and then optimizing the similarity between\ntheir respective text embeddings. Extensive experiments demonstrate that\n\\texttt{<CreTok>} enables the universal and direct generation of combinatorial\ncreativity across diverse concepts without additional training (4s vs. BASS's\n2400s per image), achieving state-of-the-art performance with improved\ntext-image alignment ($\\uparrow$0.03 in VQAScore) and higher human preference\nratings ($\\uparrow$0.009 in PickScore and $\\uparrow$0.169 in ImageReward).\nFurther evaluations with GPT-4o and user studies underscore CreTok's strengths\nin advancing creative generation.\n","authors":["Fu Feng","Yucheng Xie","Xu Yang","Jing Wang","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2410.24160v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13424v1","updated":"2024-11-20T16:09:16Z","published":"2024-11-20T16:09:16Z","title":"CAFE A Novel Code switching Dataset for Algerian Dialect French and\n  English","summary":"  The paper introduces and publicly releases (Data download link available\nafter acceptance) CAFE -- the first Code-switching dataset between Algerian\ndialect, French, and english languages. The CAFE speech data is unique for (a)\nits spontaneous speaking style in vivo human-human conversation capturing\nphenomena like code-switching and overlapping speech, (b) addresses distinct\nlinguistic challenges in North African Arabic dialect; (c) the CAFE captures\ndialectal variations from various parts of Algeria within different\nsociolinguistic contexts. CAFE data contains approximately 37 hours of speech,\nwith a subset, CAFE-small, of 2 hours and 36 minutes released with manual human\nannotation including speech segmentation, transcription, explicit annotation of\ncode-switching points, overlapping speech, and other events such as noises, and\nlaughter among others. The rest approximately 34.58 hours contain pseudo label\ntranscriptions. In addition to the data release, the paper also highlighted the\nchallenges of using state-of-the-art Automatic Speech Recognition (ASR) models\nsuch as Whisper large-v2,3 and PromptingWhisper to handle such content.\nFollowing, we benchmark CAFE data with the aforementioned Whisper models and\nshow how well-designed data processing pipelines and advanced decoding\ntechniques can improve the ASR performance in terms of Mixed Error Rate (MER)\nof 0.310, Character Error Rate (CER) of 0.329 and Word Error Rate (WER) of\n0.538.\n","authors":["Houssam Eddine-Othman Lachemat","Akli Abbas","Nourredine Oukas","Yassine El Kheir","Samia Haboussi","Absar Chowdhury Shammur"],"pdf_url":"https://arxiv.org/pdf/2411.13424v1.pdf","comment":"24 pages, submitted to tallip"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2411.13553v1","updated":"2024-11-20T18:59:58Z","published":"2024-11-20T18:59:58Z","title":"AI-generated Image Detection: Passive or Watermark?","summary":"  While text-to-image models offer numerous benefits, they also pose\nsignificant societal risks. Detecting AI-generated images is crucial for\nmitigating these risks. Detection methods can be broadly categorized into\npassive and watermark-based approaches: passive detectors rely on artifacts\npresent in AI-generated images, whereas watermark-based detectors proactively\nembed watermarks into such images. A key question is which type of detector\nperforms better in terms of effectiveness, robustness, and efficiency. However,\nthe current literature lacks a comprehensive understanding of this issue. In\nthis work, we aim to bridge that gap by developing ImageDetectBench, the first\ncomprehensive benchmark to compare the effectiveness, robustness, and\nefficiency of passive and watermark-based detectors. Our benchmark includes\nfour datasets, each containing a mix of AI-generated and non-AI-generated\nimages. We evaluate five passive detectors and four watermark-based detectors\nagainst eight types of common perturbations and three types of adversarial\nperturbations. Our benchmark results reveal several interesting findings. For\ninstance, watermark-based detectors consistently outperform passive detectors,\nboth in the presence and absence of perturbations. Based on these insights, we\nprovide recommendations for detecting AI-generated images, e.g., when both\ntypes of detectors are applicable, watermark-based detectors should be the\npreferred choice.\n","authors":["Moyang Guo","Yuepeng Hu","Zhengyuan Jiang","Zeyu Li","Amir Sadovnik","Arka Daw","Neil Gong"],"pdf_url":"https://arxiv.org/pdf/2411.13553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13552v1","updated":"2024-11-20T18:59:52Z","published":"2024-11-20T18:59:52Z","title":"REDUCIO! Generating 1024$\\times$1024 Video within 16 Seconds using\n  Extremely Compressed Motion Latents","summary":"  Commercial video generation models have exhibited realistic, high-fidelity\nresults but are still restricted to limited access. One crucial obstacle for\nlarge-scale applications is the expensive training and inference cost. In this\npaper, we argue that videos contain much more redundant information than\nimages, thus can be encoded by very few motion latents based on a content\nimage. Towards this goal, we design an image-conditioned VAE to encode a video\nto an extremely compressed motion latent space. This magic Reducio charm\nenables 64x reduction of latents compared to a common 2D VAE, without\nsacrificing the quality. Training diffusion models on such a compact\nrepresentation easily allows for generating 1K resolution videos. We then adopt\na two-stage video generation paradigm, which performs text-to-image and\ntext-image-to-video sequentially. Extensive experiments show that our\nReducio-DiT achieves strong performance in evaluation, though trained with\nlimited GPU resources. More importantly, our method significantly boost the\nefficiency of video LDMs both in training and inference. We train Reducio-DiT\nin around 3.2K training hours in total and generate a 16-frame 1024*1024 video\nclip within 15.5 seconds on a single A100 GPU. Code released at\nhttps://github.com/microsoft/Reducio-VAE .\n","authors":["Rui Tian","Qi Dai","Jianmin Bao","Kai Qiu","Yifan Yang","Chong Luo","Zuxuan Wu","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2411.13552v1.pdf","comment":"Code available at https://github.com/microsoft/Reducio-VAE"},{"id":"http://arxiv.org/abs/2411.13550v1","updated":"2024-11-20T18:59:01Z","published":"2024-11-20T18:59:01Z","title":"Find Any Part in 3D","summary":"  We study open-world part segmentation in 3D: segmenting any part in any\nobject based on any text query. Prior methods are limited in object categories\nand part vocabularies. Recent advances in AI have demonstrated effective\nopen-world recognition capabilities in 2D. Inspired by this progress, we\npropose an open-world, direct-prediction model for 3D part segmentation that\ncan be applied zero-shot to any object. Our approach, called Find3D, trains a\ngeneral-category point embedding model on large-scale 3D assets from the\ninternet without any human annotation. It combines a data engine, powered by\nfoundation models for annotating data, with a contrastive training method. We\nachieve strong performance and generalization across multiple datasets, with up\nto a 3x improvement in mIoU over the next best method. Our model is 6x to over\n300x faster than existing baselines. To encourage research in general-category\nopen-world 3D part segmentation, we also release a benchmark for general\nobjects and parts. Project website: https://ziqi-ma.github.io/find3dsite/\n","authors":["Ziqi Ma","Yisong Yue","Georgia Gkioxari"],"pdf_url":"https://arxiv.org/pdf/2411.13550v1.pdf","comment":"Project website: https://ziqi-ma.github.io/find3dsite/"},{"id":"http://arxiv.org/abs/2411.13549v1","updated":"2024-11-20T18:58:31Z","published":"2024-11-20T18:58:31Z","title":"Generating 3D-Consistent Videos from Unposed Internet Photos","summary":"  We address the problem of generating videos from unposed internet photos. A\nhandful of input images serve as keyframes, and our model interpolates between\nthem to simulate a path moving between the cameras. Given random images, a\nmodel's ability to capture underlying geometry, recognize scene identity, and\nrelate frames in terms of camera position and orientation reflects a\nfundamental understanding of 3D structure and scene layout. However, existing\nvideo models such as Luma Dream Machine fail at this task. We design a\nself-supervised method that takes advantage of the consistency of videos and\nvariability of multiview internet photos to train a scalable, 3D-aware video\nmodel without any 3D annotations such as camera parameters. We validate that\nour method outperforms all baselines in terms of geometric and appearance\nconsistency. We also show our model benefits applications that enable camera\ncontrol, such as 3D Gaussian Splatting. Our results suggest that we can scale\nup scene-level 3D learning using only 2D data such as videos and multiview\ninternet photos.\n","authors":["Gene Chou","Kai Zhang","Sai Bi","Hao Tan","Zexiang Xu","Fujun Luan","Bharath Hariharan","Noah Snavely"],"pdf_url":"https://arxiv.org/pdf/2411.13549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13548v1","updated":"2024-11-20T18:56:24Z","published":"2024-11-20T18:56:24Z","title":"HF-Diff: High-Frequency Perceptual Loss and Distribution Matching for\n  One-Step Diffusion-Based Image Super-Resolution","summary":"  Although recent diffusion-based single-step super-resolution methods achieve\nbetter performance as compared to SinSR, they are computationally complex. To\nimprove the performance of SinSR, we investigate preserving the high-frequency\ndetail features during super-resolution (SR) because the downgraded images lack\ndetailed information. For this purpose, we introduce a high-frequency\nperceptual loss by utilizing an invertible neural network (INN) pretrained on\nthe ImageNet dataset. Different feature maps of pretrained INN produce\ndifferent high-frequency aspects of an image. During the training phase, we\nimpose to preserve the high-frequency features of super-resolved and ground\ntruth (GT) images that improve the SR image quality during inference.\nFurthermore, we also utilize the Jenson-Shannon divergence between GT and SR\nimages in the pretrained DINO-v2 embedding space to match their distribution.\nBy introducing the $\\textbf{h}igh$- $\\textbf{f}requency$ preserving loss and\ndistribution matching constraint in the single-step $\\textbf{diff}usion-based$\nSR ($\\textbf{HF-Diff}$), we achieve a state-of-the-art CLIPIQA score in the\nbenchmark RealSR, RealSet65, DIV2K-Val, and ImageNet datasets. Furthermore, the\nexperimental results in several datasets demonstrate that our high-frequency\nperceptual loss yields better SR image quality than LPIPS and VGG-based\nperceptual losses. Our code will be released at\nhttps://github.com/shoaib-sami/HF-Diff.\n","authors":["Shoaib Meraj Sami","Md Mahedi Hasan","Jeremy Dawson","Nasser Nasrabadi"],"pdf_url":"https://arxiv.org/pdf/2411.13548v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2411.13545v1","updated":"2024-11-20T18:54:53Z","published":"2024-11-20T18:54:53Z","title":"Pushing the Limits of Sparsity: A Bag of Tricks for Extreme Pruning","summary":"  Pruning of deep neural networks has been an effective technique for reducing\nmodel size while preserving most of the performance of dense networks, crucial\nfor deploying models on memory and power-constrained devices. While recent\nsparse learning methods have shown promising performance up to moderate\nsparsity levels such as 95% and 98%, accuracy quickly deteriorates when pushing\nsparsities to extreme levels. Obtaining sparse networks at such extreme\nsparsity levels presents unique challenges, such as fragile gradient flow and\nheightened risk of layer collapse. In this work, we explore network performance\nbeyond the commonly studied sparsities, and propose a collection of techniques\nthat enable the continuous learning of networks without accuracy collapse even\nat extreme sparsities, including 99.90%, 99.95% and 99.99% on ResNet\narchitectures. Our approach combines 1) Dynamic ReLU phasing, where DyReLU\ninitially allows for richer parameter exploration before being gradually\nreplaced by standard ReLU, 2) weight sharing which reuses parameters within a\nresidual layer while maintaining the same number of learnable parameters, and\n3) cyclic sparsity, where both sparsity levels and sparsity patterns evolve\ndynamically throughout training to better encourage parameter exploration. We\nevaluate our method, which we term Extreme Adaptive Sparse Training (EAST) at\nextreme sparsities using ResNet-34 and ResNet-50 on CIFAR-10, CIFAR-100, and\nImageNet, achieving significant performance improvements over state-of-the-art\nmethods we compared with.\n","authors":["Andy Li","Aiden Durrant","Milan Markovic","Lu Yin","Georgios Leontidis"],"pdf_url":"https://arxiv.org/pdf/2411.13545v1.pdf","comment":"10 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2411.13544v1","updated":"2024-11-20T18:54:36Z","published":"2024-11-20T18:54:36Z","title":"DIS-Mine: Instance Segmentation for Disaster-Awareness in Poor-Light\n  Condition in Underground Mines","summary":"  Detecting disasters in underground mining, such as explosions and structural\ndamage, has been a persistent challenge over the years. This problem is\ncompounded for first responders, who often have no clear information about the\nextent or nature of the damage within the mine. The poor-light or even total\ndarkness inside the mines makes rescue efforts incredibly difficult, leading to\na tragic loss of life. In this paper, we propose a novel instance segmentation\nmethod called DIS-Mine, specifically designed to identify disaster-affected\nareas within underground mines under low-light or poor visibility conditions,\naiding first responders in rescue efforts. DIS-Mine is capable of detecting\nobjects in images, even in complete darkness, by addressing challenges such as\nhigh noise, color distortions, and reduced contrast. The key innovations of\nDIS-Mine are built upon four core components: i) Image brightness improvement,\nii) Instance segmentation with SAM integration, iii) Mask R-CNN-based\nsegmentation, and iv) Mask alignment with feature matching. On top of that, we\nhave collected real-world images from an experimental underground mine,\nintroducing a new dataset named ImageMine, specifically gathered in\nlow-visibility conditions. This dataset serves to validate the performance of\nDIS-Mine in realistic, challenging environments. Our comprehensive experiments\non the ImageMine dataset, as well as on various other datasets demonstrate that\nDIS-Mine achieves a superior F1 score of 86.0% and mIoU of 72.0%, outperforming\nstate-of-the-art instance segmentation methods, with at least 15x improvement\nand up to 80% higher precision in object detection.\n","authors":["Mizanur Rahman Jewel","Mohamed Elmahallawy","Sanjay Madria","Samuel Frimpong"],"pdf_url":"https://arxiv.org/pdf/2411.13544v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13536v1","updated":"2024-11-20T18:37:58Z","published":"2024-11-20T18:37:58Z","title":"Identity Preserving 3D Head Stylization with Multiview Score\n  Distillation","summary":"  3D head stylization transforms realistic facial features into artistic\nrepresentations, enhancing user engagement across gaming and virtual reality\napplications. While 3D-aware generators have made significant advancements,\nmany 3D stylization methods primarily provide near-frontal views and struggle\nto preserve the unique identities of original subjects, often resulting in\noutputs that lack diversity and individuality. This paper addresses these\nchallenges by leveraging the PanoHead model, synthesizing images from a\ncomprehensive 360-degree perspective. We propose a novel framework that employs\nnegative log-likelihood distillation (LD) to enhance identity preservation and\nimprove stylization quality. By integrating multi-view grid score and mirror\ngradients within the 3D GAN architecture and introducing a score rank weighing\ntechnique, our approach achieves substantial qualitative and quantitative\nimprovements. Our findings not only advance the state of 3D head stylization\nbut also provide valuable insights into effective distillation processes\nbetween diffusion models and GANs, focusing on the critical issue of identity\npreservation. Please visit the https://three-bee.github.io/head_stylization for\nmore visuals.\n","authors":["Bahri Batuhan Bilecen","Ahmet Berke Gokmen","Furkan Guzelant","Aysegul Dundar"],"pdf_url":"https://arxiv.org/pdf/2411.13536v1.pdf","comment":"https://three-bee.github.io/head_stylization"},{"id":"http://arxiv.org/abs/2411.13535v1","updated":"2024-11-20T18:37:01Z","published":"2024-11-20T18:37:01Z","title":"Comparative Analysis of Machine Learning and Deep Learning Models for\n  Classifying Squamous Epithelial Cells of the Cervix","summary":"  The cervix is the narrow end of the uterus that connects to the vagina in the\nfemale reproductive system. Abnormal cell growth in the squamous epithelial\nlining of the cervix leads to cervical cancer in females. A Pap smear is a\ndiagnostic procedure used to detect cervical cancer by gently collecting cells\nfrom the surface of the cervix with a small brush and analyzing their changes\nunder a microscope. For population-based cervical cancer screening, visual\ninspection with acetic acid is a cost-effective method with high sensitivity.\nHowever, Pap smears are also suitable for mass screening due to their higher\nspecificity. The current Pap smear analysis method is manual, time-consuming,\nlabor-intensive, and prone to human error. Therefore, an artificial\nintelligence (AI)-based approach for automatic cell classification is needed.\nIn this study, we aimed to classify cells in Pap smear images into five\ncategories: superficial-intermediate, parabasal, koilocytes, dyskeratotic, and\nmetaplastic. Various machine learning (ML) algorithms, including Gradient\nBoosting, Random Forest, Support Vector Machine, and k-Nearest Neighbor, as\nwell as deep learning (DL) approaches like ResNet-50, were employed for this\nclassification task. The ML models demonstrated high classification accuracy;\nhowever, ResNet-50 outperformed the others, achieving a classification accuracy\nof 93.06%. This study highlights the efficiency of DL models for cell-level\nclassification and their potential to aid in the early diagnosis of cervical\ncancer from Pap smear images.\n","authors":["Subhasish Das","Satish K Panda","Madhusmita Sethy","Prajna Paramita Giri","Ashwini K Nanda"],"pdf_url":"https://arxiv.org/pdf/2411.13535v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2411.13528v1","updated":"2024-11-20T18:24:11Z","published":"2024-11-20T18:24:11Z","title":"Entropy Bootstrapping for Weakly Supervised Nuclei Detection","summary":"  Microscopy structure segmentation, such as detecting cells or nuclei,\ngenerally requires a human to draw a ground truth contour around each instance.\nWeakly supervised approaches (e.g. consisting of only single point labels) have\nthe potential to reduce this workload significantly. Our approach uses\nindividual point labels for an entropy estimation to approximate an underlying\ndistribution of cell pixels. We infer full cell masks from this distribution,\nand use Mask-RCNN to produce an instance segmentation output. We compare this\npoint--annotated approach with training on the full ground truth masks. We show\nthat our method achieves a comparatively good level of performance, despite a\n95% reduction in pixel labels.\n","authors":["James Willoughby","Irina Voiculescu"],"pdf_url":"https://arxiv.org/pdf/2411.13528v1.pdf","comment":"Submitted for CVPR 2025"},{"id":"http://arxiv.org/abs/2411.13525v1","updated":"2024-11-20T18:21:58Z","published":"2024-11-20T18:21:58Z","title":"Geometric Algebra Planes: Convex Implicit Neural Volumes","summary":"  Volume parameterizations abound in recent literature, from the classic voxel\ngrid to the implicit neural representation and everything in between. While\nimplicit representations have shown impressive capacity and better memory\nefficiency compared to voxel grids, to date they require training via nonconvex\noptimization. This nonconvex training process can be slow to converge and\nsensitive to initialization and hyperparameter choices that affect the final\nconverged result. We introduce a family of models, GA-Planes, that is the first\nclass of implicit neural volume representations that can be trained by convex\noptimization. GA-Planes models include any combination of features stored in\ntensor basis elements, followed by a neural feature decoder. They generalize\nmany existing representations and can be adapted for convex, semiconvex, or\nnonconvex training as needed for different inverse problems. In the 2D setting,\nwe prove that GA-Planes is equivalent to a low-rank plus low-resolution matrix\nfactorization; we show that this approximation outperforms the classic low-rank\nplus sparse decomposition for fitting a natural image. In 3D, we demonstrate\nGA-Planes' competitive performance in terms of expressiveness, model size, and\noptimizability across three volume fitting tasks: radiance field\nreconstruction, 3D segmentation, and video segmentation.\n","authors":["Irmak Sivgin","Sara Fridovich-Keil","Gordon Wetzstein","Mert Pilanci"],"pdf_url":"https://arxiv.org/pdf/2411.13525v1.pdf","comment":"Code is available at\n  https://github.com/sivginirmak/Geometric-Algebra-Planes"},{"id":"http://arxiv.org/abs/2411.13503v1","updated":"2024-11-20T17:54:41Z","published":"2024-11-20T17:54:41Z","title":"VBench++: Comprehensive and Versatile Benchmark Suite for Video\n  Generative Models","summary":"  Video generation has witnessed significant advancements, yet evaluating these\nmodels remains a challenge. A comprehensive evaluation benchmark for video\ngeneration is indispensable for two reasons: 1) Existing metrics do not fully\nalign with human perceptions; 2) An ideal evaluation system should provide\ninsights to inform future developments of video generation. To this end, we\npresent VBench, a comprehensive benchmark suite that dissects \"video generation\nquality\" into specific, hierarchical, and disentangled dimensions, each with\ntailored prompts and evaluation methods. VBench has several appealing\nproperties: 1) Comprehensive Dimensions: VBench comprises 16 dimensions in\nvideo generation (e.g., subject identity inconsistency, motion smoothness,\ntemporal flickering, and spatial relationship, etc). The evaluation metrics\nwith fine-grained levels reveal individual models' strengths and weaknesses. 2)\nHuman Alignment: We also provide a dataset of human preference annotations to\nvalidate our benchmarks' alignment with human perception, for each evaluation\ndimension respectively. 3) Valuable Insights: We look into current models'\nability across various evaluation dimensions, and various content types. We\nalso investigate the gaps between video and image generation models. 4)\nVersatile Benchmarking: VBench++ supports evaluating text-to-video and\nimage-to-video. We introduce a high-quality Image Suite with an adaptive aspect\nratio to enable fair evaluations across different image-to-video generation\nsettings. Beyond assessing technical quality, VBench++ evaluates the\ntrustworthiness of video generative models, providing a more holistic view of\nmodel performance. 5) Full Open-Sourcing: We fully open-source VBench++ and\ncontinually add new video generation models to our leaderboard to drive forward\nthe field of video generation.\n","authors":["Ziqi Huang","Fan Zhang","Xiaojie Xu","Yinan He","Jiashuo Yu","Ziyue Dong","Qianli Ma","Nattapol Chanpaisit","Chenyang Si","Yuming Jiang","Yaohui Wang","Xinyuan Chen","Ying-Cong Chen","Limin Wang","Dahua Lin","Yu Qiao","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2411.13503v1.pdf","comment":"Leaderboard:\n  https://huggingface.co/spaces/Vchitect/VBench_Leaderboard Code:\n  https://github.com/Vchitect/VBench Project page:\n  https://vchitect.github.io/VBench-project/ extension of arXiv:2311.17982.\n  arXiv admin note: substantial text overlap with arXiv:2311.17982"},{"id":"http://arxiv.org/abs/2411.13490v1","updated":"2024-11-20T17:38:34Z","published":"2024-11-20T17:38:34Z","title":"Efficient Brain Imaging Analysis for Alzheimer's and Dementia Detection\n  Using Convolution-Derivative Operations","summary":"  Alzheimer's disease (AD) is characterized by progressive neurodegeneration\nand results in detrimental structural changes in human brains. Detecting these\nchanges is crucial for early diagnosis and timely intervention of disease\nprogression. Jacobian maps, derived from spatial normalization in voxel-based\nmorphometry (VBM), have been instrumental in interpreting volume alterations\nassociated with AD. However, the computational cost of generating Jacobian maps\nlimits its clinical adoption. In this study, we explore alternative methods and\npropose Sobel kernel angle difference (SKAD) as a computationally efficient\nalternative. SKAD is a derivative operation that offers an optimized approach\nto quantifying volumetric alterations through localized analysis of the\ngradients. By efficiently extracting gradient amplitude changes at critical\nspatial regions, this derivative operation captures regional volume variations\nEvaluation of SKAD over various medical datasets demonstrates that it is 6.3x\nfaster than Jacobian maps while still maintaining comparable accuracy. This\nmakes it an efficient and competitive approach in neuroimaging research and\nclinical practice.\n","authors":["Yasmine Mustafa","Mohamed Elmahallawy","Tie Luo"],"pdf_url":"https://arxiv.org/pdf/2411.13490v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14919v3","updated":"2024-11-20T17:20:00Z","published":"2024-10-19T00:33:51Z","title":"Adversarial Score identity Distillation: Rapidly Surpassing the Teacher\n  in One Step","summary":"  Score identity Distillation (SiD) is a data-free method that has achieved\nSOTA performance in image generation by leveraging only a pretrained diffusion\nmodel, without requiring any training data. However, its ultimate performance\nis constrained by how accurate the pretrained model captures the true data\nscores at different stages of the diffusion process. In this paper, we\nintroduce SiDA (SiD with Adversarial Loss), which not only enhances generation\nquality but also improves distillation efficiency by incorporating real images\nand adversarial loss. SiDA utilizes the encoder from the generator's score\nnetwork as a discriminator, boosting its ability to distinguish between real\nimages and those generated by SiD. The adversarial loss is batch-normalized\nwithin each GPU and then combined with the original SiD loss. This integration\neffectively incorporates the average \"fakeness\" per GPU batch into the\npixel-based SiD loss, enabling SiDA to distill a single-step generator either\nfrom scratch or by fine-tuning an existing one. SiDA converges significantly\nfaster than its predecessor when trained from scratch, and swiftly improves\nupon the original model's performance after an initial warmup period during\nfine-tuning from a pre-distilled SiD generator. This one-step adversarial\ndistillation method establishes new benchmarks in generation performance when\ndistilling EDM diffusion models pretrained on CIFAR-10 (32x32) and ImageNet\n(64x64), achieving FID score of 1.110 on ImageNet 64x64. It sets record-low FID\nscores when distilling EDM2 models trained on ImageNet (512x512), surpassing\neven the largest teacher model, EDM2-XXL. Our SiDA's results record FID scores\nof 2.156 for EDM2-XS, 1.669 for S, 1.488 for M, 1.413 for L, 1.379 for XL, and\n1.366 for XXL, demonstrating significant improvements across all model sizes.\nOur open-source code will be integrated into the SiD codebase.\n","authors":["Mingyuan Zhou","Huangjie Zheng","Yi Gu","Zhendong Wang","Hai Huang"],"pdf_url":"https://arxiv.org/pdf/2410.14919v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14386v3","updated":"2024-11-20T17:14:50Z","published":"2024-05-23T10:04:23Z","title":"Capsule Network Projectors are Equivariant and Invariant Learners","summary":"  Learning invariant representations has been the longstanding approach to\nself-supervised learning. However, recently progress has been made in\npreserving equivariant properties in representations, yet do so with highly\nprescribed architectures. In this work, we propose an invariant-equivariant\nself-supervised architecture that employs Capsule Networks (CapsNets) which\nhave been shown to capture equivariance with respect to novel viewpoints. We\ndemonstrate that the use of CapsNets in equivariant self-supervised\narchitectures achieves improved downstream performance on equivariant tasks\nwith higher efficiency and fewer network parameters. To accommodate the\narchitectural changes of CapsNets, we introduce a new objective function based\non entropy minimisation. This approach which we name CapsIE (Capsule Invariant\nEquivariant Network) achieves state-of-the-art performance on the equivariant\nrotation tasks on the 3DIEBench dataset compared to prior equivariant SSL\nmethods, while performing competitively against supervised counterparts. Our\nresults demonstrate the ability of CapsNets to learn complex and generalised\nrepresentations for large-scale, multi-task datasets compared to previous\nCapsNet benchmarks. Code is available at https://github.com/AberdeenML/CapsIE.\n","authors":["Miles Everett","Aiden Durrant","Mingjun Zhong","Georgios Leontidis"],"pdf_url":"https://arxiv.org/pdf/2405.14386v3.pdf","comment":"V3: Ignore V1 and V2 as we have fixed a bug in our code and results;\n  15 pages, 5 figures, 8 Tables"},{"id":"http://arxiv.org/abs/2411.13409v1","updated":"2024-11-20T15:48:21Z","published":"2024-11-20T15:48:21Z","title":"Unification of Balti and trans-border sister dialects in the essence of\n  LLMs and AI Technology","summary":"  The language called Balti belongs to the Sino-Tibetan, specifically the\nTibeto-Burman language family. It is understood with variations, across\npopulations in India, China, Pakistan, Nepal, Tibet, Burma, and Bhutan,\ninfluenced by local cultures and producing various dialects. Considering the\ndiverse cultural, socio-political, religious, and geographical impacts, it is\nimportant to step forward unifying the dialects, the basis of common root,\nlexica, and phonological perspectives, is vital. In the era of globalization\nand the increasingly frequent developments in AI technology, understanding the\ndiversity and the efforts of dialect unification is important to understanding\ncommonalities and shortening the gaps impacted by unavoidable circumstances.\nThis article analyzes and examines how artificial intelligence AI in the\nessence of Large Language Models LLMs, can assist in analyzing, documenting,\nand standardizing the endangered Balti Language, based on the efforts made in\ndifferent dialects so far.\n","authors":["Muhammad Sharif","Jiangyan Yi","Muhammad Shoaib"],"pdf_url":"https://arxiv.org/pdf/2411.13409v1.pdf","comment":"Accepted by IEEE conference ISCSLP 2024"},{"id":"http://arxiv.org/abs/2411.13383v1","updated":"2024-11-20T15:13:36Z","published":"2024-11-20T15:13:36Z","title":"Adversarial Diffusion Compression for Real-World Image Super-Resolution","summary":"  Real-world image super-resolution (Real-ISR) aims to reconstruct\nhigh-resolution images from low-resolution inputs degraded by complex, unknown\nprocesses. While many Stable Diffusion (SD)-based Real-ISR methods have\nachieved remarkable success, their slow, multi-step inference hinders practical\ndeployment. Recent SD-based one-step networks like OSEDiff and S3Diff alleviate\nthis issue but still incur high computational costs due to their reliance on\nlarge pretrained SD models. This paper proposes a novel Real-ISR method, AdcSR,\nby distilling the one-step diffusion network OSEDiff into a streamlined\ndiffusion-GAN model under our Adversarial Diffusion Compression (ADC)\nframework. We meticulously examine the modules of OSEDiff, categorizing them\ninto two types: (1) Removable (VAE encoder, prompt extractor, text encoder,\netc.) and (2) Prunable (denoising UNet and VAE decoder). Since direct removal\nand pruning can degrade the model's generation capability, we pretrain our\npruned VAE decoder to restore its ability to decode images and employ\nadversarial distillation to compensate for performance loss. This ADC-based\ndiffusion-GAN hybrid design effectively reduces complexity by 73% in inference\ntime, 78% in computation, and 74% in parameters, while preserving the model's\ngeneration capability. Experiments manifest that our proposed AdcSR achieves\ncompetitive recovery quality on both synthetic and real-world datasets,\noffering up to 9.3$\\times$ speedup over previous one-step diffusion-based\nmethods. Code and models will be made available.\n","authors":["Bin Chen","Gehui Li","Rongyuan Wu","Xindong Zhang","Jie Chen","Jian Zhang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.13383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13378v1","updated":"2024-11-20T14:59:47Z","published":"2024-11-20T14:59:47Z","title":"Quantum-Brain: Quantum-Inspired Neural Network Approach to Vision-Brain\n  Understanding","summary":"  Vision-brain understanding aims to extract semantic information about brain\nsignals from human perceptions. Existing deep learning methods for vision-brain\nunderstanding are usually introduced in a traditional learning paradigm missing\nthe ability to learn the connectivities between brain regions. Meanwhile, the\nquantum computing theory offers a new paradigm for designing deep learning\nmodels. Motivated by the connectivities in the brain signals and the\nentanglement properties in quantum computing, we propose a novel Quantum-Brain\napproach, a quantum-inspired neural network, to tackle the vision-brain\nunderstanding problem. To compute the connectivity between areas in brain\nsignals, we introduce a new Quantum-Inspired Voxel-Controlling module to learn\nthe impact of a brain voxel on others represented in the Hilbert space. To\neffectively learn connectivity, a novel Phase-Shifting module is presented to\ncalibrate the value of the brain signals. Finally, we introduce a new\nMeasurement-like Projection module to present the connectivity information from\nthe Hilbert space into the feature space. The proposed approach can learn to\nfind the connectivities between fMRI voxels and enhance the semantic\ninformation obtained from human perceptions. Our experimental results on the\nNatural Scene Dataset benchmarks illustrate the effectiveness of the proposed\nmethod with Top-1 accuracies of 95.1% and 95.6% on image and brain retrieval\ntasks and an Inception score of 95.3% on fMRI-to-image reconstruction task. Our\nproposed quantum-inspired network brings a potential paradigm to solving the\nvision-brain problems via the quantum computing theory.\n","authors":["Hoang-Quan Nguyen","Xuan-Bac Nguyen","Hugh Churchill","Arabinda Kumar Choudhary","Pawan Sinha","Samee U. Khan","Khoa Luu"],"pdf_url":"https://arxiv.org/pdf/2411.13378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13362v1","updated":"2024-11-20T14:36:06Z","published":"2024-11-20T14:36:06Z","title":"RTSR: A Real-Time Super-Resolution Model for AV1 Compressed Content","summary":"  Super-resolution (SR) is a key technique for improving the visual quality of\nvideo content by increasing its spatial resolution while reconstructing fine\ndetails. SR has been employed in many applications including video streaming,\nwhere compressed low-resolution content is typically transmitted to end users\nand then reconstructed with a higher resolution and enhanced quality. To\nsupport real-time playback, it is important to implement fast SR models while\npreserving reconstruction quality; however most existing solutions, in\nparticular those based on complex deep neural networks, fail to do so. To\naddress this issue, this paper proposes a low-complexity SR method, RTSR,\ndesigned to enhance the visual quality of compressed video content, focusing on\nresolution up-scaling from a) 360p to 1080p and from b) 540p to 4K. The\nproposed approach utilizes a CNN-based network architecture, which was\noptimized for AV1 (SVT)-encoded content at various quantization levels based on\na dual-teacher knowledge distillation method. This method was submitted to the\nAIM 2024 Video Super-Resolution Challenge, specifically targeting the\nEfficient/Mobile Real-Time Video Super-Resolution competition. It achieved the\nbest trade-off between complexity and coding performance (measured in PSNR,\nSSIM and VMAF) among all six submissions. The code will be available soon.\n","authors":["Yuxuan Jiang","Jakub Nawała","Chen Feng","Fan Zhang","Xiaoqing Zhu","Joel Sole","David Bull"],"pdf_url":"https://arxiv.org/pdf/2411.13362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08823v3","updated":"2024-11-20T14:33:10Z","published":"2024-02-13T22:07:29Z","title":"Random Representations Outperform Online Continually Learned\n  Representations","summary":"  Continual learning has primarily focused on the issue of catastrophic\nforgetting and the associated stability-plasticity tradeoffs. However, little\nattention has been paid to the efficacy of continually learned representations,\nas representations are learned alongside classifiers throughout the learning\nprocess. Our primary contribution is empirically demonstrating that existing\nonline continually trained deep networks produce inferior representations\ncompared to a simple pre-defined random transforms. Our approach projects raw\npixels using a fixed random transform, approximating an RBF-Kernel initialized\nbefore any data is seen. We then train a simple linear classifier on top\nwithout storing any exemplars, processing one sample at a time in an online\ncontinual learning setting. This method, called RanDumb, significantly\noutperforms state-of-the-art continually learned representations across all\nstandard online continual learning benchmarks. Our study reveals the\nsignificant limitations of representation learning, particularly in\nlow-exemplar and online continual learning scenarios. Extending our\ninvestigation to popular exemplar-free scenarios with pretrained models, we\nfind that training only a linear classifier on top of pretrained\nrepresentations surpasses most continual fine-tuning and prompt-tuning\nstrategies. Overall, our investigation challenges the prevailing assumptions\nabout effective representation learning in online continual learning. Our code\nis available at://github.com/drimpossible/RanDumb.\n","authors":["Ameya Prabhu","Shiven Sinha","Ponnurangam Kumaraguru","Philip H. S. Torr","Ozan Sener","Puneet K. Dokania"],"pdf_url":"https://arxiv.org/pdf/2402.08823v3.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.13350v1","updated":"2024-11-20T14:22:15Z","published":"2024-11-20T14:22:15Z","title":"Learning based Ge'ez character handwritten recognition","summary":"  Ge'ez, an ancient Ethiopic script of cultural and historical significance,\nhas been largely neglected in handwriting recognition research, hindering the\ndigitization of valuable manuscripts. Our study addresses this gap by\ndeveloping a state-of-the-art Ge'ez handwriting recognition system using\nConvolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM)\nnetworks. Our approach uses a two-stage recognition process. First, a CNN is\ntrained to recognize individual characters, which then acts as a feature\nextractor for an LSTM-based system for word recognition. Our dual-stage\nrecognition approach achieves new top scores in Ge'ez handwriting recognition,\noutperforming eight state-of-the-art methods, which are SVTR, ASTER, and others\nas well as human performance, as measured in the HHD-Ethiopic dataset work.\nThis research significantly advances the preservation and accessibility of\nGe'ez cultural heritage, with implications for historical document\ndigitization, educational tools, and cultural preservation. The code will be\nreleased upon acceptance.\n","authors":["Hailemicael Lulseged Yimer","Hailegabriel Dereje Degefa","Marco Cristani","Federico Cunico"],"pdf_url":"https://arxiv.org/pdf/2411.13350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13340v1","updated":"2024-11-20T14:12:34Z","published":"2024-11-20T14:12:34Z","title":"WHALES: A Multi-agent Scheduling Dataset for Enhanced Cooperation in\n  Autonomous Driving","summary":"  Achieving high levels of safety and reliability in autonomous driving remains\na critical challenge, especially due to occlusion and limited perception ranges\nin standalone systems. Cooperative perception among vehicles offers a promising\nsolution, but existing research is hindered by datasets with a limited number\nof agents. Scaling up the number of cooperating agents is non-trivial and\nintroduces significant computational and technical hurdles that have not been\naddressed in previous works. To bridge this gap, we present Wireless enHanced\nAutonomous vehicles with Large number of Engaged agentS (WHALES), a dataset\ngenerated using CARLA simulator that features an unprecedented average of 8.4\nagents per driving sequence. In addition to providing the largest number of\nagents and viewpoints among autonomous driving datasets, WHALES records agent\nbehaviors, enabling cooperation across multiple tasks. This expansion allows\nfor new supporting tasks in cooperative perception. As a demonstration, we\nconduct experiments on agent scheduling task, where the ego agent selects one\nof multiple candidate agents to cooperate with, optimizing perception gains in\nautonomous driving. The WHALES dataset and codebase can be found at\nhttps://github.com/chensiweiTHU/WHALES.\n","authors":["Siwei Chen"," Yinsong"," Wang","Ziyi Song","Sheng Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.13340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07747v3","updated":"2024-11-20T13:56:33Z","published":"2024-11-12T12:18:18Z","title":"Constraint Learning for Parametric Point Cloud","summary":"  Parametric point clouds are sampled from CAD shapes, and have become\nincreasingly prevalent in industrial manufacturing. However, most existing\npoint cloud learning methods focus on the geometric features, such as\ndeveloping efficient convolution operations, overlooking the important\nattribute of constraints inherent in CAD shapes, which limits these methods'\nability to comprehend CAD shapes fully. To address this issue, we analyzed the\neffect of constraints, and proposed its deep learning-friendly representation,\nafter that, the Constraint Feature Learning Network (CstNet) was developed to\nextract and leverage constraints. Our CstNet includes two stages. Stage 1\nextracts constraints from B-Rep data or point cloud. Stage 2 leverages\ncoordinates and constraints to enhance the comprehension of CAD shapes.\nAdditionally, we built up the Parametric 20,000 Multi-modal Dataset for the\nscarcity of labeled B-Rep datasets. Experiments demonstrate that our CstNet\nachieved state-of-the-art performance on both public and proposed CAD shape\ndatasets. To the best of our knowledge, CstNet is the first constraint-based\nlearning method tailored for CAD shape analysis.\n","authors":["Xi Cheng","Ruiqi Lei","Di Huang","Zhichao Liao","Fengyuan Piao","Yan Chen","Pingfa Feng","Long Zeng"],"pdf_url":"https://arxiv.org/pdf/2411.07747v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13317v1","updated":"2024-11-20T13:34:22Z","published":"2024-11-20T13:34:22Z","title":"Teaching VLMs to Localize Specific Objects from In-context Examples","summary":"  Vision-Language Models (VLMs) have shown remarkable capabilities across\ndiverse visual tasks, including image recognition, video understanding, and\nVisual Question Answering (VQA) when explicitly trained for these tasks.\nDespite these advances, we find that current VLMs lack a fundamental cognitive\nability: learning to localize objects in a scene by taking into account the\ncontext. In this work, we focus on the task of few-shot personalized\nlocalization, where a model is given a small set of annotated images\n(in-context examples) -- each with a category label and bounding box -- and is\ntasked with localizing the same object type in a query image. To provoke\npersonalized localization abilities in models, we present a data-centric\nsolution that fine-tunes them using carefully curated data from video object\ntracking datasets. By leveraging sequences of frames tracking the same object\nacross multiple shots, we simulate instruction-tuning dialogues that promote\ncontext awareness. To reinforce this, we introduce a novel regularization\ntechnique that replaces object labels with pseudo-names, ensuring the model\nrelies on visual context rather than prior knowledge. Our method significantly\nenhances few-shot localization performance without sacrificing generalization,\nas demonstrated on several benchmarks tailored to personalized localization.\nThis work is the first to explore and benchmark personalized few-shot\nlocalization for VLMs, laying a foundation for future research in\ncontext-driven vision-language applications. The code for our project is\navailable at https://github.com/SivanDoveh/IPLoc\n","authors":["Sivan Doveh","Nimrod Shabtay","Wei Lin","Eli Schwartz","Hilde Kuehne","Raja Giryes","Rogerio Feris","Leonid Karlinsky","James Glass","Assaf Arbelle","Shimon Ullman","M. Jehanzeb Mirza"],"pdf_url":"https://arxiv.org/pdf/2411.13317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13311v1","updated":"2024-11-20T13:26:13Z","published":"2024-11-20T13:26:13Z","title":"A Resource Efficient Fusion Network for Object Detection in Bird's-Eye\n  View using Camera and Raw Radar Data","summary":"  Cameras can be used to perceive the environment around the vehicle, while\naffordable radar sensors are popular in autonomous driving systems as they can\nwithstand adverse weather conditions unlike cameras. However, radar point\nclouds are sparser with low azimuth and elevation resolution that lack semantic\nand structural information of the scenes, resulting in generally lower radar\ndetection performance. In this work, we directly use the raw range-Doppler (RD)\nspectrum of radar data, thus avoiding radar signal processing. We independently\nprocess camera images within the proposed comprehensive image processing\npipeline. Specifically, first, we transform the camera images to Bird's-Eye\nView (BEV) Polar domain and extract the corresponding features with our camera\nencoder-decoder architecture. The resultant feature maps are fused with\nRange-Azimuth (RA) features, recovered from the RD spectrum input from the\nradar decoder to perform object detection. We evaluate our fusion strategy with\nother existing methods not only in terms of accuracy but also on computational\ncomplexity metrics on RADIal dataset.\n","authors":["Kavin Chandrasekaran","Sorin Grigorescu","Gijs Dubbelman","Pavol Jancura"],"pdf_url":"https://arxiv.org/pdf/2411.13311v1.pdf","comment":"IEEE Intelligent Transportation Systems Conference (ITSC) 2024"},{"id":"http://arxiv.org/abs/2410.07117v2","updated":"2024-11-20T13:17:08Z","published":"2024-09-20T08:42:30Z","title":"Classification of Buried Objects from Ground Penetrating Radar Images by\n  using Second Order Deep Learning Models","summary":"  In this paper, a new classification model based on covariance matrices is\nbuilt in order to classify buried objects. The inputs of the proposed models\nare the hyperbola thumbnails obtained with a classical Ground Penetrating Radar\n(GPR) system. These thumbnails are then inputs to the first layers of a\nclassical CNN, which then produces a covariance matrix using the outputs of the\nconvolutional filters. Next, the covariance matrix is given to a network\ncomposed of specific layers to classify Symmetric Positive Definite (SPD)\nmatrices. We show in a large database that our approach outperform shallow\nnetworks designed for GPR data and conventional CNNs typically used in computer\nvision applications, particularly when the number of training data decreases\nand in the presence of mislabeled data. We also illustrate the interest of our\nmodels when training data and test sets are obtained from different weather\nmodes or considerations.\n","authors":["Douba Jafuno","Ammar Mian","Guillaume Ginolhac","Nickolas Stelzenmuller"],"pdf_url":"https://arxiv.org/pdf/2410.07117v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13302v1","updated":"2024-11-20T13:15:04Z","published":"2024-11-20T13:15:04Z","title":"Can Reasons Help Improve Pedestrian Intent Estimation? A Cross-Modal\n  Approach","summary":"  With the increased importance of autonomous navigation systems has come an\nincreasing need to protect the safety of Vulnerable Road Users (VRUs) such as\npedestrians. Predicting pedestrian intent is one such challenging task, where\nprior work predicts the binary cross/no-cross intention with a fusion of visual\nand motion features. However, there has been no effort so far to hedge such\npredictions with human-understandable reasons. We address this issue by\nintroducing a novel problem setting of exploring the intuitive reasoning behind\na pedestrian's intent. In particular, we show that predicting the 'WHY' can be\nvery useful in understanding the 'WHAT'. To this end, we propose a novel,\nreason-enriched PIE++ dataset consisting of multi-label textual\nexplanations/reasons for pedestrian intent. We also introduce a novel\nmulti-task learning framework called MINDREAD, which leverages a cross-modal\nrepresentation learning framework for predicting pedestrian intent as well as\nthe reason behind the intent. Our comprehensive experiments show significant\nimprovement of 5.6% and 7% in accuracy and F1-score for the task of intent\nprediction on the PIE++ dataset using MINDREAD. We also achieved a 4.4%\nimprovement in accuracy on a commonly used JAAD dataset. Extensive evaluation\nusing quantitative/qualitative metrics and user studies shows the effectiveness\nof our approach.\n","authors":["Vaishnavi Khindkar","Vineeth Balasubramanian","Chetan Arora","Anbumani Subramanian","C. V. Jawahar"],"pdf_url":"https://arxiv.org/pdf/2411.13302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10377v2","updated":"2024-11-20T13:05:40Z","published":"2024-11-15T17:32:01Z","title":"Generation of synthetic gait data: application to multiple sclerosis\n  patients' gait patterns","summary":"  Multiple sclerosis (MS) is the leading cause of severe non-traumatic\ndisability in young adults and its incidence is increasing worldwide. The\nvariability of gait impairment in MS necessitates the development of a\nnon-invasive, sensitive, and cost-effective tool for quantitative gait\nevaluation. The eGait movement sensor, designed to characterize human gait\nthrough unit quaternion time series (QTS) representing hip rotations, is a\npromising approach. However, the small sample sizes typical of clinical studies\npose challenges for the stability of gait data analysis tools. To address these\nchallenges, this article presents two key scientific contributions. First, a\ncomprehensive framework is proposed for transforming QTS data into a form that\npreserves the essential geometric properties of gait while enabling the use of\nany tabular synthetic data generation method. Second, a synthetic data\ngeneration method is introduced, based on nearest neighbors weighting, which\nproduces high-fidelity synthetic QTS data suitable for small datasets and\nprivate data environments. The effectiveness of the proposed method, is\ndemonstrated through its application to MS gait data, showing very good\nfidelity and respect of the initial geometry of the data. Thanks to this work,\nwe are able to produce synthetic data sets and work on the stability of\nclustering methods.\n","authors":["Klervi Le Gall","Lise Bellanger","David Laplaud","Aymeric Stamm"],"pdf_url":"https://arxiv.org/pdf/2411.10377v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13291v1","updated":"2024-11-20T13:01:16Z","published":"2024-11-20T13:01:16Z","title":"DATAP-SfM: Dynamic-Aware Tracking Any Point for Robust Structure from\n  Motion in the Wild","summary":"  This paper proposes a concise, elegant, and robust pipeline to estimate\nsmooth camera trajectories and obtain dense point clouds for casual videos in\nthe wild. Traditional frameworks, such as\nParticleSfM~\\cite{zhao2022particlesfm}, address this problem by sequentially\ncomputing the optical flow between adjacent frames to obtain point\ntrajectories. They then remove dynamic trajectories through motion segmentation\nand perform global bundle adjustment. However, the process of estimating\noptical flow between two adjacent frames and chaining the matches can introduce\ncumulative errors. Additionally, motion segmentation combined with single-view\ndepth estimation often faces challenges related to scale ambiguity. To tackle\nthese challenges, we propose a dynamic-aware tracking any point (DATAP) method\nthat leverages consistent video depth and point tracking. Specifically, our\nDATAP addresses these issues by estimating dense point tracking across the\nvideo sequence and predicting the visibility and dynamics of each point. By\nincorporating the consistent video depth prior, the performance of motion\nsegmentation is enhanced. With the integration of DATAP, it becomes possible to\nestimate and optimize all camera poses simultaneously by performing global\nbundle adjustments for point tracking classified as static and visible, rather\nthan relying on incremental camera registration. Extensive experiments on\ndynamic sequences, e.g., Sintel and TUM RGBD dynamic sequences, and on the wild\nvideo, e.g., DAVIS, demonstrate that the proposed method achieves\nstate-of-the-art performance in terms of camera pose estimation even in complex\ndynamic challenge scenes.\n","authors":["Weicai Ye","Xinyu Chen","Ruohao Zhan","Di Huang","Xiaoshui Huang","Haoyi Zhu","Hujun Bao","Wanli Ouyang","Tong He","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.13291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12635v2","updated":"2024-11-20T12:54:52Z","published":"2024-11-19T16:49:24Z","title":"M3D: Dual-Stream Selective State Spaces and Depth-Driven Framework for\n  High-Fidelity Single-View 3D Reconstruction","summary":"  The precise reconstruction of 3D objects from a single RGB image in complex\nscenes presents a critical challenge in virtual reality, autonomous driving,\nand robotics. Existing neural implicit 3D representation methods face\nsignificant difficulties in balancing the extraction of global and local\nfeatures, particularly in diverse and complex environments, leading to\ninsufficient reconstruction precision and quality. We propose M3D, a novel\nsingle-view 3D reconstruction framework, to tackle these challenges. This\nframework adopts a dual-stream feature extraction strategy based on Selective\nState Spaces to effectively balance the extraction of global and local\nfeatures, thereby improving scene comprehension and representation precision.\nAdditionally, a parallel branch extracts depth information, effectively\nintegrating visual and geometric features to enhance reconstruction quality and\npreserve intricate details. Experimental results indicate that the fusion of\nmulti-scale features with depth information via the dual-branch feature\nextraction significantly boosts geometric consistency and fidelity, achieving\nstate-of-the-art reconstruction performance.\n","authors":["Luoxi Zhang","Pragyan Shrestha","Yu Zhou","Chun Xie","Itaru Kitahara"],"pdf_url":"https://arxiv.org/pdf/2411.12635v2.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2411.13287v1","updated":"2024-11-20T12:54:47Z","published":"2024-11-20T12:54:47Z","title":"Unbiased Scene Graph Generation by Type-Aware Message Passing on\n  Heterogeneous and Dual Graphs","summary":"  Although great progress has been made in the research of unbiased scene graph\ngeneration, issues still hinder improving the predictive performance of both\nhead and tail classes. An unbiased scene graph generation (TA-HDG) is proposed\nto address these issues. For modeling interactive and non-interactive\nrelations, the Interactive Graph Construction is proposed to model the\ndependence of relations on objects by combining heterogeneous and dual graph,\nwhen modeling relations between multiple objects. It also implements a\nsubject-object pair selection strategy to reduce meaningless edges. Moreover,\nthe Type-Aware Message Passing enhances the understanding of complex\ninteractions by capturing intra- and inter-type context in the Intra-Type and\nInter-Type stages. The Intra-Type stage captures the semantic context of\ninter-relaitons and inter-objects. On this basis, the Inter-Type stage captures\nthe context between objects and relations for interactive and non-interactive\nrelations, respectively. Experiments on two datasets show that TA-HDG achieves\nimprovements in the metrics of R@K and mR@K, which proves that TA-HDG can\naccurately predict the tail class while maintaining the competitive performance\nof the head class.\n","authors":["Guanglu Sun","Jin Qiu","Lili Liang"],"pdf_url":"https://arxiv.org/pdf/2411.13287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19972v2","updated":"2024-11-20T12:54:39Z","published":"2024-09-30T05:53:31Z","title":"DAOcc: 3D Object Detection Assisted Multi-Sensor Fusion for 3D Occupancy\n  Prediction","summary":"  Multi-sensor fusion significantly enhances the accuracy and robustness of 3D\nsemantic occupancy prediction, which is crucial for autonomous driving and\nrobotics. However, most existing approaches depend on large image resolutions\nand complex networks to achieve top performance, hindering their application in\npractical scenarios. Additionally, most multi-sensor fusion approaches focus on\nimproving fusion features while overlooking the exploration of supervision\nstrategies for these features. To this end, we propose DAOcc, a novel\nmulti-modal occupancy prediction framework that leverages 3D object detection\nsupervision to assist in achieving superior performance, while using a\ndeployment-friendly image feature extraction network and practical input image\nresolution. Furthermore, we introduce a BEV View Range Extension strategy to\nmitigate the adverse effects of reduced image resolution. Experimental results\nshow that DAOcc achieves new state-of-the-art performance on the Occ3D-nuScenes\nand SurroundOcc benchmarks, and surpasses other methods by a significant margin\nwhile using only ResNet50 and 256*704 input image resolution. Code will be made\navailable at https://github.com/AlphaPlusTT/DAOcc.\n","authors":["Zhen Yang","Yanpeng Dong","Heng Wang","Lichao Ma","Zijian Cui","Qi Liu","Haoran Pei"],"pdf_url":"https://arxiv.org/pdf/2409.19972v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13284v1","updated":"2024-11-20T12:52:36Z","published":"2024-11-20T12:52:36Z","title":"DATTA: Domain-Adversarial Test-Time Adaptation for Cross-Domain\n  WiFi-Based Human Activity Recognition","summary":"  Cross-domain generalization is an open problem in WiFi-based sensing due to\nvariations in environments, devices, and subjects, causing domain shifts in\nchannel state information. To address this, we propose Domain-Adversarial\nTest-Time Adaptation (DATTA), a novel framework combining domain-adversarial\ntraining (DAT), test-time adaptation (TTA), and weight resetting to facilitate\nadaptation to unseen target domains and to prevent catastrophic forgetting.\nDATTA is integrated into a lightweight, flexible architecture optimized for\nspeed. We conduct a comprehensive evaluation of DATTA, including an ablation\nstudy on all key components using publicly available data, and verify its\nsuitability for real-time applications such as human activity recognition. When\ncombining a SotA video-based variant of TTA with WiFi-based DAT and comparing\nit to DATTA, our method achieves an 8.1% higher F1-Score. The PyTorch\nimplementation of DATTA is publicly available at:\nhttps://github.com/StrohmayerJ/DATTA.\n","authors":["Julian Strohmayer","Rafael Sterzinger","Matthias Wödlinger","Martin Kampel"],"pdf_url":"https://arxiv.org/pdf/2411.13284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09860v2","updated":"2024-11-20T12:51:25Z","published":"2024-08-19T10:08:25Z","title":"3D-Aware Instance Segmentation and Tracking in Egocentric Videos","summary":"  Egocentric videos present unique challenges for 3D scene understanding due to\nrapid camera motion, frequent object occlusions, and limited object visibility.\nThis paper introduces a novel approach to instance segmentation and tracking in\nfirst-person video that leverages 3D awareness to overcome these obstacles. Our\nmethod integrates scene geometry, 3D object centroid tracking, and instance\nsegmentation to create a robust framework for analyzing dynamic egocentric\nscenes. By incorporating spatial and temporal cues, we achieve superior\nperformance compared to state-of-the-art 2D approaches. Extensive evaluations\non the challenging EPIC Fields dataset demonstrate significant improvements\nacross a range of tracking and segmentation consistency metrics. Specifically,\nour method outperforms the next best performing approach by $7$ points in\nAssociation Accuracy (AssA) and $4.5$ points in IDF1 score, while reducing the\nnumber of ID switches by $73\\%$ to $80\\%$ across various object categories.\nLeveraging our tracked instance segmentations, we showcase downstream\napplications in 3D object reconstruction and amodal video object segmentation\nin these egocentric settings.\n","authors":["Yash Bhalgat","Vadim Tschernezki","Iro Laina","João F. Henriques","Andrea Vedaldi","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2408.09860v2.pdf","comment":"Camera-ready for ACCV 2024. More experiments added"},{"id":"http://arxiv.org/abs/2411.13281v1","updated":"2024-11-20T12:48:34Z","published":"2024-11-20T12:48:34Z","title":"VideoAutoArena: An Automated Arena for Evaluating Large Multimodal\n  Models in Video Analysis through User Simulation","summary":"  Large multimodal models (LMMs) with advanced video analysis capabilities have\nrecently garnered significant attention. However, most evaluations rely on\ntraditional methods like multiple-choice questions in benchmarks such as\nVideoMME and LongVideoBench, which are prone to lack the depth needed to\ncapture the complex demands of real-world users. To address this limitation-and\ndue to the prohibitive cost and slow pace of human annotation for video\ntasks-we introduce VideoAutoArena, an arena-style benchmark inspired by LMSYS\nChatbot Arena's framework, designed to automatically assess LMMs' video\nanalysis abilities. VideoAutoArena utilizes user simulation to generate\nopen-ended, adaptive questions that rigorously assess model performance in\nvideo understanding. The benchmark features an automated, scalable evaluation\nframework, incorporating a modified ELO Rating System for fair and continuous\ncomparisons across multiple LMMs. To validate our automated judging system, we\nconstruct a 'gold standard' using a carefully curated subset of human\nannotations, demonstrating that our arena strongly aligns with human judgment\nwhile maintaining scalability. Additionally, we introduce a fault-driven\nevolution strategy, progressively increasing question complexity to push models\ntoward handling more challenging video analysis scenarios. Experimental results\ndemonstrate that VideoAutoArena effectively differentiates among\nstate-of-the-art LMMs, providing insights into model strengths and areas for\nimprovement. To further streamline our evaluation, we introduce VideoAutoBench\nas an auxiliary benchmark, where human annotators label winners in a subset of\nVideoAutoArena battles. We use GPT-4o as a judge to compare responses against\nthese human-validated answers. Together, VideoAutoArena and VideoAutoBench\noffer a cost-effective, and scalable framework for evaluating LMMs in\nuser-centric video analysis.\n","authors":["Ziyang Luo","Haoning Wu","Dongxu Li","Jing Ma","Mohan Kankanhalli","Junnan Li"],"pdf_url":"https://arxiv.org/pdf/2411.13281v1.pdf","comment":"Project Page: https://videoautoarena.github.io/"},{"id":"http://arxiv.org/abs/2411.13276v1","updated":"2024-11-20T12:43:40Z","published":"2024-11-20T12:43:40Z","title":"Analysis and Synthesis Denoisers for Forward-Backward Plug-and-Play\n  Algorithms","summary":"  In this work we study the behavior of the forward-backward (FB) algorithm\nwhen the proximity operator is replaced by a sub-iterative procedure to\napproximate a Gaussian denoiser, in a Plug-and-Play (PnP) fashion. In\nparticular, we consider both analysis and synthesis Gaussian denoisers within a\ndictionary framework, obtained by unrolling dual-FB iterations or FB\niterations, respectively. We analyze the associated minimization problems as\nwell as the asymptotic behavior of the resulting FB-PnP iterations. In\nparticular, we show that the synthesis Gaussian denoising problem can be viewed\nas a proximity operator. For each case, analysis and synthesis, we show that\nthe FB-PnP algorithms solve the same problem whether we use only one or an\ninfinite number of sub-iteration to solve the denoising problem at each\niteration. To this aim, we show that each \"one sub-iteration\" strategy within\nthe FB-PnP can be interpreted as a primal-dual algorithm when a warm-restart\nstrategy is used. We further present similar results when using a Moreau-Yosida\nsmoothing of the global problem, for an arbitrary number of sub-iterations.\nFinally, we provide numerical simulations to illustrate our theoretical\nresults. In particular we first consider a toy compressive sensing example, as\nwell as an image restoration problem in a deep dictionary framework.\n","authors":["Matthieu Kowalski","Benoît Malézieux","Thomas Moreau","Audrey Repetti"],"pdf_url":"https://arxiv.org/pdf/2411.13276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03029v3","updated":"2024-11-20T12:32:13Z","published":"2023-12-05T11:01:44Z","title":"HHAvatar: Gaussian Head Avatar with Dynamic Hairs","summary":"  Creating high-fidelity 3D head avatars has always been a research hotspot,\nbut it remains a great challenge under lightweight sparse view setups. In this\npaper, we propose HHAvatar represented by controllable 3D Gaussians for\nhigh-fidelity head avatar with dynamic hair modeling. We first use 3D Gaussians\nto represent the appearance of the head, and then jointly optimize neutral 3D\nGaussians and a fully learned MLP-based deformation field to capture complex\nexpressions. The two parts benefit each other, thereby our method can model\nfine-grained dynamic details while ensuring expression accuracy. Furthermore,\nwe devise a well-designed geometry-guided initialization strategy based on\nimplicit SDF and Deep Marching Tetrahedra for the stability and convergence of\nthe training procedure. To address the problem of dynamic hair modeling, we\nintroduce a hybrid head model into our avatar representation based Gaussian\nHead Avatar and a training method that considers timing information and an\nocclusion perception module to model the non-rigid motion of hair. Experiments\nshow that our approach outperforms other state-of-the-art sparse-view methods,\nachieving ultra high-fidelity rendering quality at 2K resolution even under\nexaggerated expressions and driving hairs reasonably with the motion of the\nhead\n","authors":["Zhanfeng Liao","Yuelang Xu","Zhe Li","Qijing Li","Boyao Zhou","Ruifeng Bai","Di Xu","Hongwen Zhang","Yebin Liu"],"pdf_url":"https://arxiv.org/pdf/2312.03029v3.pdf","comment":"Project Page: https://liaozhanfeng.github.io/HHAvatar"},{"id":"http://arxiv.org/abs/2403.15182v3","updated":"2024-11-20T12:22:53Z","published":"2024-03-22T13:11:26Z","title":"PDE-CNNs: Axiomatic Derivations and Applications","summary":"  PDE-based Group Convolutional Neural Networks (PDE-G-CNNs) use solvers of\nevolution PDEs as substitutes for the conventional components in G-CNNs.\nPDE-G-CNNs can offer several benefits simultaneously: fewer parameters,\ninherent equivariance, better accuracy, and data efficiency.\n  In this article we focus on Euclidean equivariant PDE-G-CNNs where the\nfeature maps are two-dimensional throughout. We call this variant of the\nframework a PDE-CNN.\n  From a machine learning perspective, we list several practically desirable\naxioms and derive from these which PDEs should be used in a PDE-CNN, this being\nour main contribution. Our approach to geometric learning via PDEs is inspired\nby the axioms of scale-space theory, which we generalize by introducing\nsemifield-valued signals.\n  Our theory reveals new PDEs that can be used in PDE-CNNs and we\nexperimentally examine what impact these have on the accuracy of PDE-CNNs. We\nalso confirm for small networks that PDE-CNNs offer fewer parameters, increased\naccuracy, and better data efficiency when compared to CNNs.\n","authors":["Gijs Bellaard","Sei Sakata","Bart M. N. Smets","Remco Duits"],"pdf_url":"https://arxiv.org/pdf/2403.15182v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13260v1","updated":"2024-11-20T12:21:30Z","published":"2024-11-20T12:21:30Z","title":"Paying more attention to local contrast: improving infrared small target\n  detection performance via prior knowledge","summary":"  The data-driven method for infrared small target detection (IRSTD) has\nachieved promising results. However, due to the small scale of infrared small\ntarget datasets and the limited number of pixels occupied by the targets\nthemselves, it is a challenging task for deep learning methods to directly\nlearn from these samples. Utilizing human expert knowledge to assist deep\nlearning methods in better learning is worthy of exploration. To effectively\nguide the model to focus on targets' spatial features, this paper proposes the\nLocal Contrast Attention Enhanced infrared small target detection Network\n(LCAE-Net), combining prior knowledge with data-driven deep learning methods.\nLCAE-Net is a U-shaped neural network model which consists of two developed\nmodules: a Local Contrast Enhancement (LCE) module and a Channel Attention\nEnhancement (CAE) module. The LCE module takes advantages of prior knowledge,\nleveraging handcrafted convolution operator to acquire Local Contrast Attention\n(LCA), which could realize background suppression while enhance the potential\ntarget region, thus guiding the neural network to pay more attention to\npotential infrared small targets' location information. To effectively utilize\nthe response information throughout downsampling progresses, the CAE module is\nproposed to achieve the information fusion among feature maps' different\nchannels. Experimental results indicate that our LCAE-Net outperforms existing\nstate-of-the-art methods on the three public datasets NUDT-SIRST, NUAA-SIRST,\nand IRSTD-1K, and its detection speed could reach up to 70 fps. Meanwhile, our\nmodel has a parameter count and Floating-Point Operations (FLOPs) of 1.945M and\n4.862G respectively, which is suitable for deployment on edge devices.\n","authors":["Peichao Wang","Jiabao Wang","Yao Chen","Rui Zhang","Yang Li","Zhuang Miao"],"pdf_url":"https://arxiv.org/pdf/2411.13260v1.pdf","comment":"16 pages, 8 figures"},{"id":"http://arxiv.org/abs/2411.12309v2","updated":"2024-11-20T12:18:36Z","published":"2024-11-19T07:51:44Z","title":"DGTR: Distributed Gaussian Turbo-Reconstruction for Sparse-View Vast\n  Scenes","summary":"  Novel-view synthesis (NVS) approaches play a critical role in vast scene\nreconstruction. However, these methods rely heavily on dense image inputs and\nprolonged training times, making them unsuitable where computational resources\nare limited. Additionally, few-shot methods often struggle with poor\nreconstruction quality in vast environments. This paper presents DGTR, a novel\ndistributed framework for efficient Gaussian reconstruction for sparse-view\nvast scenes. Our approach divides the scene into regions, processed\nindependently by drones with sparse image inputs. Using a feed-forward Gaussian\nmodel, we predict high-quality Gaussian primitives, followed by a global\nalignment algorithm to ensure geometric consistency. Synthetic views and depth\npriors are incorporated to further enhance training, while a distillation-based\nmodel aggregation mechanism enables efficient reconstruction. Our method\nachieves high-quality large-scale scene reconstruction and novel-view synthesis\nin significantly reduced training times, outperforming existing approaches in\nboth speed and scalability. We demonstrate the effectiveness of our framework\non vast aerial scenes, achieving high-quality results within minutes. Code will\nreleased on our [https://3d-aigc.github.io/DGTR].\n","authors":["Hao Li","Yuanyuan Gao","Haosong Peng","Chenming Wu","Weicai Ye","Yufeng Zhan","Chen Zhao","Dingwen Zhang","Jingdong Wang","Junwei Han"],"pdf_url":"https://arxiv.org/pdf/2411.12309v2.pdf","comment":"Code will released on our [https://3d-aigc.github.io/DGTR]"},{"id":"http://arxiv.org/abs/2410.08202v2","updated":"2024-11-20T12:15:08Z","published":"2024-10-10T17:59:22Z","title":"Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large\n  Language Models with Endogenous Visual Pre-training","summary":"  In this paper, we focus on monolithic Multimodal Large Language Models\n(MLLMs) that integrate visual encoding and language decoding into a single LLM.\nIn particular, we identify that existing pre-training strategies for monolithic\nMLLMs often suffer from unstable optimization or catastrophic forgetting. To\naddress this issue, our core idea is to embed a new visual parameter space into\na pre-trained LLM, thereby stably learning visual knowledge from noisy data\nwhile freezing the LLM. Based on this principle, we present Mono-InternVL, a\nnovel monolithic MLLM that seamlessly integrates a set of visual experts via a\nmultimodal mixture-of-experts structure. Moreover, we propose an innovative\npre-training strategy to maximize the visual capability of Mono-InternVL,\nnamely Endogenous Visual Pre-training (EViP). In particular, EViP is designed\nas a progressive learning process for visual experts, which aims to fully\nexploit the visual knowledge from noisy data to high-quality data. To validate\nour approach, we conduct extensive experiments on 16 benchmarks. Experimental\nresults confirm the superior performance of Mono-InternVL than existing\nmonolithic MLLMs on 13 of 16 multimodal benchmarks, e.g., +80 points over Emu3\non OCRBench. Compared to the modular baseline, i.e., InternVL-1.5,\nMono-InternVL still retains comparable multimodal performance while reducing up\nto 67% first token latency. Code and model are released at\nhttps://huggingface.co/OpenGVLab/Mono-InternVL-2B.\n","authors":["Gen Luo","Xue Yang","Wenhan Dou","Zhaokai Wang","Jiawen Liu","Jifeng Dai","Yu Qiao","Xizhou Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.08202v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13251v1","updated":"2024-11-20T12:09:43Z","published":"2024-11-20T12:09:43Z","title":"BelHouse3D: A Benchmark Dataset for Assessing Occlusion Robustness in 3D\n  Point Cloud Semantic Segmentation","summary":"  Large-scale 2D datasets have been instrumental in advancing machine learning;\nhowever, progress in 3D vision tasks has been relatively slow. This disparity\nis largely due to the limited availability of 3D benchmarking datasets. In\nparticular, creating real-world point cloud datasets for indoor scene semantic\nsegmentation presents considerable challenges, including data collection within\nconfined spaces and the costly, often inaccurate process of per-point labeling\nto generate ground truths. While synthetic datasets address some of these\nchallenges, they often fail to replicate real-world conditions, particularly\nthe occlusions that occur in point clouds collected from real environments.\nExisting 3D benchmarking datasets typically evaluate deep learning models under\nthe assumption that training and test data are independently and identically\ndistributed (IID), which affects the models' usability for real-world point\ncloud segmentation. To address these challenges, we introduce the BelHouse3D\ndataset, a new synthetic point cloud dataset designed for 3D indoor scene\nsemantic segmentation. This dataset is constructed using real-world references\nfrom 32 houses in Belgium, ensuring that the synthetic data closely aligns with\nreal-world conditions. Additionally, we include a test set with data occlusion\nto simulate out-of-distribution (OOD) scenarios, reflecting the occlusions\ncommonly encountered in real-world point clouds. We evaluate popular\npoint-based semantic segmentation methods using our OOD setting and present a\nbenchmark. We believe that BelHouse3D and its OOD setting will advance research\nin 3D point cloud semantic segmentation for indoor scenes, providing valuable\ninsights for the development of more generalizable models.\n","authors":["Umamaheswaran Raman Kumar","Abdur Razzaq Fayjie","Jurgen Hannaert","Patrick Vandewalle"],"pdf_url":"https://arxiv.org/pdf/2411.13251v1.pdf","comment":"20 pages, 6 figures, 3 tables, accepted at ECCV 2024 Workshops"},{"id":"http://arxiv.org/abs/2411.13243v1","updated":"2024-11-20T12:02:12Z","published":"2024-11-20T12:02:12Z","title":"XMask3D: Cross-modal Mask Reasoning for Open Vocabulary 3D Semantic\n  Segmentation","summary":"  Existing methodologies in open vocabulary 3D semantic segmentation primarily\nconcentrate on establishing a unified feature space encompassing 3D, 2D, and\ntextual modalities. Nevertheless, traditional techniques such as global feature\nalignment or vision-language model distillation tend to impose only approximate\ncorrespondence, struggling notably with delineating fine-grained segmentation\nboundaries. To address this gap, we propose a more meticulous mask-level\nalignment between 3D features and the 2D-text embedding space through a\ncross-modal mask reasoning framework, XMask3D. In our approach, we developed a\nmask generator based on the denoising UNet from a pre-trained diffusion model,\nleveraging its capability for precise textual control over dense pixel\nrepresentations and enhancing the open-world adaptability of the generated\nmasks. We further integrate 3D global features as implicit conditions into the\npre-trained 2D denoising UNet, enabling the generation of segmentation masks\nwith additional 3D geometry awareness. Subsequently, the generated 2D masks are\nemployed to align mask-level 3D representations with the vision-language\nfeature space, thereby augmenting the open vocabulary capability of 3D geometry\nembeddings. Finally, we fuse complementary 2D and 3D mask features, resulting\nin competitive performance across multiple benchmarks for 3D open vocabulary\nsemantic segmentation. Code is available at\nhttps://github.com/wangzy22/XMask3D.\n","authors":["Ziyi Wang","Yanbo Wang","Xumin Yu","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2411.13243v1.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2407.02182v3","updated":"2024-11-20T11:52:29Z","published":"2024-07-02T11:41:12Z","title":"Occlusion-Aware Seamless Segmentation","summary":"  Panoramic images can broaden the Field of View (FoV), occlusion-aware\nprediction can deepen the understanding of the scene, and domain adaptation can\ntransfer across viewing domains. In this work, we introduce a novel task,\nOcclusion-Aware Seamless Segmentation (OASS), which simultaneously tackles all\nthese three challenges. For benchmarking OASS, we establish a new\nhuman-annotated dataset for Blending Panoramic Amodal Seamless Segmentation,\ni.e., BlendPASS. Besides, we propose the first solution UnmaskFormer, aiming at\nunmasking the narrow FoV, occlusions, and domain gaps all at once.\nSpecifically, UnmaskFormer includes the crucial designs of Unmasking Attention\n(UA) and Amodal-oriented Mix (AoMix). Our method achieves state-of-the-art\nperformance on the BlendPASS dataset, reaching a remarkable mAPQ of 26.58% and\nmIoU of 43.66%. On public panoramic semantic segmentation datasets, i.e.,\nSynPASS and DensePASS, our method outperforms previous methods and obtains\n45.34% and 48.08% in mIoU, respectively. The fresh BlendPASS dataset and our\nsource code are available at https://github.com/yihong-97/OASS.\n","authors":["Yihong Cao","Jiaming Zhang","Hao Shi","Kunyu Peng","Yuhongxuan Zhang","Hui Zhang","Rainer Stiefelhagen","Kailun Yang"],"pdf_url":"https://arxiv.org/pdf/2407.02182v3.pdf","comment":"Accepted to ECCV 2024. The fresh dataset and source code are\n  available at https://github.com/yihong-97/OASS"},{"id":"http://arxiv.org/abs/2308.04830v3","updated":"2024-11-20T11:23:20Z","published":"2023-08-09T09:38:14Z","title":"VAST: Vivify Your Talking Avatar via Zero-Shot Expressive Facial Style\n  Transfer","summary":"  Current talking face generation methods mainly focus on speech-lip\nsynchronization. However, insufficient investigation on the facial talking\nstyle leads to a lifeless and monotonous avatar. Most previous works fail to\nimitate expressive styles from arbitrary video prompts and ensure the\nauthenticity of the generated video. This paper proposes an unsupervised\nvariational style transfer model (VAST) to vivify the neutral photo-realistic\navatars. Our model consists of three key components: a style encoder that\nextracts facial style representations from the given video prompts; a hybrid\nfacial expression decoder to model accurate speech-related movements; a\nvariational style enhancer that enhances the style space to be highly\nexpressive and meaningful. With our essential designs on facial style learning,\nour model is able to flexibly capture the expressive facial style from\narbitrary video prompts and transfer it onto a personalized image renderer in a\nzero-shot manner. Experimental results demonstrate the proposed approach\ncontributes to a more vivid talking avatar with higher authenticity and richer\nexpressiveness.\n","authors":["Liyang Chen","Zhiyong Wu","Runnan Li","Weihong Bao","Jun Ling","Xu Tan","Sheng Zhao"],"pdf_url":"https://arxiv.org/pdf/2308.04830v3.pdf","comment":"Accepted by ICCV2023"},{"id":"http://arxiv.org/abs/2411.13211v1","updated":"2024-11-20T11:19:22Z","published":"2024-11-20T11:19:22Z","title":"ViSTa Dataset: Do vision-language models understand sequential tasks?","summary":"  Using vision-language models (VLMs) as reward models in reinforcement\nlearning holds promise for reducing costs and improving safety. So far, VLM\nreward models have only been used for goal-oriented tasks, where the agent must\nreach a particular final outcome. We explore VLMs' potential to supervise tasks\nthat cannot be scored by the final state alone. To this end, we introduce\nViSTa, a dataset for evaluating Vision-based understanding of Sequential Tasks.\nViSTa comprises over 4,000 videos with step-by-step descriptions in virtual\nhome, Minecraft, and real-world environments. Its novel hierarchical structure\n-- basic single-step tasks composed into more and more complex sequential tasks\n-- allows a fine-grained understanding of how well VLMs can judge tasks with\nvarying complexity. To illustrate this, we use ViSTa to evaluate\nstate-of-the-art VLMs, including CLIP, ViCLIP, and GPT-4o. We find that, while\nthey are all good at object recognition, they fail to understand sequential\ntasks, with only GPT-4o achieving non-trivial performance.\n","authors":["Evžen Wybitul","Evan Ryan Gunter","Mikhail Seleznyov"],"pdf_url":"https://arxiv.org/pdf/2411.13211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13205v1","updated":"2024-11-20T11:07:37Z","published":"2024-11-20T11:07:37Z","title":"An Integrated Approach to Robotic Object Grasping and Manipulation","summary":"  In response to the growing challenges of manual labor and efficiency in\nwarehouse operations, Amazon has embarked on a significant transformation by\nincorporating robotics to assist with various tasks. While a substantial number\nof robots have been successfully deployed for tasks such as item transportation\nwithin warehouses, the complex process of object picking from shelves remains a\nsignificant challenge. This project addresses the issue by developing an\ninnovative robotic system capable of autonomously fulfilling a simulated order\nby efficiently selecting specific items from shelves. A distinguishing feature\nof the proposed robotic system is its capacity to navigate the challenge of\nuncertain object positions within each bin of the shelf. The system is\nengineered to autonomously adapt its approach, employing strategies that enable\nit to efficiently locate and retrieve the desired items, even in the absence of\npre-established knowledge about their placements.\n","authors":["Owais Ahmed","M Huzaifa","M Areeb","Hamza Ali Khan"],"pdf_url":"https://arxiv.org/pdf/2411.13205v1.pdf","comment":"5 PAGES"},{"id":"http://arxiv.org/abs/2411.13198v1","updated":"2024-11-20T10:58:47Z","published":"2024-11-20T10:58:47Z","title":"Intensity-Spatial Dual Masked Autoencoder for Multi-Scale Feature\n  Learning in Chest CT Segmentation","summary":"  In the field of medical image segmentation, challenges such as indistinct\nlesion features, ambiguous boundaries,and multi-scale characteristics have long\nrevailed. This paper proposes an improved method named Intensity-Spatial Dual\nMasked AutoEncoder (ISD-MAE). Based on the tissue-contrast semi-masked\nautoencoder, a Masked AutoEncoder (MAE) branch is introduced to perform\nintensity masking and spatial masking operations on chest CT images for\nmulti-scale feature learning and segmentation tasks. The model utilizes a\ndual-branch structure and contrastive learning to enhance the ability to learn\ntissue features and boundary details. Experiments are conducted on multiple 2D\nand 3D datasets. The results show that ISD-MAE significantly outperforms other\nmethods in 2D pneumonia and mediastinal tumor segmentation tasks. For example,\nthe Dice score reaches 90.10% on the COVID19 LESION dataset, and the\nperformance is relatively stable. However, there is still room for improvement\non 3D datasets. In response to this, improvement directions are proposed,\nincluding optimizing the loss function, using enhanced 3D convolution blocks,\nand processing datasets from multiple perspectives.Our code is available\nat:https://github.com/prowontheus/ISD-MAE.\n","authors":["Yuexing Ding","Jun Wang","Hongbing Lyu"],"pdf_url":"https://arxiv.org/pdf/2411.13198v1.pdf","comment":"10 pages,6 figures,3 tables"},{"id":"http://arxiv.org/abs/2405.14475v3","updated":"2024-11-20T10:43:51Z","published":"2024-05-23T12:04:51Z","title":"MagicDrive3D: Controllable 3D Generation for Any-View Rendering in\n  Street Scenes","summary":"  While controllable generative models for images and videos have achieved\nremarkable success, high-quality models for 3D scenes, particularly in\nunbounded scenarios like autonomous driving, remain underdeveloped due to high\ndata acquisition costs. In this paper, we introduce MagicDrive3D, a novel\npipeline for controllable 3D street scene generation that supports\nmulti-condition control, including BEV maps, 3D objects, and text descriptions.\nUnlike previous methods that reconstruct before training the generative models,\nMagicDrive3D first trains a video generation model and then reconstructs from\nthe generated data. This innovative approach enables easily controllable\ngeneration and static scene acquisition, resulting in high-quality scene\nreconstruction. To address the minor errors in generated content, we propose\ndeformable Gaussian splatting with monocular depth initialization and\nappearance modeling to manage exposure discrepancies across viewpoints.\nValidated on the nuScenes dataset, MagicDrive3D generates diverse, high-quality\n3D driving scenes that support any-view rendering and enhance downstream tasks\nlike BEV segmentation. Our results demonstrate the framework's superior\nperformance, showcasing its potential for autonomous driving simulation and\nbeyond.\n","authors":["Ruiyuan Gao","Kai Chen","Zhihao Li","Lanqing Hong","Zhenguo Li","Qiang Xu"],"pdf_url":"https://arxiv.org/pdf/2405.14475v3.pdf","comment":"Project Page: https://flymin.github.io/magicdrive3d"},{"id":"http://arxiv.org/abs/2411.13186v1","updated":"2024-11-20T10:36:41Z","published":"2024-11-20T10:36:41Z","title":"VADet: Multi-frame LiDAR 3D Object Detection using Variable Aggregation","summary":"  Input aggregation is a simple technique used by state-of-the-art LiDAR 3D\nobject detectors to improve detection. However, increasing aggregation is known\nto have diminishing returns and even performance degradation, due to objects\nresponding differently to the number of aggregated frames. To address this\nlimitation, we propose an efficient adaptive method, which we call Variable\nAggregation Detection (VADet). Instead of aggregating the entire scene using a\nfixed number of frames, VADet performs aggregation per object, with the number\nof frames determined by an object's observed properties, such as speed and\npoint density. VADet thus reduces the inherent trade-offs of fixed aggregation\nand is not architecture specific. To demonstrate its benefits, we apply VADet\nto three popular single-stage detectors and achieve state-of-the-art\nperformance on the Waymo dataset.\n","authors":["Chengjie Huang","Vahdat Abdelzad","Sean Sedwards","Krzysztof Czarnecki"],"pdf_url":"https://arxiv.org/pdf/2411.13186v1.pdf","comment":"Accepted by WACV 2025"},{"id":"http://arxiv.org/abs/2409.07267v4","updated":"2024-11-20T10:34:21Z","published":"2024-09-11T13:43:01Z","title":"MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D\n  Features as Text Tokens for Autonomous Driving","summary":"  Vision-language models (VLMs) serve as general-purpose end-to-end models in\nautonomous driving, performing subtasks such as prediction, planning, and\nperception through question-and-answer interactions. However, most existing\nmethods rely on computationally expensive visual encoders and large language\nmodels (LLMs), making them difficult to deploy in real-world scenarios and\nreal-time applications. Meanwhile, most existing VLMs lack the ability to\nprocess multiple images, making it difficult to adapt to multi-camera\nperception in autonomous driving. To address these issues, we propose a novel\nframework called MiniDrive, which incorporates our proposed Feature Engineering\nMixture of Experts (FE-MoE) module and Dynamic Instruction Adapter\n(DI-Adapter). The FE-MoE effectively maps 2D features into visual token\nembeddings before being input into the language model. The DI-Adapter enables\nthe visual token embeddings to dynamically change with the instruction text\nembeddings, resolving the issue of static visual token embeddings for the same\nimage in previous approaches. Compared to previous works, MiniDrive achieves\nstate-of-the-art performance in terms of parameter size, floating point\noperations, and response efficiency, with the smallest version containing only\n83M parameters.\n","authors":["Enming Zhang","Xingyuan Dai","Yisheng Lv","Qinghai Miao"],"pdf_url":"https://arxiv.org/pdf/2409.07267v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13183v1","updated":"2024-11-20T10:30:33Z","published":"2024-11-20T10:30:33Z","title":"Click; Single Object Tracking; Video Object Segmentation; Real-time\n  Interaction","summary":"  Single object tracking(SOT) relies on precise object bounding box\ninitialization. In this paper, we reconsidered the deficiencies in the current\napproaches to initializing single object trackers and propose a new paradigm\nfor single object tracking algorithms, ClickTrack, a new paradigm using\nclicking interaction for real-time scenarios. Moreover, click as an input type\ninherently lack hierarchical information. To address ambiguity in certain\nspecial scenarios, we designed the Guided Click Refiner(GCR), which accepts\npoint and optional textual information as inputs, transforming the point into\nthe bounding box expected by the operator. The bounding box will be used as\ninput of single object trackers. Experiments on LaSOT and GOT-10k benchmarks\nshow that tracker combined with GCR achieves stable performance in real-time\ninteractive scenarios. Furthermore, we explored the integration of GCR into the\nSegment Anything model(SAM), significantly reducing ambiguity issues when SAM\nreceives point inputs.\n","authors":["Kuiran Wang","Xuehui Yu","Wenwen Yu","Guorong Li","Xiangyuan Lan","Qixiang Ye","Jianbin Jiao","Zhenjun Han"],"pdf_url":"https://arxiv.org/pdf/2411.13183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13181v1","updated":"2024-11-20T10:27:12Z","published":"2024-11-20T10:27:12Z","title":"Cross-Camera Distracted Driver Classification through Feature\n  Disentanglement and Contrastive Learning","summary":"  The classification of distracted drivers is pivotal for ensuring safe\ndriving. Previous studies demonstrated the effectiveness of neural networks in\nautomatically predicting driver distraction, fatigue, and potential hazards.\nHowever, recent research has uncovered a significant loss of accuracy in these\nmodels when applied to samples acquired under conditions that differ from the\ntraining data. In this paper, we introduce a robust model designed to withstand\nchanges in camera position within the vehicle. Our Driver Behavior Monitoring\nNetwork (DBMNet) relies on a lightweight backbone and integrates a\ndisentanglement module to discard camera view information from features,\ncoupled with contrastive learning to enhance the encoding of various driver\nactions. Experiments conducted on the daytime and nighttime subsets of the\n100-Driver dataset validate the effectiveness of our approach with an increment\non average of 9\\% in Top-1 accuracy in comparison with the state of the art. In\naddition, cross-dataset and cross-camera experiments conducted on three\nbenchmark datasets, namely AUCDD-V1, EZZ2021 and SFD, demonstrate the superior\ngeneralization capability of the proposed method.\n","authors":["Simone Bianco","Luigi Celona","Paolo Napoletano"],"pdf_url":"https://arxiv.org/pdf/2411.13181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13179v1","updated":"2024-11-20T10:23:21Z","published":"2024-11-20T10:23:21Z","title":"SONNET: Enhancing Time Delay Estimation by Leveraging Simulated Audio","summary":"  Time delay estimation or Time-Difference-Of-Arrival estimates is a critical\ncomponent for multiple localization applications such as multilateration,\ndirection of arrival, and self-calibration. The task is to estimate the time\ndifference between a signal arriving at two different sensors. For the audio\nsensor modality, most current systems are based on classical methods such as\nthe Generalized Cross-Correlation Phase Transform (GCC-PHAT) method. In this\npaper we demonstrate that learning based methods can, even based on synthetic\ndata, significantly outperform GCC-PHAT on novel real world data. To overcome\nthe lack of data with ground truth for the task, we train our model on a\nsimulated dataset which is sufficiently large and varied, and that captures the\nrelevant characteristics of the real world problem. We provide our trained\nmodel, SONNET (Simulation Optimized Neural Network Estimator of Timeshifts),\nwhich is runnable in real-time and works on novel data out of the box for many\nreal data applications, i.e. without re-training. We further demonstrate\ngreatly improved performance on the downstream task of self-calibration when\nusing our model compared to classical methods.\n","authors":["Erik Tegler","Magnus Oskarsson","Kalle Åström"],"pdf_url":"https://arxiv.org/pdf/2411.13179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13152v1","updated":"2024-11-20T09:41:41Z","published":"2024-11-20T09:41:41Z","title":"AGLP: A Graph Learning Perspective for Semi-supervised Domain Adaptation","summary":"  In semi-supervised domain adaptation (SSDA), the model aims to leverage\npartially labeled target domain data along with a large amount of labeled\nsource domain data to enhance its generalization capability for the target\ndomain. A key advantage of SSDA is its ability to significantly reduce reliance\non labeled data, thereby lowering the costs and time associated with data\npreparation. Most existing SSDA methods utilize information from domain labels\nand class labels but overlook the structural information of the data. To\naddress this issue, this paper proposes a graph learning perspective (AGLP) for\nsemi-supervised domain adaptation. We apply the graph convolutional network to\nthe instance graph which allows structural information to propagate along the\nweighted graph edges. The proposed AGLP model has several advantages. First, to\nthe best of our knowledge, this is the first work to model structural\ninformation in SSDA. Second, the proposed model can effectively learn\ndomain-invariant and semantic representations, reducing domain discrepancies in\nSSDA. Extensive experimental results on multiple standard benchmarks\ndemonstrate that the proposed AGLP algorithm outperforms state-of-the-art\nsemi-supervised domain adaptation methods.\n","authors":["Houcheng Su","Mengzhu Wang","Jiao Li","Nan Yin","Li Shen"],"pdf_url":"https://arxiv.org/pdf/2411.13152v1.pdf","comment":"8page"},{"id":"http://arxiv.org/abs/2405.00448v4","updated":"2024-11-20T09:40:14Z","published":"2024-05-01T11:04:22Z","title":"MMTryon: Multi-Modal Multi-Reference Control for High-Quality Fashion\n  Generation","summary":"  This paper introduces MMTryon, a multi-modal multi-reference VIrtual Try-ON\n(VITON) framework, which can generate high-quality compositional try-on results\nby taking a text instruction and multiple garment images as inputs. Our MMTryon\naddresses three problems overlooked in prior literature: 1) Support of multiple\ntry-on items. Existing methods are commonly designed for single-item try-on\ntasks (e.g., upper/lower garments, dresses). 2)Specification of dressing style.\nExisting methods are unable to customize dressing styles based on instructions\n(e.g., zipped/unzipped, tuck-in/tuck-out, etc.) 3) Segmentation Dependency.\nThey further heavily rely on category-specific segmentation models to identify\nthe replacement regions, with segmentation errors directly leading to\nsignificant artifacts in the try-on results. To address the first two issues,\nour MMTryon introduces a novel multi-modality and multi-reference attention\nmechanism to combine the garment information from reference images and\ndressing-style information from text instructions. Besides, to remove the\nsegmentation dependency, MMTryon uses a parsing-free garment encoder and\nleverages a novel scalable data generation pipeline to convert existing VITON\ndatasets to a form that allows MMTryon to be trained without requiring any\nexplicit segmentation. Extensive experiments on high-resolution benchmarks and\nin-the-wild test sets demonstrate MMTryon's superiority over existing SOTA\nmethods both qualitatively and quantitatively. MMTryon's impressive performance\non multi-item and style-controllable virtual try-on scenarios and its ability\nto try on any outfit in a large variety of scenarios from any source image,\nopens up a new avenue for future investigation in the fashion community.\n","authors":["Xujie Zhang","Ente Lin","Xiu Li","Yuxuan Luo","Michael Kampffmeyer","Xin Dong","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2405.00448v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13150v1","updated":"2024-11-20T09:40:12Z","published":"2024-11-20T09:40:12Z","title":"RAW-Diffusion: RGB-Guided Diffusion Models for High-Fidelity RAW Image\n  Generation","summary":"  Current deep learning approaches in computer vision primarily focus on RGB\ndata sacrificing information. In contrast, RAW images offer richer\nrepresentation, which is crucial for precise recognition, particularly in\nchallenging conditions like low-light environments. The resultant demand for\ncomprehensive RAW image datasets contrasts with the labor-intensive process of\ncreating specific datasets for individual sensors. To address this, we propose\na novel diffusion-based method for generating RAW images guided by RGB images.\nOur approach integrates an RGB-guidance module for feature extraction from RGB\ninputs, then incorporates these features into the reverse diffusion process\nwith RGB-guided residual blocks across various resolutions. This approach\nyields high-fidelity RAW images, enabling the creation of camera-specific RAW\ndatasets. Our RGB2RAW experiments on four DSLR datasets demonstrate\nstate-of-the-art performance. Moreover, RAW-Diffusion demonstrates exceptional\ndata efficiency, achieving remarkable performance with as few as 25 training\nsamples or even fewer. We extend our method to create BDD100K-RAW and\nCityscapes-RAW datasets, revealing its effectiveness for object detection in\nRAW imagery, significantly reducing the amount of required RAW images.\n","authors":["Christoph Reinders","Radu Berdan","Beril Besbinar","Junji Otsuka","Daisuke Iso"],"pdf_url":"https://arxiv.org/pdf/2411.13150v1.pdf","comment":"Accepted at WACV 2025"},{"id":"http://arxiv.org/abs/2411.13149v1","updated":"2024-11-20T09:32:22Z","published":"2024-11-20T09:32:22Z","title":"YCB-LUMA: YCB Object Dataset with Luminance Keying for Object\n  Localization","summary":"  Localizing target objects in images is an important task in computer vision.\nOften it is the first step towards solving a variety of applications in\nautonomous driving, maintenance, quality insurance, robotics, and augmented\nreality. Best in class solutions for this task rely on deep neural networks,\nwhich require a set of representative training data for best performance.\nCreating sets of sufficient quality, variety, and size is often difficult,\nerror prone, and expensive. This is where the method of luminance keying can\nhelp: it provides a simple yet effective solution to record high quality data\nfor training object detection and segmentation. We extend previous work that\npresented luminance keying on the common YCB-V set of household objects by\nrecording the remaining objects of the YCB superset. The additional variety of\nobjects - addition of transparency, multiple color variations, non-rigid\nobjects - further demonstrates the usefulness of luminance keying and might be\nused to test the applicability of the approach on new 2D object detection and\nsegmentation algorithms.\n","authors":["Thomas Pöllabauer"],"pdf_url":"https://arxiv.org/pdf/2411.13149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.05779v4","updated":"2024-11-20T09:31:15Z","published":"2024-01-11T09:30:36Z","title":"Erasing Undesirable Influence in Diffusion Models","summary":"  Diffusion models are highly effective at generating high-quality images but\npose risks, such as the unintentional generation of NSFW (not safe for work)\ncontent. Although various techniques have been proposed to mitigate unwanted\ninfluences in diffusion models while preserving overall performance, achieving\na balance between these goals remains challenging. In this work, we introduce\nEraseDiff, an algorithm designed to preserve the utility of the diffusion model\non retained data while removing the unwanted information associated with the\ndata to be forgotten. Our approach formulates this task as a constrained\noptimization problem using the value function, resulting in a natural\nfirst-order algorithm for solving the optimization problem. By altering the\ngenerative process to deviate away from the ground-truth denoising trajectory,\nwe update parameters for preservation while controlling constraint reduction to\nensure effective erasure, striking an optimal trade-off. Extensive experiments\nand thorough comparisons with state-of-the-art algorithms demonstrate that\nEraseDiff effectively preserves the model's utility, efficacy, and efficiency.\n","authors":["Jing Wu","Trung Le","Munawar Hayat","Mehrtash Harandi"],"pdf_url":"https://arxiv.org/pdf/2401.05779v4.pdf","comment":"Diffusion Model, Machine Unlearning"},{"id":"http://arxiv.org/abs/2312.04328v2","updated":"2024-11-20T09:27:42Z","published":"2023-12-07T14:40:05Z","title":"A Multi-scale Information Integration Framework for Infrared and Visible\n  Image Fusion","summary":"  Infrared and visible image fusion aims at generating a fused image containing\nthe intensity and detail information of source images, and the key issue is\neffectively measuring and integrating the complementary information of\nmulti-modality images from the same scene. Existing methods mostly adopt a\nsimple weight in the loss function to decide the information retention of each\nmodality rather than adaptively measuring complementary information for\ndifferent image pairs. In this study, we propose a multi-scale dual attention\n(MDA) framework for infrared and visible image fusion, which is designed to\nmeasure and integrate complementary information in both structure and loss\nfunction at the image and patch level. In our method, the residual downsample\nblock decomposes source images into three scales first. Then, dual attention\nfusion block integrates complementary information and generates a spatial and\nchannel attention map at each scale for feature fusion. Finally, the output\nimage is reconstructed by the residual reconstruction block. Loss function\nconsists of image-level, feature-level and patch-level three parts, of which\nthe calculation of the image-level and patch-level two parts are based on the\nweights generated by the complementary information measurement. Indeed, to\nconstrain the pixel intensity distribution between the output and infrared\nimage, a style loss is added. Our fusion results perform robust and informative\nacross different scenarios. Qualitative and quantitative results on two\ndatasets illustrate that our method is able to preserve both thermal radiation\nand detailed information from two modalities and achieve comparable results\ncompared with the other state-of-the-art methods. Ablation experiments show the\neffectiveness of our information integration architecture and adaptively\nmeasure complementary information retention in the loss function.\n","authors":["Guang Yang","Jie Li","Hanxiao Lei","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2312.04328v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13147v1","updated":"2024-11-20T09:24:46Z","published":"2024-11-20T09:24:46Z","title":"GraphCL: Graph-based Clustering for Semi-Supervised Medical Image\n  Segmentation","summary":"  Semi-supervised learning (SSL) has made notable advancements in medical image\nsegmentation (MIS), particularly in scenarios with limited labeled data and\nsignificantly enhancing data utilization efficiency. Previous methods primarily\nfocus on complex training strategies to utilize unlabeled data but neglect the\nimportance of graph structural information. Different from existing methods, we\npropose a graph-based clustering for semi-supervised medical image segmentation\n(GraphCL) by jointly modeling graph data structure in a unified deep model. The\nproposed GraphCL model enjoys several advantages. Firstly, to the best of our\nknowledge, this is the first work to model the data structure information for\nsemi-supervised medical image segmentation (SSMIS). Secondly, to get the\nclustered features across different graphs, we integrate both pairwise\naffinities between local image features and raw features as inputs. Extensive\nexperimental results on three standard benchmarks show that the proposed\nGraphCL algorithm outperforms state-of-the-art semi-supervised medical image\nsegmentation methods.\n","authors":["Mengzhu Wang","Jiao Li","Houcheng Su","Nan Yin","Shen Li"],"pdf_url":"https://arxiv.org/pdf/2411.13147v1.pdf","comment":"9page"},{"id":"http://arxiv.org/abs/2411.13145v1","updated":"2024-11-20T09:19:12Z","published":"2024-11-20T09:19:12Z","title":"Globally Correlation-Aware Hard Negative Generation","summary":"  Hard negative generation aims to generate informative negative samples that\nhelp to determine the decision boundaries and thus facilitate advancing deep\nmetric learning. Current works select pair/triplet samples, learn their\ncorrelations, and fuse them to generate hard negatives. However, these works\nmerely consider the local correlations of selected samples, ignoring global\nsample correlations that would provide more significant information to generate\nmore informative negatives. In this work, we propose a Globally\nCorrelation-Aware Hard Negative Generation (GCA-HNG) framework, which first\nlearns sample correlations from a global perspective and exploits these\ncorrelations to guide generating hardness-adaptive and diverse negatives.\nSpecifically, this approach begins by constructing a structured graph to model\nsample correlations, where each node represents a specific sample and each edge\nrepresents the correlations between corresponding samples. Then, we introduce\nan iterative graph message propagation to propagate the messages of node and\nedge through the whole graph and thus learn the sample correlations globally.\nFinally, with the guidance of the learned global correlations, we propose a\nchannel-adaptive manner to combine an anchor and multiple negatives for HNG.\nCompared to current methods, GCA-HNG allows perceiving sample correlations with\nnumerous negatives from a global and comprehensive perspective and generates\nthe negatives with better hardness and diversity. Extensive experiment results\ndemonstrate that the proposed GCA-HNG is superior to related methods on four\nimage retrieval benchmark datasets. Codes and trained models are available at\n\\url{https://github.com/PWenJay/GCA-HNG}.\n","authors":["Wenjie Peng","Hongxiang Huang","Tianshui Chen","Quhui Ke","Gang Dai","Shuangping Huang"],"pdf_url":"https://arxiv.org/pdf/2411.13145v1.pdf","comment":"Accepted by IJCV'24"},{"id":"http://arxiv.org/abs/2411.13144v1","updated":"2024-11-20T09:19:10Z","published":"2024-11-20T09:19:10Z","title":"CopyrightMeter: Revisiting Copyright Protection in Text-to-image Models","summary":"  Text-to-image diffusion models have emerged as powerful tools for generating\nhigh-quality images from textual descriptions. However, their increasing\npopularity has raised significant copyright concerns, as these models can be\nmisused to reproduce copyrighted content without authorization. In response,\nrecent studies have proposed various copyright protection methods, including\nadversarial perturbation, concept erasure, and watermarking techniques.\nHowever, their effectiveness and robustness against advanced attacks remain\nlargely unexplored. Moreover, the lack of unified evaluation frameworks has\nhindered systematic comparison and fair assessment of different approaches. To\nbridge this gap, we systematize existing copyright protection methods and\nattacks, providing a unified taxonomy of their design spaces. We then develop\nCopyrightMeter, a unified evaluation framework that incorporates 17\nstate-of-the-art protections and 16 representative attacks. Leveraging\nCopyrightMeter, we comprehensively evaluate protection methods across multiple\ndimensions, thereby uncovering how different design choices impact fidelity,\nefficacy, and resilience under attacks. Our analysis reveals several key\nfindings: (i) most protections (16/17) are not resilient against attacks; (ii)\nthe \"best\" protection varies depending on the target priority; (iii) more\nadvanced attacks significantly promote the upgrading of protections. These\ninsights provide concrete guidance for developing more robust protection\nmethods, while its unified evaluation protocol establishes a standard benchmark\nfor future copyright protection research in text-to-image generation.\n","authors":["Naen Xu","Changjiang Li","Tianyu Du","Minxi Li","Wenjie Luo","Jiacheng Liang","Yuyuan Li","Xuhong Zhang","Meng Han","Jianwei Yin","Ting Wang"],"pdf_url":"https://arxiv.org/pdf/2411.13144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.05128v3","updated":"2024-11-20T09:13:36Z","published":"2023-08-09T08:49:29Z","title":"High-Level Parallelism and Nested Features for Dynamic Inference Cost\n  and Top-Down Attention","summary":"  This paper introduces a novel network topology that seamlessly integrates\ndynamic inference cost with a top-down attention mechanism, addressing two\nsignificant gaps in traditional deep learning models. Drawing inspiration from\nhuman perception, we combine sequential processing of generic low-level\nfeatures with parallelism and nesting of high-level features. This design not\nonly reflects a finding from recent neuroscience research regarding - spatially\nand contextually distinct neural activations - in human cortex, but also\nintroduces a novel \"cutout\" technique: the ability to selectively activate\n%segments of the network for task-relevant only network segments of\ntask-relevant categories to optimize inference cost and eliminate the need for\nre-training. We believe this paves the way for future network designs that are\nlightweight and adaptable, making them suitable for a wide range of\napplications, from compact edge devices to large-scale clouds. Our proposed\ntopology also comes with a built-in top-down attention mechanism, which allows\nprocessing to be directly influenced by either enhancing or inhibiting\ncategory-specific high-level features, drawing parallels to the selective\nattention mechanism observed in human cognition. Using targeted external\nsignals, we experimentally enhanced predictions across all tested models. In\nterms of dynamic inference cost our methodology can achieve an exclusion of up\nto $73.48\\,\\%$ of parameters and $84.41\\,\\%$ fewer giga-multiply-accumulate\n(GMAC) operations, analysis against comparative baselines show an average\nreduction of $40\\,\\%$ in parameters and $8\\,\\%$ in GMACs across the cases we\nevaluated.\n","authors":["André Peter Kelm","Niels Hannemann","Bruno Heberle","Lucas Schmidt","Tim Rolff","Christian Wilms","Ehsan Yaghoubi","Simone Frintrop"],"pdf_url":"https://arxiv.org/pdf/2308.05128v3.pdf","comment":"Paper's findings on high-level parallelism and nested features\n  directly contributes to 'Selecting High-Level Features: Efficient Experts\n  from a Hierarchical Classification Network,' accepted at ICLR 2024's\n  Practical ML for Low Resource Settings (PML4LRS) workshop (non-archival); a\n  modified version has been accepted for presentation at the ICPR 2024"},{"id":"http://arxiv.org/abs/2411.13136v1","updated":"2024-11-20T08:58:59Z","published":"2024-11-20T08:58:59Z","title":"TAPT: Test-Time Adversarial Prompt Tuning for Robust Inference in\n  Vision-Language Models","summary":"  Large pre-trained Vision-Language Models (VLMs) such as CLIP have\ndemonstrated excellent zero-shot generalizability across various downstream\ntasks. However, recent studies have shown that the inference performance of\nCLIP can be greatly degraded by small adversarial perturbations, especially its\nvisual modality, posing significant safety threats. To mitigate this\nvulnerability, in this paper, we propose a novel defense method called\nTest-Time Adversarial Prompt Tuning (TAPT) to enhance the inference robustness\nof CLIP against visual adversarial attacks. TAPT is a test-time defense method\nthat learns defensive bimodal (textual and visual) prompts to robustify the\ninference process of CLIP. Specifically, it is an unsupervised method that\noptimizes the defensive prompts for each test sample by minimizing a multi-view\nentropy and aligning adversarial-clean distributions. We evaluate the\neffectiveness of TAPT on 11 benchmark datasets, including ImageNet and 10 other\nzero-shot datasets, demonstrating that it enhances the zero-shot adversarial\nrobustness of the original CLIP by at least 48.9% against AutoAttack (AA),\nwhile largely maintaining performance on clean examples. Moreover, TAPT\noutperforms existing adversarial prompt tuning methods across various\nbackbones, achieving an average robustness improvement of at least 36.6%.\n","authors":["Xin Wang","Kai Chen","Jiaming Zhang","Jingjing Chen","Xingjun Ma"],"pdf_url":"https://arxiv.org/pdf/2411.13136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.07919v4","updated":"2024-11-20T08:52:24Z","published":"2024-05-13T16:50:42Z","title":"Exploring the Low-Pass Filtering Behavior in Image Super-Resolution","summary":"  Deep neural networks for image super-resolution (ISR) have shown significant\nadvantages over traditional approaches like the interpolation. However, they\nare often criticized as 'black boxes' compared to traditional approaches with\nsolid mathematical foundations. In this paper, we attempt to interpret the\nbehavior of deep neural networks in ISR using theories from the field of signal\nprocessing. First, we report an intriguing phenomenon, referred to as `the sinc\nphenomenon.' It occurs when an impulse input is fed to a neural network. Then,\nbuilding on this observation, we propose a method named Hybrid Response\nAnalysis (HyRA) to analyze the behavior of neural networks in ISR tasks.\nSpecifically, HyRA decomposes a neural network into a parallel connection of a\nlinear system and a non-linear system and demonstrates that the linear system\nfunctions as a low-pass filter while the non-linear system injects\nhigh-frequency information. Finally, to quantify the injected high-frequency\ninformation, we introduce a metric for image-to-image tasks called Frequency\nSpectrum Distribution Similarity (FSDS). FSDS reflects the distribution\nsimilarity of different frequency components and can capture nuances that\ntraditional metrics may overlook. Code, videos and raw experimental results for\nthis paper can be found in: https://github.com/RisingEntropy/LPFInISR.\n","authors":["Haoyu Deng","Zijing Xu","Yule Duan","Xiao Wu","Wenjie Shu","Liang-Jian Deng"],"pdf_url":"https://arxiv.org/pdf/2405.07919v4.pdf","comment":"Accepted by ICML 2024"},{"id":"http://arxiv.org/abs/2312.15701v2","updated":"2024-11-20T08:44:06Z","published":"2023-12-25T11:53:06Z","title":"Rotation Equivariant Proximal Operator for Deep Unfolding Methods in\n  Image Restoration","summary":"  The deep unfolding approach has attracted significant attention in computer\nvision tasks, which well connects conventional image processing modeling\nmanners with more recent deep learning techniques. Specifically, by\nestablishing a direct correspondence between algorithm operators at each\nimplementation step and network modules within each layer, one can rationally\nconstruct an almost ``white box'' network architecture with high\ninterpretability. In this architecture, only the predefined component of the\nproximal operator, known as a proximal network, needs manual configuration,\nenabling the network to automatically extract intrinsic image priors in a\ndata-driven manner. In current deep unfolding methods, such a proximal network\nis generally designed as a CNN architecture, whose necessity has been proven by\na recent theory. That is, CNN structure substantially delivers the\ntranslational invariant image prior, which is the most universally possessed\nstructural prior across various types of images. However, standard CNN-based\nproximal networks have essential limitations in capturing the rotation symmetry\nprior, another universal structural prior underlying general images. This\nleaves a large room for further performance improvement in deep unfolding\napproaches. To address this issue, this study makes efforts to suggest a\nhigh-accuracy rotation equivariant proximal network that effectively embeds\nrotation symmetry priors into the deep unfolding framework. Especially, we\ndeduce, for the first time, the theoretical equivariant error for such a\ndesigned proximal network with arbitrary layers under arbitrary rotation\ndegrees. This analysis should be the most refined theoretical conclusion for\nsuch error evaluation to date and is also indispensable for supporting the\nrationale behind such networks with intrinsic interpretability requirements.\n","authors":["Jiahong Fu","Qi Xie","Deyu Meng","Zongben Xu"],"pdf_url":"https://arxiv.org/pdf/2312.15701v2.pdf","comment":"Published in TPAMI 2024"},{"id":"http://arxiv.org/abs/2411.13127v1","updated":"2024-11-20T08:37:39Z","published":"2024-11-20T08:37:39Z","title":"Adapting Vision Foundation Models for Robust Cloud Segmentation in\n  Remote Sensing Images","summary":"  Cloud segmentation is a critical challenge in remote sensing image\ninterpretation, as its accuracy directly impacts the effectiveness of\nsubsequent data processing and analysis. Recently, vision foundation models\n(VFM) have demonstrated powerful generalization capabilities across various\nvisual tasks. In this paper, we present a parameter-efficient adaptive\napproach, termed Cloud-Adapter, designed to enhance the accuracy and robustness\nof cloud segmentation. Our method leverages a VFM pretrained on general domain\ndata, which remains frozen, eliminating the need for additional training.\nCloud-Adapter incorporates a lightweight spatial perception module that\ninitially utilizes a convolutional neural network (ConvNet) to extract dense\nspatial representations. These multi-scale features are then aggregated and\nserve as contextual inputs to an adapting module, which modulates the frozen\ntransformer layers within the VFM. Experimental results demonstrate that the\nCloud-Adapter approach, utilizing only 0.6% of the trainable parameters of the\nfrozen backbone, achieves substantial performance gains. Cloud-Adapter\nconsistently attains state-of-the-art (SOTA) performance across a wide variety\nof cloud segmentation datasets from multiple satellite sources, sensor series,\ndata processing levels, land cover scenarios, and annotation granularities. We\nhave released the source code and pretrained models at\nhttps://github.com/XavierJiezou/Cloud-Adapter to support further research.\n","authors":["Xuechao Zou","Shun Zhang","Kai Li","Shiying Wang","Junliang Xing","Lei Jin","Congyan Lang","Pin Tao"],"pdf_url":"https://arxiv.org/pdf/2411.13127v1.pdf","comment":"13 pages, 9 figures"},{"id":"http://arxiv.org/abs/2411.13120v1","updated":"2024-11-20T08:30:11Z","published":"2024-11-20T08:30:11Z","title":"Virtual Staining of Label-Free Tissue in Imaging Mass Spectrometry","summary":"  Imaging mass spectrometry (IMS) is a powerful tool for untargeted, highly\nmultiplexed molecular mapping of tissue in biomedical research. IMS offers a\nmeans of mapping the spatial distributions of molecular species in biological\ntissue with unparalleled chemical specificity and sensitivity. However, most\nIMS platforms are not able to achieve microscopy-level spatial resolution and\nlack cellular morphological contrast, necessitating subsequent histochemical\nstaining, microscopic imaging and advanced image registration steps to enable\nmolecular distributions to be linked to specific tissue features and cell\ntypes. Here, we present a virtual histological staining approach that enhances\nspatial resolution and digitally introduces cellular morphological contrast\ninto mass spectrometry images of label-free human tissue using a diffusion\nmodel. Blind testing on human kidney tissue demonstrated that the virtually\nstained images of label-free samples closely match their histochemically\nstained counterparts (with Periodic Acid-Schiff staining), showing high\nconcordance in identifying key renal pathology structures despite utilizing IMS\ndata with 10-fold larger pixel size. Additionally, our approach employs an\noptimized noise sampling technique during the diffusion model's inference\nprocess to reduce variance in the generated images, yielding reliable and\nrepeatable virtual staining. We believe this virtual staining method will\nsignificantly expand the applicability of IMS in life sciences and open new\navenues for mass spectrometry-based biomedical research.\n","authors":["Yijie Zhang","Luzhe Huang","Nir Pillar","Yuzhu Li","Lukasz G. Migas","Raf Van de Plas","Jeffrey M. Spraggins","Aydogan Ozcan"],"pdf_url":"https://arxiv.org/pdf/2411.13120v1.pdf","comment":"33 Pages, 6 Figures"},{"id":"http://arxiv.org/abs/2411.13112v1","updated":"2024-11-20T08:14:01Z","published":"2024-11-20T08:14:01Z","title":"DriveMLLM: A Benchmark for Spatial Understanding with Multimodal Large\n  Language Models in Autonomous Driving","summary":"  Autonomous driving requires a comprehensive understanding of 3D environments\nto facilitate high-level tasks such as motion prediction, planning, and\nmapping. In this paper, we introduce DriveMLLM, a benchmark specifically\ndesigned to evaluate the spatial understanding capabilities of multimodal large\nlanguage models (MLLMs) in autonomous driving. DriveMLLM includes 2,734\nfront-facing camera images and introduces both absolute and relative spatial\nreasoning tasks, accompanied by linguistically diverse natural language\nquestions. To measure MLLMs' performance, we propose novel evaluation metrics\nfocusing on spatial understanding. We evaluate several state-of-the-art MLLMs\non DriveMLLM, and our results reveal the limitations of current models in\nunderstanding complex spatial relationships in driving contexts. We believe\nthese findings underscore the need for more advanced MLLM-based spatial\nreasoning methods and highlight the potential for DriveMLLM to drive further\nresearch in autonomous driving. Code will be available at\n\\url{https://github.com/XiandaGuo/Drive-MLLM}.\n","authors":["Xianda Guo","Ruijun Zhang","Yiqun Duan","Yuhang He","Chenming Zhang","Shuai Liu","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2411.13112v1.pdf","comment":"Code will be available at\n  \\url{https://github.com/XiandaGuo/Drive-MLLM}"},{"id":"http://arxiv.org/abs/2403.06505v2","updated":"2024-11-20T08:11:36Z","published":"2024-03-11T08:28:51Z","title":"Voxel-Mesh Hybrid Representation for Real-Time View Synthesis","summary":"  The neural radiance fields (NeRF) have emerged as a prominent methodology for\nsynthesizing realistic images of novel views. While neural radiance\nrepresentations based on voxels or mesh individually offer distinct advantages,\nexcelling in either rendering quality or speed, each has limitations in the\nother aspect. In response, we propose a hybrid representation named Vosh,\nseamlessly combining both voxel and mesh components in hybrid rendering for\nview synthesis. Vosh is meticulously crafted by optimizing the voxel grid based\non neural rendering, strategically meshing a portion of the volumetric density\nfield to surface. Therefore, it excels in fast rendering scenes with simple\ngeometry and textures through its mesh component, while simultaneously enabling\nhigh-quality rendering in intricate regions by leveraging voxel component. The\nflexibility of Vosh is showcased through the ability to adjust hybrid ratios,\nproviding users the ability to control the balance between rendering quality\nand speed based on flexible usage. Experimental results demonstrate that our\nmethod achieves commendable trade-off between rendering quality and speed, and\nnotably has real-time performance on mobile devices. The interactive web demo\nand code are available at https://zyyzyy06.github.io/Vosh.\n","authors":["Chenhao Zhang","Yongyang Zhou","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.06505v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13108v1","updated":"2024-11-20T08:06:25Z","published":"2024-11-20T08:06:25Z","title":"Demonstrating the Suitability of Neuromorphic, Event-Based, Dynamic\n  Vision Sensors for In Process Monitoring of Metallic Additive Manufacturing\n  and Welding","summary":"  We demonstrate the suitability of high dynamic range, high-speed,\nneuromorphic event-based, dynamic vision sensors for metallic additive\nmanufacturing and welding for in-process monitoring applications. In-process\nmonitoring to enable quality control of mission critical components produced\nusing metallic additive manufacturing is of high interest. However, the extreme\nlight environment and high speed dynamics of metallic melt pools have made this\na difficult environment in which to make measurements. Event-based sensing is\nan alternative measurement paradigm where data is only transmitted/recorded\nwhen a measured quantity exceeds a threshold resolution. The result is that\nevent-based sensors consume less power and less memory/bandwidth, and they\noperate across a wide range of timescales and dynamic ranges. Event-driven\ndriven imagers stand out from conventional imager technology in that they have\na very high dynamic range of approximately 120 dB. Conventional 8 bit imagers\nonly have a dynamic range of about 48 dB. This high dynamic range makes them a\ngood candidate for monitoring manufacturing processes that feature high\nintensity light sources/generation such as metallic additive manufacturing and\nwelding. In addition event based imagers are able to capture data at timescales\non the order of 100 {\\mu}s, which makes them attractive to capturing fast\ndynamics in a metallic melt pool. In this work we demonstrate that event-driven\nimagers have been shown to be able to observe tungsten inert gas (TIG) and\nlaser welding melt pools. The results of this effort suggest that with\nadditional engineering effort, neuromorphic event imagers should be capable of\n3D geometry measurements of the melt pool, and anomaly\ndetection/classification/prediction.\n","authors":["David Mascareñas","Andre Green","Ashlee Liao","Michael Torrez","Alessandro Cattaneo","Amber Black","John Bernardin","Garrett Kenyon"],"pdf_url":"https://arxiv.org/pdf/2411.13108v1.pdf","comment":"This work is a derivative work of a conference proceedings paper\n  submitted to the International Modal Analysis Conference 2024, and is subject\n  to some copyright restrictions associated with the Society of Experimental\n  Mechanics. A variation of this paper is also published in the Weapons\n  Engineering Symposium and Journal (WESJ) which is not publically accessible"},{"id":"http://arxiv.org/abs/2411.13105v1","updated":"2024-11-20T07:59:55Z","published":"2024-11-20T07:59:55Z","title":"Superpixel Cost Volume Excitation for Stereo Matching","summary":"  In this work, we concentrate on exciting the intrinsic local consistency of\nstereo matching through the incorporation of superpixel soft constraints, with\nthe objective of mitigating inaccuracies at the boundaries of predicted\ndisparity maps. Our approach capitalizes on the observation that neighboring\npixels are predisposed to belong to the same object and exhibit closely similar\nintensities within the probability volume of superpixels. By incorporating this\ninsight, our method encourages the network to generate consistent probability\ndistributions of disparity within each superpixel, aiming to improve the\noverall accuracy and coherence of predicted disparity maps. Experimental evalua\ntions on widely-used datasets validate the efficacy of our proposed approach,\ndemonstrating its ability to assist cost volume-based matching networks in\nrestoring competitive performance.\n","authors":["Shanglong Liu","Lin Qi","Junyu Dong","Wenxiang Gu","Liyi Xu"],"pdf_url":"https://arxiv.org/pdf/2411.13105v1.pdf","comment":"13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2411.13093v1","updated":"2024-11-20T07:44:34Z","published":"2024-11-20T07:44:34Z","title":"Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension","summary":"  Existing large video-language models (LVLMs) struggle to comprehend long\nvideos correctly due to limited context. To address this problem, fine-tuning\nlong-context LVLMs and employing GPT-based agents have emerged as promising\nsolutions. However, fine-tuning LVLMs would require extensive high-quality data\nand substantial GPU resources, while GPT-based agents would rely on proprietary\nmodels (e.g., GPT-4o). In this paper, we propose Video Retrieval-Augmented\nGeneration (Video-RAG), a training-free and cost-effective pipeline that\nemploys visually-aligned auxiliary texts to help facilitate cross-modality\nalignment while providing additional information beyond the visual content.\nSpecifically, we leverage open-source external tools to extract\nvisually-aligned information from pure video data (e.g., audio, optical\ncharacter, and object detection), and incorporate the extracted information\ninto an existing LVLM as auxiliary texts, alongside video frames and queries,\nin a plug-and-play manner. Our Video-RAG offers several key advantages: (i)\nlightweight with low computing overhead due to single-turn retrieval; (ii) easy\nimplementation and compatibility with any LVLM; and (iii) significant,\nconsistent performance gains across long video understanding benchmarks,\nincluding Video-MME, MLVU, and LongVideoBench. Notably, our model demonstrates\nsuperior performance over proprietary models like Gemini-1.5-Pro and GPT-4o\nwhen utilized with a 72B model.\n","authors":["Yongdong Luo","Xiawu Zheng","Xiao Yang","Guilin Li","Haojia Lin","Jinfa Huang","Jiayi Ji","Fei Chao","Jiebo Luo","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2411.13093v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.09865v2","updated":"2024-11-20T07:38:20Z","published":"2024-10-13T14:58:21Z","title":"SynFER: Towards Boosting Facial Expression Recognition with Synthetic\n  Data","summary":"  Facial expression datasets remain limited in scale due to privacy concerns,\nthe subjectivity of annotations, and the labor-intensive nature of data\ncollection. This limitation poses a significant challenge for developing modern\ndeep learning-based facial expression analysis models, particularly foundation\nmodels, that rely on large-scale data for optimal performance. To tackle the\noverarching and complex challenge, we introduce SynFER (Synthesis of Facial\nExpressions with Refined Control), a novel framework for synthesizing facial\nexpression image data based on high-level textual descriptions as well as more\nfine-grained and precise control through facial action units. To ensure the\nquality and reliability of the synthetic data, we propose a semantic guidance\ntechnique to steer the generation process and a pseudo-label generator to help\nrectify the facial expression labels for the synthetic images. To demonstrate\nthe generation fidelity and the effectiveness of the synthetic data from\nSynFER, we conduct extensive experiments on representation learning using both\nsynthetic data and real-world data. Experiment results validate the efficacy of\nthe proposed approach and the synthetic data. Notably, our approach achieves a\n67.23% classification accuracy on AffectNet when training solely with synthetic\ndata equivalent to the AffectNet training set size, which increases to 69.84%\nwhen scaling up to five times the original size. Our code will be made publicly\navailable.\n","authors":["Xilin He","Cheng Luo","Xiaole Xian","Bing Li","Siyang Song","Muhammad Haris Khan","Weicheng Xie","Linlin Shen","Zongyuan Ge"],"pdf_url":"https://arxiv.org/pdf/2410.09865v2.pdf","comment":"Updated Results"},{"id":"http://arxiv.org/abs/2411.13089v1","updated":"2024-11-20T07:37:37Z","published":"2024-11-20T07:37:37Z","title":"ESARM: 3D Emotional Speech-to-Animation via Reward Model from\n  Automatically-Ranked Demonstrations","summary":"  This paper proposes a novel 3D speech-to-animation (STA) generation framework\ndesigned to address the shortcomings of existing models in producing diverse\nand emotionally resonant animations. Current STA models often generate\nanimations that lack emotional depth and variety, failing to align with human\nexpectations. To overcome these limitations, we introduce a novel STA model\ncoupled with a reward model. This combination enables the decoupling of emotion\nand content under audio conditions through a cross-coupling training approach.\nAdditionally, we develop a training methodology that leverages automatic\nquality evaluation of generated facial animations to guide the reinforcement\nlearning process. This methodology encourages the STA model to explore a\nbroader range of possibilities, resulting in the generation of diverse and\nemotionally expressive facial animations of superior quality. We conduct\nextensive empirical experiments on a benchmark dataset, and the results\nvalidate the effectiveness of our proposed framework in generating\nhigh-quality, emotionally rich 3D animations that are better aligned with human\npreferences.\n","authors":["Xulong Zhang","Xiaoyang Qu","Haoxiang Shi","Chunguang Xiao","Jianzong Wang"],"pdf_url":"https://arxiv.org/pdf/2411.13089v1.pdf","comment":"Accepted by the 26th IEEE International Conference on High\n  Performance Computing and Communications (HPCC2024)"},{"id":"http://arxiv.org/abs/2411.13081v1","updated":"2024-11-20T07:17:16Z","published":"2024-11-20T07:17:16Z","title":"Practical Compact Deep Compressed Sensing","summary":"  Recent years have witnessed the success of deep networks in compressed\nsensing (CS), which allows for a significant reduction in sampling cost and has\ngained growing attention since its inception. In this paper, we propose a new\npractical and compact network dubbed PCNet for general image CS. Specifically,\nin PCNet, a novel collaborative sampling operator is designed, which consists\nof a deep conditional filtering step and a dual-branch fast sampling step. The\nformer learns an implicit representation of a linear transformation matrix into\na few convolutions and first performs adaptive local filtering on the input\nimage, while the latter then uses a discrete cosine transform and a scrambled\nblock-diagonal Gaussian matrix to generate under-sampled measurements. Our\nPCNet is equipped with an enhanced proximal gradient descent algorithm-unrolled\nnetwork for reconstruction. It offers flexibility, interpretability, and strong\nrecovery performance for arbitrary sampling rates once trained. Additionally,\nwe provide a deployment-oriented extraction scheme for single-pixel CS imaging\nsystems, which allows for the convenient conversion of any linear sampling\noperator to its matrix form to be loaded onto hardware like digital\nmicro-mirror devices. Extensive experiments on natural image CS, quantized CS,\nand self-supervised CS demonstrate the superior reconstruction accuracy and\ngeneralization ability of PCNet compared to existing state-of-the-art methods,\nparticularly for high-resolution images. Code is available at\nhttps://github.com/Guaishou74851/PCNet.\n","authors":["Bin Chen","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.13081v1.pdf","comment":"Accepted by IEEE T-PAMI"},{"id":"http://arxiv.org/abs/2106.15989v2","updated":"2024-11-20T07:16:16Z","published":"2021-06-30T11:30:06Z","title":"Word-level Sign Language Recognition with Multi-stream Neural Networks\n  Focusing on Local Regions and Skeletal Information","summary":"  Word-level sign language recognition (WSLR) has attracted attention because\nit is expected to overcome the communication barrier between people with speech\nimpairment and those who can hear. In the WSLR problem, a method designed for\naction recognition has achieved the state-of-the-art accuracy. Indeed, it\nsounds reasonable for an action recognition method to perform well on WSLR\nbecause sign language is regarded as an action. However, a careful evaluation\nof the tasks reveals that the tasks of action recognition and WSLR are\ninherently different. Hence, in this paper, we propose a novel WSLR method that\ntakes into account information specifically useful for the WSLR problem. We\nrealize it as a multi-stream neural network (MSNN), which consist of three\nstreams: 1) base stream, 2) local image stream, and 3) skeleton stream. Each\nstream is designed to handle different types of information. The base stream\ndeals with quick and detailed movements of the hands and body, the local image\nstream focuses on handshapes and facial expressions, and the skeleton stream\ncaptures the relative positions of the body and both hands. This approach\nallows us to combine various types of data for more comprehensive gesture\nanalysis. Experimental results on the WLASL and MS-ASL datasets show the\neffectiveness of the proposed method; it achieved an improvement of\napproximately 10\\%--15\\% in Top-1 accuracy when compared with conventional\nmethods.\n","authors":["Mizuki Maruyama","Shrey Singh","Katsufumi Inoue","Partha Pratim Roy","Masakazu Iwamura","Michifumi Yoshioka"],"pdf_url":"https://arxiv.org/pdf/2106.15989v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13076v1","updated":"2024-11-20T06:58:33Z","published":"2024-11-20T06:58:33Z","title":"Hints of Prompt: Enhancing Visual Representation for Multimodal LLMs in\n  Autonomous Driving","summary":"  In light of the dynamic nature of autonomous driving environments and\nstringent safety requirements, general MLLMs combined with CLIP alone often\nstruggle to represent driving-specific scenarios accurately, particularly in\ncomplex interactions and long-tail cases. To address this, we propose the Hints\nof Prompt (HoP) framework, which introduces three key enhancements: Affinity\nhint to emphasize instance-level structure by strengthening token-wise\nconnections, Semantic hint to incorporate high-level information relevant to\ndriving-specific cases, such as complex interactions among vehicles and traffic\nsigns, and Question hint to align visual features with the query context,\nfocusing on question-relevant regions. These hints are fused through a Hint\nFusion module, enriching visual representations and enhancing multimodal\nreasoning for autonomous driving VQA tasks. Extensive experiments confirm the\neffectiveness of the HoP framework, showing it significantly outperforms\nprevious state-of-the-art methods across all key metrics.\n","authors":["Hao Zhou","Zhanning Gao","Maosheng Ye","Zhili Chen","Qifeng Chen","Tongyi Cao","Honggang Qi"],"pdf_url":"https://arxiv.org/pdf/2411.13076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15674v2","updated":"2024-11-20T06:58:02Z","published":"2024-10-21T06:37:13Z","title":"TALoS: Enhancing Semantic Scene Completion via Test-time Adaptation on\n  the Line of Sight","summary":"  Semantic Scene Completion (SSC) aims to perform geometric completion and\nsemantic segmentation simultaneously. Despite the promising results achieved by\nexisting studies, the inherently ill-posed nature of the task presents\nsignificant challenges in diverse driving scenarios. This paper introduces\nTALoS, a novel test-time adaptation approach for SSC that excavates the\ninformation available in driving environments. Specifically, we focus on that\nobservations made at a certain moment can serve as Ground Truth (GT) for scene\ncompletion at another moment. Given the characteristics of the LiDAR sensor, an\nobservation of an object at a certain location confirms both 1) the occupation\nof that location and 2) the absence of obstacles along the line of sight from\nthe LiDAR to that point. TALoS utilizes these observations to obtain\nself-supervision about occupancy and emptiness, guiding the model to adapt to\nthe scene in test time. In a similar manner, we aggregate reliable SSC\npredictions among multiple moments and leverage them as semantic pseudo-GT for\nadaptation. Further, to leverage future observations that are not accessible at\nthe current time, we present a dual optimization scheme using the model in\nwhich the update is delayed until the future observation is available.\nEvaluations on the SemanticKITTI validation and test sets demonstrate that\nTALoS significantly improves the performance of the pre-trained SSC model. Our\ncode is available at https://github.com/blue-531/TALoS.\n","authors":["Hyun-Kurl Jang","Jihun Kim","Hyeokjun Kweon","Kuk-Jin Yoon"],"pdf_url":"https://arxiv.org/pdf/2410.15674v2.pdf","comment":"Accepted at NeurIPS 2024. Code is available at\n  https://github.com/blue-531/TALoS"},{"id":"http://arxiv.org/abs/2411.13073v1","updated":"2024-11-20T06:50:50Z","published":"2024-11-20T06:50:50Z","title":"Improving OOD Generalization of Pre-trained Encoders via Aligned\n  Embedding-Space Ensembles","summary":"  The quality of self-supervised pre-trained embeddings on out-of-distribution\n(OOD) data is poor without fine-tuning. A straightforward and simple approach\nto improving the generalization of pre-trained representation to OOD data is\nthe use of deep ensembles. However, obtaining an effective ensemble in the\nembedding space with only unlabeled data remains an unsolved problem. We first\nperform a theoretical analysis that reveals the relationship between individual\nhyperspherical embedding spaces in an ensemble. We then design a principled\nmethod to align these embedding spaces in an unsupervised manner. Experimental\nresults on the MNIST dataset show that our embedding-space ensemble method\nimproves pre-trained embedding quality on in-distribution and OOD data compared\nto single encoders.\n","authors":["Shuman Peng","Arash Khoeini","Sharan Vaswani","Martin Ester"],"pdf_url":"https://arxiv.org/pdf/2411.13073v1.pdf","comment":"Accepted at the Self-Supervised Learning Workshop and the Unifying\n  Representations in Neural Models Workshop at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2308.05286v2","updated":"2024-11-20T06:35:55Z","published":"2023-08-10T02:04:01Z","title":"Informative Scene Graph Generation via Debiasing","summary":"  Scene graph generation aims to detect visual relationship triplets, (subject,\npredicate, object). Due to biases in data, current models tend to predict\ncommon predicates, e.g. \"on\" and \"at\", instead of informative ones, e.g.\n\"standing on\" and \"looking at\". This tendency results in the loss of precise\ninformation and overall performance. If a model only uses \"stone on road\"\nrather than \"stone blocking road\" to describe an image, it may be a grave\nmisunderstanding. We argue that this phenomenon is caused by two imbalances:\nsemantic space level imbalance and training sample level imbalance. For this\nproblem, we propose DB-SGG, an effective framework based on debiasing but not\nthe conventional distribution fitting. It integrates two components: Semantic\nDebiasing (SD) and Balanced Predicate Learning (BPL), for these imbalances. SD\nutilizes a confusion matrix and a bipartite graph to construct predicate\nrelationships. BPL adopts a random undersampling strategy and an ambiguity\nremoving strategy to focus on informative predicates. Benefiting from the\nmodel-agnostic process, our method can be easily applied to SGG models and\noutperforms Transformer by 136.3%, 119.5%, and 122.6% on mR@20 at three SGG\nsub-tasks on the SGG-VG dataset. Our method is further verified on another\ncomplex SGG dataset (SGG-GQA) and two downstream tasks (sentence-to-graph\nretrieval and image captioning).\n","authors":["Lianli Gao","Xinyu Lyu","Yuyu Guo","Yuxuan Hu","Yuan-Fang Li","Lu Xu","Heng Tao Shen","Jingkuan Song"],"pdf_url":"https://arxiv.org/pdf/2308.05286v2.pdf","comment":"The author requests to withdraw this paper due to a critical\n  definitional error in Informative Scene Graph Generation via Debiasing. This\n  error aligned with the definition of Informative Scene Graph Generation\n  tasks, resulting in an unfair comparison with state-of- the-art (SOTA)\n  methods, which in turn, hindered the ability to evaluate the paper's\n  contributions"},{"id":"http://arxiv.org/abs/2411.13069v1","updated":"2024-11-20T06:34:47Z","published":"2024-11-20T06:34:47Z","title":"Automatic marker-free registration based on similar tetrahedras for\n  single-tree point clouds","summary":"  In recent years, terrestrial laser scanning technology has been widely used\nto collect tree point cloud data, aiding in measurements of diameter at breast\nheight, biomass, and other forestry survey data. Since a single scan from\nterrestrial laser systems captures data from only one angle, multiple scans\nmust be registered and fused to obtain complete tree point cloud data. This\npaper proposes a marker-free automatic registration method for single-tree\npoint clouds based on similar tetrahedras. First, two point clouds from two\nscans of the same tree are used to generate tree skeletons, and key point sets\nare constructed from these skeletons. Tetrahedra are then filtered and matched\naccording to similarity principles, with the vertices of these two matched\ntetrahedras selected as matching point pairs, thus completing the coarse\nregistration of the point clouds from the two scans. Subsequently, the ICP\nmethod is applied to the coarse-registered leaf point clouds to obtain fine\nregistration parameters, completing the precise registration of the two tree\npoint clouds. Experiments were conducted using terrestrial laser scanning data\nfrom eight trees, each from different species and with varying shapes. The\nproposed method was evaluated using RMSE and Hausdorff distance, compared\nagainst the traditional ICP and NDT methods. The experimental results\ndemonstrate that the proposed method significantly outperforms both ICP and NDT\nin registration accuracy, achieving speeds up to 593 times and 113 times faster\nthan ICP and NDT, respectively. In summary, the proposed method shows good\nrobustness in single-tree point cloud registration, with significant advantages\nin accuracy and speed compared to traditional ICP and NDT methods, indicating\nexcellent application prospects in practical registration scenarios.\n","authors":["Jing Ren","Pei Wang","Hanlong Li","Yuhan Wu","Yuhang Gao","Wenxin Chen","Mingtai Zhang","Lingyun Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.13069v1.pdf","comment":"remote sensing; terrestrial lidar; multi-scan cloud registration"},{"id":"http://arxiv.org/abs/2411.13059v1","updated":"2024-11-20T06:15:28Z","published":"2024-11-20T06:15:28Z","title":"Towards Unbiased and Robust Spatio-Temporal Scene Graph Generation and\n  Anticipation","summary":"  Spatio-Temporal Scene Graphs (STSGs) provide a concise and expressive\nrepresentation of dynamic scenes by modelling objects and their evolving\nrelationships over time. However, real-world visual relationships often exhibit\na long-tailed distribution, causing existing methods for tasks like Video Scene\nGraph Generation (VidSGG) and Scene Graph Anticipation (SGA) to produce biased\nscene graphs. To this end, we propose ImparTail, a novel training framework\nthat leverages curriculum learning and loss masking to mitigate bias in the\ngeneration and anticipation of spatio-temporal scene graphs. Our approach\ngradually decreases the dominance of the head relationship classes during\ntraining and focuses more on tail classes, leading to more balanced training.\nFurthermore, we introduce two new tasks, Robust Spatio-Temporal Scene Graph\nGeneration and Robust Scene Graph Anticipation, designed to evaluate the\nrobustness of STSG models against distribution shifts. Extensive experiments on\nthe Action Genome dataset demonstrate that our framework significantly enhances\nthe unbiased performance and robustness of STSG models compared to existing\nmethods.\n","authors":["Rohith Peddi"," Saurabh","Ayush Abhay Shrivastava","Parag Singla","Vibhav Gogate"],"pdf_url":"https://arxiv.org/pdf/2411.13059v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2411.13056v1","updated":"2024-11-20T06:08:21Z","published":"2024-11-20T06:08:21Z","title":"Efficient Masked AutoEncoder for Video Object Counting and A Large-Scale\n  Benchmark","summary":"  The dynamic imbalance of the fore-background is a major challenge in video\nobject counting, which is usually caused by the sparsity of foreground objects.\nThis often leads to severe under- and over-prediction problems and has been\nless studied in existing works. To tackle this issue in video object counting,\nwe propose a density-embedded Efficient Masked Autoencoder Counting (E-MAC)\nframework in this paper. To effectively capture the dynamic variations across\nframes, we utilize an optical flow-based temporal collaborative fusion that\naligns features to derive multi-frame density residuals. The counting accuracy\nof the current frame is boosted by harnessing the information from adjacent\nframes. More importantly, to empower the representation ability of dynamic\nforeground objects for intra-frame, we first take the density map as an\nauxiliary modality to perform $\\mathtt{D}$ensity-$\\mathtt{E}$mbedded\n$\\mathtt{M}$asked m$\\mathtt{O}$deling ($\\mathtt{DEMO}$) for multimodal\nself-representation learning to regress density map. However, as\n$\\mathtt{DEMO}$ contributes effective cross-modal regression guidance, it also\nbrings in redundant background information and hard to focus on foreground\nregions. To handle this dilemma, we further propose an efficient spatial\nadaptive masking derived from density maps to boost efficiency. In addition,\nconsidering most existing datasets are limited to human-centric scenarios, we\nfirst propose a large video bird counting dataset $\\textit{DroneBird}$, in\nnatural scenarios for migratory bird protection. Extensive experiments on three\ncrowd datasets and our $\\textit{DroneBird}$ validate our superiority against\nthe counterparts.\n","authors":["Bing Cao","Quanhao Lu","Jiekang Feng","Pengfei Zhu","Qinghua Hu","Qilong Wang"],"pdf_url":"https://arxiv.org/pdf/2411.13056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13053v1","updated":"2024-11-20T05:57:00Z","published":"2024-11-20T05:57:00Z","title":"MEGL: Multimodal Explanation-Guided Learning","summary":"  Explaining the decision-making processes of Artificial Intelligence (AI)\nmodels is crucial for addressing their \"black box\" nature, particularly in\ntasks like image classification. Traditional eXplainable AI (XAI) methods\ntypically rely on unimodal explanations, either visual or textual, each with\ninherent limitations. Visual explanations highlight key regions but often lack\nrationale, while textual explanations provide context without spatial\ngrounding. Further, both explanation types can be inconsistent or incomplete,\nlimiting their reliability. To address these challenges, we propose a novel\nMultimodal Explanation-Guided Learning (MEGL) framework that leverages both\nvisual and textual explanations to enhance model interpretability and improve\nclassification performance. Our Saliency-Driven Textual Grounding (SDTG)\napproach integrates spatial information from visual explanations into textual\nrationales, providing spatially grounded and contextually rich explanations.\nAdditionally, we introduce Textual Supervision on Visual Explanations to align\nvisual explanations with textual rationales, even in cases where ground truth\nvisual annotations are missing. A Visual Explanation Distribution Consistency\nloss further reinforces visual coherence by aligning the generated visual\nexplanations with dataset-level patterns, enabling the model to effectively\nlearn from incomplete multimodal supervision. We validate MEGL on two new\ndatasets, Object-ME and Action-ME, for image classification with multimodal\nexplanations. Experimental results demonstrate that MEGL outperforms previous\napproaches in prediction accuracy and explanation quality across both visual\nand textual domains. Our code will be made available upon the acceptance of the\npaper.\n","authors":["Yifei Zhang","Tianxu Jiang","Bo Pan","Jingyu Wang","Guangji Bai","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2411.13053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13047v1","updated":"2024-11-20T05:40:20Z","published":"2024-11-20T05:40:20Z","title":"Bounding-box Watermarking: Defense against Model Extraction Attacks on\n  Object Detectors","summary":"  Deep neural networks (DNNs) deployed in a cloud often allow users to query\nmodels via the APIs. However, these APIs expose the models to model extraction\nattacks (MEAs). In this attack, the attacker attempts to duplicate the target\nmodel by abusing the responses from the API. Backdoor-based DNN watermarking is\nknown as a promising defense against MEAs, wherein the defender injects a\nbackdoor into extracted models via API responses. The backdoor is used as a\nwatermark of the model; if a suspicious model has the watermark (i.e.,\nbackdoor), it is verified as an extracted model. This work focuses on object\ndetection (OD) models. Existing backdoor attacks on OD models are not\napplicable for model watermarking as the defense against MEAs on a realistic\nthreat model. Our proposed approach involves inserting a backdoor into\nextracted models via APIs by stealthily modifying the bounding-boxes (BBs) of\nobjects detected in queries while keeping the OD capability. In our experiments\non three OD datasets, the proposed approach succeeded in identifying the\nextracted models with 100% accuracy in a wide variety of experimental\nscenarios.\n","authors":["Satoru Koda","Ikuya Morikawa"],"pdf_url":"https://arxiv.org/pdf/2411.13047v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01925v2","updated":"2024-11-20T05:35:37Z","published":"2024-11-04T09:43:33Z","title":"Exploiting Contextual Uncertainty of Visual Data for Efficient Training\n  of Deep Models","summary":"  Objects, in the real world, rarely occur in isolation and exhibit typical\narrangements governed by their independent utility, and their expected\ninteraction with humans and other objects in the context. For example, a chair\nis expected near a table, and a computer is expected on top. Humans use this\nspatial context and relative placement as an important cue for visual\nrecognition in case of ambiguities. Similar to human's, DNN's exploit\ncontextual information from data to learn representations. Our research focuses\non harnessing the contextual aspects of visual data to optimize data annotation\nand enhance the training of deep networks. Our contributions can be summarized\nas follows: (1) We introduce the notion of contextual diversity for active\nlearning CDAL and show its applicability in three different visual tasks\nsemantic segmentation, object detection and image classification, (2) We\npropose a data repair algorithm to curate contextually fair data to reduce\nmodel bias, enabling the model to detect objects out of their obvious context,\n(3) We propose Class-based annotation, where contextually relevant classes are\nselected that are complementary for model training under domain shift.\nUnderstanding the importance of well-curated data, we also emphasize the\nnecessity of involving humans in the loop to achieve accurate annotations and\nto develop novel interaction strategies that allow humans to serve as\nfact-checkers. In line with this we are working on developing image retrieval\nsystem for wildlife camera trap images and reliable warning system for poor\nquality rural roads. For large-scale annotation, we are employing a strategic\ncombination of human expertise and zero-shot models, while also integrating\nhuman input at various stages for continuous feedback.\n","authors":["Sharat Agarwal"],"pdf_url":"https://arxiv.org/pdf/2411.01925v2.pdf","comment":"ICVGIP, Young Researchers Symposium"},{"id":"http://arxiv.org/abs/2411.13042v1","updated":"2024-11-20T05:16:31Z","published":"2024-11-20T05:16:31Z","title":"Attentive Contextual Attention for Cloud Removal","summary":"  Cloud cover can significantly hinder the use of remote sensing images for\nEarth observation, prompting urgent advancements in cloud removal technology.\nRecently, deep learning strategies have shown strong potential in restoring\ncloud-obscured areas. These methods utilize convolution to extract intricate\nlocal features and attention mechanisms to gather long-range information,\nimproving the overall comprehension of the scene. However, a common drawback of\nthese approaches is that the resulting images often suffer from blurriness,\nartifacts, and inconsistencies. This is partly because attention mechanisms\napply weights to all features based on generalized similarity scores, which can\ninadvertently introduce noise and irrelevant details from cloud-covered areas.\nTo overcome this limitation and better capture relevant distant context, we\nintroduce a novel approach named Attentive Contextual Attention (AC-Attention).\nThis method enhances conventional attention mechanisms by dynamically learning\ndata-driven attentive selection scores, enabling it to filter out noise and\nirrelevant features effectively. By integrating the AC-Attention module into\nthe DSen2-CR cloud removal framework, we significantly improve the model's\nability to capture essential distant information, leading to more effective\ncloud removal. Our extensive evaluation of various datasets shows that our\nmethod outperforms existing ones regarding image reconstruction quality.\nAdditionally, we conducted ablation studies by integrating AC-Attention into\nmultiple existing methods and widely used network architectures. These studies\ndemonstrate the effectiveness and adaptability of AC-Attention and reveal its\nability to focus on relevant features, thereby improving the overall\nperformance of the networks. The code is available at\n\\url{https://github.com/huangwenwenlili/ACA-CRNet}.\n","authors":["Wenli Huang","Ye Deng","Yang Wu","Jinjun Wang"],"pdf_url":"https://arxiv.org/pdf/2411.13042v1.pdf","comment":"13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2411.13040v1","updated":"2024-11-20T05:10:48Z","published":"2024-11-20T05:10:48Z","title":"RobustFormer: Noise-Robust Pre-training for images and videos","summary":"  While deep learning models are powerful tools that revolutionized many areas,\nthey are also vulnerable to noise as they rely heavily on learning patterns and\nfeatures from the exact details of the clean data. Transformers, which have\nbecome the backbone of modern vision models, are no exception. Current Discrete\nWavelet Transforms (DWT) based methods do not benefit from masked autoencoder\n(MAE) pre-training since the inverse DWT (iDWT) introduced in these approaches\nis computationally inefficient and lacks compatibility with video inputs in\ntransformer architectures.\n  In this work, we present RobustFormer, a method that overcomes these\nlimitations by enabling noise-robust pre-training for both images and videos;\nimproving the efficiency of DWT-based methods by removing the need for\ncomputationally iDWT steps and simplifying the attention mechanism. To our\nknowledge, the proposed method is the first DWT-based method compatible with\nvideo inputs and masked pre-training. Our experiments show that MAE-based\npre-training allows us to bypass the iDWT step, greatly reducing computation.\nThrough extensive tests on benchmark datasets, RobustFormer achieves\nstate-of-the-art results for both image and video tasks.\n","authors":["Ashish Bastola","Nishant Luitel","Hao Wang","Danda Pani Paudel","Roshani Poudel","Abolfazl Razi"],"pdf_url":"https://arxiv.org/pdf/2411.13040v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2411.12279v2","updated":"2024-11-20T05:05:48Z","published":"2024-11-19T06:57:45Z","title":"HouseLLM: LLM-Assisted Two-Phase Text-to-Floorplan Generation","summary":"  This paper proposes a two-phase text-to-floorplan generation method, which\nguides a Large Language Model (LLM) to generate an initial layout (Layout-LLM)\nand refines them into the final floorplans through conditional diffusion model.\nWe incorporate a Chain-of-Thought approach to prompt the LLM based on user text\nspecifications, enabling a more user-friendly and intuitive house layout\ndesign. This method allows users to describe their needs in natural language,\nenhancing accessibility and providing clearer geometric constraints. The final\nfloorplans generated by Layout-LLM through conditional diffusion refinement are\nmore accurate and better meet user requirements. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance across all\nmetrics, validating its effectiveness in practical home design applications. We\nplan to release our code for public use.\n","authors":["Ziyang Zong","Zhaohuan Zhan","Guang Tan"],"pdf_url":"https://arxiv.org/pdf/2411.12279v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13036v1","updated":"2024-11-20T04:56:19Z","published":"2024-11-20T04:56:19Z","title":"Unsupervised Homography Estimation on Multimodal Image Pair via\n  Alternating Optimization","summary":"  Estimating the homography between two images is crucial for mid- or\nhigh-level vision tasks, such as image stitching and fusion. However, using\nsupervised learning methods is often challenging or costly due to the\ndifficulty of collecting ground-truth data. In response, unsupervised learning\napproaches have emerged. Most early methods, though, assume that the given\nimage pairs are from the same camera or have minor lighting differences.\nConsequently, while these methods perform effectively under such conditions,\nthey generally fail when input image pairs come from different domains,\nreferred to as multimodal image pairs. To address these limitations, we propose\nAltO, an unsupervised learning framework for estimating homography in\nmultimodal image pairs. Our method employs a two-phase alternating optimization\nframework, similar to Expectation-Maximization (EM), where one phase reduces\nthe geometry gap and the other addresses the modality gap. To handle these\ngaps, we use Barlow Twins loss for the modality gap and propose an extended\nversion, Geometry Barlow Twins, for the geometry gap. As a result, we\ndemonstrate that our method, AltO, can be trained on multimodal datasets\nwithout any ground-truth data. It not only outperforms other unsupervised\nmethods but is also compatible with various architectures of homography\nestimators. The source code can be found\nat:~\\url{https://github.com/songsang7/AltO}\n","authors":["Sanghyeob Song","Jaihyun Lew","Hyemi Jang","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2411.13036v1.pdf","comment":"This paper is accepted to the Thirty-Eighth Annual Conference on\n  Neural Information Processing Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2411.13033v1","updated":"2024-11-20T04:43:37Z","published":"2024-11-20T04:43:37Z","title":"LMM-driven Semantic Image-Text Coding for Ultra Low-bitrate Learned\n  Image Compression","summary":"  Supported by powerful generative models, low-bitrate learned image\ncompression (LIC) models utilizing perceptual metrics have become feasible.\nSome of the most advanced models achieve high compression rates and superior\nperceptual quality by using image captions as sub-information. This paper\ndemonstrates that using a large multi-modal model (LMM), it is possible to\ngenerate captions and compress them within a single model. We also propose a\nnovel semantic-perceptual-oriented fine-tuning method applicable to any LIC\nnetwork, resulting in a 41.58\\% improvement in LPIPS BD-rate compared to\nexisting methods. Our implementation and pre-trained weights are available at\nhttps://github.com/tokkiwa/ImageTextCoding.\n","authors":["Shimon Murai","Heming Sun","Jiro Katto"],"pdf_url":"https://arxiv.org/pdf/2411.13033v1.pdf","comment":"IEEE VCIP 2024 poster"},{"id":"http://arxiv.org/abs/2411.12440v2","updated":"2024-11-20T04:27:10Z","published":"2024-11-19T11:59:54Z","title":"Beyond Gaussians: Fast and High-Fidelity 3D Splatting with Linear\n  Kernels","summary":"  Recent advancements in 3D Gaussian Splatting (3DGS) have substantially\nimproved novel view synthesis, enabling high-quality reconstruction and\nreal-time rendering. However, blurring artifacts, such as floating primitives\nand over-reconstruction, remain challenging. Current methods address these\nissues by refining scene structure, enhancing geometric representations,\naddressing blur in training images, improving rendering consistency, and\noptimizing density control, yet the role of kernel design remains\nunderexplored. We identify the soft boundaries of Gaussian ellipsoids as one of\nthe causes of these artifacts, limiting detail capture in high-frequency\nregions. To bridge this gap, we introduce 3D Linear Splatting (3DLS), which\nreplaces Gaussian kernels with linear kernels to achieve sharper and more\nprecise results, particularly in high-frequency regions. Through evaluations on\nthree datasets, 3DLS demonstrates state-of-the-art fidelity and accuracy, along\nwith a 30% FPS improvement over baseline 3DGS. The implementation will be made\npublicly available upon acceptance.\n","authors":["Haodong Chen","Runnan Chen","Qiang Qu","Zhaoqing Wang","Tongliang Liu","Xiaoming Chen","Yuk Ying Chung"],"pdf_url":"https://arxiv.org/pdf/2411.12440v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13026v1","updated":"2024-11-20T04:18:11Z","published":"2024-11-20T04:18:11Z","title":"X as Supervision: Contending with Depth Ambiguity in Unsupervised\n  Monocular 3D Pose Estimation","summary":"  Recent unsupervised methods for monocular 3D pose estimation have endeavored\nto reduce dependence on limited annotated 3D data, but most are solely\nformulated in 2D space, overlooking the inherent depth ambiguity issue. Due to\nthe information loss in 3D-to-2D projection, multiple potential depths may\nexist, yet only some of them are plausible in human structure. To tackle depth\nambiguity, we propose a novel unsupervised framework featuring a\nmulti-hypothesis detector and multiple tailored pretext tasks. The detector\nextracts multiple hypotheses from a heatmap within a local window, effectively\nmanaging the multi-solution problem. Furthermore, the pretext tasks harness 3D\nhuman priors from the SMPL model to regularize the solution space of pose\nestimation, aligning it with the empirical distribution of 3D human structures.\nThis regularization is partially achieved through a GCN-based discriminator\nwithin the discriminative learning, and is further complemented with synthetic\nimages through rendering, ensuring plausible estimations. Consequently, our\napproach demonstrates state-of-the-art unsupervised 3D pose estimation\nperformance on various human datasets. Further evaluations on data scale-up and\none animal dataset highlight its generalization capabilities. Code will be\navailable at https://github.com/Charrrrrlie/X-as-Supervision.\n","authors":["Yuchen Yang","Xuanyi Liu","Xing Gao","Zhihang Zhong","Xiao Sun"],"pdf_url":"https://arxiv.org/pdf/2411.13026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13025v1","updated":"2024-11-20T04:13:43Z","published":"2024-11-20T04:13:43Z","title":"ORID: Organ-Regional Information Driven Framework for Radiology Report\n  Generation","summary":"  The objective of Radiology Report Generation (RRG) is to automatically\ngenerate coherent textual analyses of diseases based on radiological images,\nthereby alleviating the workload of radiologists. Current AI-based methods for\nRRG primarily focus on modifications to the encoder-decoder model architecture.\nTo advance these approaches, this paper introduces an Organ-Regional\nInformation Driven (ORID) framework which can effectively integrate multi-modal\ninformation and reduce the influence of noise from unrelated organs.\nSpecifically, based on the LLaVA-Med, we first construct an RRG-related\ninstruction dataset to improve organ-regional diagnosis description ability and\nget the LLaVA-Med-RRG. After that, we propose an organ-based cross-modal fusion\nmodule to effectively combine the information from the organ-regional diagnosis\ndescription and radiology image. To further reduce the influence of noise from\nunrelated organs on the radiology report generation, we introduce an organ\nimportance coefficient analysis module, which leverages Graph Neural Network\n(GNN) to examine the interconnections of the cross-modal information of each\norgan region. Extensive experiments an1d comparisons with state-of-the-art\nmethods across various evaluation metrics demonstrate the superior performance\nof our proposed method.\n","authors":["Tiancheng Gu","Kaicheng Yang","Xiang An","Ziyong Feng","Dongnan Liu","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2411.13025v1.pdf","comment":"13 pages, 11 figures, WACV2025"},{"id":"http://arxiv.org/abs/2411.13024v1","updated":"2024-11-20T04:13:05Z","published":"2024-11-20T04:13:05Z","title":"Prior-based Objective Inference Mining Potential Uncertainty for Facial\n  Expression Recognition","summary":"  Annotation ambiguity caused by the inherent subjectivity of visual judgment\nhas always been a major challenge for Facial Expression Recognition (FER)\ntasks, particularly for largescale datasets from in-the-wild scenarios. A\npotential solution is the evaluation of relatively objective emotional\ndistributions to help mitigate the ambiguity of subjective annotations. To this\nend, this paper proposes a novel Prior-based Objective Inference (POI) network.\nThis network employs prior knowledge to derive a more objective and varied\nemotional distribution and tackles the issue of subjective annotation ambiguity\nthrough dynamic knowledge transfer. POI comprises two key networks: Firstly,\nthe Prior Inference Network (PIN) utilizes the prior knowledge of AUs and\nemotions to capture intricate motion details. To reduce over-reliance on priors\nand facilitate objective emotional inference, PIN aggregates inferential\nknowledge from various key facial subregions, encouraging mutual learning.\nSecondly, the Target Recognition Network (TRN) integrates subjective emotion\nannotations and objective inference soft labels provided by the PIN, fostering\nan understanding of inherent facial expression diversity, thus resolving\nannotation ambiguity. Moreover, we introduce an uncertainty estimation module\nto quantify and balance facial expression confidence. This module enables a\nflexible approach to dealing with the uncertainties of subjective annotations.\nExtensive experiments show that POI exhibits competitive performance on both\nsynthetic noisy datasets and multiple real-world datasets. All codes and\ntraining logs will be publicly available at https://github.com/liuhw01/POI.\n","authors":["Hanwei Liu","Huiling Cai","Qingcheng Lin","Xuefeng Li","Hui Xiao"],"pdf_url":"https://arxiv.org/pdf/2411.13024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13022v1","updated":"2024-11-20T03:53:41Z","published":"2024-11-20T03:53:41Z","title":"Training Physics-Driven Deep Learning Reconstruction without Raw Data\n  Access for Equitable Fast MRI","summary":"  Physics-driven deep learning (PD-DL) approaches have become popular for\nimproved reconstruction of fast magnetic resonance imaging (MRI) scans. Even\nthough PD-DL offers higher acceleration rates compared to existing clinical\nfast MRI techniques, their use has been limited outside specialized MRI\ncenters. One impediment for their deployment is the difficulties with\ngeneralization to pathologies or population groups that are not\nwell-represented in training sets. This has been noted in several studies, and\nfine-tuning on target populations to improve reconstruction has been suggested.\nHowever, current approaches for PD-DL training require access to raw k-space\nmeasurements, which is typically only available at specialized MRI centers that\nhave research agreements for such data access. This is especially an issue for\nrural and underserved areas, where commercial MRI scanners only provide access\nto a final reconstructed image. To tackle these challenges, we propose\nCompressibility-inspired Unsupervised Learning via Parallel Imaging Fidelity\n(CUPID) for high-quality PD-DL training, using only routine clinical\nreconstructed images exported from an MRI scanner. CUPID evaluates the goodness\nof the output with a compressibility-based approach, while ensuring that the\noutput stays consistent with the clinical parallel imaging reconstruction\nthrough well-designed perturbations. Our results show that CUPID achieves\nsimilar quality compared to well-established PD-DL training strategies that\nrequire raw k-space data access, while outperforming conventional compressed\nsensing (CS) and state-of-the-art generative methods. We also demonstrate its\neffectiveness in a zero-shot training setup for retrospectively and\nprospectively sub-sampled acquisitions, attesting to its minimal training\nburden.\n","authors":["Yaşar Utku Alçalar","Merve Gülle","Mehmet Akçakaya"],"pdf_url":"https://arxiv.org/pdf/2411.13022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13021v1","updated":"2024-11-20T03:53:32Z","published":"2024-11-20T03:53:32Z","title":"Chanel-Orderer: A Channel-Ordering Predictor for Tri-Channel Natural\n  Images","summary":"  This paper shows a proof-of-concept that, given a typical 3-channel images\nbut in a randomly permuted channel order, a model (termed as Chanel-Orderer)\nwith ad-hoc inductive biases in terms of both architecture and loss functions\ncan accurately predict the channel ordering and knows how to make it right.\nSpecifically, Chanel-Orderer learns to score each of the three channels with\nthe priors of object semantics and uses the resulting scores to predict the\nchannel ordering. This brings up benefits into a typical scenario where an\n\\texttt{RGB} image is often mis-displayed in the \\texttt{BGR} format and needs\nto be corrected into the right order. Furthermore, as a byproduct, the\nresulting model Chanel-Orderer is able to tell whether a given image is a\nnear-gray-scale image (near-monochromatic) or not (polychromatic). Our research\nsuggests that Chanel-Orderer mimics human visual coloring of our physical\nnatural world.\n","authors":["Shen Li","Lei Jiang","Wei Wang","Hongwei Hu","Liang Li"],"pdf_url":"https://arxiv.org/pdf/2411.13021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11187v3","updated":"2024-11-20T03:49:23Z","published":"2024-10-15T02:04:05Z","title":"Multiview Scene Graph","summary":"  A proper scene representation is central to the pursuit of spatial\nintelligence where agents can robustly reconstruct and efficiently understand\n3D scenes. A scene representation is either metric, such as landmark maps in 3D\nreconstruction, 3D bounding boxes in object detection, or voxel grids in\noccupancy prediction, or topological, such as pose graphs with loop closures in\nSLAM or visibility graphs in SfM. In this work, we propose to build Multiview\nScene Graphs (MSG) from unposed images, representing a scene topologically with\ninterconnected place and object nodes. The task of building MSG is challenging\nfor existing representation learning methods since it needs to jointly address\nboth visual place recognition, object detection, and object association from\nimages with limited fields of view and potentially large viewpoint changes. To\nevaluate any method tackling this task, we developed an MSG dataset and\nannotation based on a public 3D dataset. We also propose an evaluation metric\nbased on the intersection-over-union score of MSG edges. Moreover, we develop a\nnovel baseline method built on mainstream pretrained vision models, combining\nvisual place recognition and object association into one Transformer decoder\narchitecture. Experiments demonstrate that our method has superior performance\ncompared to existing relevant baselines.\n","authors":["Juexiao Zhang","Gao Zhu","Sihang Li","Xinhao Liu","Haorui Song","Xinran Tang","Chen Feng"],"pdf_url":"https://arxiv.org/pdf/2410.11187v3.pdf","comment":"NeurIPS 2024. Website at https://ai4ce.github.io/MSG/"},{"id":"http://arxiv.org/abs/2411.13019v1","updated":"2024-11-20T03:45:48Z","published":"2024-11-20T03:45:48Z","title":"Open-World Amodal Appearance Completion","summary":"  Understanding and reconstructing occluded objects is a challenging problem,\nespecially in open-world scenarios where categories and contexts are diverse\nand unpredictable. Traditional methods, however, are typically restricted to\nclosed sets of object categories, limiting their use in complex, open-world\nscenes. We introduce Open-World Amodal Appearance Completion, a training-free\nframework that expands amodal completion capabilities by accepting flexible\ntext queries as input. Our approach generalizes to arbitrary objects specified\nby both direct terms and abstract queries. We term this capability reasoning\namodal completion, where the system reconstructs the full appearance of the\nqueried object based on the provided image and language query. Our framework\nunifies segmentation, occlusion analysis, and inpainting to handle complex\nocclusions and generates completed objects as RGBA elements, enabling seamless\nintegration into applications such as 3D reconstruction and image editing.\nExtensive evaluations demonstrate the effectiveness of our approach in\ngeneralizing to novel objects and occlusions, establishing a new benchmark for\namodal completion in open-world settings. The code and datasets will be\nreleased after paper acceptance.\n","authors":["Jiayang Ao","Yanbei Jiang","Qiuhong Ke","Krista A. Ehinger"],"pdf_url":"https://arxiv.org/pdf/2411.13019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12319v2","updated":"2024-11-20T03:31:17Z","published":"2024-11-19T08:23:52Z","title":"CLIP Unreasonable Potential in Single-Shot Face Recognition","summary":"  Face recognition is a core task in computer vision designed to identify and\nauthenticate individuals by analyzing facial patterns and features. This field\nintersects with artificial intelligence image processing and machine learning\nwith applications in security authentication and personalization. Traditional\napproaches in facial recognition focus on capturing facial features like the\neyes, nose and mouth and matching these against a database to verify\nidentities. However challenges such as high false positive rates have persisted\noften due to the similarity among individuals facial features. Recently\nContrastive Language Image Pretraining (CLIP) a model developed by OpenAI has\nshown promising advancements by linking natural language processing with vision\ntasks allowing it to generalize across modalities. Using CLIP's vision language\ncorrespondence and single-shot finetuning the model can achieve lower false\npositive rates upon deployment without the need of mass facial features\nextraction. This integration demonstrating CLIP's potential to address\npersistent issues in face recognition model performance without complicating\nour training paradigm.\n","authors":["Nhan T. Luu"],"pdf_url":"https://arxiv.org/pdf/2411.12319v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13006v1","updated":"2024-11-20T03:03:49Z","published":"2024-11-20T03:03:49Z","title":"Automating Sonologists USG Commands with AI and Voice Interface","summary":"  This research presents an advanced AI-powered ultrasound imaging system that\nincorporates real-time image processing, organ tracking, and voice commands to\nenhance the efficiency and accuracy of diagnoses in clinical practice.\nTraditional ultrasound diagnostics often require significant time and introduce\na degree of subjectivity due to user interaction. The goal of this innovative\nsolution is to provide Sonologists with a more predictable and productive\nimaging procedure utilizing artificial intelligence, computer vision, and voice\ntechnology. The functionality of the system employs computer vision and deep\nlearning algorithms, specifically adopting the Mask R-CNN model from Detectron2\nfor semantic segmentation of organs and key landmarks. This automation improves\ndiagnostic accuracy by enabling the extraction of valuable information with\nminimal human input. Additionally, it includes a voice recognition feature that\nallows for hands-free operation, enabling users to control the system with\ncommands such as freeze or liver, all while maintaining their focus on the\npatient. The architecture comprises video processing and real-time segmentation\nmodules that prepare the system to perform essential imaging functions, such as\nfreezing and zooming in on frames. The liver histopathology module, optimized\nfor detecting fibrosis, achieved an impressive accuracy of 98.6%. Furthermore,\nthe organ segmentation module produces output confidence levels between 50% and\n95%, demonstrating its efficacy in organ detection.\n","authors":["Emad Mohamed","Shruti Tiwari","Sheena Christabel Pravin"],"pdf_url":"https://arxiv.org/pdf/2411.13006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13005v1","updated":"2024-11-20T03:02:51Z","published":"2024-11-20T03:02:51Z","title":"DT-LSD: Deformable Transformer-based Line Segment Detection","summary":"  Line segment detection is a fundamental low-level task in computer vision,\nand improvements in this task can impact more advanced methods that depend on\nit. Most new methods developed for line segment detection are based on\nConvolutional Neural Networks (CNNs). Our paper seeks to address challenges\nthat prevent the wider adoption of transformer-based methods for line segment\ndetection. More specifically, we introduce a new model called Deformable\nTransformer-based Line Segment Detection (DT-LSD) that supports cross-scale\ninteractions and can be trained quickly. This work proposes a novel Deformable\nTransformer-based Line Segment Detector (DT-LSD) that addresses LETR's\ndrawbacks. For faster training, we introduce Line Contrastive DeNoising (LCDN),\na technique that stabilizes the one-to-one matching process and speeds up\ntraining by 34$\\times$. We show that DT-LSD is faster and more accurate than\nits predecessor transformer-based model (LETR) and outperforms all CNN-based\nmodels in terms of accuracy. In the Wireframe dataset, DT-LSD achieves 71.7 for\n$sAP^{10}$ and 73.9 for $sAP^{15}$; while 33.2 for $sAP^{10}$ and 35.1 for\n$sAP^{15}$ in the YorkUrban dataset.\n","authors":["Sebastian Janampa","Marios Pattichis"],"pdf_url":"https://arxiv.org/pdf/2411.13005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10362v2","updated":"2024-11-20T02:58:25Z","published":"2024-03-15T14:53:31Z","title":"CPGA: Coding Priors-Guided Aggregation Network for Compressed Video\n  Quality Enhancement","summary":"  Recently, numerous approaches have achieved notable success in compressed\nvideo quality enhancement (VQE). However, these methods usually ignore the\nutilization of valuable coding priors inherently embedded in compressed videos,\nsuch as motion vectors and residual frames, which carry abundant temporal and\nspatial information. To remedy this problem, we propose the Coding\nPriors-Guided Aggregation (CPGA) network to utilize temporal and spatial\ninformation from coding priors. The CPGA mainly consists of an inter-frame\ntemporal aggregation (ITA) module and a multi-scale non-local aggregation (MNA)\nmodule. Specifically, the ITA module aggregates temporal information from\nconsecutive frames and coding priors, while the MNA module globally captures\nspatial information guided by residual frames. In addition, to facilitate\nresearch in VQE task, we newly construct the Video Coding Priors (VCP) dataset,\ncomprising 300 videos with various coding priors extracted from corresponding\nbitstreams. It remedies the shortage of previous datasets on the lack of coding\ninformation. Experimental results demonstrate the superiority of our method\ncompared to existing state-of-the-art methods. The code and dataset will be\nreleased at https://github.com/VQE-CPGA/CPGA.git .\n","authors":["Qiang Zhu","Jinhua Hao","Yukang Ding","Yu Liu","Qiao Mo","Ming Sun","Chao Zhou","Shuyuan Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.10362v2.pdf","comment":"11 pages, 8 figures, 6 tables"},{"id":"http://arxiv.org/abs/2411.07976v4","updated":"2024-11-20T02:57:56Z","published":"2024-11-12T17:55:39Z","title":"DINO-LG: A Task-Specific DINO Model for Coronary Calcium Scoring","summary":"  Coronary artery disease (CAD), one of the most common cause of mortality in\nthe world. Coronary artery calcium (CAC) scoring using computed tomography (CT)\nis key for risk assessment to prevent coronary disease. Previous studies on\nrisk assessment and calcification detection in CT scans primarily use\napproaches based on UNET architecture, frequently implemented on pre-built\nmodels. However, these models are limited by the availability of annotated CT\nscans containing CAC and suffering from imbalanced dataset, decreasing\nperformance of CAC segmentation and scoring. In this study, we extend this\napproach by incorporating the self-supervised learning (SSL) technique of DINO\n(self-distillation with no labels) to eliminate limitations of scarce annotated\ndata in CT scans. The DINO model's ability to train without requiring CAC area\nannotations enhances its robustness in generating distinct features. The DINO\nmodel is trained on to focus specifically on calcified areas by using labels,\naiming to generate features that effectively capture and highlight key\ncharacteristics. The label-guided DINO (DINO-LG) enhances classification by\ndistinguishing CT slices that contain calcification from those that do not,\nperforming 57% better than the standard DINO model in this task. CAC scoring\nand segmentation tasks are performed by a basic U-NET architecture, fed\nspecifically with CT slices containing calcified areas as identified by the\nDINO-LG model. This targeted identification performed by DINO-LG model improves\nCAC segmentation performance by approximately 10% and significant increase in\nCAC scoring accuracy.\n","authors":["Mahmut S. Gokmen","Caner Ozcan","Cody Bumgardner"],"pdf_url":"https://arxiv.org/pdf/2411.07976v4.pdf","comment":"Developed by Center for Applied Artificial Intelligence (CAAI),\n  University of Kentucky"},{"id":"http://arxiv.org/abs/2411.13001v1","updated":"2024-11-20T02:57:35Z","published":"2024-11-20T02:57:35Z","title":"Collaborative Feature-Logits Contrastive Learning for Open-Set\n  Semi-Supervised Object Detection","summary":"  Current Semi-Supervised Object Detection (SSOD) methods enhance detector\nperformance by leveraging large amounts of unlabeled data, assuming that both\nlabeled and unlabeled data share the same label space. However, in open-set\nscenarios, the unlabeled dataset contains both in-distribution (ID) classes and\nout-of-distribution (OOD) classes. Applying semi-supervised detectors in such\nsettings can lead to misclassifying OOD class as ID classes. To alleviate this\nissue, we propose a simple yet effective method, termed Collaborative\nFeature-Logits Detector (CFL-Detector). Specifically, we introduce a\nfeature-level clustering method using contrastive loss to clarify vector\nboundaries in the feature space and highlight class differences. Additionally,\nby optimizing the logits-level uncertainty classification loss, the model\nenhances its ability to effectively distinguish between ID and OOD classes.\nExtensive experiments demonstrate that our method achieves state-of-the-art\nperformance compared to existing methods.\n","authors":["Xinhao Zhong","Siyu Jiao","Yao Zhao","Yunchao Wei"],"pdf_url":"https://arxiv.org/pdf/2411.13001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.09209v3","updated":"2024-11-20T02:56:02Z","published":"2024-11-14T06:13:05Z","title":"JoyVASA: Portrait and Animal Image Animation with Diffusion-Based\n  Audio-Driven Facial Dynamics and Head Motion Generation","summary":"  Audio-driven portrait animation has made significant advances with\ndiffusion-based models, improving video quality and lipsync accuracy. However,\nthe increasing complexity of these models has led to inefficiencies in training\nand inference, as well as constraints on video length and inter-frame\ncontinuity. In this paper, we propose JoyVASA, a diffusion-based method for\ngenerating facial dynamics and head motion in audio-driven facial animation.\nSpecifically, in the first stage, we introduce a decoupled facial\nrepresentation framework that separates dynamic facial expressions from static\n3D facial representations. This decoupling allows the system to generate longer\nvideos by combining any static 3D facial representation with dynamic motion\nsequences. Then, in the second stage, a diffusion transformer is trained to\ngenerate motion sequences directly from audio cues, independent of character\nidentity. Finally, a generator trained in the first stage uses the 3D facial\nrepresentation and the generated motion sequences as inputs to render\nhigh-quality animations. With the decoupled facial representation and the\nidentity-independent motion generation process, JoyVASA extends beyond human\nportraits to animate animal faces seamlessly. The model is trained on a hybrid\ndataset of private Chinese and public English data, enabling multilingual\nsupport. Experimental results validate the effectiveness of our approach.\nFuture work will focus on improving real-time performance and refining\nexpression control, further expanding the applications in portrait animation.\nThe code is available at: https://github.com/jdh-algo/JoyVASA.\n","authors":["Xuyang Cao","Guoxin Wang","Sheng Shi","Jun Zhao","Yang Yao","Jintao Fei","Minyu Gao"],"pdf_url":"https://arxiv.org/pdf/2411.09209v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12560v2","updated":"2024-11-20T02:51:08Z","published":"2024-11-19T15:23:59Z","title":"Topological Symmetry Enhanced Graph Convolution for Skeleton-Based\n  Action Recognition","summary":"  Skeleton-based action recognition has achieved remarkable performance with\nthe development of graph convolutional networks (GCNs). However, most of these\nmethods tend to construct complex topology learning mechanisms while neglecting\nthe inherent symmetry of the human body. Additionally, the use of temporal\nconvolutions with certain fixed receptive fields limits their capacity to\neffectively capture dependencies in time sequences. To address the issues, we\n(1) propose a novel Topological Symmetry Enhanced Graph Convolution (TSE-GC) to\nenable distinct topology learning across different channel partitions while\nincorporating topological symmetry awareness and (2) construct a Multi-Branch\nDeformable Temporal Convolution (MBDTC) for skeleton-based action recognition.\nThe proposed TSE-GC emphasizes the inherent symmetry of the human body while\nenabling efficient learning of dynamic topologies. Meanwhile, the design of\nMBDTC introduces the concept of deformable modeling, leading to more flexible\nreceptive fields and stronger modeling capacity of temporal dependencies.\nCombining TSE-GC with MBDTC, our final model, TSE-GCN, achieves competitive\nperformance with fewer parameters compared with state-of-the-art methods on\nthree large datasets, NTU RGB+D, NTU RGB+D 120, and NW-UCLA. On the\ncross-subject and cross-set evaluations of NTU RGB+D 120, the accuracies of our\nmodel reach 90.0\\% and 91.1\\%, with 1.1M parameters and 1.38 GFLOPS for one\nstream.\n","authors":["Zeyu Liang","Hailun Xia","Naichuan Zheng","Huan Xu"],"pdf_url":"https://arxiv.org/pdf/2411.12560v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.09315v3","updated":"2024-11-20T02:48:31Z","published":"2022-08-19T12:59:46Z","title":"Self-Supervised Place Recognition by Refining Temporal and Featural\n  Pseudo Labels from Panoramic Data","summary":"  Visual place recognition (VPR) using deep networks has achieved\nstate-of-the-art performance. However, most of them require a training set with\nground truth sensor poses to obtain positive and negative samples of each\nobservation's spatial neighborhood for supervised learning. When such\ninformation is unavailable, temporal neighborhoods from a sequentially\ncollected data stream could be exploited for self-supervised training, although\nwe find its performance suboptimal. Inspired by noisy label learning, we\npropose a novel self-supervised framework named TF-VPR that uses temporal\nneighborhoods and learnable feature neighborhoods to discover unknown spatial\nneighborhoods. Our method follows an iterative training paradigm which\nalternates between: (1) representation learning with data augmentation, (2)\npositive set expansion to include the current feature space neighbors, and (3)\npositive set contraction via geometric verification. We conduct auto-labeling\nand generalization tests on both simulated and real datasets, with either RGB\nimages or point clouds as inputs. The results show that our method outperforms\nself-supervised baselines in recall rate, robustness, and heading diversity, a\nnovel metric we propose for VPR. Our code and datasets can be found at\nhttps://ai4ce.github.io/TF-VPR/\n","authors":["Chao Chen","Zegang Cheng","Xinhao Liu","Yiming Li","Li Ding","Ruoyu Wang","Chen Feng"],"pdf_url":"https://arxiv.org/pdf/2208.09315v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.11367v2","updated":"2024-11-20T02:47:25Z","published":"2023-05-19T01:06:08Z","title":"Smart Pressure e-Mat for Human Sleeping Posture and Dynamic Activity\n  Recognition","summary":"  With the emphasis on healthcare, early childhood education, and fitness,\nnon-invasive measurement and recognition methods have received more attention.\nPressure sensing has been extensively studied because of its advantages of\nsimple structure, easy access, visualization application, and harmlessness.\nThis paper introduces a Smart Pressure e-Mat (SPeM) system based on\npiezoresistive material, Velostat, for human monitoring applications, including\nrecognition of sleeping postures, sports, and yoga. After a subsystem scans the\ne-mat readings and processes the signal, it generates a pressure image stream.\nDeep neural networks (DNNs) are used to fit and train the pressure image stream\nand recognize the corresponding human behavior. Four sleeping postures and 13\ndynamic activities inspired by Nintendo Switch Ring Fit Adventure (RFA) are\nused as a preliminary validation of the proposed SPeM system. The SPeM system\nachieves high accuracies in both applications, demonstrating the high accuracy\nand generalizability of the models. Compared with other pressure sensor-based\nsystems, SPeM possesses more flexible applications and commercial application\nprospects, with reliable, robust, and repeatable properties.\n","authors":["Liangqi Yuan","Yuan Wei","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2305.11367v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10318v2","updated":"2024-11-20T02:37:27Z","published":"2024-10-14T09:24:48Z","title":"QIANets: Quantum-Integrated Adaptive Networks for Reduced Latency and\n  Improved Inference Times in CNN Models","summary":"  Convolutional neural networks (CNNs) have made significant advances in\ncomputer vision tasks, yet their high inference times and latency often limit\nreal-world applicability. While model compression techniques have gained\npopularity as solutions, they often overlook the critical balance between low\nlatency and uncompromised accuracy. By harnessing quantum-inspired pruning,\ntensor decomposition, and annealing-based matrix factorization - three\nquantum-inspired concepts - we introduce QIANets: a novel approach of\nredesigning the traditional GoogLeNet, DenseNet, and ResNet-18 model\narchitectures to process more parameters and computations whilst maintaining\nlow inference times. Despite experimental limitations, the method was tested\nand evaluated, demonstrating reductions in inference times, along with\neffective accuracy preservations.\n","authors":["Zhumazhan Balapanov","Vanessa Matvei","Olivia Holmberg","Edward Magongo","Jonathan Pei","Kevin Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.10318v2.pdf","comment":"Accepted to NeurIPS 2024 workshop on Neural Compression"},{"id":"http://arxiv.org/abs/2411.11305v2","updated":"2024-11-20T02:24:26Z","published":"2024-11-18T06:01:00Z","title":"TP-UNet: Temporal Prompt Guided UNet for Medical Image Segmentation","summary":"  The advancement of medical image segmentation techniques has been propelled\nby the adoption of deep learning techniques, particularly UNet-based\napproaches, which exploit semantic information to improve the accuracy of\nsegmentations. However, the order of organs in scanned images has been\ndisregarded by current medical image segmentation approaches based on UNet.\nFurthermore, the inherent network structure of UNet does not provide direct\ncapabilities for integrating temporal information. To efficiently integrate\ntemporal information, we propose TP-UNet that utilizes temporal prompts,\nencompassing organ-construction relationships, to guide the segmentation UNet\nmodel. Specifically, our framework is featured with cross-attention and\nsemantic alignment based on unsupervised contrastive learning to combine\ntemporal prompts and image features effectively. Extensive evaluations on two\nmedical image segmentation datasets demonstrate the state-of-the-art\nperformance of TP-UNet. Our implementation will be open-sourced after\nacceptance.\n","authors":["Ranmin Wang","Limin Zhuang","Hongkun Chen","Boyan Xu","Ruichu Cai"],"pdf_url":"https://arxiv.org/pdf/2411.11305v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12981v1","updated":"2024-11-20T02:15:23Z","published":"2024-11-20T02:15:23Z","title":"GazeGaussian: High-Fidelity Gaze Redirection with 3D Gaussian Splatting","summary":"  Gaze estimation encounters generalization challenges when dealing with\nout-of-distribution data. To address this problem, recent methods use neural\nradiance fields (NeRF) to generate augmented data. However, existing methods\nbased on NeRF are computationally expensive and lack facial details. 3D\nGaussian Splatting (3DGS) has become the prevailing representation of neural\nfields. While 3DGS has been extensively examined in head avatars, it faces\nchallenges with accurate gaze control and generalization across different\nsubjects. In this work, we propose GazeGaussian, a high-fidelity gaze\nredirection method that uses a two-stream 3DGS model to represent the face and\neye regions separately. By leveraging the unstructured nature of 3DGS, we\ndevelop a novel eye representation for rigid eye rotation based on the target\ngaze direction. To enhance synthesis generalization across various subjects, we\nintegrate an expression-conditional module to guide the neural renderer.\nComprehensive experiments show that GazeGaussian outperforms existing methods\nin rendering speed, gaze redirection accuracy, and facial synthesis across\nmultiple datasets. We also demonstrate that existing gaze estimation methods\ncan leverage GazeGaussian to improve their generalization performance. The code\nwill be available at: https://ucwxb.github.io/GazeGaussian/.\n","authors":["Xiaobao Wei","Peng Chen","Guangyu Li","Ming Lu","Hui Chen","Feng Tian"],"pdf_url":"https://arxiv.org/pdf/2411.12981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12980v1","updated":"2024-11-20T02:14:07Z","published":"2024-11-20T02:14:07Z","title":"LaVida Drive: Vision-Text Interaction VLM for Autonomous Driving with\n  Token Selection, Recovery and Enhancement","summary":"  Recent advancements in Visual Language Models (VLMs) have made them crucial\nfor visual question answering (VQA) in autonomous driving, enabling natural\nhuman-vehicle interactions. However, existing methods often struggle in dynamic\ndriving environments, as they usually focus on static images or videos and rely\non downsampling to manage computational costs. This results in the loss of\ncritical details and the difficulty in effectively integrating spatial and\ntemporal information, undermining fine-grained perception and temporal\ncoherence essential for effective decision-making. To tackle these challenges,\nwe introduce LaVida Drive, a novel and efficient VQA framework for autonomous\ndriving. LaVida Drive seamlessly integrates temporal data while maintaining\nhigh-resolution inputs for detailed visual perception. It optimizes spatial\nprocessing by retaining high-resolution data for intricate details and using\nlower-resolution inputs for temporal analysis to focus on motion-related\nfeatures, thereby boosting computational efficiency. The core of LaVida Drive\nconsists of two modules: the \\textit{Query-aware Token Selection} module and\nthe \\textit{Spatial-Temporal Token Recovery and Enhancement} module. The former\ndynamically selects the most relevant visual tokens based on semantic alignment\nwith the input query, reducing the token count from high-resolution spatial\ninput. The latter ensures smooth and coherent interactions between spatial and\ntemporal information, preserving contextual continuity across frames. Extensive\nexperiments on various autonomous driving question-answering benchmarks show\nthat LaVida Drive significantly reduces visual tokens, enhances efficiency, and\nimproves overall performance.\n","authors":["Siwen Jiao","Yangyi Fang"],"pdf_url":"https://arxiv.org/pdf/2411.12980v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03013v2","updated":"2024-11-20T01:24:53Z","published":"2024-11-05T11:25:19Z","title":"CRT-Fusion: Camera, Radar, Temporal Fusion Using Motion Information for\n  3D Object Detection","summary":"  Accurate and robust 3D object detection is a critical component in autonomous\nvehicles and robotics. While recent radar-camera fusion methods have made\nsignificant progress by fusing information in the bird's-eye view (BEV)\nrepresentation, they often struggle to effectively capture the motion of\ndynamic objects, leading to limited performance in real-world scenarios. In\nthis paper, we introduce CRT-Fusion, a novel framework that integrates temporal\ninformation into radar-camera fusion to address this challenge. Our approach\ncomprises three key modules: Multi-View Fusion (MVF), Motion Feature Estimator\n(MFE), and Motion Guided Temporal Fusion (MGTF). The MVF module fuses radar and\nimage features within both the camera view and bird's-eye view, thereby\ngenerating a more precise unified BEV representation. The MFE module conducts\ntwo simultaneous tasks: estimation of pixel-wise velocity information and BEV\nsegmentation. Based on the velocity and the occupancy score map obtained from\nthe MFE module, the MGTF module aligns and fuses feature maps across multiple\ntimestamps in a recurrent manner. By considering the motion of dynamic objects,\nCRT-Fusion can produce robust BEV feature maps, thereby improving detection\naccuracy and robustness. Extensive evaluations on the challenging nuScenes\ndataset demonstrate that CRT-Fusion achieves state-of-the-art performance for\nradar-camera-based 3D object detection. Our approach outperforms the previous\nbest method in terms of NDS by +1.7%, while also surpassing the leading\napproach in mAP by +1.4%. These significant improvements in both metrics\nshowcase the effectiveness of our proposed fusion strategy in enhancing the\nreliability and accuracy of 3D object detection.\n","authors":["Jisong Kim","Minjae Seong","Jun Won Choi"],"pdf_url":"https://arxiv.org/pdf/2411.03013v2.pdf","comment":"Accepted at NeurIPS2024"},{"id":"http://arxiv.org/abs/2308.07298v3","updated":"2024-11-20T01:20:02Z","published":"2023-08-14T17:36:39Z","title":"Accurate Eye Tracking from Dense 3D Surface Reconstructions using\n  Single-Shot Deflectometry","summary":"  Eye-tracking plays a crucial role in the development of virtual reality\ndevices, neuroscience research, and psychology. Despite its significance in\nnumerous applications, achieving an accurate, robust, and fast eye-tracking\nsolution remains a considerable challenge for current state-of-the-art methods.\nWhile existing reflection-based techniques (e.g., \"glint tracking\") are\nconsidered to be very accurate, their performance is limited by their reliance\non sparse 3D surface data acquired solely from the cornea surface. In this\npaper, we rethink the way how specular reflections can be used for eye\ntracking: We propose a novel method for accurate and fast evaluation of the\ngaze direction that exploits teachings from single-shot\nphase-measuring-deflectometry(PMD). In contrast to state-of-the-art\nreflection-based methods, our method acquires dense 3D surface information of\nboth cornea and sclera within only one single camera frame (single-shot). For a\ntypical measurement, we acquire $>3000 \\times$ more surface reflection points\n(\"glints\") than conventional methods. We show the feasibility of our approach\nwith experimentally evaluated gaze errors on a realistic model eye below only\n$0.12^\\circ$. Moreover, we demonstrate quantitative measurements on real human\neyes in vivo, reaching accuracy values between only $0.46^\\circ$ and\n$0.97^\\circ$.\n","authors":["Jiazhang Wang","Tianfu Wang","Bingjie Xu","Oliver Cossairt","Florian Willomitzer"],"pdf_url":"https://arxiv.org/pdf/2308.07298v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12951v1","updated":"2024-11-20T00:47:17Z","published":"2024-11-20T00:47:17Z","title":"On the Consistency of Video Large Language Models in Temporal\n  Comprehension","summary":"  Video large language models (Video-LLMs) can temporally ground language\nqueries and retrieve video moments. Yet, such temporal comprehension\ncapabilities are neither well-studied nor understood. So we conduct a study on\nprediction consistency -- a key indicator for robustness and trustworthiness of\ntemporal grounding. After the model identifies an initial moment within the\nvideo content, we apply a series of probes to check if the model's responses\nalign with this initial grounding as an indicator of reliable comprehension.\nOur results reveal that current Video-LLMs are sensitive to variations in video\ncontents, language queries, and task settings, unveiling severe deficiencies in\nmaintaining consistency. We further explore common prompting and\ninstruction-tuning methods as potential solutions, but find that their\nimprovements are often unstable. To that end, we propose event temporal\nverification tuning that explicitly accounts for consistency, and demonstrate\nsignificant improvements for both grounding and consistency. Our data and code\nwill be available at https://github.com/minjoong507/Consistency-of-Video-LLM.\n","authors":["Minjoon Jung","Junbin Xiao","Byoung-Tak Zhang","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2411.12951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11016v2","updated":"2024-11-20T00:30:01Z","published":"2024-11-17T09:39:50Z","title":"Time Step Generating: A Universal Synthesized Deepfake Image Detector","summary":"  Currently, high-fidelity text-to-image models are developed in an\naccelerating pace. Among them, Diffusion Models have led to a remarkable\nimprovement in the quality of image generation, making it vary challenging to\ndistinguish between real and synthesized images. It simultaneously raises\nserious concerns regarding privacy and security. Some methods are proposed to\ndistinguish the diffusion model generated images through reconstructing.\nHowever, the inversion and denoising processes are time-consuming and heavily\nreliant on the pre-trained generative model. Consequently, if the pre-trained\ngenerative model meet the problem of out-of-domain, the detection performance\ndeclines. To address this issue, we propose a universal synthetic image\ndetector Time Step Generating (TSG), which does not rely on pre-trained models'\nreconstructing ability, specific datasets, or sampling algorithms. Our method\nutilizes a pre-trained diffusion model's network as a feature extractor to\ncapture fine-grained details, focusing on the subtle differences between real\nand synthetic images. By controlling the time step t of the network input, we\ncan effectively extract these distinguishing detail features. Then, those\nfeatures can be passed through a classifier (i.e. Resnet), which efficiently\ndetects whether an image is synthetic or real. We test the proposed TSG on the\nlarge-scale GenImage benchmark and it achieves significant improvements in both\naccuracy and generalizability.\n","authors":["Ziyue Zeng","Haoyuan Liu","Dingjie Peng","Luoxu Jing","Hiroshi Watanabe"],"pdf_url":"https://arxiv.org/pdf/2411.11016v2.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2411.12943v1","updated":"2024-11-20T00:27:01Z","published":"2024-11-20T00:27:01Z","title":"Enhancing Thermal MOT: A Novel Box Association Method Leveraging Thermal\n  Identity and Motion Similarity","summary":"  Multiple Object Tracking (MOT) in thermal imaging presents unique challenges\ndue to the lack of visual features and the complexity of motion patterns. This\npaper introduces an innovative approach to improve MOT in the thermal domain by\ndeveloping a novel box association method that utilizes both thermal object\nidentity and motion similarity. Our method merges thermal feature sparsity and\ndynamic object tracking, enabling more accurate and robust MOT performance.\nAdditionally, we present a new dataset comprised of a large-scale collection of\nthermal and RGB images captured in diverse urban environments, serving as both\na benchmark for our method and a new resource for thermal imaging. We conduct\nextensive experiments to demonstrate the superiority of our approach over\nexisting methods, showing significant improvements in tracking accuracy and\nrobustness under various conditions. Our findings suggest that incorporating\nthermal identity with motion data enhances MOT performance. The newly collected\ndataset and source code is available at https://github.com/wassimea/thermalMOT\n","authors":["Wassim El Ahmar","Dhanvin Kolhatkar","Farzan Nowruzi","Robert Laganiere"],"pdf_url":"https://arxiv.org/pdf/2411.12943v1.pdf","comment":"Workshop on Towards a Complete Analysis of People, part of the\n  European Conference on Computer Vision (ECCV) 2024"},{"id":"http://arxiv.org/abs/2405.09530v2","updated":"2024-11-20T00:10:28Z","published":"2024-05-01T15:18:01Z","title":"A community palm model","summary":"  Palm oil production has been identified as one of the major drivers of\ndeforestation for tropical countries. To meet supply chain objectives,\ncommodity producers and other stakeholders need timely information of land\ncover dynamics in their supply shed. However, such data are difficult to obtain\nfrom suppliers who may lack digital geographic representations of their supply\nsheds and production locations. Here we present a \"community model,\" a machine\nlearning model trained on pooled data sourced from many different stakeholders,\nto produce a map of palm probability at global scale. An advantage of this\nmethod is the inclusion of varied inputs, the ability to easily update the\nmodel as new training data becomes available and run the model on any year that\ninput imagery is available. Inclusion of diverse data sources into one\nprobability map can help establish a shared understanding across stakeholders\non the presence and absence of a land cover or commodity (in this case oil\npalm). The model predictors are annual composites built from publicly available\nsatellite imagery provided by Sentinel-1, Sentinel-2, and ALOS-2, and terrain\ndata from Jaxa (AW3D30) and Copernicus (GLO-30). We provide map outputs as the\nprobability of palm in a given pixel, to reflect the uncertainty of the\nunderlying state (palm or not palm). This version of this model provides global\naccuracy estimated to be 92% (at 0.5 probability threshold) on an independent\ntest set. This model, and resulting oil palm probability map products are\nuseful for accurately identifying the geographic footprint of palm cultivation.\nUsed in conjunction with timely deforestation information, this palm model is\nuseful for understanding the risk of continued oil palm plantation expansion in\nsensitive forest areas.\n","authors":["Nicholas Clinton","Andreas Vollrath","Remi D'annunzio","Desheng Liu","Henry B. Glick","Adrià Descals","Alicia Sullivan","Oliver Guinan","Jacob Abramowitz","Fred Stolle","Chris Goodman","Tanya Birch","David Quinn","Olga Danylo","Tijs Lips","Daniel Coelho","Enikoe Bihari","Bryce Cronkite-Ratcliff","Ate Poortinga","Atena Haghighattalab","Evan Notman","Michael DeWitt","Aaron Yonas","Gennadii Donchyts","Devaja Shah","David Saah","Karis Tenneson","Nguyen Hanh Quyen","Megha Verma","Andrew Wilcox"],"pdf_url":"https://arxiv.org/pdf/2405.09530v2.pdf","comment":"v03"},{"id":"http://arxiv.org/abs/2411.13754v1","updated":"2024-11-20T23:39:54Z","published":"2024-11-20T23:39:54Z","title":"Learning to Reason Iteratively and Parallelly for Complex Visual\n  Reasoning Scenarios","summary":"  Complex visual reasoning and question answering (VQA) is a challenging task\nthat requires compositional multi-step processing and higher-level reasoning\ncapabilities beyond the immediate recognition and localization of objects and\nevents. Here, we introduce a fully neural Iterative and Parallel Reasoning\nMechanism (IPRM) that combines two distinct forms of computation -- iterative\nand parallel -- to better address complex VQA scenarios. Specifically, IPRM's\n\"iterative\" computation facilitates compositional step-by-step reasoning for\nscenarios wherein individual operations need to be computed, stored, and\nrecalled dynamically (e.g. when computing the query \"determine the color of pen\nto the left of the child in red t-shirt sitting at the white table\").\nMeanwhile, its \"parallel\" computation allows for the simultaneous exploration\nof different reasoning paths and benefits more robust and efficient execution\nof operations that are mutually independent (e.g. when counting individual\ncolors for the query: \"determine the maximum occurring color amongst all\nt-shirts\"). We design IPRM as a lightweight and fully-differentiable neural\nmodule that can be conveniently applied to both transformer and non-transformer\nvision-language backbones. It notably outperforms prior task-specific methods\nand transformer-based attention modules across various image and video VQA\nbenchmarks testing distinct complex reasoning capabilities such as\ncompositional spatiotemporal reasoning (AGQA), situational reasoning (STAR),\nmulti-hop reasoning generalization (CLEVR-Humans) and causal event linking\n(CLEVRER-Humans). Further, IPRM's internal computations can be visualized\nacross reasoning steps, aiding interpretability and diagnosis of its errors.\n","authors":["Shantanu Jaiswal","Debaditya Roy","Basura Fernando","Cheston Tan"],"pdf_url":"https://arxiv.org/pdf/2411.13754v1.pdf","comment":"NeurIPS 2024 camera ready; source code to be released at:\n  https://github.com/shantanuj/IPRM_Iterative_and_Parallel_Reasoning_Mechanism"},{"id":"http://arxiv.org/abs/2411.13753v1","updated":"2024-11-20T23:36:46Z","published":"2024-11-20T23:36:46Z","title":"FAST-Splat: Fast, Ambiguity-Free Semantics Transfer in Gaussian\n  Splatting","summary":"  We present FAST-Splat for fast, ambiguity-free semantic Gaussian Splatting,\nwhich seeks to address the main limitations of existing semantic Gaussian\nSplatting methods, namely: slow training and rendering speeds; high memory\nusage; and ambiguous semantic object localization. In deriving FAST-Splat , we\nformulate open-vocabulary semantic Gaussian Splatting as the problem of\nextending closed-set semantic distillation to the open-set (open-vocabulary)\nsetting, enabling FAST-Splat to provide precise semantic object localization\nresults, even when prompted with ambiguous user-provided natural-language\nqueries. Further, by exploiting the explicit form of the Gaussian Splatting\nscene representation to the fullest extent, FAST-Splat retains the remarkable\ntraining and rendering speeds of Gaussian Splatting. Specifically, while\nexisting semantic Gaussian Splatting methods distill semantics into a separate\nneural field or utilize neural models for dimensionality reduction, FAST-Splat\ndirectly augments each Gaussian with specific semantic codes, preserving the\ntraining, rendering, and memory-usage advantages of Gaussian Splatting over\nneural field methods. These Gaussian-specific semantic codes, together with a\nhash-table, enable semantic similarity to be measured with open-vocabulary user\nprompts and further enable FAST-Splat to respond with unambiguous semantic\nobject labels and 3D masks, unlike prior methods. In experiments, we\ndemonstrate that FAST-Splat is 4x to 6x faster to train with a 13x faster data\npre-processing step, achieves between 18x to 75x faster rendering speeds, and\nrequires about 3x smaller GPU memory, compared to the best-competing semantic\nGaussian Splatting methods. Further, FAST-Splat achieves relatively similar or\nbetter semantic segmentation performance compared to existing methods. After\nthe review period, we will provide links to the project website and the\ncodebase.\n","authors":["Ola Shorinwa","Jiankai Sun","Mac Schwager"],"pdf_url":"https://arxiv.org/pdf/2411.13753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10011v3","updated":"2024-11-20T23:27:25Z","published":"2023-09-18T07:04:40Z","title":"Universal Photorealistic Style Transfer: A Lightweight and Adaptive\n  Approach","summary":"  Photorealistic style transfer aims to apply stylization while preserving the\nrealism and structure of input content. However, existing methods often\nencounter challenges such as color tone distortions, dependency on pair-wise\npre-training, inefficiency with high-resolution inputs, and the need for\nadditional constraints in video style transfer tasks. To address these issues,\nwe propose a Universal Photorealistic Style Transfer (UPST) framework that\ndelivers accurate photorealistic style transfer on high-resolution images and\nvideos without relying on pre-training. Our approach incorporates a lightweight\nStyleNet for per-instance transfer, ensuring color tone accuracy while\nsupporting high-resolution inputs, maintaining rapid processing speeds, and\neliminating the need for pretraining. To further enhance photorealism and\nefficiency, we introduce instance-adaptive optimization, which features an\nadaptive coefficient to prioritize content image realism and employs early\nstopping to accelerate network convergence. Additionally, UPST enables seamless\nvideo style transfer without additional constraints due to its strong non-color\ninformation preservation ability. Experimental results show that UPST\nconsistently produces photorealistic outputs and significantly reduces GPU\nmemory usage, making it an effective and universal solution for various\nphotorealistic style transfer tasks.\n","authors":["Rong Liu","Enyu Zhao","Zhiyuan Liu","Andrew Feng","Scott John Easley"],"pdf_url":"https://arxiv.org/pdf/2309.10011v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18299v4","updated":"2024-11-20T23:23:40Z","published":"2024-05-28T15:51:18Z","title":"Deep Learning Innovations for Underwater Waste Detection: An In-Depth\n  Analysis","summary":"  Addressing the issue of submerged underwater trash is crucial for\nsafeguarding aquatic ecosystems and preserving marine life. While identifying\ndebris present on the surface of water bodies is straightforward, assessing the\nunderwater submerged waste is a challenge due to the image distortions caused\nby factors such as light refraction, absorption, suspended particles, color\nshifts, and occlusion. This paper conducts a comprehensive review of\nstate-of-the-art architectures and on the existing datasets to establish a\nbaseline for submerged waste and trash detection. The primary goal remains to\nestablish the benchmark of the object localization techniques to be leveraged\nby advanced underwater sensors and autonomous underwater vehicles. The ultimate\nobjective is to explore the underwater environment, to identify, and remove\nunderwater debris. The absence of benchmarks (dataset or algorithm) in many\nresearches emphasizes the need for a more robust algorithmic solution. Through\nthis research, we aim to give performance comparative analysis of various\nunderwater trash detection algorithms.\n","authors":["Jaskaran Singh Walia","Pavithra L K"],"pdf_url":"https://arxiv.org/pdf/2405.18299v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13731v1","updated":"2024-11-20T22:15:10Z","published":"2024-11-20T22:15:10Z","title":"Delta-Influence: Unlearning Poisons via Influence Functions","summary":"  Addressing data integrity challenges, such as unlearning the effects of data\npoisoning after model training, is necessary for the reliable deployment of\nmachine learning models. State-of-the-art influence functions, such as EK-FAC,\noften fail to accurately attribute abnormal model behavior to the specific\npoisoned training data responsible for the data poisoning attack. In addition,\ntraditional unlearning algorithms often struggle to effectively remove the\ninfluence of poisoned samples, particularly when only a few affected examples\ncan be identified. To address these challenge, we introduce $\\Delta$-Influence,\na novel approach that leverages influence functions to trace abnormal model\nbehavior back to the responsible poisoned training data using as little as just\none poisoned test example. $\\Delta$-Influence applies data transformations that\nsever the link between poisoned training data and compromised test points\nwithout significantly affecting clean data. This allows $\\Delta$-Influence to\ndetect large negative shifts in influence scores following data\ntransformations, a phenomenon we term as influence collapse, thereby accurately\nidentifying poisoned training data. Unlearning this subset, e.g. through\nretraining, effectively eliminates the data poisoning. We validate our method\nacross three vision-based poisoning attacks and three datasets, benchmarking\nagainst four detection algorithms and five unlearning strategies. We show that\n$\\Delta$-Influence consistently achieves the best unlearning across all\nsettings, showing the promise of influence functions for corrective unlearning.\nOur code is publicly available at:\n\\url{https://github.com/andyisokay/delta-influence}\n","authors":["Wenjie Li","Jiawei Li","Christian Schroeder de Witt","Ameya Prabhu","Amartya Sanyal"],"pdf_url":"https://arxiv.org/pdf/2411.13731v1.pdf","comment":"Accepted at NeurIPS Workshop on Attributing Model Behavior at Scale\n  (ATTRIB @ NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2311.15414v3","updated":"2024-11-20T22:14:07Z","published":"2023-11-26T20:35:19Z","title":"KOPPA: Improving Prompt-based Continual Learning with Key-Query\n  Orthogonal Projection and Prototype-based One-Versus-All","summary":"  Drawing inspiration from prompt tuning techniques applied to Large Language\nModels, recent methods based on pre-trained ViT networks have achieved\nremarkable results in the field of Continual Learning. Specifically, these\napproaches propose to maintain a set of prompts and allocate a subset of them\nto learn each task using a key-query matching strategy. However, they may\nencounter limitations when lacking control over the correlations between old\ntask queries and keys of future tasks, the shift of features in the latent\nspace, and the relative separation of latent vectors learned in independent\ntasks. In this work, we introduce a novel key-query learning strategy based on\northogonal projection, inspired by model-agnostic meta-learning, to enhance\nprompt matching efficiency and address the challenge of shifting features.\nFurthermore, we introduce a One-Versus-All (OVA) prototype-based component that\nenhances the classification head distinction. Experimental results on benchmark\ndatasets demonstrate that our method empowers the model to achieve results\nsurpassing those of current state-of-the-art approaches by a large margin of up\nto 20%.\n","authors":["Quyen Tran","Hoang Phan","Lam Tran","Khoat Than","Toan Tran","Dinh Phung","Trung Le"],"pdf_url":"https://arxiv.org/pdf/2311.15414v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13716v1","updated":"2024-11-20T21:27:13Z","published":"2024-11-20T21:27:13Z","title":"Developing Normative Gait Cycle Parameters for Clinical Analysis Using\n  Human Pose Estimation","summary":"  Gait analysis using computer vision is an emerging field in AI, offering\nclinicians an objective, multi-feature approach to analyse complex movements.\nDespite its promise, current applications using RGB video data alone are\nlimited in measuring clinically relevant spatial and temporal kinematics and\nestablishing normative parameters essential for identifying movement\nabnormalities within a gait cycle. This paper presents a data-driven method\nusing RGB video data and 2D human pose estimation for developing normative\nkinematic gait parameters. By analysing joint angles, an established kinematic\nmeasure in biomechanics and clinical practice, we aim to enhance gait analysis\ncapabilities and improve explainability. Our cycle-wise kinematic analysis\nenables clinicians to simultaneously measure and compare multiple joint angles,\nassessing individuals against a normative population using just monocular RGB\nvideo. This approach expands clinical capacity, supports objective\ndecision-making, and automates the identification of specific spatial and\ntemporal deviations and abnormalities within the gait cycle.\n","authors":["Rahm Ranjan","David Ahmedt-Aristizabal","Mohammad Ali Armin","Juno Kim"],"pdf_url":"https://arxiv.org/pdf/2411.13716v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2003.03653v4","updated":"2024-11-20T20:47:25Z","published":"2020-03-07T20:17:06Z","title":"SalsaNext: Fast, Uncertainty-aware Semantic Segmentation of LiDAR Point\n  Clouds for Autonomous Driving","summary":"  In this paper, we introduce SalsaNext for the uncertainty-aware semantic\nsegmentation of a full 3D LiDAR point cloud in real-time. SalsaNext is the next\nversion of SalsaNet [1] which has an encoder-decoder architecture where the\nencoder unit has a set of ResNet blocks and the decoder part combines upsampled\nfeatures from the residual blocks. In contrast to SalsaNet, we introduce a new\ncontext module, replace the ResNet encoder blocks with a new residual dilated\nconvolution stack with gradually increasing receptive fields and add the\npixel-shuffle layer in the decoder. Additionally, we switch from stride\nconvolution to average pooling and also apply central dropout treatment. To\ndirectly optimize the Jaccard index, we further combine the weighted\ncross-entropy loss with Lovasz-Softmax loss [2]. We finally inject a Bayesian\ntreatment to compute the epistemic and aleatoric uncertainties for each point\nin the cloud. We provide a thorough quantitative evaluation on the\nSemantic-KITTI dataset [3], which demonstrates that the proposed SalsaNext\noutperforms other state-of-the-art semantic segmentation networks and ranks\nfirst on the Semantic-KITTI leaderboard. We also release our source code\nhttps://github.com/TiagoCortinhal/SalsaNext.\n","authors":["Tiago Cortinhal","George Tzelepis","Eren Erdal Aksoy"],"pdf_url":"https://arxiv.org/pdf/2003.03653v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13697v1","updated":"2024-11-20T20:28:04Z","published":"2024-11-20T20:28:04Z","title":"Decompose and Leverage Preferences from Expert Models for Improving\n  Trustworthiness of MLLMs","summary":"  Multimodal Large Language Models (MLLMs) can enhance trustworthiness by\naligning with human preferences. As human preference labeling is laborious,\nrecent works employ evaluation models for assessing MLLMs' responses, using the\nmodel-based assessments to automate preference dataset construction. This\napproach, however, faces challenges with MLLMs' lengthy and compositional\nresponses, which often require diverse reasoning skills that a single\nevaluation model may not fully possess. Additionally, most existing methods\nrely on closed-source models as evaluators. To address limitations, we propose\nDecompGen, a decomposable framework that uses an ensemble of open-sourced\nexpert models. DecompGen breaks down each response into atomic verification\ntasks, assigning each task to an appropriate expert model to generate\nfine-grained assessments. The DecompGen feedback is used to automatically\nconstruct our preference dataset, DGPref. MLLMs aligned with DGPref via\npreference learning show improvements in trustworthiness, demonstrating the\neffectiveness of DecompGen.\n","authors":["Rui Cao","Yuming Jiang","Michael Schlichtkrull","Andreas Vlachos"],"pdf_url":"https://arxiv.org/pdf/2411.13697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13683v1","updated":"2024-11-20T20:00:38Z","published":"2024-11-20T20:00:38Z","title":"Extending Video Masked Autoencoders to 128 frames","summary":"  Video understanding has witnessed significant progress with recent video\nfoundation models demonstrating strong performance owing to self-supervised\npre-training objectives; Masked Autoencoders (MAE) being the design of choice.\nNevertheless, the majority of prior works that leverage MAE pre-training have\nfocused on relatively short video representations (16 / 32 frames in length)\nlargely due to hardware memory and compute limitations that scale poorly with\nvideo length due to the dense memory-intensive self-attention decoding. One\nnatural strategy to address these challenges is to subsample tokens to\nreconstruct during decoding (or decoder masking). In this work, we propose an\neffective strategy for prioritizing tokens which allows training on longer\nvideo sequences (128 frames) and gets better performance than, more typical,\nrandom and uniform masking strategies. The core of our approach is an adaptive\ndecoder masking strategy that prioritizes the most important tokens and uses\nquantized tokens as reconstruction objectives. Our adaptive strategy leverages\na powerful MAGVIT-based tokenizer that jointly learns the tokens and their\npriority. We validate our design choices through exhaustive ablations and\nobserve improved performance of the resulting long-video (128 frames) encoders\nover short-video (32 frames) counterparts. With our long-video masked\nautoencoder (LVMAE) strategy, we surpass state-of-the-art on Diving48 by 3.9\npoints and EPIC-Kitchens-100 verb classification by 2.5 points while relying on\na simple core architecture and video-only pre-training (unlike some of the\nprior works that require millions of labeled video-text pairs or specialized\nencoders).\n","authors":["Nitesh Bharadwaj Gundavarapu","Luke Friedman","Raghav Goyal","Chaitra Hegde","Eirikur Agustsson","Sagar M. Waghmare","Mikhail Sirotenko","Ming-Hsuan Yang","Tobias Weyand","Boqing Gong","Leonid Sigal"],"pdf_url":"https://arxiv.org/pdf/2411.13683v1.pdf","comment":"10.5 pages of main paper, 25 pages total, 4 figures and 10 tables. To\n  appear in NeurIPS'24"},{"id":"http://arxiv.org/abs/2402.00712v5","updated":"2024-11-20T19:57:16Z","published":"2024-02-01T16:07:12Z","title":"ChaosBench: A Multi-Channel, Physics-Based Benchmark for\n  Subseasonal-to-Seasonal Climate Prediction","summary":"  Accurate prediction of climate in the subseasonal-to-seasonal scale is\ncrucial for disaster preparedness and robust decision making amidst climate\nchange. Yet, forecasting beyond the weather timescale is challenging because it\ndeals with problems other than initial condition, including boundary\ninteraction, butterfly effect, and our inherent lack of physical understanding.\nAt present, existing benchmarks tend to have shorter forecasting range of up-to\n15 days, do not include a wide range of operational baselines, and lack\nphysics-based constraints for explainability. Thus, we propose ChaosBench, a\nchallenging benchmark to extend the predictability range of data-driven weather\nemulators to S2S timescale. First, ChaosBench is comprised of variables beyond\nthe typical surface-atmospheric ERA5 to also include ocean, ice, and land\nreanalysis products that span over 45 years to allow for full Earth system\nemulation that respects boundary conditions. We also propose physics-based, in\naddition to deterministic and probabilistic metrics, to ensure a\nphysically-consistent ensemble that accounts for butterfly effect. Furthermore,\nwe evaluate on a diverse set of physics-based forecasts from four national\nweather agencies as baselines to our data-driven counterpart such as\nViT/ClimaX, PanguWeather, GraphCast, and FourCastNetV2. Overall, we find\nmethods originally developed for weather-scale applications fail on S2S task:\ntheir performance simply collapse to an unskilled climatology. Nonetheless, we\noutline and demonstrate several strategies that can extend the predictability\nrange of existing weather emulators, including the use of ensembles, robust\ncontrol of error propagation, and the use of physics-informed models. Our\nbenchmark, datasets, and instructions are available at\nhttps://leap-stc.github.io/ChaosBench.\n","authors":["Juan Nathaniel","Yongquan Qu","Tung Nguyen","Sungduk Yu","Julius Busecke","Aditya Grover","Pierre Gentine"],"pdf_url":"https://arxiv.org/pdf/2402.00712v5.pdf","comment":"NeurIPS 2024 D&B Track (Oral)"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2407.15462v3","updated":"2024-11-20T18:30:19Z","published":"2024-07-22T08:19:34Z","title":"Retrieval with Learned Similarities","summary":"  Retrieval plays a fundamental role in recommendation systems, search, and\nnatural language processing (NLP) by efficiently finding relevant items from a\nlarge corpus given a query. Dot products have been widely used as the\nsimilarity function in such tasks, enabled by Maximum Inner Product Search\n(MIPS) algorithms for efficient retrieval. However, state-of-the-art retrieval\nalgorithms have migrated to learned similarities. These advanced approaches\nencompass multiple query embeddings, complex neural networks, direct item ID\ndecoding via beam search, and hybrid solutions. Unfortunately, we lack\nefficient solutions for retrieval in these state-of-the-art setups. Our work\naddresses this gap by investigating efficient retrieval techniques with\nexpressive learned similarity functions. We establish Mixture-of-Logits (MoL)\nas a universal approximator of similarity functions, demonstrate that MoL's\nexpressiveness can be realized empirically to achieve superior performance on\ndiverse retrieval scenarios, and propose techniques to retrieve the approximate\ntop-k results using MoL with tight error bounds. Through extensive\nexperimentation, we show that MoL, enhanced by our proposed mutual\ninformation-based load balancing loss, sets new state-of-the-art results across\nheterogeneous scenarios, including sequential retrieval models in\nrecommendation systems and finetuning language models for question answering;\nand our approximate top-$k$ algorithms outperform baselines by up to 66x in\nlatency while achieving >.99 recall rate compared to exact algorithms.\n","authors":["Bailu Ding","Jiaqi Zhai"],"pdf_url":"https://arxiv.org/pdf/2407.15462v3.pdf","comment":"21 pages, 3 figures. Our code and pre-trained model checkpoints are\n  available at https://github.com/bailuding/rails"},{"id":"http://arxiv.org/abs/2411.13477v1","updated":"2024-11-20T17:23:40Z","published":"2024-11-20T17:23:40Z","title":"PatentEdits: Framing Patent Novelty as Textual Entailment","summary":"  A patent must be deemed novel and non-obvious in order to be granted by the\nUS Patent Office (USPTO). If it is not, a US patent examiner will cite the\nprior work, or prior art, that invalidates the novelty and issue a non-final\nrejection. Predicting what claims of the invention should change given the\nprior art is an essential and crucial step in securing invention rights, yet\nhas not been studied before as a learnable task. In this work we introduce the\nPatentEdits dataset, which contains 105K examples of successful revisions that\novercome objections to novelty. We design algorithms to label edits sentence by\nsentence, then establish how well these edits can be predicted with large\nlanguage models (LLMs). We demonstrate that evaluating textual entailment\nbetween cited references and draft sentences is especially effective in\npredicting which inventive claims remained unchanged or are novel in relation\nto prior art.\n","authors":["Ryan Lee","Alexander Spangher","Xuezhe Ma"],"pdf_url":"https://arxiv.org/pdf/2411.13477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13415v1","updated":"2024-11-20T16:02:14Z","published":"2024-11-20T16:02:14Z","title":"Unleashing the Power of Large Language Models for Group POI\n  Recommendations","summary":"  Group Point-of-Interest (POI) recommendations aim to predict the next POI\nthat satisfies the diverse preferences of a group of users. This task is more\nchallenging than traditional individual POI recommendations due to complex\ngroup decision-making and extremely sparse group-level check-in data. Existing\nmethods for group POI recommendations primarily rely on single ID-based\nfeatures from check-in data, capturing only statistical correlations and\nfailing to fully utilize the rich semantic information contained in the\ncheck-ins, resulting in suboptimal performance. To this end, we propose a\nframework that unleashes the power of the Large Language Model (LLM) for\ncontext-aware group POI recommendations (LLMGPR). Our approach first introduces\nPOI tokens alongside the original word tokens of the LLM, which are initialized\nby applying the LLM to the rich information of each POI. We then propose a\nnovel sequencing adapter guided by Quantized Low-Rank Adaptation (QLORA) to\nmodify the LLM. The enhanced LLM can learn sequence representations by\ncombining semantic-enhanced POI tokens and rich contextual information\nincluding positional encodings and spatio-temporal differences. This approach\ncan be adapted for learning either group or user representations depending on\nthe sequence type. Furthermore, we enhance group representations by aggregating\nindividual member representations with another QLORA-based aggregation adapter\nand introducing a self-supervised learning task that predicts the purpose of\ncheck-in sequences, alleviating the data sparsity issue. Our experimental\nresults demonstrate that LLMGPR outperforms existing methods, effectively\naddressing group-level data sparsity and providing superior recommendations.\n","authors":["Jing Long","Liang Qu","Guanhua Ye","Tong Chen","Quoc Viet Hung Nguyen","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2411.13415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06256v2","updated":"2024-11-20T15:28:42Z","published":"2024-11-09T19:07:58Z","title":"Annotative Indexing","summary":"  This paper introduces annotative indexing, a novel framework that unifies and\ngeneralizes traditional inverted indexes, column stores, object stores, and\ngraph databases. As a result, annotative indexing can provide the underlying\nindexing framework for databases that support knowledge graphs, entity\nretrieval, semi-structured data, and ranked retrieval. While we primarily focus\non human language data in the form of text, annotative indexing is sufficiently\ngeneral to support a range of other datatypes, and we provide examples of\nSQL-like queries over a JSON store that includes numbers and dates. Taking\nadvantage of the flexibility of annotative indexing, we also demonstrate a\nfully dynamic annotative index incorporating support for ACID properties of\ntransactions with hundreds of multiple concurrent readers and writers.\n","authors":["Charles L. A. Clarke"],"pdf_url":"https://arxiv.org/pdf/2411.06256v2.pdf","comment":"Code at https://github.com/claclark/Cottontail"},{"id":"http://arxiv.org/abs/2411.13322v1","updated":"2024-11-20T13:44:59Z","published":"2024-11-20T13:44:59Z","title":"Scaling Laws for Online Advertisement Retrieval","summary":"  The scaling law is a notable property of neural network models and has\nsignificantly propelled the development of large language models. Scaling laws\nhold great promise in guiding model design and resource allocation. Recent\nresearch increasingly shows that scaling laws are not limited to NLP tasks or\nTransformer architectures; they also apply to domains such as recommendation.\nHowever, there is still a lack of literature on scaling law research in online\nadvertisement retrieval systems. This may be because 1) identifying the scaling\nlaw for resource cost and online revenue is often expensive in both time and\ntraining resources for large-scale industrial applications, and 2) varying\nsettings for different systems prevent the scaling law from being applied\nacross various scenarios. To address these issues, we propose a lightweight\nparadigm to identify the scaling law of online revenue and machine cost for a\ncertain online advertisement retrieval scenario with a low experimental cost.\nSpecifically, we focus on a sole factor (FLOPs) and propose an offline metric\nnamed R/R* that exhibits a high linear correlation with online revenue for\nretrieval models. We estimate the machine cost offline via a simulation\nalgorithm. Thus, we can transform most online experiments into low-cost offline\nexperiments. We conduct comprehensive experiments to verify the effectiveness\nof our proposed metric R/R* and to identify the scaling law in the online\nadvertisement retrieval system of Kuaishou. With the scaling law, we\ndemonstrate practical applications for ROI-constrained model designing and\nmulti-scenario resource allocation in Kuaishou advertising system. To the best\nof our knowledge, this is the first work to study the scaling laws for online\nadvertisement retrieval of real-world systems, showing great potential for\nscaling law in advertising system optimization.\n","authors":["Yunli Wang","Zixuan Yang","Zhen Zhang","Zhiqiang Wang","Jian Yang","Shiyang Wen","Peng Jiang","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2411.13322v1.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2411.13212v1","updated":"2024-11-20T11:19:35Z","published":"2024-11-20T11:19:35Z","title":"On the Statistical Significance with Relevance Assessments of Large\n  Language Models","summary":"  Test collections are an integral part of Information Retrieval (IR) research.\nThey allow researchers to evaluate and compare ranking algorithms in a quick,\neasy and reproducible way. However, constructing these datasets requires great\nefforts in manual labelling and logistics, and having only few human relevance\njudgements can introduce biases in the comparison. Recent research has explored\nthe use of Large Language Models (LLMs) for labelling the relevance of\ndocuments for building new retrieval test collections. Their strong\ntext-understanding capabilities and low cost compared to human-made judgements\nmakes them an appealing tool for gathering relevance judgements. Results\nsuggest that LLM-generated labels are promising for IR evaluation in terms of\nranking correlation, but nothing is said about the implications in terms of\nstatistical significance. In this work, we look at how LLM-generated judgements\npreserve the same pairwise significance evaluation as human judgements. Our\nresults show that LLM judgements detect most of the significant differences\nwhile maintaining acceptable numbers of false positives. However, we also show\nthat some systems are treated differently under LLM-generated labels,\nsuggesting that evaluation with LLM judgements might not be entirely fair. Our\nwork represents a step forward in the evaluation of statistical testing results\nprovided by LLM judgements. We hope that this will serve as a basis for other\nresearchers to develop reliable models for automatic relevance assessments.\n","authors":["David Otero","Javier Parapar","Álvaro Barreiro"],"pdf_url":"https://arxiv.org/pdf/2411.13212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13173v1","updated":"2024-11-20T10:17:09Z","published":"2024-11-20T10:17:09Z","title":"Writing Style Matters: An Examination of Bias and Fairness in\n  Information Retrieval Systems","summary":"  The rapid advancement of Language Model technologies has opened new\nopportunities, but also introduced new challenges related to bias and fairness.\nThis paper explores the uncharted territory of potential biases in\nstate-of-the-art universal text embedding models towards specific document and\nquery writing styles within Information Retrieval (IR) systems. Our\ninvestigation reveals that different embedding models exhibit different\npreferences of document writing style, while more informal and emotive styles\nare less favored by most embedding models. In terms of query writing styles,\nmany embedding models tend to match the style of the query with the style of\nthe retrieved documents, but some show a consistent preference for specific\nstyles. Text embedding models fine-tuned on synthetic data generated by LLMs\ndisplay a consistent preference for certain style of generated data. These\nbiases in text embedding based IR systems can inadvertently silence or\nmarginalize certain communication styles, thereby posing a significant threat\nto fairness in information retrieval. Finally, we also compare the answer\nstyles of Retrieval Augmented Generation (RAG) systems based on different LLMs\nand find out that most text embedding models are biased towards LLM's answer\nstyles when used as evaluation metrics for answer correctness. This study sheds\nlight on the critical issue of writing style based bias in IR systems, offering\nvaluable insights for the development of more fair and robust models.\n","authors":["Hongliu Cao"],"pdf_url":"https://arxiv.org/pdf/2411.13173v1.pdf","comment":"In Proceedings of the Eighteenth ACM International Conference on Web\n  Search and Data Mining (WSDM 25)"},{"id":"http://arxiv.org/abs/2411.12449v2","updated":"2024-11-20T10:06:05Z","published":"2024-11-19T12:17:43Z","title":"Neon: News Entity-Interaction Extraction for Enhanced Question Answering","summary":"  Capturing fresh information in near real-time and using it to augment\nexisting large language models (LLMs) is essential to generate up-to-date,\ngrounded, and reliable output. This problem becomes particularly challenging\nwhen LLMs are used for informational tasks in rapidly evolving fields, such as\nWeb search related to recent or unfolding events involving entities, where\ngenerating temporally relevant responses requires access to up-to-the-hour news\nsources. However, the information modeled by the parametric memory of LLMs is\noften outdated, and Web results from prototypical retrieval systems may fail to\ncapture the latest relevant information and struggle to handle conflicting\nreports in evolving news. To address this challenge, we present the NEON\nframework, designed to extract emerging entity interactions -- such as events\nor activities -- as described in news articles. NEON constructs an\nentity-centric timestamped knowledge graph that captures such interactions,\nthereby facilitating enhanced QA capabilities related to news events. Our\nframework innovates by integrating open Information Extraction (openIE) style\ntuples into LLMs to enable in-context retrieval-augmented generation. This\nintegration demonstrates substantial improvements in QA performance when\ntackling temporal, entity-centric search queries. Through NEON, LLMs can\ndeliver more accurate, reliable, and up-to-date responses.\n","authors":["Sneha Singhania","Silviu Cucerzan","Allen Herring","Sujay Kumar Jauhar"],"pdf_url":"https://arxiv.org/pdf/2411.12449v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13154v1","updated":"2024-11-20T09:43:30Z","published":"2024-11-20T09:43:30Z","title":"DMQR-RAG: Diverse Multi-Query Rewriting for RAG","summary":"  Large language models often encounter challenges with static knowledge and\nhallucinations, which undermine their reliability. Retrieval-augmented\ngeneration (RAG) mitigates these issues by incorporating external information.\nHowever, user queries frequently contain noise and intent deviations,\nnecessitating query rewriting to improve the relevance of retrieved documents.\nIn this paper, we introduce DMQR-RAG, a Diverse Multi-Query Rewriting framework\ndesigned to improve the performance of both document retrieval and final\nresponses in RAG. Specifically, we investigate how queries with varying\ninformation quantities can retrieve a diverse array of documents, presenting\nfour rewriting strategies that operate at different levels of information to\nenhance the performance of baseline approaches. Additionally, we propose an\nadaptive strategy selection method that minimizes the number of rewrites while\noptimizing overall performance. Our methods have been rigorously validated\nthrough extensive experiments conducted in both academic and industry settings.\n","authors":["Zhicong Li","Jiahao Wang","Zhishu Jiang","Hangyu Mao","Zhongxia Chen","Jiazhen Du","Yuanxing Zhang","Fuzheng Zhang","Di Zhang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2411.13154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12706v2","updated":"2024-11-20T09:35:09Z","published":"2024-05-21T11:54:16Z","title":"Crocodile: Cross Experts Covariance for Disentangled Learning in\n  Multi-Domain Recommendation","summary":"  Multi-domain learning (MDL) has become a prominent topic in enhancing the\nquality of personalized services. It's critical to learn commonalities between\ndomains and preserve the distinct characteristics of each domain. However, this\nleads to a challenging dilemma in MDL. On the one hand, a model needs to\nleverage domain-aware modules such as experts or embeddings to preserve each\ndomain's distinctiveness. On the other hand, real-world datasets often exhibit\nlong-tailed distributions across domains, where some domains may lack\nsufficient samples to effectively train their specific modules. Unfortunately,\nnearly all existing work falls short of resolving this dilemma. To this end, we\npropose a novel Cross-experts Covariance Loss for Disentangled Learning model\n(Crocodile), which employs multiple embedding tables to make the model\ndomain-aware at the embeddings which consist most parameters in the model, and\na covariance loss upon these embeddings to disentangle them, enabling the model\nto capture diverse user interests among domains. Empirical analysis\ndemonstrates that our method successfully addresses both challenges and\noutperforms all state-of-the-art methods on public datasets. During online A/B\ntesting in Tencent's advertising platform, Crocodile achieves 0.72% CTR lift\nand 0.73% GMV lift on a primary advertising scenario.\n","authors":["Zhutian Lin","Junwei Pan","Haibin Yu","Xi Xiao","Ximei Wang","Zhixiang Feng","Shifeng Wen","Shudong Huang","Dapeng Liu","Lei Xiao"],"pdf_url":"https://arxiv.org/pdf/2405.12706v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13057v1","updated":"2024-11-20T06:10:06Z","published":"2024-11-20T06:10:06Z","title":"Branches, Assemble! Multi-Branch Cooperation Network for Large-Scale\n  Click-Through Rate Prediction at Taobao","summary":"  Existing click-through rate (CTR) prediction works have studied the role of\nfeature interaction through a variety of techniques. Each interaction technique\nexhibits its own strength, and solely using one type could constrain the\nmodel's capability to capture the complex feature relationships, especially for\nindustrial large-scale data with enormous users and items. Recent research\nshows that effective CTR models often combine an MLP network with a dedicated\nfeature interaction network in a two-parallel structure. However, the interplay\nand cooperative dynamics between different streams or branches remain\nunder-researched. In this work, we introduce a novel Multi-Branch Cooperation\nNetwork (MBCnet) which enables multiple branch networks to collaborate with\neach other for better complex feature interaction modeling. Specifically,\nMBCnet consists of three branches: the Expert-based Feature Grouping and\nCrossing (EFGC) branch that promotes the model's memorization ability of\nspecific feature fields, the low rank Cross Net branch and Deep branch to\nenhance both explicit and implicit feature crossing for improved\ngeneralization. Among branches, a novel cooperation scheme is proposed based on\ntwo principles: branch co-teaching and moderate differentiation. Branch\nco-teaching encourages well-learned branches to support poorly-learned ones on\nspecific training samples. Moderate differentiation advocates branches to\nmaintain a reasonable level of difference in their feature representations. The\ncooperation strategy improves learning through mutual knowledge sharing via\nco-teaching and boosts the discovery of diverse feature interactions across\nbranches. Extensive experiments on large-scale industrial datasets and online\nA/B test demonstrate MBCnet's superior performance, delivering a 0.09 point\nincrease in CTR, 1.49% growth in deals, and 1.62% rise in GMV. Core codes will\nbe released soon.\n","authors":["Xu Chen","Zida Cheng","Yuangang Pan","Shuai Xiao","Xiaoming Liu","Jinsong Lan","Qingwen Liu","Ivor W. Tsang"],"pdf_url":"https://arxiv.org/pdf/2411.13057v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2411.13052v1","updated":"2024-11-20T05:56:31Z","published":"2024-11-20T05:56:31Z","title":"On-device Content-based Recommendation with Single-shot Embedding\n  Pruning: A Cooperative Game Perspective","summary":"  Content-based Recommender Systems (CRSs) play a crucial role in shaping user\nexperiences in e-commerce, online advertising, and personalized\nrecommendations. However, due to the vast amount of categorical features, the\nembedding tables used in CRS models pose a significant storage bottleneck for\nreal-world deployment, especially on resource-constrained devices. To address\nthis problem, various embedding pruning methods have been proposed, but most\nexisting ones require expensive retraining steps for each target parameter\nbudget, leading to enormous computation costs. In reality, this computation\ncost is a major hurdle in real-world applications with diverse storage\nrequirements, such as federated learning and streaming settings. In this paper,\nwe propose Shapley Value-guided Embedding Reduction (Shaver) as our response.\nWith Shaver, we view the problem from a cooperative game perspective, and\nquantify each embedding parameter's contribution with Shapley values to\nfacilitate contribution-based parameter pruning. To address the inherently high\ncomputation costs of Shapley values, we propose an efficient and unbiased\nmethod to estimate Shapley values of a CRS's embedding parameters. Moreover, in\nthe pruning stage, we put forward a field-aware codebook to mitigate the\ninformation loss in the traditional zero-out treatment. Through extensive\nexperiments on three real-world datasets, Shaver has demonstrated competitive\nperformance with lightweight recommendation models across various parameter\nbudgets. The source code is available at\nhttps://anonymous.4open.science/r/shaver-E808\n","authors":["Hung Vinh Tran","Tong Chen","Guanhua Ye","Quoc Viet Hung Nguyen","Kai Zheng","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2411.13052v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13045v1","updated":"2024-11-20T05:30:15Z","published":"2024-11-20T05:30:15Z","title":"Explainable LLM-driven Multi-dimensional Distillation for E-Commerce\n  Relevance Learning","summary":"  Effective query-item relevance modeling is pivotal for enhancing user\nexperience and safeguarding user satisfaction in e-commerce search systems.\nRecently, benefiting from the vast inherent knowledge, Large Language Model\n(LLM) approach demonstrates strong performance and long-tail generalization\nability compared with previous neural-based specialized relevance learning\nmethods. Though promising, current LLM-based methods encounter the following\ninadequacies in practice: First, the massive parameters and computational\ndemands make it difficult to be deployed online. Second, distilling LLM models\nto online models is a feasible direction, but the LLM relevance modeling is a\nblack box, and its rich intrinsic knowledge is difficult to extract and apply\nonline. To improve the interpretability of LLM and boost the performance of\nonline relevance models via LLM, we propose an Explainable LLM-driven\nMulti-dimensional Distillation framework for e-commerce relevance learning,\nwhich comprises two core components: (1) An Explainable LLM for relevance\nmodeling (ELLM-rele), which decomposes the relevance learning into intermediate\nsteps and models relevance learning as a Chain-of-Thought (CoT) reasoning,\nthereby enhancing both interpretability and performance of LLM. (2) A\nMulti-dimensional Knowledge Distillation (MKD) architecture that transfers the\nknowledge of ELLM-rele to current deployable interaction-based and\nrepresentation-based student models from both the relevance score distribution\nand CoT reasoning aspects. Through distilling the probabilistic and CoT\nreasoning knowledge, MKD improves both the semantic interaction and long-tail\ngeneralization abilities of student models. Extensive offline evaluations and\nonline experiments on Taobao search ad scene demonstrate that our proposed\nframework significantly enhances e-commerce relevance learning performance and\nuser experience.\n","authors":["Gang Zhao","Ximing Zhang","Chenji Lu","Hui Zhao","Tianshu Wu","Pengjie Wang","Jian Xu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2411.13045v1.pdf","comment":"Submitted to WWW 2025"},{"id":"http://arxiv.org/abs/2411.12989v1","updated":"2024-11-20T02:34:21Z","published":"2024-11-20T02:34:21Z","title":"Data Watermarking for Sequential Recommender Systems","summary":"  In the era of large foundation models, data has become a crucial component\nfor building high-performance AI systems. As the demand for high-quality and\nlarge-scale data continues to rise, data copyright protection is attracting\nincreasing attention. In this work, we explore the problem of data watermarking\nfor sequential recommender systems, where a watermark is embedded into the\ntarget dataset and can be detected in models trained on that dataset. We\naddress two specific challenges: dataset watermarking, which protects the\nownership of the entire dataset, and user watermarking, which safeguards the\ndata of individual users. We systematically define these problems and present a\nmethod named DWRS to address them. Our approach involves randomly selecting\nunpopular items to create a watermark sequence, which is then inserted into\nnormal users' interaction sequences. Extensive experiments on five\nrepresentative sequential recommendation models and three benchmark datasets\ndemonstrate the effectiveness of DWRS in protecting data copyright while\npreserving model utility.\n","authors":["Sixiao Zhang","Cheng Long","Wei Yuan","Hongxu Chen","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2411.12989v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12949v1","updated":"2024-11-20T00:43:32Z","published":"2024-11-20T00:43:32Z","title":"Epidemiology-informed Network for Robust Rumor Detection","summary":"  The rapid spread of rumors on social media has posed significant challenges\nto maintaining public trust and information integrity. Since an information\ncascade process is essentially a propagation tree, recent rumor detection\nmodels leverage graph neural networks to additionally capture information\npropagation patterns, thus outperforming text-only solutions. Given the\nvariations in topics and social impact of the root node, different source\ninformation naturally has distinct outreach capabilities, resulting in\ndifferent heights of propagation trees. This variation, however, impedes the\ndata-driven design of existing graph-based rumor detectors. Given a shallow\npropagation tree with limited interactions, it is unlikely for graph-based\napproaches to capture sufficient cascading patterns, questioning their ability\nto handle less popular news or early detection needs. In contrast, a deep\npropagation tree is prone to noisy user responses, and this can in turn\nobfuscate the predictions. In this paper, we propose a novel\nEpidemiology-informed Network (EIN) that integrates epidemiological knowledge\nto enhance performance by overcoming data-driven methods sensitivity to data\nquality. Meanwhile, to adapt epidemiology theory to rumor detection, it is\nexpected that each users stance toward the source information will be\nannotated. To bypass the costly and time-consuming human labeling process, we\ntake advantage of large language models to generate stance labels, facilitating\noptimization objectives for learning epidemiology-informed representations. Our\nexperimental results demonstrate that the proposed EIN not only outperforms\nstate-of-the-art methods on real-world datasets but also exhibits enhanced\nrobustness across varying tree depths.\n","authors":["Wei Jiang","Tong Chen","Xinyi Gao","Wentao Zhang","Lizhen Cui","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2411.12949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13700v1","updated":"2024-11-20T20:38:56Z","published":"2024-11-20T20:38:56Z","title":"A Collaborative Ensemble Framework for CTR Prediction","summary":"  Recent advances in foundation models have established scaling laws that\nenable the development of larger models to achieve enhanced performance,\nmotivating extensive research into large-scale recommendation models. However,\nsimply increasing the model size in recommendation systems, even with large\namounts of data, does not always result in the expected performance\nimprovements. In this paper, we propose a novel framework, Collaborative\nEnsemble Training Network (CETNet), to leverage multiple distinct models, each\nwith its own embedding table, to capture unique feature interaction patterns.\nUnlike naive model scaling, our approach emphasizes diversity and collaboration\nthrough collaborative learning, where models iteratively refine their\npredictions. To dynamically balance contributions from each model, we introduce\na confidence-based fusion mechanism using general softmax, where model\nconfidence is computed via negation entropy. This design ensures that more\nconfident models have a greater influence on the final prediction while\nbenefiting from the complementary strengths of other models. We validate our\nframework on three public datasets (AmazonElectronics, TaobaoAds, and\nKuaiVideo) as well as a large-scale industrial dataset from Meta, demonstrating\nits superior performance over individual models and state-of-the-art baselines.\nAdditionally, we conduct further experiments on the Criteo and Avazu datasets\nto compare our method with the multi-embedding paradigm. Our results show that\nour framework achieves comparable or better performance with smaller embedding\nsizes, offering a scalable and efficient solution for CTR prediction tasks.\n","authors":["Xiaolong Liu","Zhichen Zeng","Xiaoyi Liu","Siyang Yuan","Weinan Song","Mengyue Hang","Yiqun Liu","Chaofei Yang","Donghyun Kim","Wen-Yen Chen","Jiyan Yang","Yiping Han","Rong Jin","Bo Long","Hanghang Tong","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2411.13700v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2411.13553v1","updated":"2024-11-20T18:59:58Z","published":"2024-11-20T18:59:58Z","title":"AI-generated Image Detection: Passive or Watermark?","summary":"  While text-to-image models offer numerous benefits, they also pose\nsignificant societal risks. Detecting AI-generated images is crucial for\nmitigating these risks. Detection methods can be broadly categorized into\npassive and watermark-based approaches: passive detectors rely on artifacts\npresent in AI-generated images, whereas watermark-based detectors proactively\nembed watermarks into such images. A key question is which type of detector\nperforms better in terms of effectiveness, robustness, and efficiency. However,\nthe current literature lacks a comprehensive understanding of this issue. In\nthis work, we aim to bridge that gap by developing ImageDetectBench, the first\ncomprehensive benchmark to compare the effectiveness, robustness, and\nefficiency of passive and watermark-based detectors. Our benchmark includes\nfour datasets, each containing a mix of AI-generated and non-AI-generated\nimages. We evaluate five passive detectors and four watermark-based detectors\nagainst eight types of common perturbations and three types of adversarial\nperturbations. Our benchmark results reveal several interesting findings. For\ninstance, watermark-based detectors consistently outperform passive detectors,\nboth in the presence and absence of perturbations. Based on these insights, we\nprovide recommendations for detecting AI-generated images, e.g., when both\ntypes of detectors are applicable, watermark-based detectors should be the\npreferred choice.\n","authors":["Moyang Guo","Yuepeng Hu","Zhengyuan Jiang","Zeyu Li","Amir Sadovnik","Arka Daw","Neil Gong"],"pdf_url":"https://arxiv.org/pdf/2411.13553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04327v2","updated":"2024-11-20T18:59:23Z","published":"2024-10-06T01:30:40Z","title":"Leveraging Hierarchical Taxonomies in Prompt-based Continual Learning","summary":"  Drawing inspiration from human learning behaviors, this work proposes a novel\napproach to mitigate catastrophic forgetting in Prompt-based Continual Learning\nmodels by exploiting the relationships between continuously emerging class\ndata. We find that applying human habits of organizing and connecting\ninformation can serve as an efficient strategy when training deep learning\nmodels. Specifically, by building a hierarchical tree structure based on the\nexpanding set of labels, we gain fresh insights into the data, identifying\ngroups of similar classes could easily cause confusion. Additionally, we delve\ndeeper into the hidden connections between classes by exploring the original\npretrained model's behavior through an optimal transport-based approach. From\nthese insights, we propose a novel regularization loss function that encourages\nmodels to focus more on challenging knowledge areas, thereby enhancing overall\nperformance. Experimentally, our method demonstrated significant superiority\nover the most robust state-of-the-art models on various benchmarks.\n","authors":["Quyen Tran","Hoang Phan","Minh Le","Tuan Truong","Dinh Phung","Linh Ngo","Thien Nguyen","Nhat Ho","Trung Le"],"pdf_url":"https://arxiv.org/pdf/2410.04327v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13548v1","updated":"2024-11-20T18:56:24Z","published":"2024-11-20T18:56:24Z","title":"HF-Diff: High-Frequency Perceptual Loss and Distribution Matching for\n  One-Step Diffusion-Based Image Super-Resolution","summary":"  Although recent diffusion-based single-step super-resolution methods achieve\nbetter performance as compared to SinSR, they are computationally complex. To\nimprove the performance of SinSR, we investigate preserving the high-frequency\ndetail features during super-resolution (SR) because the downgraded images lack\ndetailed information. For this purpose, we introduce a high-frequency\nperceptual loss by utilizing an invertible neural network (INN) pretrained on\nthe ImageNet dataset. Different feature maps of pretrained INN produce\ndifferent high-frequency aspects of an image. During the training phase, we\nimpose to preserve the high-frequency features of super-resolved and ground\ntruth (GT) images that improve the SR image quality during inference.\nFurthermore, we also utilize the Jenson-Shannon divergence between GT and SR\nimages in the pretrained DINO-v2 embedding space to match their distribution.\nBy introducing the $\\textbf{h}igh$- $\\textbf{f}requency$ preserving loss and\ndistribution matching constraint in the single-step $\\textbf{diff}usion-based$\nSR ($\\textbf{HF-Diff}$), we achieve a state-of-the-art CLIPIQA score in the\nbenchmark RealSR, RealSet65, DIV2K-Val, and ImageNet datasets. Furthermore, the\nexperimental results in several datasets demonstrate that our high-frequency\nperceptual loss yields better SR image quality than LPIPS and VGG-based\nperceptual losses. Our code will be released at\nhttps://github.com/shoaib-sami/HF-Diff.\n","authors":["Shoaib Meraj Sami","Md Mahedi Hasan","Jeremy Dawson","Nasser Nasrabadi"],"pdf_url":"https://arxiv.org/pdf/2411.13548v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2411.13546v1","updated":"2024-11-20T18:55:51Z","published":"2024-11-20T18:55:51Z","title":"Promoting User Data Autonomy During the Dissolution of a Monopolistic\n  Firm","summary":"  The deployment of AI in consumer products is currently focused on the use of\nso-called foundation models, large neural networks pre-trained on massive\ncorpora of digital records. This emphasis on scaling up datasets and\npre-training computation raises the risk of further consolidating the industry,\nand enabling monopolistic (or oligopolistic) behavior. Judges and regulators\nseeking to improve market competition may employ various remedies. This paper\nexplores dissolution -- the breaking up of a monopolistic entity into smaller\nfirms -- as one such remedy, focusing in particular on the technical challenges\nand opportunities involved in the breaking up of large models and datasets. We\nshow how the framework of Conscious Data Contribution can enable user autonomy\nduring under dissolution. Through a simulation study, we explore how\nfine-tuning and the phenomenon of \"catastrophic forgetting\" could actually\nprove beneficial as a type of machine unlearning that allows users to specify\nwhich data they want used for what purposes.\n","authors":["Rushabh Solanki","Elliot Creager"],"pdf_url":"https://arxiv.org/pdf/2411.13546v1.pdf","comment":"This paper appeared at the 2nd Workshop on Regulatable ML at NeurIPS\n  2024"},{"id":"http://arxiv.org/abs/2411.13537v1","updated":"2024-11-20T18:41:03Z","published":"2024-11-20T18:41:03Z","title":"Metacognition for Unknown Situations and Environments (MUSE)","summary":"  Metacognition--the awareness and regulation of one's cognitive processes--is\ncentral to human adaptability in unknown situations. In contrast, current\nautonomous agents often struggle in novel environments due to their limited\ncapacity for adaptation. We hypothesize that metacognition is a critical\nmissing ingredient in adaptive autonomous systems, equipping them with the\ncognitive flexibility needed to tackle unfamiliar challenges. Given the broad\nscope of metacognitive abilities, we focus on two key aspects: competence\nawareness and strategy selection for novel tasks. To this end, we propose the\nMetacognition for Unknown Situations and Environments (MUSE) framework, which\nintegrates metacognitive processes--specifically self-awareness and\nself-regulation--into autonomous agents. We present two initial implementations\nof MUSE: one based on world modeling and another leveraging large language\nmodels (LLMs), both instantiating the metacognitive cycle. Our system\ncontinuously learns to assess its competence on a given task and uses this\nself-awareness to guide iterative cycles of strategy selection. MUSE agents\nshow significant improvements in self-awareness and self-regulation, enabling\nthem to solve novel, out-of-distribution tasks more effectively compared to\nDreamer-v3-based reinforcement learning and purely prompt-based LLM agent\napproaches. This work highlights the promise of approaches inspired by\ncognitive and neural systems in enabling autonomous systems to adapt to new\nenvironments, overcoming the limitations of current methods that rely heavily\non extensive training data.\n","authors":["Rodolfo Valiente","Praveen K. Pilly"],"pdf_url":"https://arxiv.org/pdf/2411.13537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13536v1","updated":"2024-11-20T18:37:58Z","published":"2024-11-20T18:37:58Z","title":"Identity Preserving 3D Head Stylization with Multiview Score\n  Distillation","summary":"  3D head stylization transforms realistic facial features into artistic\nrepresentations, enhancing user engagement across gaming and virtual reality\napplications. While 3D-aware generators have made significant advancements,\nmany 3D stylization methods primarily provide near-frontal views and struggle\nto preserve the unique identities of original subjects, often resulting in\noutputs that lack diversity and individuality. This paper addresses these\nchallenges by leveraging the PanoHead model, synthesizing images from a\ncomprehensive 360-degree perspective. We propose a novel framework that employs\nnegative log-likelihood distillation (LD) to enhance identity preservation and\nimprove stylization quality. By integrating multi-view grid score and mirror\ngradients within the 3D GAN architecture and introducing a score rank weighing\ntechnique, our approach achieves substantial qualitative and quantitative\nimprovements. Our findings not only advance the state of 3D head stylization\nbut also provide valuable insights into effective distillation processes\nbetween diffusion models and GANs, focusing on the critical issue of identity\npreservation. Please visit the https://three-bee.github.io/head_stylization for\nmore visuals.\n","authors":["Bahri Batuhan Bilecen","Ahmet Berke Gokmen","Furkan Guzelant","Aysegul Dundar"],"pdf_url":"https://arxiv.org/pdf/2411.13536v1.pdf","comment":"https://three-bee.github.io/head_stylization"},{"id":"http://arxiv.org/abs/2407.15462v3","updated":"2024-11-20T18:30:19Z","published":"2024-07-22T08:19:34Z","title":"Retrieval with Learned Similarities","summary":"  Retrieval plays a fundamental role in recommendation systems, search, and\nnatural language processing (NLP) by efficiently finding relevant items from a\nlarge corpus given a query. Dot products have been widely used as the\nsimilarity function in such tasks, enabled by Maximum Inner Product Search\n(MIPS) algorithms for efficient retrieval. However, state-of-the-art retrieval\nalgorithms have migrated to learned similarities. These advanced approaches\nencompass multiple query embeddings, complex neural networks, direct item ID\ndecoding via beam search, and hybrid solutions. Unfortunately, we lack\nefficient solutions for retrieval in these state-of-the-art setups. Our work\naddresses this gap by investigating efficient retrieval techniques with\nexpressive learned similarity functions. We establish Mixture-of-Logits (MoL)\nas a universal approximator of similarity functions, demonstrate that MoL's\nexpressiveness can be realized empirically to achieve superior performance on\ndiverse retrieval scenarios, and propose techniques to retrieve the approximate\ntop-k results using MoL with tight error bounds. Through extensive\nexperimentation, we show that MoL, enhanced by our proposed mutual\ninformation-based load balancing loss, sets new state-of-the-art results across\nheterogeneous scenarios, including sequential retrieval models in\nrecommendation systems and finetuning language models for question answering;\nand our approximate top-$k$ algorithms outperform baselines by up to 66x in\nlatency while achieving >.99 recall rate compared to exact algorithms.\n","authors":["Bailu Ding","Jiaqi Zhai"],"pdf_url":"https://arxiv.org/pdf/2407.15462v3.pdf","comment":"21 pages, 3 figures. Our code and pre-trained model checkpoints are\n  available at https://github.com/bailuding/rails"},{"id":"http://arxiv.org/abs/2309.01837v3","updated":"2024-11-20T18:26:03Z","published":"2023-09-04T22:16:35Z","title":"Delegating Data Collection in Decentralized Machine Learning","summary":"  Motivated by the emergence of decentralized machine learning (ML) ecosystems,\nwe study the delegation of data collection. Taking the field of contract theory\nas our starting point, we design optimal and near-optimal contracts that deal\nwith two fundamental information asymmetries that arise in decentralized ML:\nuncertainty in the assessment of model quality and uncertainty regarding the\noptimal performance of any model. We show that a principal can cope with such\nasymmetry via simple linear contracts that achieve 1-1/e fraction of the\noptimal utility. To address the lack of a priori knowledge regarding the\noptimal performance, we give a convex program that can adaptively and\nefficiently compute the optimal contract. We also study linear contracts and\nderive the optimal utility in the more complex setting of multiple\ninteractions.\n","authors":["Nivasini Ananthakrishnan","Stephen Bates","Michael I. Jordan","Nika Haghtalab"],"pdf_url":"https://arxiv.org/pdf/2309.01837v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.11655v4","updated":"2024-11-20T18:25:16Z","published":"2023-07-21T15:43:32Z","title":"Preferences Evolve And So Should Your Bandits: Bandits with Evolving\n  States for Online Platforms","summary":"  We propose a model for learning with bandit feedback while accounting for\ndeterministically evolving and unobservable states that we call Bandits with\nDeterministically Evolving States ($B$-$DES$). The workhorse applications of\nour model are learning for recommendation systems and learning for online ads.\nIn both cases, the reward that the algorithm obtains at each round is a\nfunction of the short-term reward of the action chosen and how \"healthy\" the\nsystem is (i.e., as measured by its state). For example, in recommendation\nsystems, the reward that the platform obtains from a user's engagement with a\nparticular type of content depends not only on the inherent features of the\nspecific content, but also on how the user's preferences have evolved as a\nresult of interacting with other types of content on the platform. Our general\nmodel accounts for the different rate $\\lambda \\in [0,1]$ at which the state\nevolves (e.g., how fast a user's preferences shift as a result of previous\ncontent consumption) and encompasses standard multi-armed bandits as a special\ncase. The goal of the algorithm is to minimize a notion of regret against the\nbest-fixed sequence of arms pulled, which is significantly harder to attain\ncompared to standard benchmark of the best-fixed action in hindsight. We\npresent online learning algorithms for any possible value of the evolution rate\n$\\lambda$ and we show the robustness of our results to various model\nmisspecifications.\n","authors":["Khashayar Khosravi","Renato Paes Leme","Chara Podimata","Apostolis Tsorvantzis"],"pdf_url":"https://arxiv.org/pdf/2307.11655v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13520v1","updated":"2024-11-20T18:11:17Z","published":"2024-11-20T18:11:17Z","title":"Quantum Attention for Vision Transformers in High Energy Physics","summary":"  We present a novel hybrid quantum-classical vision transformer architecture\nincorporating quantum orthogonal neural networks (QONNs) to enhance performance\nand computational efficiency in high-energy physics applications. Building on\nadvancements in quantum vision transformers, our approach addresses limitations\nof prior models by leveraging the inherent advantages of QONNs, including\nstability and efficient parameterization in high-dimensional spaces. We\nevaluate the proposed architecture using multi-detector jet images from CMS\nOpen Data, focusing on the task of distinguishing quark-initiated from\ngluon-initiated jets. The results indicate that embedding quantum orthogonal\ntransformations within the attention mechanism can provide robust performance\nwhile offering promising scalability for machine learning challenges associated\nwith the upcoming High Luminosity Large Hadron Collider. This work highlights\nthe potential of quantum-enhanced models to address the computational demands\nof next-generation particle physics experiments.\n","authors":["Alessandro Tesi","Gopal Ramesh Dahale","Sergei Gleyzer","Kyoungchul Kong","Tom Magorsch","Konstantin T. Matchev","Katia Matcheva"],"pdf_url":"https://arxiv.org/pdf/2411.13520v1.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2411.13513v1","updated":"2024-11-20T18:06:55Z","published":"2024-11-20T18:06:55Z","title":"Procurement Auctions via Approximately Optimal Submodular Optimization","summary":"  We study procurement auctions, where an auctioneer seeks to acquire services\nfrom strategic sellers with private costs. The quality of services is measured\nby a submodular function known to the auctioneer. Our goal is to design\ncomputationally efficient procurement auctions that (approximately) maximize\nthe difference between the quality of the acquired services and the total cost\nof the sellers, while ensuring incentive compatibility (IC), individual\nrationality (IR) for sellers, and non-negative surplus (NAS) for the\nauctioneer.\n  Our contributions are twofold: (i) we provide an improved analysis of\nexisting algorithms for non-positive submodular function maximization, and (ii)\nwe design efficient frameworks that transform submodular optimization\nalgorithms into mechanisms that are IC, IR, NAS, and approximation-preserving.\nThese frameworks apply to both the offline setting, where all sellers' bids and\nservices are available simultaneously, and the online setting, where sellers\narrive in an adversarial order, requiring the auctioneer to make irrevocable\ndecisions.\n  We also explore whether state-of-the-art submodular optimization algorithms\ncan be converted into descending auctions in adversarial settings, where the\nschedule of descending prices is determined by an adversary. We show that a\nsubmodular optimization algorithm satisfying bi-criteria $(1/2,\n1)$-approximation in welfare can be effectively adapted to a descending\nauction. Additionally, we establish a connection between descending auctions\nand online submodular optimization.\n  Finally, we demonstrate the practical applications of our frameworks by\ninstantiating them with state-of-the-art submodular optimization algorithms and\nempirically comparing their welfare performance on publicly available datasets\nwith thousands of sellers.\n","authors":["Yuan Deng","Amin Karbasi","Vahab Mirrokni","Renato Paes Leme","Grigoris Velegkas","Song Zuo"],"pdf_url":"https://arxiv.org/pdf/2411.13513v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16838v2","updated":"2024-11-20T17:57:26Z","published":"2024-06-24T17:45:59Z","title":"From Decoding to Meta-Generation: Inference-time Algorithms for Large\n  Language Models","summary":"  One of the most striking findings in modern research on large language models\n(LLMs) is that scaling up compute during training leads to better results.\nHowever, less attention has been given to the benefits of scaling compute\nduring inference. This survey focuses on these inference-time approaches. We\nexplore three areas under a unified mathematical formalism: token-level\ngeneration algorithms, meta-generation algorithms, and efficient generation.\nToken-level generation algorithms, often called decoding algorithms, operate by\nsampling a single token at a time or constructing a token-level search space\nand then selecting an output. These methods typically assume access to a\nlanguage model's logits, next-token distributions, or probability scores.\nMeta-generation algorithms work on partial or full sequences, incorporating\ndomain knowledge, enabling backtracking, and integrating external information.\nEfficient generation methods aim to reduce token costs and improve the speed of\ngeneration. Our survey unifies perspectives from three research communities:\ntraditional natural language processing, modern LLMs, and machine learning\nsystems.\n","authors":["Sean Welleck","Amanda Bertsch","Matthew Finlayson","Hailey Schoelkopf","Alex Xie","Graham Neubig","Ilia Kulikov","Zaid Harchaoui"],"pdf_url":"https://arxiv.org/pdf/2406.16838v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13496v1","updated":"2024-11-20T17:45:03Z","published":"2024-11-20T17:45:03Z","title":"Advancing Heatwave Forecasting via Distribution Informed-Graph Neural\n  Networks (DI-GNNs): Integrating Extreme Value Theory with GNNs","summary":"  Heatwaves, prolonged periods of extreme heat, have intensified in frequency\nand severity due to climate change, posing substantial risks to public health,\necosystems, and infrastructure. Despite advancements in Machine Learning (ML)\nmodeling, accurate heatwave forecasting at weather scales (1--15 days) remains\nchallenging due to the non-linear interactions between atmospheric drivers and\nthe rarity of these extreme events. Traditional models relying on heuristic\nfeature engineering often fail to generalize across diverse climates and\ncapture the complexities of heatwave dynamics. This study introduces the\nDistribution-Informed Graph Neural Network (DI-GNN), a novel framework that\nintegrates principles from Extreme Value Theory (EVT) into the graph neural\nnetwork architecture. DI-GNN incorporates Generalized Pareto Distribution\n(GPD)-derived descriptors into the feature space, adjacency matrix, and loss\nfunction to enhance its sensitivity to rare heatwave occurrences. By\nprioritizing the tails of climatic distributions, DI-GNN addresses the\nlimitations of existing methods, particularly in imbalanced datasets where\ntraditional metrics like accuracy are misleading. Empirical evaluations using\nweather station data from British Columbia, Canada, demonstrate the superior\nperformance of DI-GNN compared to baseline models. DI-GNN achieved significant\nimprovements in balanced accuracy, recall, and precision, with high AUC and\naverage precision scores, reflecting its robustness in distinguishing heatwave\nevents.\n","authors":["Farrukh A. Chishtie","Dominique Brunet","Rachel H. White","Daniel Michelson","Jing Jiang","Vicky Lucas","Emily Ruboonga","Sayana Imaash","Melissa Westland","Timothy Chui","Rana Usman Ali","Mujtaba Hassan","Roland Stull","David Hudak"],"pdf_url":"https://arxiv.org/pdf/2411.13496v1.pdf","comment":"23 pages, 13 figures, pdf format"},{"id":"http://arxiv.org/abs/2411.13485v1","updated":"2024-11-20T17:35:21Z","published":"2024-11-20T17:35:21Z","title":"Utilizing Large Language Models to Synthesize Product Desirability\n  Datasets","summary":"  This research explores the application of large language models (LLMs) to\ngenerate synthetic datasets for Product Desirability Toolkit (PDT) testing, a\nkey component in evaluating user sentiment and product experience. Utilizing\ngpt-4o-mini, a cost-effective alternative to larger commercial LLMs, three\nmethods, Word+Review, Review+Word, and Supply-Word, were each used to\nsynthesize 1000 product reviews. The generated datasets were assessed for\nsentiment alignment, textual diversity, and data generation cost. Results\ndemonstrated high sentiment alignment across all methods, with Pearson\ncorrelations ranging from 0.93 to 0.97. Supply-Word exhibited the highest\ndiversity and coverage of PDT terms, although with increased generation costs.\nDespite minor biases toward positive sentiments, in situations with limited\ntest data, LLM-generated synthetic data offers significant advantages,\nincluding scalability, cost savings, and flexibility in dataset production.\n","authors":["John D. Hastings","Sherri Weitl-Harms","Joseph Doty","Zachary L. Myers","Warren Thompson"],"pdf_url":"https://arxiv.org/pdf/2411.13485v1.pdf","comment":"9 pages, 2 figures, 6 tables"},{"id":"http://arxiv.org/abs/2411.13479v1","updated":"2024-11-20T17:26:26Z","published":"2024-11-20T17:26:26Z","title":"Conformal Prediction for Hierarchical Data","summary":"  Reconciliation has become an essential tool in multivariate point forecasting\nfor hierarchical time series. However, there is still a lack of understanding\nof the theoretical properties of probabilistic Forecast Reconciliation\ntechniques. Meanwhile, Conformal Prediction is a general framework with growing\nappeal that provides prediction sets with probabilistic guarantees in finite\nsample. In this paper, we propose a first step towards combining Conformal\nPrediction and Forecast Reconciliation by analyzing how including a\nreconciliation step in the Split Conformal Prediction (SCP) procedure enhances\nthe resulting prediction sets. In particular, we show that the validity granted\nby SCP remains while improving the efficiency of the prediction sets. We also\nadvocate a variation of the theoretical procedure for practical use. Finally,\nwe illustrate these results with simulations.\n","authors":["Guillaume Principato","Yvenn Amara-Ouali","Yannig Goude","Bachir Hamrouche","Jean-Michel Poggi","Gilles Stoltz"],"pdf_url":"https://arxiv.org/pdf/2411.13479v1.pdf","comment":"14 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.14919v3","updated":"2024-11-20T17:20:00Z","published":"2024-10-19T00:33:51Z","title":"Adversarial Score identity Distillation: Rapidly Surpassing the Teacher\n  in One Step","summary":"  Score identity Distillation (SiD) is a data-free method that has achieved\nSOTA performance in image generation by leveraging only a pretrained diffusion\nmodel, without requiring any training data. However, its ultimate performance\nis constrained by how accurate the pretrained model captures the true data\nscores at different stages of the diffusion process. In this paper, we\nintroduce SiDA (SiD with Adversarial Loss), which not only enhances generation\nquality but also improves distillation efficiency by incorporating real images\nand adversarial loss. SiDA utilizes the encoder from the generator's score\nnetwork as a discriminator, boosting its ability to distinguish between real\nimages and those generated by SiD. The adversarial loss is batch-normalized\nwithin each GPU and then combined with the original SiD loss. This integration\neffectively incorporates the average \"fakeness\" per GPU batch into the\npixel-based SiD loss, enabling SiDA to distill a single-step generator either\nfrom scratch or by fine-tuning an existing one. SiDA converges significantly\nfaster than its predecessor when trained from scratch, and swiftly improves\nupon the original model's performance after an initial warmup period during\nfine-tuning from a pre-distilled SiD generator. This one-step adversarial\ndistillation method establishes new benchmarks in generation performance when\ndistilling EDM diffusion models pretrained on CIFAR-10 (32x32) and ImageNet\n(64x64), achieving FID score of 1.110 on ImageNet 64x64. It sets record-low FID\nscores when distilling EDM2 models trained on ImageNet (512x512), surpassing\neven the largest teacher model, EDM2-XXL. Our SiDA's results record FID scores\nof 2.156 for EDM2-XS, 1.669 for S, 1.488 for M, 1.413 for L, 1.379 for XL, and\n1.366 for XXL, demonstrating significant improvements across all model sizes.\nOur open-source code will be integrated into the SiD codebase.\n","authors":["Mingyuan Zhou","Huangjie Zheng","Yi Gu","Zhendong Wang","Hai Huang"],"pdf_url":"https://arxiv.org/pdf/2410.14919v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.13105v3","updated":"2024-11-20T17:16:01Z","published":"2023-01-30T17:44:05Z","title":"Generalization on the Unseen, Logic Reasoning and Degree Curriculum","summary":"  This paper considers the learning of logical (Boolean) functions with a focus\non the generalization on the unseen (GOTU) setting, a strong case of\nout-of-distribution generalization. This is motivated by the fact that the rich\ncombinatorial nature of data in certain reasoning tasks (e.g.,\narithmetic/logic) makes representative data sampling challenging, and learning\nsuccessfully under GOTU gives a first vignette of an 'extrapolating' or\n'reasoning' learner. We study how different network architectures trained by\n(S)GD perform under GOTU and provide both theoretical and experimental evidence\nthat for sparse functions and a class of network models including instances of\nTransformers, random features models, and linear networks, a\nmin-degree-interpolator is learned on the unseen. More specifically, this means\nan interpolator of the training data that has minimal Fourier mass on the\nhigher degree basis elements. These findings lead to two implications: (1) we\nprovide an explanation to the length generalization problem for Boolean\nfunctions (e.g., Anil et al. 2022); (2) we introduce a curriculum learning\nalgorithm called Degree-Curriculum that learns monomials more efficiently by\nincrementing supports. Finally, we discuss extensions to other models or\nnon-sparse regimes where the min-degree bias may still occur or fade, as well\nas how it can be potentially corrected when undesirable.\n","authors":["Emmanuel Abbe","Samy Bengio","Aryo Lotfi","Kevin Rizk"],"pdf_url":"https://arxiv.org/pdf/2301.13105v3.pdf","comment":"extended JMLR version of the original ICML 2023 paper"},{"id":"http://arxiv.org/abs/2406.00599v3","updated":"2024-11-20T17:12:50Z","published":"2024-06-02T03:11:31Z","title":"Robust Fair Clustering with Group Membership Uncertainty Sets","summary":"  We study the canonical fair clustering problem where each cluster is\nconstrained to have close to population-level representation of each group.\nDespite significant attention, the salient issue of having incomplete knowledge\nabout the group membership of each point has been superficially addressed. In\nthis paper, we consider a setting where the assigned group memberships are\nnoisy. We introduce a simple noise model that requires a small number of\nparameters to be given by the decision maker. We then present an algorithm for\nfair clustering with provable \\emph{robustness} guarantees. Our framework\nenables the decision maker to trade off between the robustness and the\nclustering quality. Unlike previous work, our algorithms are backed by\nworst-case theoretical guarantees. Finally, we empirically verify the\nperformance of our algorithm on real world datasets and show its superior\nperformance over existing baselines.\n","authors":["Sharmila Duppala","Juan Luque","John P. Dickerson","Seyed A. Esmaeili"],"pdf_url":"https://arxiv.org/pdf/2406.00599v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07679v2","updated":"2024-11-20T17:11:21Z","published":"2024-11-12T09:49:16Z","title":"Safe Exploitative Play with Untrusted Type Beliefs","summary":"  The combination of the Bayesian game and learning has a rich history, with\nthe idea of controlling a single agent in a system composed of multiple agents\nwith unknown behaviors given a set of types, each specifying a possible\nbehavior for the other agents. The idea is to plan an agent's own actions with\nrespect to those types which it believes are most likely to maximize the\npayoff. However, the type beliefs are often learned from past actions and\nlikely to be incorrect. With this perspective in mind, we consider an agent in\na game with type predictions of other components, and investigate the impact of\nincorrect beliefs to the agent's payoff. In particular, we formally define a\ntradeoff between risk and opportunity by comparing the payoff obtained against\nthe optimal payoff, which is represented by a gap caused by trusting or\ndistrusting the learned beliefs. Our main results characterize the tradeoff by\nestablishing upper and lower bounds on the Pareto front for both normal-form\nand stochastic Bayesian games, with numerical results provided.\n","authors":["Tongxin Li","Tinashe Handina","Shaolei Ren","Adam Wierman"],"pdf_url":"https://arxiv.org/pdf/2411.07679v2.pdf","comment":"26 pages, NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.13462v1","updated":"2024-11-20T17:10:24Z","published":"2024-11-20T17:10:24Z","title":"Sampling and Integration of Logconcave Functions by Algorithmic\n  Diffusion","summary":"  We study the complexity of sampling, rounding, and integrating arbitrary\nlogconcave functions. Our new approach provides the first complexity\nimprovements in nearly two decades for general logconcave functions for all\nthree problems, and matches the best-known complexities for the special case of\nuniform distributions on convex bodies. For the sampling problem, our output\nguarantees are significantly stronger than previously known, and lead to a\nstreamlined analysis of statistical estimation based on dependent random\nsamples.\n","authors":["Yunbum Kook","Santosh S. Vempala"],"pdf_url":"https://arxiv.org/pdf/2411.13462v1.pdf","comment":"60 pages, 1 figure"},{"id":"http://arxiv.org/abs/2411.13459v1","updated":"2024-11-20T17:08:38Z","published":"2024-11-20T17:08:38Z","title":"SoK: A Systems Perspective on Compound AI Threats and Countermeasures","summary":"  Large language models (LLMs) used across enterprises often use proprietary\nmodels and operate on sensitive inputs and data. The wide range of attack\nvectors identified in prior research - targeting various software and hardware\ncomponents used in training and inference - makes it extremely challenging to\nenforce confidentiality and integrity policies.\n  As we advance towards constructing compound AI inference pipelines that\nintegrate multiple large language models (LLMs), the attack surfaces expand\nsignificantly. Attackers now focus on the AI algorithms as well as the software\nand hardware components associated with these systems. While current research\noften examines these elements in isolation, we find that combining cross-layer\nattack observations can enable powerful end-to-end attacks with minimal\nassumptions about the threat model. Given, the sheer number of existing attacks\nat each layer, we need a holistic and systemized understanding of different\nattack vectors at each layer.\n  This SoK discusses different software and hardware attacks applicable to\ncompound AI systems and demonstrates how combining multiple attack mechanisms\ncan reduce the threat model assumptions required for an isolated attack. Next,\nwe systematize the ML attacks in lines with the Mitre Att&ck framework to\nbetter position each attack based on the threat model. Finally, we outline the\nexisting countermeasures for both software and hardware layers and discuss the\nnecessity of a comprehensive defense strategy to enable the secure and\nhigh-performance deployment of compound AI systems.\n","authors":["Sarbartha Banerjee","Prateek Sahu","Mulong Luo","Anjo Vahldiek-Oberwagner","Neeraja J. Yadwadkar","Mohit Tiwari"],"pdf_url":"https://arxiv.org/pdf/2411.13459v1.pdf","comment":"13 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2411.10544v2","updated":"2024-11-20T17:06:26Z","published":"2024-11-15T19:32:01Z","title":"Debias-CLR: A Contrastive Learning Based Debiasing Method for\n  Algorithmic Fairness in Healthcare Applications","summary":"  Artificial intelligence based predictive models trained on the clinical notes\ncan be demographically biased. This could lead to adverse healthcare\ndisparities in predicting outcomes like length of stay of the patients. Thus,\nit is necessary to mitigate the demographic biases within these models. We\nproposed an implicit in-processing debiasing method to combat disparate\ntreatment which occurs when the machine learning model predict different\noutcomes for individuals based on the sensitive attributes like gender,\nethnicity, race, and likewise. For this purpose, we used clinical notes of\nheart failure patients and used diagnostic codes, procedure reports and\nphysiological vitals of the patients. We used Clinical BERT to obtain feature\nembeddings within the diagnostic codes and procedure reports, and LSTM\nautoencoders to obtain feature embeddings within the physiological vitals.\nThen, we trained two separate deep learning contrastive learning frameworks,\none for gender and the other for ethnicity to obtain debiased representations\nwithin those demographic traits. We called this debiasing framework Debias-CLR.\nWe leveraged clinical phenotypes of the patients identified in the diagnostic\ncodes and procedure reports in the previous study to measure fairness\nstatistically. We found that Debias-CLR was able to reduce the Single-Category\nWord Embedding Association Test (SC-WEAT) effect size score when debiasing for\ngender and ethnicity. We further found that to obtain fair representations in\nthe embedding space using Debias-CLR, the accuracy of the predictive models on\ndownstream tasks like predicting length of stay of the patients did not get\nreduced as compared to using the un-debiased counterparts for training the\npredictive models. Hence, we conclude that our proposed approach, Debias-CLR is\nfair and representative in mitigating demographic biases and can reduce health\ndisparities.\n","authors":["Ankita Agarwal","Tanvi Banerjee","William Romine","Mia Cajita"],"pdf_url":"https://arxiv.org/pdf/2411.10544v2.pdf","comment":"9 pages, 1 figure, 4 tables. Manuscript accepted at 7th Special\n  Session on HealthCare Data in IEEE Big Data 2024, Washington, D.C"},{"id":"http://arxiv.org/abs/2411.13451v1","updated":"2024-11-20T16:54:15Z","published":"2024-11-20T16:54:15Z","title":"AdaptAgent: Adapting Multimodal Web Agents with Few-Shot Learning from\n  Human Demonstrations","summary":"  State-of-the-art multimodal web agents, powered by Multimodal Large Language\nModels (MLLMs), can autonomously execute many web tasks by processing user\ninstructions and interacting with graphical user interfaces (GUIs). Current\nstrategies for building web agents rely on (i) the generalizability of\nunderlying MLLMs and their steerability via prompting, and (ii) large-scale\nfine-tuning of MLLMs on web-related tasks. However, web agents still struggle\nto automate tasks on unseen websites and domains, limiting their applicability\nto enterprise-specific and proprietary platforms. Beyond generalization from\nlarge-scale pre-training and fine-tuning, we propose building agents for\nfew-shot adaptability using human demonstrations. We introduce the AdaptAgent\nframework that enables both proprietary and open-weights multimodal web agents\nto adapt to new websites and domains using few human demonstrations (up to 2).\nOur experiments on two popular benchmarks -- Mind2Web & VisualWebArena -- show\nthat using in-context demonstrations (for proprietary models) or\nmeta-adaptation demonstrations (for meta-learned open-weights models) boosts\ntask success rate by 3.36% to 7.21% over non-adapted state-of-the-art models,\ncorresponding to a relative increase of 21.03% to 65.75%. Furthermore, our\nadditional analyses (a) show the effectiveness of multimodal demonstrations\nover text-only ones, (b) shed light on the influence of different data\nselection strategies during meta-learning on the generalization of the agent,\nand (c) demonstrate the effect of number of few-shot examples on the web\nagent's success rate. Overall, our results unlock a complementary axis for\ndeveloping widely applicable multimodal web agents beyond large-scale\npre-training and fine-tuning, emphasizing few-shot adaptability.\n","authors":["Gaurav Verma","Rachneet Kaur","Nishan Srishankar","Zhen Zeng","Tucker Balch","Manuela Veloso"],"pdf_url":"https://arxiv.org/pdf/2411.13451v1.pdf","comment":"18 pages, 3 figures, an abridged version to appear in NeurIPS 2024\n  AFM Workshop"},{"id":"http://arxiv.org/abs/2410.20886v2","updated":"2024-11-20T16:47:44Z","published":"2024-10-28T10:12:06Z","title":"CODES: Benchmarking Coupled ODE Surrogates","summary":"  We introduce CODES, a benchmark for comprehensive evaluation of surrogate\narchitectures for coupled ODE systems. Besides standard metrics like mean\nsquared error (MSE) and inference time, CODES provides insights into surrogate\nbehaviour across multiple dimensions like interpolation, extrapolation, sparse\ndata, uncertainty quantification and gradient correlation. The benchmark\nemphasizes usability through features such as integrated parallel training, a\nweb-based configuration generator, and pre-implemented baseline models and\ndatasets. Extensive documentation ensures sustainability and provides the\nfoundation for collaborative improvement. By offering a fair and multi-faceted\ncomparison, CODES helps researchers select the most suitable surrogate for\ntheir specific dataset and application while deepening our understanding of\nsurrogate learning behaviour.\n","authors":["Robin Janssen","Immanuel Sulzer","Tobias Buck"],"pdf_url":"https://arxiv.org/pdf/2410.20886v2.pdf","comment":"13 pages, 10 figures, accepted for the Machine Learning and the\n  Physical Sciences workshop at NeurIPS 2024, source code available on GitHub\n  at https://github.com/robin-janssen/CODES-Benchmark"},{"id":"http://arxiv.org/abs/2411.13428v1","updated":"2024-11-20T16:11:20Z","published":"2024-11-20T16:11:20Z","title":"SynEHRgy: Synthesizing Mixed-Type Structured Electronic Health Records\n  using Decoder-Only Transformers","summary":"  Generating synthetic Electronic Health Records (EHRs) offers significant\npotential for data augmentation, privacy-preserving data sharing, and improving\nmachine learning model training. We propose a novel tokenization strategy\ntailored for structured EHR data, which encompasses diverse data types such as\ncovariates, ICD codes, and irregularly sampled time series. Using a GPT-like\ndecoder-only transformer model, we demonstrate the generation of high-quality\nsynthetic EHRs. Our approach is evaluated using the MIMIC-III dataset, and we\nbenchmark the fidelity, utility, and privacy of the generated data against\nstate-of-the-art models.\n","authors":["Hojjat Karami","David Atienza","Anisoara Ionescu"],"pdf_url":"https://arxiv.org/pdf/2411.13428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13425v1","updated":"2024-11-20T16:09:22Z","published":"2024-11-20T16:09:22Z","title":"WaterPark: A Robustness Assessment of Language Model Watermarking","summary":"  To mitigate the misuse of large language models (LLMs), such as\ndisinformation, automated phishing, and academic cheating, there is a pressing\nneed for the capability of identifying LLM-generated texts. Watermarking\nemerges as one promising solution: it plants statistical signals into LLMs'\ngenerative processes and subsequently verifies whether LLMs produce given\ntexts. Various watermarking methods (``watermarkers'') have been proposed; yet,\ndue to the lack of unified evaluation platforms, many critical questions remain\nunder-explored: i) What are the strengths/limitations of various watermarkers,\nespecially their attack robustness? ii) How do various design choices impact\ntheir robustness? iii) How to optimally operate watermarkers in adversarial\nenvironments?\n  To fill this gap, we systematize existing LLM watermarkers and watermark\nremoval attacks, mapping out their design spaces. We then develop WaterPark, a\nunified platform that integrates 10 state-of-the-art watermarkers and 12\nrepresentative attacks. More importantly, leveraging WaterPark, we conduct a\ncomprehensive assessment of existing watermarkers, unveiling the impact of\nvarious design choices on their attack robustness. For instance, a\nwatermarker's resilience to increasingly intensive attacks hinges on its\ncontext dependency. We further explore the best practices to operate\nwatermarkers in adversarial environments. For instance, using a generic\ndetector alongside a watermark-specific detector improves the security of\nvulnerable watermarkers. We believe our study sheds light on current LLM\nwatermarking techniques while WaterPark serves as a valuable testbed to\nfacilitate future research.\n","authors":["Jiacheng Liang","Zian Wang","Lauren Hong","Shouling Ji","Ting Wang"],"pdf_url":"https://arxiv.org/pdf/2411.13425v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2405.00662v3","updated":"2024-11-20T16:07:04Z","published":"2024-05-01T17:50:16Z","title":"No Representation, No Trust: Connecting Representation, Collapse, and\n  Trust Issues in PPO","summary":"  Reinforcement learning (RL) is inherently rife with non-stationarity since\nthe states and rewards the agent observes during training depend on its\nchanging policy. Therefore, networks in deep RL must be capable of adapting to\nnew observations and fitting new targets. However, previous works have observed\nthat networks trained under non-stationarity exhibit an inability to continue\nlearning, termed loss of plasticity, and eventually a collapse in performance.\nFor off-policy deep value-based RL methods, this phenomenon has been correlated\nwith a decrease in representation rank and the ability to fit random targets,\ntermed capacity loss. Although this correlation has generally been attributed\nto neural network learning under non-stationarity, the connection to\nrepresentation dynamics has not been carefully studied in on-policy policy\noptimization methods. In this work, we empirically study representation\ndynamics in Proximal Policy Optimization (PPO) on the Atari and MuJoCo\nenvironments, revealing that PPO agents are also affected by feature rank\ndeterioration and capacity loss. We show that this is aggravated by stronger\nnon-stationarity, ultimately driving the actor's performance to collapse,\nregardless of the performance of the critic. We ask why the trust region,\nspecific to methods like PPO, cannot alleviate or prevent the collapse and find\na connection between representation collapse and the degradation of the trust\nregion, one exacerbating the other. Finally, we present Proximal Feature\nOptimization (PFO), a novel auxiliary loss that, along with other\ninterventions, shows that regularizing the representation dynamics mitigates\nthe performance collapse of PPO agents.\n","authors":["Skander Moalla","Andrea Miele","Daniil Pyatko","Razvan Pascanu","Caglar Gulcehre"],"pdf_url":"https://arxiv.org/pdf/2405.00662v3.pdf","comment":"NeurIPS2024 version. Code and run histories are available at\n  https://github.com/CLAIRE-Labo/no-representation-no-trust"},{"id":"http://arxiv.org/abs/2411.13420v1","updated":"2024-11-20T16:06:28Z","published":"2024-11-20T16:06:28Z","title":"Heuristically Adaptive Diffusion-Model Evolutionary Strategy","summary":"  Diffusion Models represent a significant advancement in generative modeling,\nemploying a dual-phase process that first degrades domain-specific information\nvia Gaussian noise and restores it through a trainable model. This framework\nenables pure noise-to-data generation and modular reconstruction of, images or\nvideos. Concurrently, evolutionary algorithms employ optimization methods\ninspired by biological principles to refine sets of numerical parameters\nencoding potential solutions to rugged objective functions. Our research\nreveals a fundamental connection between diffusion models and evolutionary\nalgorithms through their shared underlying generative mechanisms: both methods\ngenerate high-quality samples via iterative refinement on random initial\ndistributions. By employing deep learning-based diffusion models as generative\nmodels across diverse evolutionary tasks and iteratively refining diffusion\nmodels with heuristically acquired databases, we can iteratively sample\npotentially better-adapted offspring parameters, integrating them into\nsuccessive generations of the diffusion model. This approach achieves efficient\nconvergence toward high-fitness parameters while maintaining explorative\ndiversity. Diffusion models introduce enhanced memory capabilities into\nevolutionary algorithms, retaining historical information across generations\nand leveraging subtle data correlations to generate refined samples. We elevate\nevolutionary algorithms from procedures with shallow heuristics to frameworks\nwith deep memory. By deploying classifier-free guidance for conditional\nsampling at the parameter level, we achieve precise control over evolutionary\nsearch dynamics to further specific genotypical, phenotypical, or\npopulation-wide traits. Our framework marks a major heuristic and algorithmic\ntransition, offering increased flexibility, precision, and control in\nevolutionary optimization processes.\n","authors":["Benedikt Hartl","Yanbo Zhang","Hananel Hazan","Michael Levin"],"pdf_url":"https://arxiv.org/pdf/2411.13420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13410v1","updated":"2024-11-20T15:52:03Z","published":"2024-11-20T15:52:03Z","title":"A Survey On Enhancing Reinforcement Learning in Complex Environments:\n  Insights from Human and LLM Feedback","summary":"  Reinforcement learning (RL) is one of the active fields in machine learning,\ndemonstrating remarkable potential in tackling real-world challenges. Despite\nits promising prospects, this methodology has encountered with issues and\nchallenges, hindering it from achieving the best performance. In particular,\nthese approaches lack decent performance when navigating environments and\nsolving tasks with large observation space, often resulting in\nsample-inefficiency and prolonged learning times. This issue, commonly referred\nto as the curse of dimensionality, complicates decision-making for RL agents,\nnecessitating a careful balance between attention and decision-making. RL\nagents, when augmented with human or large language models' (LLMs) feedback,\nmay exhibit resilience and adaptability, leading to enhanced performance and\naccelerated learning. Such feedback, conveyed through various modalities or\ngranularities including natural language, serves as a guide for RL agents,\naiding them in discerning relevant environmental cues and optimizing\ndecision-making processes. In this survey paper, we mainly focus on problems of\ntwo-folds: firstly, we focus on humans or an LLMs assistance, investigating the\nways in which these entities may collaborate with the RL agent in order to\nfoster optimal behavior and expedite learning; secondly, we delve into the\nresearch papers dedicated to addressing the intricacies of environments\ncharacterized by large observation space.\n","authors":["Alireza Rashidi Laleh","Majid Nili Ahmadabadi"],"pdf_url":"https://arxiv.org/pdf/2411.13410v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13405v1","updated":"2024-11-20T15:45:08Z","published":"2024-11-20T15:45:08Z","title":"On the Way to LLM Personalization: Learning to Remember User\n  Conversations","summary":"  Large Language Models (LLMs) have quickly become an invaluable assistant for\na variety of tasks. However, their effectiveness is constrained by their\nability to tailor responses to human preferences and behaviors via\npersonalization. Prior work in LLM personalization has largely focused on style\ntransfer or incorporating small factoids about the user, as knowledge injection\nremains an open challenge. In this paper, we explore injecting knowledge of\nprior conversations into LLMs to enable future work on less redundant,\npersonalized conversations. We identify two real-world constraints: (1)\nconversations are sequential in time and must be treated as such during\ntraining, and (2) per-user personalization is only viable in\nparameter-efficient settings. To this aim, we propose PLUM, a pipeline\nperforming data augmentation for up-sampling conversations as question-answer\npairs, that are then used to finetune a low-rank adaptation adapter with a\nweighted cross entropy loss. Even in this first exploration of the problem, we\nperform competitively with baselines such as RAG, attaining an accuracy of\n81.5% across 100 conversations.\n","authors":["Lucie Charlotte Magister","Katherine Metcalf","Yizhe Zhang","Maartje ter Hoeve"],"pdf_url":"https://arxiv.org/pdf/2411.13405v1.pdf","comment":"16 pages, 6 tables, 3 figures"},{"id":"http://arxiv.org/abs/2411.12600v2","updated":"2024-11-20T15:01:04Z","published":"2024-11-19T16:04:31Z","title":"Provable unlearning in topic modeling and downstream tasks","summary":"  Machine unlearning algorithms are increasingly important as legal concerns\narise around the provenance of training data, but verifying the success of\nunlearning is often difficult. Provable guarantees for unlearning are often\nlimited to supervised learning settings. In this paper, we provide the first\ntheoretical guarantees for unlearning in the pre-training and fine-tuning\nparadigm by studying topic models, simple bag-of-words language models that can\nbe adapted to solve downstream tasks like retrieval and classification. First,\nwe design a provably effective unlearning algorithm for topic models that\nincurs a computational overhead independent of the size of the original\ndataset. Our analysis additionally quantifies the deletion capacity of the\nmodel -- i.e., the number of examples that can be unlearned without incurring a\nsignificant cost in model performance. Finally, we formally extend our analyses\nto account for adaptation to a given downstream task. In particular, we design\nan efficient algorithm to perform unlearning after fine-tuning the topic model\nvia a linear head. Notably, we show that it is easier to unlearn pre-training\ndata from models that have been fine-tuned to a particular task, and one can\nunlearn this data without modifying the base model.\n","authors":["Stanley Wei","Sadhika Malladi","Sanjeev Arora","Amartya Sanyal"],"pdf_url":"https://arxiv.org/pdf/2411.12600v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13376v1","updated":"2024-11-20T14:58:32Z","published":"2024-11-20T14:58:32Z","title":"ODTE -- An ensemble of multi-class SVM-based oblique decision trees","summary":"  We propose ODTE, a new ensemble that uses oblique decision trees as base\nclassifiers. Additionally, we introduce STree, the base algorithm for growing\noblique decision trees, which leverages support vector machines to define\nhyperplanes within the decision nodes. We embed a multiclass strategy --\none-vs-one or one-vs-rest -- at the decision nodes, allowing the model to\ndirectly handle non-binary classification tasks without the need to cluster\ninstances into two groups, as is common in other approaches from the\nliterature. In each decision node, only the best-performing model SVM -- the\none that minimizes an impurity measure for the n-ary classification -- is\nretained, even if the learned SVM addresses a binary classification subtask. An\nextensive experimental study involving 49 datasets and various state-of-the-art\nalgorithms for oblique decision tree ensembles has been conducted. Our results\nshow that ODTE ranks consistently above its competitors, achieving significant\nperformance gains when hyperparameters are carefully tuned. Moreover, the\noblique decision trees learned through STree are more compact than those\nproduced by other algorithms evaluated in our experiments.\n","authors":["Ricardo Montañana","José A. Gámez","José M. Puerta"],"pdf_url":"https://arxiv.org/pdf/2411.13376v1.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2411.13366v1","updated":"2024-11-20T14:42:53Z","published":"2024-11-20T14:42:53Z","title":"Predicting Wall Thickness Changes in Cold Forging Processes: An\n  Integrated FEM and Neural Network approach","summary":"  This study presents a novel approach for predicting wall thickness changes in\ntubes during the nosing process. Specifically, we first provide a thorough\nanalysis of nosing processes and the influencing parameters. We further set-up\na Finite Element Method (FEM) simulation to better analyse the effects of\nvarying process parameters. As however traditional FEM simulations, while\naccurate, are time-consuming and computationally intensive, which renders them\ninapplicable for real-time application, we present a novel modeling framework\nbased on specifically designed graph neural networks as surrogate models. To\nthis end, we extend the neural network architecture by directly incorporating\ninformation about the nosing process by adding different types of edges and\ntheir corresponding encoders to model object interactions. This augmentation\nenhances model accuracy and opens the possibility for employing precise\nsurrogate models within closed-loop production processes. The proposed approach\nis evaluated using a new evaluation metric termed area between thickness curves\n(ABTC). The results demonstrate promising performance and highlight the\npotential of neural networks as surrogate models in predicting wall thickness\nchanges during nosing forging processes.\n","authors":["Sasa Ilic","Abdulkerim Karaman","Johannes Pöppelbaum","Jan Niclas Reimann","Michael Marré","Andreas Schwung"],"pdf_url":"https://arxiv.org/pdf/2411.13366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13365v1","updated":"2024-11-20T14:42:23Z","published":"2024-11-20T14:42:23Z","title":"Explainable Finite-Memory Policies for Partially Observable Markov\n  Decision Processes","summary":"  Partially Observable Markov Decision Processes (POMDPs) are a fundamental\nframework for decision-making under uncertainty and partial observability.\nSince in general optimal policies may require infinite memory, they are hard to\nimplement and often render most problems undecidable. Consequently,\nfinite-memory policies are mostly considered instead. However, the algorithms\nfor computing them are typically very complex, and so are the resulting\npolicies. Facing the need for their explainability, we provide a representation\nof such policies, both (i) in an interpretable formalism and (ii) typically of\nsmaller size, together yielding higher explainability. To that end, we combine\nmodels of Mealy machines and decision trees; the latter describing simple,\nstationary parts of the policies and the former describing how to switch among\nthem. We design a translation for policies of the finite-state-controller (FSC)\nform from standard literature and show how our method smoothly generalizes to\nother variants of finite-memory policies. Further, we identify specific\nproperties of recently used \"attractor-based\" policies, which allow us to\nconstruct yet simpler and smaller representations. Finally, we illustrate the\nhigher explainability in a few case studies.\n","authors":["Muqsit Azeem","Debraj Chakraborty","Sudeep Kanav","Jan Kretinsky"],"pdf_url":"https://arxiv.org/pdf/2411.13365v1.pdf","comment":"Preprint -- Under Review"},{"id":"http://arxiv.org/abs/2402.08823v3","updated":"2024-11-20T14:33:10Z","published":"2024-02-13T22:07:29Z","title":"Random Representations Outperform Online Continually Learned\n  Representations","summary":"  Continual learning has primarily focused on the issue of catastrophic\nforgetting and the associated stability-plasticity tradeoffs. However, little\nattention has been paid to the efficacy of continually learned representations,\nas representations are learned alongside classifiers throughout the learning\nprocess. Our primary contribution is empirically demonstrating that existing\nonline continually trained deep networks produce inferior representations\ncompared to a simple pre-defined random transforms. Our approach projects raw\npixels using a fixed random transform, approximating an RBF-Kernel initialized\nbefore any data is seen. We then train a simple linear classifier on top\nwithout storing any exemplars, processing one sample at a time in an online\ncontinual learning setting. This method, called RanDumb, significantly\noutperforms state-of-the-art continually learned representations across all\nstandard online continual learning benchmarks. Our study reveals the\nsignificant limitations of representation learning, particularly in\nlow-exemplar and online continual learning scenarios. Extending our\ninvestigation to popular exemplar-free scenarios with pretrained models, we\nfind that training only a linear classifier on top of pretrained\nrepresentations surpasses most continual fine-tuning and prompt-tuning\nstrategies. Overall, our investigation challenges the prevailing assumptions\nabout effective representation learning in online continual learning. Our code\nis available at://github.com/drimpossible/RanDumb.\n","authors":["Ameya Prabhu","Shiven Sinha","Ponnurangam Kumaraguru","Philip H. S. Torr","Ozan Sener","Puneet K. Dokania"],"pdf_url":"https://arxiv.org/pdf/2402.08823v3.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.13358v1","updated":"2024-11-20T14:29:59Z","published":"2024-11-20T14:29:59Z","title":"Vertical Validation: Evaluating Implicit Generative Models for Graphs on\n  Thin Support Regions","summary":"  There has been a growing excitement that implicit graph generative models\ncould be used to design or discover new molecules for medicine or material\ndesign. Because these molecules have not been discovered, they naturally lie in\nunexplored or scarcely supported regions of the distribution of known\nmolecules. However, prior evaluation methods for implicit graph generative\nmodels have focused on validating statistics computed from the thick support\n(e.g., mean and variance of a graph property). Therefore, there is a mismatch\nbetween the goal of generating novel graphs and the evaluation methods. To\naddress this evaluation gap, we design a novel evaluation method called\nVertical Validation (VV) that systematically creates thin support regions\nduring the train-test splitting procedure and then reweights generated samples\nso that they can be compared to the held-out test data. This procedure can be\nseen as a generalization of the standard train-test procedure except that the\nsplits are dependent on sample features. We demonstrate that our method can be\nused to perform model selection if performance on thin support regions is the\ndesired goal. As a side benefit, we also show that our approach can better\ndetect overfitting as exemplified by memorization.\n","authors":["Mai Elkady","Thu Bui","Bruno Ribeiro","David I. Inouye"],"pdf_url":"https://arxiv.org/pdf/2411.13358v1.pdf","comment":"Accepted to UAI 2024"},{"id":"http://arxiv.org/abs/2310.19460v3","updated":"2024-11-20T14:24:25Z","published":"2023-10-30T11:33:01Z","title":"Conditional Denoising Diffusion Probabilistic Models for Data\n  Reconstruction Enhancement in Wireless Communications","summary":"  In this paper, conditional denoising diffusion probabilistic models (DDPMs)\nare proposed to enhance the data transmission and reconstruction over wireless\nchannels. The underlying mechanism of DDPM is to decompose the data generation\nprocess over the so-called \"denoising\" steps. Inspired by this, the key idea is\nto leverage the generative prior of diffusion models in learning a\n\"noisy-to-clean\" transformation of the information signal to help enhance data\nreconstruction. The proposed scheme could be beneficial for communication\nscenarios in which a prior knowledge of the information content is available,\ne.g., in multimedia transmission. Hence, instead of employing complicated\nchannel codes that reduce the information rate, one can exploit diffusion\npriors for reliable data reconstruction, especially under extreme channel\nconditions due to low signal-to-noise ratio (SNR), or hardware-impaired\ncommunications. The proposed DDPM-assisted receiver is tailored for the\nscenario of wireless image transmission using MNIST dataset. Our numerical\nresults highlight the reconstruction performance of our scheme compared to the\nconventional digital communication, as well as the deep neural network\n(DNN)-based benchmark. It is also shown that more than 10 dB improvement in the\nreconstruction could be achieved in low SNR regimes, without the need to reduce\nthe information rate for error correction.\n","authors":["Mehdi Letafati","Samad Ali","Matti Latva-aho"],"pdf_url":"https://arxiv.org/pdf/2310.19460v3.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2309.08568"},{"id":"http://arxiv.org/abs/2312.05356v5","updated":"2024-11-20T14:22:06Z","published":"2023-12-08T20:28:08Z","title":"Neuron Patching: Semantic-based Neuron-level Language Model Repair for\n  Code Generation","summary":"  Language Models (LMs) have become widely used in software engineering,\nespecially for tasks such as code generation, where they are referred to as\ncode LMs. These models have proven effective in generating code, making it\neasier for developers to automate coding activities. However, research has\nhighlighted a significant limitation: despite their effectiveness, LMs often\nproduce code that is incorrect, buggy, or not fully functional. Updating these\nmodels with limited data can be prohibitively challenging, yet it is essential\nto maximize their utility. This may require hot-fix techniques (updating models\nwith limited data) to resolve. In this paper, we propose \\ul{M}odel\n\\ul{I}mprovement via \\ul{N}euron \\ul{T}argeting (\\textsc{MINT}), a novel\napproach for repairing code LMs. MINT leverages the semantic property of\nlanguage models to perform neuron-level repairs in a novel way. Further, by\nanalyzing the relationships between the model's latent representations, the\nincorrect outputs, and the desired outputs, \\textsc{MINT} determines which\nneurons are worth updating. This approach ensures that only the neurons crucial\nto the model's failure are targeted, avoiding unnecessary changes and allowing\nfor a more efficient and precise repair process. \\textsc{MINT} is effective,\nefficient, and reliable, capable of correcting a neural model by patching a\nminimum number of neurons (usually one or two neurons). Our approach is\nevaluated on three coding tasks: line-level code generation, shellcode\ngeneration, and intent-to-bash translation. The experimental results\ndemonstrate that the proposed approach significantly outperforms the\nstate-of-the-art in both effectiveness and efficiency measures. In addition, we\nanalyze and discuss the side effects of model repair techniques, including the\nbalance between generalization and specificity, and the performance after\nmultiple repairs in succession.\n","authors":["Jian Gu","Aldeida Aleti","Chunyang Chen","Hongyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.05356v5.pdf","comment":"13 pages, 7 figures, 7 tables, under peer-review"},{"id":"http://arxiv.org/abs/2411.13332v1","updated":"2024-11-20T13:57:32Z","published":"2024-11-20T13:57:32Z","title":"Verifying Machine Unlearning with Explainable AI","summary":"  We investigate the effectiveness of Explainable AI (XAI) in verifying Machine\nUnlearning (MU) within the context of harbor front monitoring, focusing on data\nprivacy and regulatory compliance. With the increasing need to adhere to\nprivacy legislation such as the General Data Protection Regulation (GDPR),\ntraditional methods of retraining ML models for data deletions prove\nimpractical due to their complexity and resource demands. MU offers a solution\nby enabling models to selectively forget specific learned patterns without full\nretraining. We explore various removal techniques, including data relabeling,\nand model perturbation. Then, we leverage attribution-based XAI to discuss the\neffects of unlearning on model performance. Our proof-of-concept introduces\nfeature importance as an innovative verification step for MU, expanding beyond\ntraditional metrics and demonstrating techniques' ability to reduce reliance on\nundesired patterns. Additionally, we propose two novel XAI-based metrics,\nHeatmap Coverage (HC) and Attention Shift (AS), to evaluate the effectiveness\nof these methods. This approach not only highlights how XAI can complement MU\nby providing effective verification, but also sets the stage for future\nresearch to enhance their joint integration.\n","authors":["Àlex Pujol Vidal","Anders S. Johansen","Mohammad N. S. Jahromi","Sergio Escalera","Kamal Nasrollahi","Thomas B. Moeslund"],"pdf_url":"https://arxiv.org/pdf/2411.13332v1.pdf","comment":"ICPRW2024"},{"id":"http://arxiv.org/abs/2209.10081v4","updated":"2024-11-20T13:52:42Z","published":"2022-09-21T03:01:36Z","title":"Revisiting Discrete Soft Actor-Critic","summary":"  We study the adaption of Soft Actor-Critic (SAC), which is considered as a\nstate-of-the-art reinforcement learning (RL) algorithm, from continuous action\nspace to discrete action space. We revisit vanilla discrete SAC and provide an\nin-depth understanding of its Q value underestimation and performance\ninstability issues when applied to discrete settings. We thereby propose Stable\nDiscrete SAC (SDSAC), an algorithm that leverages entropy-penalty and double\naverage Q-learning with Q-clip to address these issues. Extensive experiments\non typical benchmarks with discrete action space, including Atari games and a\nlarge-scale MOBA game, show the efficacy of our proposed method. Our code is\nat: https://github.com/coldsummerday/SD-SAC.git.\n","authors":["Haibin Zhou","Tong Wei","Zichuan Lin","junyou li","Junliang Xing","Yuanchun Shi","Li Shen","Chao Yu","Deheng Ye"],"pdf_url":"https://arxiv.org/pdf/2209.10081v4.pdf","comment":"Accepted by Transactions on Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2411.13323v1","updated":"2024-11-20T13:46:04Z","published":"2024-11-20T13:46:04Z","title":"Are Large Language Models Memorizing Bug Benchmarks?","summary":"  Large Language Models (LLMs) have become integral to various software\nengineering tasks, including code generation, bug detection, and repair. To\nevaluate model performance in these domains, numerous bug benchmarks containing\nreal-world bugs from software projects have been developed. However, a growing\nconcern within the software engineering community is that these benchmarks may\nnot reliably reflect true LLM performance due to the risk of data leakage.\nDespite this concern, limited research has been conducted to quantify the\nimpact of potential leakage.\n  In this paper, we systematically evaluate popular LLMs to assess their\nsusceptibility to data leakage from widely used bug benchmarks. To identify\npotential leakage, we use multiple metrics, including a study of benchmark\nmembership within commonly used training datasets, as well as analyses of\nnegative log-likelihood and n-gram accuracy. Our findings show that certain\nmodels, in particular codegen-multi, exhibit significant evidence of\nmemorization in widely used benchmarks like Defects4J, while newer models\ntrained on larger datasets like LLaMa 3.1 exhibit limited signs of leakage.\nThese results highlight the need for careful benchmark selection and the\nadoption of robust metrics to adequately assess models capabilities.\n","authors":["Daniel Ramos","Claudia Mamede","Kush Jain","Paulo Canelas","Catarina Gamboa","Claire Le Goues"],"pdf_url":"https://arxiv.org/pdf/2411.13323v1.pdf","comment":"pre-print"},{"id":"http://arxiv.org/abs/2411.13322v1","updated":"2024-11-20T13:44:59Z","published":"2024-11-20T13:44:59Z","title":"Scaling Laws for Online Advertisement Retrieval","summary":"  The scaling law is a notable property of neural network models and has\nsignificantly propelled the development of large language models. Scaling laws\nhold great promise in guiding model design and resource allocation. Recent\nresearch increasingly shows that scaling laws are not limited to NLP tasks or\nTransformer architectures; they also apply to domains such as recommendation.\nHowever, there is still a lack of literature on scaling law research in online\nadvertisement retrieval systems. This may be because 1) identifying the scaling\nlaw for resource cost and online revenue is often expensive in both time and\ntraining resources for large-scale industrial applications, and 2) varying\nsettings for different systems prevent the scaling law from being applied\nacross various scenarios. To address these issues, we propose a lightweight\nparadigm to identify the scaling law of online revenue and machine cost for a\ncertain online advertisement retrieval scenario with a low experimental cost.\nSpecifically, we focus on a sole factor (FLOPs) and propose an offline metric\nnamed R/R* that exhibits a high linear correlation with online revenue for\nretrieval models. We estimate the machine cost offline via a simulation\nalgorithm. Thus, we can transform most online experiments into low-cost offline\nexperiments. We conduct comprehensive experiments to verify the effectiveness\nof our proposed metric R/R* and to identify the scaling law in the online\nadvertisement retrieval system of Kuaishou. With the scaling law, we\ndemonstrate practical applications for ROI-constrained model designing and\nmulti-scenario resource allocation in Kuaishou advertising system. To the best\nof our knowledge, this is the first work to study the scaling laws for online\nadvertisement retrieval of real-world systems, showing great potential for\nscaling law in advertising system optimization.\n","authors":["Yunli Wang","Zixuan Yang","Zhen Zhang","Zhiqiang Wang","Jian Yang","Shiyang Wen","Peng Jiang","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2411.13322v1.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2411.06406v2","updated":"2024-11-20T13:39:23Z","published":"2024-11-10T09:57:13Z","title":"Locally Adaptive One-Class Classifier Fusion with Dynamic $\\ell$p-Norm\n  Constraints for Robust Anomaly Detection","summary":"  This paper presents a novel approach to one-class classifier fusion through\nlocally adaptive learning with dynamic $\\ell$p-norm constraints. We introduce a\nframework that dynamically adjusts fusion weights based on local data\ncharacteristics, addressing fundamental challenges in ensemble-based anomaly\ndetection. Our method incorporates an interior-point optimization technique\nthat significantly improves computational efficiency compared to traditional\nFrank-Wolfe approaches, achieving up to 19-fold speed improvements in complex\nscenarios. The framework is extensively evaluated on standard UCI benchmark\ndatasets and specialized temporal sequence datasets, demonstrating superior\nperformance across diverse anomaly types. Statistical validation through\nSkillings-Mack tests confirms our method's significant advantages over existing\napproaches, with consistent top rankings in both pure and non-pure learning\nscenarios. The framework's ability to adapt to local data patterns while\nmaintaining computational efficiency makes it particularly valuable for\nreal-time applications where rapid and accurate anomaly detection is crucial.\n","authors":["Sepehr Nourmohammadi","Arda Sarp Yenicesu","Shervin Rahimzadeh Arashloo","Ozgur S. Oguz"],"pdf_url":"https://arxiv.org/pdf/2411.06406v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12254v2","updated":"2024-11-20T13:24:11Z","published":"2024-11-19T05:58:22Z","title":"Predicting User Intents and Musical Attributes from Music Discovery\n  Conversations","summary":"  Intent classification is a text understanding task that identifies user needs\nfrom input text queries. While intent classification has been extensively\nstudied in various domains, it has not received much attention in the music\ndomain. In this paper, we investigate intent classification models for music\ndiscovery conversation, focusing on pre-trained language models. Rather than\nonly predicting functional needs: intent classification, we also include a task\nfor classifying musical needs: musical attribute classification. Additionally,\nwe propose a method of concatenating previous chat history with just\nsingle-turn user queries in the input text, allowing the model to understand\nthe overall conversation context better. Our proposed model significantly\nimproves the F1 score for both user intent and musical attribute\nclassification, and surpasses the zero-shot and few-shot performance of the\npretrained Llama 3 model.\n","authors":["Daeyong Kwon","SeungHeon Doh","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2411.12254v2.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2311.07633v5","updated":"2024-11-20T13:20:45Z","published":"2023-11-13T13:19:34Z","title":"Benchmarking PtO and PnO Methods in the Predictive Combinatorial\n  Optimization Regime","summary":"  Predictive combinatorial optimization, where the parameters of combinatorial\noptimization (CO) are unknown at the decision-making time, is the precise\nmodeling of many real-world applications, including energy cost-aware\nscheduling and budget allocation on advertising. Tackling such a problem\nusually involves a prediction model and a CO solver. These two modules are\nintegrated into the predictive CO pipeline following two design principles:\n\"Predict-then-Optimize (PtO)\", which learns predictions by supervised training\nand subsequently solves CO using predicted coefficients, while the other, named\n\"Predict-and-Optimize (PnO)\", directly optimizes towards the ultimate decision\nquality and claims to yield better decisions than traditional PtO approaches.\nHowever, there lacks a systematic benchmark of both approaches, including the\nspecific design choices at the module level, as well as an evaluation dataset\nthat covers representative real-world scenarios. To this end, we develop a\nmodular framework to benchmark 11 existing PtO/PnO methods on 8 problems,\nincluding a new industrial dataset for combinatorial advertising that will be\nreleased. Our study shows that PnO approaches are better than PtO on 7 out of 8\nbenchmarks, but there is no silver bullet found for the specific design choices\nof PnO. A comprehensive categorization of current approaches and integration of\ntypical scenarios are provided under a unified benchmark. Therefore, this paper\ncould serve as a comprehensive benchmark for future PnO approach development\nand also offer fast prototyping for application-focused development. The code\nis available at https://github.com/Thinklab-SJTU/PredictiveCO-Benchmark.\n","authors":["Haoyu Geng","Hang Ruan","Runzhong Wang","Yang Li","Yang Wang","Lei Chen","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2311.07633v5.pdf","comment":"NeurIPS 2024 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2410.07117v2","updated":"2024-11-20T13:17:08Z","published":"2024-09-20T08:42:30Z","title":"Classification of Buried Objects from Ground Penetrating Radar Images by\n  using Second Order Deep Learning Models","summary":"  In this paper, a new classification model based on covariance matrices is\nbuilt in order to classify buried objects. The inputs of the proposed models\nare the hyperbola thumbnails obtained with a classical Ground Penetrating Radar\n(GPR) system. These thumbnails are then inputs to the first layers of a\nclassical CNN, which then produces a covariance matrix using the outputs of the\nconvolutional filters. Next, the covariance matrix is given to a network\ncomposed of specific layers to classify Symmetric Positive Definite (SPD)\nmatrices. We show in a large database that our approach outperform shallow\nnetworks designed for GPR data and conventional CNNs typically used in computer\nvision applications, particularly when the number of training data decreases\nand in the presence of mislabeled data. We also illustrate the interest of our\nmodels when training data and test sets are obtained from different weather\nmodes or considerations.\n","authors":["Douba Jafuno","Ammar Mian","Guillaume Ginolhac","Nickolas Stelzenmuller"],"pdf_url":"https://arxiv.org/pdf/2410.07117v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11730v2","updated":"2024-11-20T13:01:18Z","published":"2024-11-18T16:59:44Z","title":"Lifted Model Construction without Normalisation: A Vectorised Approach\n  to Exploit Symmetries in Factor Graphs","summary":"  Lifted probabilistic inference exploits symmetries in a probabilistic model\nto allow for tractable probabilistic inference with respect to domain sizes of\nlogical variables. We found that the current state-of-the-art algorithm to\nconstruct a lifted representation in form of a parametric factor graph misses\nsymmetries between factors that are exchangeable but scaled differently,\nthereby leading to a less compact representation. In this paper, we propose a\ngeneralisation of the advanced colour passing (ACP) algorithm, which is the\nstate of the art to construct a parametric factor graph. Our proposed algorithm\nallows for potentials of factors to be scaled arbitrarily and efficiently\ndetects more symmetries than the original ACP algorithm. By detecting strictly\nmore symmetries than ACP, our algorithm significantly reduces online query\ntimes for probabilistic inference when the resulting model is applied, which we\nalso confirm in our experiments.\n","authors":["Malte Luttermann","Ralf Möller","Marcel Gehrke"],"pdf_url":"https://arxiv.org/pdf/2411.11730v2.pdf","comment":"Accepted to the Proceedings of the 3rd Learning on Graphs Conference\n  (LoG 2024)"},{"id":"http://arxiv.org/abs/2411.13284v1","updated":"2024-11-20T12:52:36Z","published":"2024-11-20T12:52:36Z","title":"DATTA: Domain-Adversarial Test-Time Adaptation for Cross-Domain\n  WiFi-Based Human Activity Recognition","summary":"  Cross-domain generalization is an open problem in WiFi-based sensing due to\nvariations in environments, devices, and subjects, causing domain shifts in\nchannel state information. To address this, we propose Domain-Adversarial\nTest-Time Adaptation (DATTA), a novel framework combining domain-adversarial\ntraining (DAT), test-time adaptation (TTA), and weight resetting to facilitate\nadaptation to unseen target domains and to prevent catastrophic forgetting.\nDATTA is integrated into a lightweight, flexible architecture optimized for\nspeed. We conduct a comprehensive evaluation of DATTA, including an ablation\nstudy on all key components using publicly available data, and verify its\nsuitability for real-time applications such as human activity recognition. When\ncombining a SotA video-based variant of TTA with WiFi-based DAT and comparing\nit to DATTA, our method achieves an 8.1% higher F1-Score. The PyTorch\nimplementation of DATTA is publicly available at:\nhttps://github.com/StrohmayerJ/DATTA.\n","authors":["Julian Strohmayer","Rafael Sterzinger","Matthias Wödlinger","Martin Kampel"],"pdf_url":"https://arxiv.org/pdf/2411.13284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09860v2","updated":"2024-11-20T12:51:25Z","published":"2024-08-19T10:08:25Z","title":"3D-Aware Instance Segmentation and Tracking in Egocentric Videos","summary":"  Egocentric videos present unique challenges for 3D scene understanding due to\nrapid camera motion, frequent object occlusions, and limited object visibility.\nThis paper introduces a novel approach to instance segmentation and tracking in\nfirst-person video that leverages 3D awareness to overcome these obstacles. Our\nmethod integrates scene geometry, 3D object centroid tracking, and instance\nsegmentation to create a robust framework for analyzing dynamic egocentric\nscenes. By incorporating spatial and temporal cues, we achieve superior\nperformance compared to state-of-the-art 2D approaches. Extensive evaluations\non the challenging EPIC Fields dataset demonstrate significant improvements\nacross a range of tracking and segmentation consistency metrics. Specifically,\nour method outperforms the next best performing approach by $7$ points in\nAssociation Accuracy (AssA) and $4.5$ points in IDF1 score, while reducing the\nnumber of ID switches by $73\\%$ to $80\\%$ across various object categories.\nLeveraging our tracked instance segmentations, we showcase downstream\napplications in 3D object reconstruction and amodal video object segmentation\nin these egocentric settings.\n","authors":["Yash Bhalgat","Vadim Tschernezki","Iro Laina","João F. Henriques","Andrea Vedaldi","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2408.09860v2.pdf","comment":"Camera-ready for ACCV 2024. More experiments added"},{"id":"http://arxiv.org/abs/2411.13264v1","updated":"2024-11-20T12:34:06Z","published":"2024-11-20T12:34:06Z","title":"Transformers with Sparse Attention for Granger Causality","summary":"  Temporal causal analysis means understanding the underlying causes behind\nobserved variables over time. Deep learning based methods such as transformers\nare increasingly used to capture temporal dynamics and causal relationships\nbeyond mere correlations. Recent works suggest self-attention weights of\ntransformers as a useful indicator of causal links. We leverage this to propose\na novel modification to the self-attention module to establish causal links\nbetween the variables of multivariate time-series data with varying lag\ndependencies. Our Sparse Attention Transformer captures causal relationships\nusing a two-fold approach - performing temporal attention first followed by\nattention between the variables across the time steps masking them individually\nto compute Granger Causality indices. The key novelty in our approach is the\nability of the model to assert importance and pick the most significant past\ntime instances for its prediction task against manually feeding a fixed time\nlag value. We demonstrate the effectiveness of our approach via extensive\nexperimentation on several synthetic benchmark datasets. Furthermore, we\ncompare the performance of our model with the traditional Vector Autoregression\nbased Granger Causality method that assumes fixed lag length.\n","authors":["Riya Mahesh","Rahul Vashisht","Chandrashekar Lakshminarayanan"],"pdf_url":"https://arxiv.org/pdf/2411.13264v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15182v3","updated":"2024-11-20T12:22:53Z","published":"2024-03-22T13:11:26Z","title":"PDE-CNNs: Axiomatic Derivations and Applications","summary":"  PDE-based Group Convolutional Neural Networks (PDE-G-CNNs) use solvers of\nevolution PDEs as substitutes for the conventional components in G-CNNs.\nPDE-G-CNNs can offer several benefits simultaneously: fewer parameters,\ninherent equivariance, better accuracy, and data efficiency.\n  In this article we focus on Euclidean equivariant PDE-G-CNNs where the\nfeature maps are two-dimensional throughout. We call this variant of the\nframework a PDE-CNN.\n  From a machine learning perspective, we list several practically desirable\naxioms and derive from these which PDEs should be used in a PDE-CNN, this being\nour main contribution. Our approach to geometric learning via PDEs is inspired\nby the axioms of scale-space theory, which we generalize by introducing\nsemifield-valued signals.\n  Our theory reveals new PDEs that can be used in PDE-CNNs and we\nexperimentally examine what impact these have on the accuracy of PDE-CNNs. We\nalso confirm for small networks that PDE-CNNs offer fewer parameters, increased\naccuracy, and better data efficiency when compared to CNNs.\n","authors":["Gijs Bellaard","Sei Sakata","Bart M. N. Smets","Remco Duits"],"pdf_url":"https://arxiv.org/pdf/2403.15182v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13251v1","updated":"2024-11-20T12:09:43Z","published":"2024-11-20T12:09:43Z","title":"BelHouse3D: A Benchmark Dataset for Assessing Occlusion Robustness in 3D\n  Point Cloud Semantic Segmentation","summary":"  Large-scale 2D datasets have been instrumental in advancing machine learning;\nhowever, progress in 3D vision tasks has been relatively slow. This disparity\nis largely due to the limited availability of 3D benchmarking datasets. In\nparticular, creating real-world point cloud datasets for indoor scene semantic\nsegmentation presents considerable challenges, including data collection within\nconfined spaces and the costly, often inaccurate process of per-point labeling\nto generate ground truths. While synthetic datasets address some of these\nchallenges, they often fail to replicate real-world conditions, particularly\nthe occlusions that occur in point clouds collected from real environments.\nExisting 3D benchmarking datasets typically evaluate deep learning models under\nthe assumption that training and test data are independently and identically\ndistributed (IID), which affects the models' usability for real-world point\ncloud segmentation. To address these challenges, we introduce the BelHouse3D\ndataset, a new synthetic point cloud dataset designed for 3D indoor scene\nsemantic segmentation. This dataset is constructed using real-world references\nfrom 32 houses in Belgium, ensuring that the synthetic data closely aligns with\nreal-world conditions. Additionally, we include a test set with data occlusion\nto simulate out-of-distribution (OOD) scenarios, reflecting the occlusions\ncommonly encountered in real-world point clouds. We evaluate popular\npoint-based semantic segmentation methods using our OOD setting and present a\nbenchmark. We believe that BelHouse3D and its OOD setting will advance research\nin 3D point cloud semantic segmentation for indoor scenes, providing valuable\ninsights for the development of more generalizable models.\n","authors":["Umamaheswaran Raman Kumar","Abdur Razzaq Fayjie","Jurgen Hannaert","Patrick Vandewalle"],"pdf_url":"https://arxiv.org/pdf/2411.13251v1.pdf","comment":"20 pages, 6 figures, 3 tables, accepted at ECCV 2024 Workshops"},{"id":"http://arxiv.org/abs/2411.13248v1","updated":"2024-11-20T12:07:19Z","published":"2024-11-20T12:07:19Z","title":"On lower bounds of the density of planar periodic sets without unit\n  distances","summary":"  Determining the maximal density $m_1(\\mathbb{R}^2)$ of planar sets without\nunit distances is a fundamental problem in combinatorial geometry. This paper\ninvestigates lower bounds for this quantity. We introduce a novel approach to\nestimating $m_1(\\mathbb{R}^2)$ by reformulating the problem as a Maximal\nIndependent Set (MIS) problem on graphs constructed from flat torus, focusing\non periodic sets with respect to two non-collinear vectors. Our experimental\nresults supported by theoretical justifications of proposed method demonstrate\nthat for a sufficiently wide range of parameters this approach does not improve\nthe known lower bound $0.22936 \\le m_1(\\mathbb{R}^2)$. The best discrete sets\nfound are approximations of Croft's construction. In addition, several open\nsource software packages for MIS problem are compared on this task.\n","authors":["Alexander Tolmachev"],"pdf_url":"https://arxiv.org/pdf/2411.13248v1.pdf","comment":"21 pages, 9 figures"},{"id":"http://arxiv.org/abs/2411.13223v1","updated":"2024-11-20T11:35:22Z","published":"2024-11-20T11:35:22Z","title":"Existential Conversations with Large Language Models: Content,\n  Community, and Culture","summary":"  Contemporary conversational AI systems based on large language models (LLMs)\ncan engage users on a wide variety of topics, including philosophy,\nspirituality, and religion. Suitably prompted, LLMs can be coaxed into\ndiscussing such existentially significant matters as their own putative\nconsciousness and the role of artificial intelligence in the fate of the\nCosmos. Here we examine two lengthy conversations of this type. We trace likely\nsources, both ancient and modern, for the extensive repertoire of images,\nmyths, metaphors, and conceptual esoterica that the language model draws on\nduring these conversations, and foreground the contemporary communities and\ncultural movements that deploy related motifs, especially in their online\nactivity. Finally, we consider the larger societal impacts of such engagements\nwith LLMs.\n","authors":["Murray Shanahan","Beth Singler"],"pdf_url":"https://arxiv.org/pdf/2411.13223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13211v1","updated":"2024-11-20T11:19:22Z","published":"2024-11-20T11:19:22Z","title":"ViSTa Dataset: Do vision-language models understand sequential tasks?","summary":"  Using vision-language models (VLMs) as reward models in reinforcement\nlearning holds promise for reducing costs and improving safety. So far, VLM\nreward models have only been used for goal-oriented tasks, where the agent must\nreach a particular final outcome. We explore VLMs' potential to supervise tasks\nthat cannot be scored by the final state alone. To this end, we introduce\nViSTa, a dataset for evaluating Vision-based understanding of Sequential Tasks.\nViSTa comprises over 4,000 videos with step-by-step descriptions in virtual\nhome, Minecraft, and real-world environments. Its novel hierarchical structure\n-- basic single-step tasks composed into more and more complex sequential tasks\n-- allows a fine-grained understanding of how well VLMs can judge tasks with\nvarying complexity. To illustrate this, we use ViSTa to evaluate\nstate-of-the-art VLMs, including CLIP, ViCLIP, and GPT-4o. We find that, while\nthey are all good at object recognition, they fail to understand sequential\ntasks, with only GPT-4o achieving non-trivial performance.\n","authors":["Evžen Wybitul","Evan Ryan Gunter","Mikhail Seleznyov"],"pdf_url":"https://arxiv.org/pdf/2411.13211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13207v1","updated":"2024-11-20T11:09:55Z","published":"2024-11-20T11:09:55Z","title":"The Information Security Awareness of Large Language Models","summary":"  The popularity of large language models (LLMs) continues to increase, and\nLLM-based assistants have become ubiquitous, assisting people of diverse\nbackgrounds in many aspects of life. Significant resources have been invested\nin the safety of LLMs and their alignment with social norms. However, research\nexamining their behavior from the information security awareness (ISA)\nperspective is lacking. Chatbots and LLM-based assistants may put unwitting\nusers in harm's way by facilitating unsafe behavior. We observe that the ISA\ninherent in some of today's most popular LLMs varies significantly, with most\nmodels requiring user prompts with a clear security context to utilize their\nsecurity knowledge and provide safe responses to users. Based on this\nobservation, we created a comprehensive set of 30 scenarios to assess the ISA\nof LLMs. These scenarios benchmark the evaluated models with respect to all\nfocus areas defined in a mobile ISA taxonomy. Among our findings is that ISA is\nmildly affected by changing the model's temperature, whereas adjusting the\nsystem prompt can substantially impact it. This underscores the necessity of\nsetting the right system prompt to mitigate ISA weaknesses. Our findings also\nhighlight the importance of ISA assessment for the development of future\nLLM-based assistants.\n","authors":["Ofir Cohen","Gil Ari Agmon","Asaf Shabtai","Rami Puzis"],"pdf_url":"https://arxiv.org/pdf/2411.13207v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13187v1","updated":"2024-11-20T10:40:08Z","published":"2024-11-20T10:40:08Z","title":"Engagement-Driven Content Generation with Large Language Models","summary":"  Large Language Models (LLMs) exhibit significant persuasion capabilities in\none-on-one interactions, but their influence within social networks remains\nunderexplored. This study investigates the potential social impact of LLMs in\nthese environments, where interconnected users and complex opinion dynamics\npose unique challenges. In particular, we address the following research\nquestion: can LLMs learn to generate meaningful content that maximizes user\nengagement on social networks?\n  To answer this question, we define a pipeline to guide the LLM-based content\ngeneration which employs reinforcement learning with simulated feedback. In our\nframework, the reward is based on an engagement model borrowed from the\nliterature on opinion dynamics and information propagation. Moreover, we force\nthe text generated by the LLM to be aligned with a given topic and to satisfy a\nminimum fluency requirement.\n  Using our framework, we analyze the capabilities and limitations of LLMs in\ntackling the given task, specifically considering the relative positions of the\nLLM as an agent within the social network and the distribution of opinions in\nthe network on the given topic. Our findings show the full potential of LLMs in\ncreating social engagement. Notable properties of our approach are that the\nlearning procedure is adaptive to the opinion distribution of the underlying\nnetwork and agnostic to the specifics of the engagement model, which is\nembedded as a plug-and-play component. In this regard, our approach can be\neasily refined for more complex engagement tasks and interventions in\ncomputational social science.\n  The code used for the experiments is publicly available at\nhttps://anonymous.4open.science/r/EDCG/.\n","authors":["Erica Coppolillo","Marco Minici","Federico Cinus","Francesco Bonchi","Giuseppe Manco"],"pdf_url":"https://arxiv.org/pdf/2411.13187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11886v2","updated":"2024-11-20T10:38:55Z","published":"2024-11-05T11:47:59Z","title":"How Much Data is Enough? Optimization of Data Collection for Artifact\n  Detection in EEG Recordings","summary":"  Objective. Electroencephalography (EEG) is a widely used neuroimaging\ntechnique known for its cost-effectiveness and user-friendliness. However,\nvarious artifacts, particularly biological artifacts like Electromyography\n(EMG) signals, lead to a poor signal-to-noise ratio, limiting the precision of\nanalyses and applications. The currently reported EEG data cleaning performance\nlargely depends on the data used for validation, and in the case of machine\nlearning approaches, also on the data used for training. The data are typically\ngathered either by recruiting subjects to perform specific artifact tasks or by\nintegrating existing datasets. Prevailing approaches, however, tend to rely on\nintuitive, concept-oriented data collection with minimal justification for the\nselection of artifacts and their quantities. Given the substantial costs\nassociated with biological data collection and the pressing need for effective\ndata utilization, we propose an optimization procedure for data-oriented data\ncollection design using deep learning-based artifact detection. Approach. We\napply a binary classification between artifact epochs (time intervals\ncontaining artifacts) and non-artifact epochs (time intervals containing no\nartifact) using three different neural architectures. Our aim is to minimize\ndata collection efforts while preserving the cleaning efficiency. Main results.\nWe were able to reduce the number of artifact tasks from twelve to three and\ndecrease repetitions of isometric contraction tasks from ten to three or\nsometimes even just one. Significance. Our work addresses the need for\neffective data utilization in biological data collection, offering a systematic\nand dynamic quantitative approach. By providing clear justifications for the\nchoices of artifacts and their quantity, we aim to guide future studies toward\nmore effective and economical data collection in EEG and EMG research.\n","authors":["Lu Wang-Nöth","Philipp Heiler","Hai Huang","Daniel Lichtenstern","Alexandra Reichenbach","Luis Flacke","Linus Maisch","Helmut Mayer"],"pdf_url":"https://arxiv.org/pdf/2411.11886v2.pdf","comment":"Several changes of wording. Caption of figure 10 corrected"},{"id":"http://arxiv.org/abs/2401.17739v2","updated":"2024-11-20T10:38:29Z","published":"2024-01-31T10:59:57Z","title":"Operator learning without the adjoint","summary":"  There is a mystery at the heart of operator learning: how can one recover a\nnon-self-adjoint operator from data without probing the adjoint? Current\npractical approaches suggest that one can accurately recover an operator while\nonly using data generated by the forward action of the operator without access\nto the adjoint. However, naively, it seems essential to sample the action of\nthe adjoint. In this paper, we partially explain this mystery by proving that\nwithout querying the adjoint, one can approximate a family of non-self-adjoint\ninfinite-dimensional compact operators via projection onto a Fourier basis. We\nthen apply the result to recovering Green's functions of elliptic partial\ndifferential operators and derive an adjoint-free sample complexity bound.\nWhile existing theory justifies low sample complexity in operator learning,\nours is the first adjoint-free analysis that attempts to close the gap between\ntheory and practice.\n","authors":["Nicolas Boullé","Diana Halikias","Samuel E. Otto","Alex Townsend"],"pdf_url":"https://arxiv.org/pdf/2401.17739v2.pdf","comment":"54 pages, 5 figures, to appear in Journal of Machine Learning\n  Research"},{"id":"http://arxiv.org/abs/2410.11807v2","updated":"2024-11-20T10:33:05Z","published":"2024-10-15T17:34:50Z","title":"Regional Ocean Forecasting with Hierarchical Graph Neural Networks","summary":"  Accurate ocean forecasting systems are vital for understanding marine\ndynamics, which play a crucial role in environmental management and climate\nadaptation strategies. Traditional numerical solvers, while effective, are\ncomputationally expensive and time-consuming. Recent advancements in machine\nlearning have revolutionized weather forecasting, offering fast and\nenergy-efficient alternatives. Building on these advancements, we introduce\nSeaCast, a neural network designed for high-resolution, medium-range ocean\nforecasting. SeaCast employs a graph-based framework to effectively handle the\ncomplex geometry of ocean grids and integrates external forcing data tailored\nto the regional ocean context. Our approach is validated through experiments at\na high spatial resolution using the operational numerical model of the\nMediterranean Sea provided by the Copernicus Marine Service, along with both\nnumerical and data-driven atmospheric forcings.\n","authors":["Daniel Holmberg","Emanuela Clementi","Teemu Roos"],"pdf_url":"https://arxiv.org/pdf/2410.11807v2.pdf","comment":"28 pages, 35 figures. Accepted to the Tackling Climate Change with\n  Machine Learning workshop at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.13169v1","updated":"2024-11-20T10:08:22Z","published":"2024-11-20T10:08:22Z","title":"A Unified Analysis for Finite Weight Averaging","summary":"  Averaging iterations of Stochastic Gradient Descent (SGD) have achieved\nempirical success in training deep learning models, such as Stochastic Weight\nAveraging (SWA), Exponential Moving Average (EMA), and LAtest Weight Averaging\n(LAWA). Especially, with a finite weight averaging method, LAWA can attain\nfaster convergence and better generalization. However, its theoretical\nexplanation is still less explored since there are fundamental differences\nbetween finite and infinite settings. In this work, we first generalize SGD and\nLAWA as Finite Weight Averaging (FWA) and explain their advantages compared to\nSGD from the perspective of optimization and generalization. A key challenge is\nthe inapplicability of traditional methods in the sense of expectation or\noptimal values for infinite-dimensional settings in analyzing FWA's\nconvergence. Second, the cumulative gradients introduced by FWA introduce\nadditional confusion to the generalization analysis, especially making it more\ndifficult to discuss them under different assumptions. Extending the final\niteration convergence analysis to the FWA, this paper, under a convexity\nassumption, establishes a convergence bound\n$\\mathcal{O}(\\log\\left(\\frac{T}{k}\\right)/\\sqrt{T})$, where $k\\in[1, T/2]$ is a\nconstant representing the last $k$ iterations. Compared to SGD with\n$\\mathcal{O}(\\log(T)/\\sqrt{T})$, we prove theoretically that FWA has a faster\nconvergence rate and explain the effect of the number of average points. In the\ngeneralization analysis, we find a recursive representation for bounding the\ncumulative gradient using mathematical induction. We provide bounds for\nconstant and decay learning rates and the convex and non-convex cases to show\nthe good generalization performance of FWA. Finally, experimental results on\nseveral benchmarks verify our theoretical results.\n","authors":["Peng Wang","Li Shen","Zerui Tao","Yan Sun","Guodong Zheng","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2411.13169v1.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2411.13163v1","updated":"2024-11-20T09:59:12Z","published":"2024-11-20T09:59:12Z","title":"Unlocking Historical Clinical Trial Data with ALIGN: A Compositional\n  Large Language Model System for Medical Coding","summary":"  The reuse of historical clinical trial data has significant potential to\naccelerate medical research and drug development. However, interoperability\nchallenges, particularly with missing medical codes, hinders effective data\nintegration across studies. While Large Language Models (LLMs) offer a\npromising solution for automated coding without labeled data, current\napproaches face challenges on complex coding tasks. We introduce ALIGN, a novel\ncompositional LLM-based system for automated, zero-shot medical coding. ALIGN\nfollows a three-step process: (1) diverse candidate code generation; (2)\nself-evaluation of codes and (3) confidence scoring and uncertainty estimation\nenabling human deferral to ensure reliability. We evaluate ALIGN on harmonizing\nmedication terms into Anatomical Therapeutic Chemical (ATC) and medical history\nterms into Medical Dictionary for Regulatory Activities (MedDRA) codes\nextracted from 22 immunology trials. ALIGN outperformed the LLM baselines,\nwhile also providing capabilities for trustworthy deployment. For MedDRA\ncoding, ALIGN achieved high accuracy across all levels, matching RAG and\nexcelling at the most specific levels (87-90% for HLGT). For ATC coding, ALIGN\ndemonstrated superior performance, particularly at lower hierarchy levels (ATC\nLevel 4), with 72-73% overall accuracy and 86-89% accuracy for common\nmedications, outperforming baselines by 7-22%. ALIGN's uncertainty-based\ndeferral improved accuracy by 17% to 90% accuracy with 30% deferral, notably\nenhancing performance on uncommon medications. ALIGN achieves this\ncost-efficiently at \\$0.0007 and \\$0.02 per code for GPT-4o-mini and GPT-4o,\nreducing barriers to clinical adoption. ALIGN advances automated medical coding\nfor clinical trial data, contributing to enhanced data interoperability and\nreusability, positioning it as a promising tool to improve clinical research\nand accelerate drug development.\n","authors":["Nabeel Seedat","Caterina Tozzi","Andrea Hita Ardiaca","Mihaela van der Schaar","James Weatherall","Adam Taylor"],"pdf_url":"https://arxiv.org/pdf/2411.13163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13157v1","updated":"2024-11-20T09:46:30Z","published":"2024-11-20T09:46:30Z","title":"Closer Look at Efficient Inference Methods: A Survey of Speculative\n  Decoding","summary":"  Efficient inference in large language models (LLMs) has become a critical\nfocus as their scale and complexity grow. Traditional autoregressive decoding,\nwhile effective, suffers from computational inefficiencies due to its\nsequential token generation process. Speculative decoding addresses this\nbottleneck by introducing a two-stage framework: drafting and verification. A\nsmaller, efficient model generates a preliminary draft, which is then refined\nby a larger, more sophisticated model. This paper provides a comprehensive\nsurvey of speculative decoding methods, categorizing them into draft-centric\nand model-centric approaches. We discuss key ideas associated with each method,\nhighlighting their potential for scaling LLM inference. This survey aims to\nguide future research in optimizing speculative decoding and its integration\ninto real-world LLM applications.\n","authors":["Hyun Ryu","Eric Kim"],"pdf_url":"https://arxiv.org/pdf/2411.13157v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13153v1","updated":"2024-11-20T09:42:08Z","published":"2024-11-20T09:42:08Z","title":"Long-term Detection System for Six Kinds of Abnormal Behavior of the\n  Elderly Living Alone","summary":"  The proportion of elderly people is increasing worldwide, particularly those\nliving alone in Japan. As elderly people get older, their risks of physical\ndisabilities and health issues increase. To automatically discover these issues\nat a low cost in daily life, sensor-based detection in a smart home is\npromising. As part of the effort towards early detection of abnormal behaviors,\nwe propose a simulator-based detection systems for six typical anomalies: being\nsemi-bedridden, being housebound, forgetting, wandering, fall while walking and\nfall while standing. Our detection system can be customized for various room\nlayout, sensor arrangement and resident's characteristics by training detection\nclassifiers using the simulator with the parameters fitted to individual cases.\nConsidering that the six anomalies that our system detects have various\noccurrence durations, such as being housebound for weeks or lying still for\nseconds after a fall, the detection classifiers of our system produce anomaly\nlabels depending on each anomaly's occurrence duration, e.g., housebound per\nday and falls per second. We propose a method that standardizes the processing\nof sensor data, and uses a simple detection approach. Although the validity\ndepends on the realism of the simulation, numerical evaluations using sensor\ndata that includes a variety of resident behavior patterns over nine years as\ntest data show that (1) the methods for detecting wandering and falls are\ncomparable to previous methods, and (2) the methods for detecting being\nsemi-bedridden, being housebound, and forgetting achieve a sensitivity of over\n0.9 with fewer than one false alarm every 50 days.\n","authors":["Kai Tanaka","Mineichi Kudo","Keigo Kimura","Atsuyoshi Nakamura"],"pdf_url":"https://arxiv.org/pdf/2411.13153v1.pdf","comment":"20 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.23306v2","updated":"2024-11-20T09:11:56Z","published":"2024-10-26T14:27:17Z","title":"Securing Healthcare with Deep Learning: A CNN-Based Model for medical\n  IoT Threat Detection","summary":"  The increasing integration of the Internet of Medical Things (IoMT) into\nhealthcare systems has significantly enhanced patient care but has also\nintroduced critical cybersecurity challenges. This paper presents a novel\napproach based on Convolutional Neural Networks (CNNs) for detecting\ncyberattacks within IoMT environments. Unlike previous studies that\npredominantly utilized traditional machine learning (ML) models or simpler Deep\nNeural Networks (DNNs), the proposed model leverages the capabilities of CNNs\nto effectively analyze the temporal characteristics of network traffic data.\nTrained and evaluated on the CICIoMT2024 dataset, which comprises 18 distinct\ntypes of cyberattacks across a range of IoMT devices, the proposed CNN model\ndemonstrates superior performance compared to previous state-of-the-art\nmethods, achieving a perfect accuracy of 99% in binary, categorical, and\nmulticlass classification tasks. This performance surpasses that of\nconventional ML models such as Logistic Regression, AdaBoost, DNNs, and Random\nForests. These findings highlight the potential of CNNs to substantially\nimprove IoMT cybersecurity, thereby ensuring the protection and integrity of\nconnected healthcare systems.\n","authors":["Alireza Mohamadi","Hosna Ghahramani","Seyyed Amir Asghari","Mehdi Aminian"],"pdf_url":"https://arxiv.org/pdf/2410.23306v2.pdf","comment":"7 pages, 4 figures, Accepted at Iranian Conference on Intelligent\n  Systems (ICIS) 23-24 October, 2024, Sirjan University of Technology, Sirjan,\n  Kerman, Iran. \\c{opyright} 2024 IEEE. Personal use of this material is\n  permitted. The accepted version is shared here. For the final published\n  version, refer to the IEEE Xplore Digital Library"},{"id":"http://arxiv.org/abs/2410.15665v3","updated":"2024-11-20T09:08:14Z","published":"2024-10-21T06:09:30Z","title":"Long Term Memory: The Foundation of AI Self-Evolution","summary":"  Large language models (LLMs) like GPTs, trained on vast datasets, have\ndemonstrated impressive capabilities in language understanding, reasoning, and\nplanning, achieving human-level performance in various tasks. Most studies\nfocus on enhancing these models by training on ever-larger datasets to build\nmore powerful foundation models. While training stronger models is important,\nenabling models to evolve during inference is equally crucial, a process we\nrefer to as AI self-evolution. Unlike large-scale training, self-evolution may\nrely on limited data or interactions. Inspired by the columnar organization of\nthe human cerebral cortex, we hypothesize that AI models could develop\ncognitive abilities and build internal representations through iterative\ninteractions with their environment. To achieve this, models need long-term\nmemory (LTM) to store and manage processed interaction data. LTM supports\nself-evolution by representing diverse experiences across environments and\nagents. In this report, we explore AI self-evolution and its potential to\nenhance models during inference. We examine LTM's role in lifelong learning,\nallowing models to evolve based on accumulated interactions. We outline the\nstructure of LTM and the systems needed for effective data retention and\nrepresentation. We also classify approaches for building personalized models\nwith LTM data and show how these models achieve self-evolution through\ninteraction. Using LTM, our multi-agent framework OMNE achieved first place on\nthe GAIA benchmark, demonstrating LTM's potential for AI self-evolution.\nFinally, we present a roadmap for future research, emphasizing the importance\nof LTM for advancing AI technology and its practical applications.\n","authors":["Xun Jiang","Feng Li","Han Zhao","Jiaying Wang","Jun Shao","Shihao Xu","Shu Zhang","Weiling Chen","Xavier Tang","Yize Chen","Mengyue Wu","Weizhi Ma","Mengdi Wang","Tianqiao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.15665v3.pdf","comment":"56 pages, 13 figures"},{"id":"http://arxiv.org/abs/2411.13137v1","updated":"2024-11-20T09:05:36Z","published":"2024-11-20T09:05:36Z","title":"Domain Adaptive Unfolded Graph Neural Networks","summary":"  Over the last decade, graph neural networks (GNNs) have made significant\nprogress in numerous graph machine learning tasks. In real-world applications,\nwhere domain shifts occur and labels are often unavailable for a new target\ndomain, graph domain adaptation (GDA) approaches have been proposed to\nfacilitate knowledge transfer from the source domain to the target domain.\nPrevious efforts in tackling distribution shifts across domains have mainly\nfocused on aligning the node embedding distributions generated by the GNNs in\nthe source and target domains. However, as the core part of GDA approaches, the\nimpact of the underlying GNN architecture has received limited attention. In\nthis work, we explore this orthogonal direction, i.e., how to facilitate GDA\nwith architectural enhancement. In particular, we consider a class of GNNs that\nare designed explicitly based on optimization problems, namely unfolded GNNs\n(UGNNs), whose training process can be represented as bi-level optimization.\nEmpirical and theoretical analyses demonstrate that when transferring from the\nsource domain to the target domain, the lower-level objective value generated\nby the UGNNs significantly increases, resulting in an increase in the\nupper-level objective as well. Motivated by this observation, we propose a\nsimple yet effective strategy called cascaded propagation (CP), which is\nguaranteed to decrease the lower-level objective value. The CP strategy is\nwidely applicable to general UGNNs, and we evaluate its efficacy with three\nrepresentative UGNN architectures. Extensive experiments on five real-world\ndatasets demonstrate that the UGNNs integrated with CP outperform\nstate-of-the-art GDA baselines.\n","authors":["Zepeng Zhang","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2411.13137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15894v2","updated":"2024-11-20T09:04:29Z","published":"2024-05-24T19:32:48Z","title":"Derivatives of Stochastic Gradient Descent in parametric optimization","summary":"  We consider stochastic optimization problems where the objective depends on\nsome parameter, as commonly found in hyperparameter optimization for instance.\nWe investigate the behavior of the derivatives of the iterates of Stochastic\nGradient Descent (SGD) with respect to that parameter and show that they are\ndriven by an inexact SGD recursion on a different objective function, perturbed\nby the convergence of the original SGD. This enables us to establish that the\nderivatives of SGD converge to the derivative of the solution mapping in terms\nof mean squared error whenever the objective is strongly convex. Specifically,\nwe demonstrate that with constant step-sizes, these derivatives stabilize\nwithin a noise ball centered at the solution derivative, and that with\nvanishing step-sizes they exhibit $O(\\log(k)^2 / k)$ convergence rates.\nAdditionally, we prove exponential convergence in the interpolation regime. Our\ntheoretical findings are illustrated by numerical experiments on synthetic\ntasks.\n","authors":["Franck Iutzeler","Edouard Pauwels","Samuel Vaiter"],"pdf_url":"https://arxiv.org/pdf/2405.15894v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01215v5","updated":"2024-11-20T08:54:05Z","published":"2024-08-02T12:04:19Z","title":"ZNorm: Z-Score Gradient Normalization Accelerating Skip-Connected\n  Network Training without Architectural Modification","summary":"  The rapid advancements in deep learning necessitate better training methods\nfor deep neural networks (DNNs). As models grow in complexity, vanishing and\nexploding gradients impede performance, particularly in skip-connected\narchitectures like Deep Residual Networks. We propose Z-Score Normalization for\nGradient Descent (ZNorm), an innovative technique that adjusts only the\ngradients without modifying the network architecture to accelerate training and\nimprove model performance. ZNorm normalizes the overall gradients, providing\nconsistent gradient scaling across layers, effectively reducing the risks of\nvanishing and exploding gradients and achieving superior performance. Extensive\nexperiments on CIFAR-10 and medical datasets confirm that ZNorm consistently\noutperforms existing methods under the same experimental settings. In medical\nimaging applications, ZNorm significantly enhances tumor prediction and\nsegmentation accuracy, underscoring its practical utility. These findings\nhighlight ZNorm's potential as a robust and versatile tool for enhancing the\ntraining and effectiveness of deep neural networks, especially in\nskip-connected architectures, across various applications.\n","authors":["Juyoung Yun"],"pdf_url":"https://arxiv.org/pdf/2408.01215v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.15701v2","updated":"2024-11-20T08:44:06Z","published":"2023-12-25T11:53:06Z","title":"Rotation Equivariant Proximal Operator for Deep Unfolding Methods in\n  Image Restoration","summary":"  The deep unfolding approach has attracted significant attention in computer\nvision tasks, which well connects conventional image processing modeling\nmanners with more recent deep learning techniques. Specifically, by\nestablishing a direct correspondence between algorithm operators at each\nimplementation step and network modules within each layer, one can rationally\nconstruct an almost ``white box'' network architecture with high\ninterpretability. In this architecture, only the predefined component of the\nproximal operator, known as a proximal network, needs manual configuration,\nenabling the network to automatically extract intrinsic image priors in a\ndata-driven manner. In current deep unfolding methods, such a proximal network\nis generally designed as a CNN architecture, whose necessity has been proven by\na recent theory. That is, CNN structure substantially delivers the\ntranslational invariant image prior, which is the most universally possessed\nstructural prior across various types of images. However, standard CNN-based\nproximal networks have essential limitations in capturing the rotation symmetry\nprior, another universal structural prior underlying general images. This\nleaves a large room for further performance improvement in deep unfolding\napproaches. To address this issue, this study makes efforts to suggest a\nhigh-accuracy rotation equivariant proximal network that effectively embeds\nrotation symmetry priors into the deep unfolding framework. Especially, we\ndeduce, for the first time, the theoretical equivariant error for such a\ndesigned proximal network with arbitrary layers under arbitrary rotation\ndegrees. This analysis should be the most refined theoretical conclusion for\nsuch error evaluation to date and is also indispensable for supporting the\nrationale behind such networks with intrinsic interpretability requirements.\n","authors":["Jiahong Fu","Qi Xie","Deyu Meng","Zongben Xu"],"pdf_url":"https://arxiv.org/pdf/2312.15701v2.pdf","comment":"Published in TPAMI 2024"},{"id":"http://arxiv.org/abs/2403.05601v2","updated":"2024-11-20T08:42:04Z","published":"2024-03-08T00:02:42Z","title":"Select High-Level Features: Efficient Experts from a Hierarchical\n  Classification Network","summary":"  This study introduces a novel expert generation method that dynamically\nreduces task and computational complexity without compromising predictive\nperformance. It is based on a new hierarchical classification network topology\nthat combines sequential processing of generic low-level features with\nparallelism and nesting of high-level features. This structure allows for the\ninnovative extraction technique: the ability to select only high-level features\nof task-relevant categories. In certain cases, it is possible to skip almost\nall unneeded high-level features, which can significantly reduce the inference\ncost and is highly beneficial in resource-constrained conditions. We believe\nthis method paves the way for future network designs that are lightweight and\nadaptable, making them suitable for a wide range of applications, from compact\nedge devices to large-scale clouds. In terms of dynamic inference our\nmethodology can achieve an exclusion of up to 88.7\\,\\% of parameters and\n73.4\\,\\% fewer giga-multiply accumulate (GMAC) operations, analysis against\ncomparative baselines showing an average reduction of 47.6\\,\\% in parameters\nand 5.8\\,\\% in GMACs across the cases we evaluated.\n","authors":["André Kelm","Niels Hannemann","Bruno Heberle","Lucas Schmidt","Tim Rolff","Christian Wilms","Ehsan Yaghoubi","Simone Frintrop"],"pdf_url":"https://arxiv.org/pdf/2403.05601v2.pdf","comment":"This two-page paper was accepted for a poster presentation at the 5th\n  ICLR 2024 Workshop on Practical ML for Limited/Low Resource Settings\n  (PML4LRS)"},{"id":"http://arxiv.org/abs/2411.13120v1","updated":"2024-11-20T08:30:11Z","published":"2024-11-20T08:30:11Z","title":"Virtual Staining of Label-Free Tissue in Imaging Mass Spectrometry","summary":"  Imaging mass spectrometry (IMS) is a powerful tool for untargeted, highly\nmultiplexed molecular mapping of tissue in biomedical research. IMS offers a\nmeans of mapping the spatial distributions of molecular species in biological\ntissue with unparalleled chemical specificity and sensitivity. However, most\nIMS platforms are not able to achieve microscopy-level spatial resolution and\nlack cellular morphological contrast, necessitating subsequent histochemical\nstaining, microscopic imaging and advanced image registration steps to enable\nmolecular distributions to be linked to specific tissue features and cell\ntypes. Here, we present a virtual histological staining approach that enhances\nspatial resolution and digitally introduces cellular morphological contrast\ninto mass spectrometry images of label-free human tissue using a diffusion\nmodel. Blind testing on human kidney tissue demonstrated that the virtually\nstained images of label-free samples closely match their histochemically\nstained counterparts (with Periodic Acid-Schiff staining), showing high\nconcordance in identifying key renal pathology structures despite utilizing IMS\ndata with 10-fold larger pixel size. Additionally, our approach employs an\noptimized noise sampling technique during the diffusion model's inference\nprocess to reduce variance in the generated images, yielding reliable and\nrepeatable virtual staining. We believe this virtual staining method will\nsignificantly expand the applicability of IMS in life sciences and open new\navenues for mass spectrometry-based biomedical research.\n","authors":["Yijie Zhang","Luzhe Huang","Nir Pillar","Yuzhu Li","Lukasz G. Migas","Raf Van de Plas","Jeffrey M. Spraggins","Aydogan Ozcan"],"pdf_url":"https://arxiv.org/pdf/2411.13120v1.pdf","comment":"33 Pages, 6 Figures"},{"id":"http://arxiv.org/abs/2411.13117v1","updated":"2024-11-20T08:21:53Z","published":"2024-11-20T08:21:53Z","title":"Compute Optimal Inference and Provable Amortisation Gap in Sparse\n  Autoencoders","summary":"  A recent line of work has shown promise in using sparse autoencoders (SAEs)\nto uncover interpretable features in neural network representations. However,\nthe simple linear-nonlinear encoding mechanism in SAEs limits their ability to\nperform accurate sparse inference. In this paper, we investigate sparse\ninference and learning in SAEs through the lens of sparse coding. Specifically,\nwe show that SAEs perform amortised sparse inference with a computationally\nrestricted encoder and, using compressed sensing theory, we prove that this\nmapping is inherently insufficient for accurate sparse inference, even in\nsolvable cases. Building on this theory, we empirically explore conditions\nwhere more sophisticated sparse inference methods outperform traditional SAE\nencoders. Our key contribution is the decoupling of the encoding and decoding\nprocesses, which allows for a comparison of various sparse encoding strategies.\nWe evaluate these strategies on two dimensions: alignment with true underlying\nsparse features and correct inference of sparse codes, while also accounting\nfor computational costs during training and inference. Our results reveal that\nsubstantial performance gains can be achieved with minimal increases in compute\ncost. We demonstrate that this generalises to SAEs applied to large language\nmodels (LLMs), where advanced encoders achieve similar interpretability. This\nwork opens new avenues for understanding neural network representations and\noffers important implications for improving the tools we use to analyse the\nactivations of large language models.\n","authors":["Charles O'Neill","David Klindt"],"pdf_url":"https://arxiv.org/pdf/2411.13117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11405v2","updated":"2024-11-20T08:20:35Z","published":"2024-11-18T09:27:49Z","title":"Extended Neural Contractive Dynamical Systems: On Multiple Tasks and\n  Riemannian Safety Regions","summary":"  Stability guarantees are crucial when ensuring that a fully autonomous robot\ndoes not take undesirable or potentially harmful actions. We recently proposed\nthe Neural Contractive Dynamical Systems (NCDS), which is a neural network\narchitecture that guarantees contractive stability. With this,\nlearning-from-demonstrations approaches can trivially provide stability\nguarantees. However, our early work left several unanswered questions, which we\nhere address. Beyond providing an in-depth explanation of NCDS, this paper\nextends the framework with more careful regularization, a conditional variant\nof the framework for handling multiple tasks, and an uncertainty-driven\napproach to latent obstacle avoidance. Experiments verify that the developed\nsystem has the flexibility of ordinary neural networks while providing the\nstability guarantees needed for autonomous robotics.\n","authors":["Hadi Beik Mohammadi","Søren Hauberg","Georgios Arvanitidis","Gerhard Neumann","Leonel Rozo"],"pdf_url":"https://arxiv.org/pdf/2411.11405v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2401.09352"},{"id":"http://arxiv.org/abs/2411.13116v1","updated":"2024-11-20T08:20:29Z","published":"2024-11-20T08:20:29Z","title":"Provably Efficient Action-Manipulation Attack Against Continuous\n  Reinforcement Learning","summary":"  Manipulating the interaction trajectories between the intelligent agent and\nthe environment can control the agent's training and behavior, exposing the\npotential vulnerabilities of reinforcement learning (RL). For example, in\nCyber-Physical Systems (CPS) controlled by RL, the attacker can manipulate the\nactions of the adopted RL to other actions during the training phase, which\nwill lead to bad consequences. Existing work has studied action-manipulation\nattacks in tabular settings, where the states and actions are discrete. As seen\nin many up-and-coming RL applications, such as autonomous driving, continuous\naction space is widely accepted, however, its action-manipulation attacks have\nnot been thoroughly investigated yet. In this paper, we consider this crucial\nproblem in both white-box and black-box scenarios. Specifically, utilizing the\nknowledge derived exclusively from trajectories, we propose a black-box attack\nalgorithm named LCBT, which uses the Monte Carlo tree search method for\nefficient action searching and manipulation. Additionally, we demonstrate that\nfor an agent whose dynamic regret is sub-linearly related to the total number\nof steps, LCBT can teach the agent to converge to target policies with only\nsublinear attack cost, i.e., $O\\left(\\mathcal{R}(T) + MH^3K^E\\log\n(MT)\\right)(0<E<1)$, where $H$ is the number of steps per episode, $K$ is the\ntotal number of episodes, $T=KH$ is the total number of steps, $M$ is the\nnumber of subspaces divided in the state space, and $\\mathcal{R}(T)$ is the\nbound of the RL algorithm's regret. We conduct our proposed attack methods on\nthree aggressive algorithms: DDPG, PPO, and TD3 in continuous settings, which\nshow a promising attack performance.\n","authors":["Zhi Luo","Xiyuan Yang","Pan Zhou","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2411.13116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11641v2","updated":"2024-11-20T08:04:43Z","published":"2024-11-18T15:19:54Z","title":"TSINR: Capturing Temporal Continuity via Implicit Neural Representations\n  for Time Series Anomaly Detection","summary":"  Time series anomaly detection aims to identify unusual patterns in data or\ndeviations from systems' expected behavior. The reconstruction-based methods\nare the mainstream in this task, which learn point-wise representation via\nunsupervised learning. However, the unlabeled anomaly points in training data\nmay cause these reconstruction-based methods to learn and reconstruct anomalous\ndata, resulting in the challenge of capturing normal patterns. In this paper,\nwe propose a time series anomaly detection method based on implicit neural\nrepresentation (INR) reconstruction, named TSINR, to address this challenge.\nDue to the property of spectral bias, TSINR enables prioritizing low-frequency\nsignals and exhibiting poorer performance on high-frequency abnormal data.\nSpecifically, we adopt INR to parameterize time series data as a continuous\nfunction and employ a transformer-based architecture to predict the INR of\ngiven data. As a result, the proposed TSINR method achieves the advantage of\ncapturing the temporal continuity and thus is more sensitive to discontinuous\nanomaly data. In addition, we further design a novel form of INR continuous\nfunction to learn inter- and intra-channel information, and leverage a\npre-trained large language model to amplify the intense fluctuations in\nanomalies. Extensive experiments demonstrate that TSINR achieves superior\noverall performance on both univariate and multivariate time series anomaly\ndetection benchmarks compared to other state-of-the-art reconstruction-based\nmethods. Our codes are available.\n","authors":["Mengxuan Li","Ke Liu","Hongyang Chen","Jiajun Bu","Hongwei Wang","Haishuai Wang"],"pdf_url":"https://arxiv.org/pdf/2411.11641v2.pdf","comment":"Accepted by SIGKDD 2025"},{"id":"http://arxiv.org/abs/2411.13104v1","updated":"2024-11-20T07:59:35Z","published":"2024-11-20T07:59:35Z","title":"DRL-Based Optimization for AoI and Energy Consumption in C-V2X Enabled\n  IoV","summary":"  To address communication latency issues, the Third Generation Partnership\nProject (3GPP) has defined Cellular-Vehicle to Everything (C-V2X) technology,\nwhich includes Vehicle-to-Vehicle (V2V) communication for direct\nvehicle-to-vehicle communication. However, this method requires vehicles to\nautonomously select communication resources based on the Semi-Persistent\nScheduling (SPS) protocol, which may lead to collisions due to different\nvehicles sharing the same communication resources, thereby affecting\ncommunication effectiveness. Non-Orthogonal Multiple Access (NOMA) is\nconsidered a potential solution for handling large-scale vehicle communication,\nas it can enhance the Signal-to-Interference-plus-Noise Ratio (SINR) by\nemploying Successive Interference Cancellation (SIC), thereby reducing the\nnegative impact of communication collisions. When evaluating vehicle\ncommunication performance, traditional metrics such as reliability and\ntransmission delay present certain contradictions. Introducing the new metric\nAge of Information (AoI) provides a more comprehensive evaluation of\ncommunication system. Additionally, to ensure service quality, user terminals\nneed to possess high computational capabilities, which may lead to increased\nenergy consumption, necessitating a trade-off between communication energy\nconsumption and effectiveness. Given the complexity and dynamics of\ncommunication systems, Deep Reinforcement Learning (DRL) serves as an\nintelligent learning method capable of learning optimal strategies in dynamic\nenvironments. Therefore, this paper analyzes the effects of multi-priority\nqueues and NOMA on AoI in the C-V2X vehicular communication system and proposes\nan energy consumption and AoI optimization method based on DRL. Finally,\nthrough comparative simulations with baseline methods, the proposed approach\ndemonstrates its advances in terms of energy consumption and AoI.\n","authors":["Zheng Zhang","Qiong Wu","Pingyi Fan","Nan Cheng","Wen Chen","Khaled B. Letaief"],"pdf_url":"https://arxiv.org/pdf/2411.13104v1.pdf","comment":"This paper has been submitted to IEEE Journal. The source code has\n  been released at:\n  https://github.com/qiongwu86/DRL-Based-Optimization-for-Information-of-Age-and-Energy-Consumption-in-C-V2X-Enabled-IoV"},{"id":"http://arxiv.org/abs/2409.18696v3","updated":"2024-11-20T07:51:18Z","published":"2024-09-27T12:34:08Z","title":"Rethinking the Power of Timestamps for Robust Time Series Forecasting: A\n  Global-Local Fusion Perspective","summary":"  Time series forecasting has played a pivotal role across various industries,\nincluding finance, transportation, energy, healthcare, and climate. Due to the\nabundant seasonal information they contain, timestamps possess the potential to\noffer robust global guidance for forecasting techniques. However, existing\nworks primarily focus on local observations, with timestamps being treated\nmerely as an optional supplement that remains underutilized. When data gathered\nfrom the real world is polluted, the absence of global information will damage\nthe robust prediction capability of these algorithms. To address these\nproblems, we propose a novel framework named GLAFF. Within this framework, the\ntimestamps are modeled individually to capture the global dependencies. Working\nas a plugin, GLAFF adaptively adjusts the combined weights for global and local\ninformation, enabling seamless collaboration with any time series forecasting\nbackbone. Extensive experiments conducted on nine real-world datasets\ndemonstrate that GLAFF significantly enhances the average performance of widely\nused mainstream forecasting models by 12.5%, surpassing the previous\nstate-of-the-art method by 5.5%.\n","authors":["Chengsen Wang","Qi Qi","Jingyu Wang","Haifeng Sun","Zirui Zhuang","Jinming Wu","Jianxin Liao"],"pdf_url":"https://arxiv.org/pdf/2409.18696v3.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.13097v1","updated":"2024-11-20T07:49:51Z","published":"2024-11-20T07:49:51Z","title":"Incremental Label Distribution Learning with Scalable Graph\n  Convolutional Networks","summary":"  Label Distribution Learning (LDL) is an effective approach for handling label\nambiguity, as it can analyze all labels at once and indicate the extent to\nwhich each label describes a given sample. Most existing LDL methods consider\nthe number of labels to be static. However, in various LDL-specific contexts\n(e.g., disease diagnosis), the label count grows over time (such as the\ndiscovery of new diseases), a factor that existing methods overlook. Learning\nsamples with new labels directly means learning all labels at once, thus\nwasting more time on the old labels and even risking overfitting the old\nlabels. At the same time, learning new labels by the LDL model means\nreconstructing the inter-label relationships. How to make use of constructed\nrelationships is also a crucial challenge. To tackle these challenges, we\nintroduce Incremental Label Distribution Learning (ILDL), analyze its key\nissues regarding training samples and inter-label relationships, and propose\nScalable Graph Label Distribution Learning (SGLDL) as a practical framework for\nimplementing ILDL. Specifically, in SGLDL, we develop a New-label-aware\nGradient Compensation Loss to speed up the learning of new labels and represent\ninter-label relationships as a graph to reduce the time required to reconstruct\ninter-label relationships. Experimental results on the classical LDL dataset\nshow the clear advantages of unique algorithms and illustrate the importance of\na dedicated design for the ILDL problem.\n","authors":["Ziqi Jia","Xiaoyang Qu","Chenghao Liu","Jianzong Wang"],"pdf_url":"https://arxiv.org/pdf/2411.13097v1.pdf","comment":"Accepted by the 26th IEEE International Conference on High\n  Performance Computing and Communications (HPCC2024)"},{"id":"http://arxiv.org/abs/2408.08272v2","updated":"2024-11-20T07:35:07Z","published":"2024-08-15T17:17:56Z","title":"Is Knowledge Power? On the (Im)possibility of Learning from Strategic\n  Interactions","summary":"  When learning in strategic environments, a key question is whether agents can\novercome uncertainty about their preferences to achieve outcomes they could\nhave achieved absent any uncertainty. Can they do this solely through\ninteractions with each other? We focus this question on the ability of agents\nto attain the value of their Stackelberg optimal strategy and study the impact\nof information asymmetry. We study repeated interactions in fully strategic\nenvironments where players' actions are decided based on learning algorithms\nthat take into account their observed histories and knowledge of the game. We\nstudy the pure Nash equilibria (PNE) of a meta-game where players choose these\nalgorithms as their actions. We demonstrate that if one player has perfect\nknowledge about the game, then any initial informational gap persists. That is,\nwhile there is always a PNE in which the informed agent achieves her\nStackelberg value, there is a game where no PNE of the meta-game allows the\npartially informed player to achieve her Stackelberg value. On the other hand,\nif both players start with some uncertainty about the game, the quality of\ninformation alone does not determine which agent can achieve her Stackelberg\nvalue. In this case, the concept of information asymmetry becomes nuanced and\ndepends on the game's structure. Overall, our findings suggest that repeated\nstrategic interactions alone cannot facilitate learning effectively enough to\nearn an uninformed player her Stackelberg value.\n","authors":["Nivasini Ananthakrishnan","Nika Haghtalab","Chara Podimata","Kunhe Yang"],"pdf_url":"https://arxiv.org/pdf/2408.08272v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13083v1","updated":"2024-11-20T07:20:49Z","published":"2024-11-20T07:20:49Z","title":"Omnipredicting Single-Index Models with Multi-Index Models","summary":"  Recent work on supervised learning [GKR+22] defined the notion of\nomnipredictors, i.e., predictor functions $p$ over features that are\nsimultaneously competitive for minimizing a family of loss functions\n$\\mathcal{L}$ against a comparator class $\\mathcal{C}$. Omniprediction requires\napproximating the Bayes-optimal predictor beyond the loss minimization\nparadigm, and has generated significant interest in the learning theory\ncommunity. However, even for basic settings such as agnostically learning\nsingle-index models (SIMs), existing omnipredictor constructions require\nimpractically-large sample complexities and runtimes, and output complex,\nhighly-improper hypotheses.\n  Our main contribution is a new, simple construction of omnipredictors for\nSIMs. We give a learner outputting an omnipredictor that is\n$\\varepsilon$-competitive on any matching loss induced by a monotone, Lipschitz\nlink function, when the comparator class is bounded linear predictors. Our\nalgorithm requires $\\approx \\varepsilon^{-4}$ samples and runs in nearly-linear\ntime, and its sample complexity improves to $\\approx \\varepsilon^{-2}$ if link\nfunctions are bi-Lipschitz. This significantly improves upon the only prior\nknown construction, due to [HJKRR18, GHK+23], which used $\\gtrsim\n\\varepsilon^{-10}$ samples.\n  We achieve our construction via a new, sharp analysis of the classical\nIsotron algorithm [KS09, KKKS11] in the challenging agnostic learning setting,\nof potential independent interest. Previously, Isotron was known to properly\nlearn SIMs in the realizable setting, as well as constant-factor competitive\nhypotheses under the squared loss [ZWDD24]. As they are based on Isotron, our\nomnipredictors are multi-index models with $\\approx \\varepsilon^{-2}$\nprediction heads, bringing us closer to the tantalizing goal of proper\nomniprediction for general loss families and comparators.\n","authors":["Lunjia Hu","Kevin Tian","Chutong Yang"],"pdf_url":"https://arxiv.org/pdf/2411.13083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00055v5","updated":"2024-11-20T07:08:22Z","published":"2024-08-21T04:47:26Z","title":"SORSA: Singular Values and Orthonormal Regularized Singular Vectors\n  Adaptation of Large Language Models","summary":"  In this paper, we propose Singular Values and Orthonormal Regularized\nSingular Vectors Adaptation, or SORSA, a novel PEFT method. Each SORSA adapter\nconsists of two main parts: trainable principal singular weights $W_p = U_p\n\\text{diag}(S_p) V^\\top_p$, and frozen residual weights $W_r = U_r\n\\text{diag}(S_r) V^\\top_r$. These parts are initialized by performing singular\nvalue decomposition (SVD) on pre-trained weights. Moreover, we implement and\nanalyze an orthonormal regularizer, which we prove could decrease the condition\nnumber of $W_p$ and make the optimization more efficient. SORSA adapters could\nbe merged during inference, thus eliminating any inference latency. We also\nintroduce a method to analyze the variation of the parameters by performing SVD\nand discuss and analyze SORSA's superiority in minimizing the alteration in the\nSVD aspect. After all, SORSA shows a faster convergence than LoRA and PiSSA in\nour experiments. On the GSM-8K benchmark, Llama 2 7B adapted using SORSA\nachieved 56.03% accuracy, surpassing LoRA (42.30%), AdaLoRA (47.30%), Full FT\n(49.05%), and PiSSA (53.07%). On the MATH benchmark, SORSA achieved 10.36%\naccuracy, outperforming LoRA (5.50%), AdaLoRA (6.48%), Full FT (7.22%), and\nPiSSA (7.44%). We conclude that SORSA offers a new perspective on\nparameter-efficient fine-tuning, demonstrating remarkable performance.\n","authors":["Yang Cao"],"pdf_url":"https://arxiv.org/pdf/2409.00055v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03022v3","updated":"2024-11-20T07:07:41Z","published":"2023-12-05T07:27:08Z","title":"Beyond Isolation: Multi-Agent Synergy for Improving Knowledge Graph\n  Construction","summary":"  This paper introduces CooperKGC, a novel framework challenging the\nconventional solitary approach of large language models (LLMs) in knowledge\ngraph construction (KGC). CooperKGC establishes a collaborative processing\nnetwork, assembling a team capable of concurrently addressing entity, relation,\nand event extraction tasks. Experimentation demonstrates that fostering\ncollaboration within CooperKGC enhances knowledge selection, correction, and\naggregation capabilities across multiple rounds of interactions.\n","authors":["Hongbin Ye","Honghao Gui","Aijia Zhang","Tong Liu","Weiqiang Jia"],"pdf_url":"https://arxiv.org/pdf/2312.03022v3.pdf","comment":"Accepted by CCKS 2024, best english candidate paper"},{"id":"http://arxiv.org/abs/2410.11061v4","updated":"2024-11-20T07:03:40Z","published":"2024-10-14T20:14:39Z","title":"Learning to Optimize for Mixed-Integer Non-linear Programming","summary":"  Mixed-integer non-linear programs (MINLPs) arise in various domains, such as\nenergy systems and transportation, but are notoriously difficult to solve.\nRecent advances in machine learning have led to remarkable successes in\noptimization tasks, an area broadly known as learning to optimize. This\napproach includes using predictive models to generate solutions for\noptimization problems with continuous decision variables, thereby avoiding the\nneed for computationally expensive optimization algorithms. However, applying\nlearning to MINLPs remains challenging primarily due to the presence of integer\ndecision variables, which complicate gradient-based learning. To address this\nlimitation, we propose two differentiable correction layers that generate\ninteger outputs while preserving gradient information. Combined with a soft\npenalty for constraint violation, our framework can tackle both the integrality\nand non-linear constraints in a MINLP. Experiments on three problem classes\nwith convex/non-convex objective/constraints and integer/mixed-integer\nvariables show that the proposed learning-based approach consistently produces\nhigh-quality solutions for parametric MINLPs extremely quickly. As problem size\nincreases, traditional exact solvers and heuristic methods struggle to find\nfeasible solutions, whereas our approach continues to deliver reliable results.\nOur work extends the scope of learning-to-optimize to MINLP, paving the way for\nintegrating integer constraints into deep learning models. Our code is\navailable at https://github.com/pnnl/L2O-pMINLP.\n","authors":["Bo Tang","Elias B. Khalil","Ján Drgoňa"],"pdf_url":"https://arxiv.org/pdf/2410.11061v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01441v2","updated":"2024-11-20T06:59:55Z","published":"2024-02-02T14:34:22Z","title":"Learning the Market: Sentiment-Based Ensemble Trading Agents","summary":"  We propose and study the integration of sentiment analysis and deep\nreinforcement learning ensemble algorithms for stock trading by evaluating\nstrategies capable of dynamically altering their active agent given the\nconcurrent market environment. In particular, we design a simple-yet-effective\nmethod for extracting financial sentiment and combine this with improvements on\nexisting trading agents, resulting in a strategy that effectively considers\nboth qualitative market factors and quantitative stock data. We show that our\napproach results in a strategy that is profitable, robust, and risk-minimal -\noutperforming the traditional ensemble strategy as well as single agent\nalgorithms and market metrics. Our findings suggest that the conventional\npractice of switching and reevaluating agents in ensemble every fixed-number of\nmonths is sub-optimal, and that a dynamic sentiment-based framework greatly\nunlocks additional performance. Furthermore, as we have designed our algorithm\nwith simplicity and efficiency in mind, we hypothesize that the transition of\nour method from historical evaluation towards real-time trading with live data\nto be relatively simple.\n","authors":["Andrew Ye","James Xu","Vidyut Veedgav","Yi Wang","Yifan Yu","Daniel Yan","Ryan Chen","Vipin Chaudhary","Shuai Xu"],"pdf_url":"https://arxiv.org/pdf/2402.01441v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01744v2","updated":"2024-11-20T06:56:31Z","published":"2024-09-03T09:41:07Z","title":"Surface Flux Transport Modeling using Physics Informed Neural Networks","summary":"  Studying the magnetic field properties on the solar surface is crucial for\nunderstanding the solar and heliospheric activities, which in turn shape space\nweather in the solar system. Surface Flux Transport (SFT) modeling helps us to\nsimulate and analyse the transport and evolution of magnetic flux on the solar\nsurface, providing valuable insights into the mechanisms responsible for solar\nactivity. In this work, we demonstrate the use of machine learning techniques\nin solving magnetic flux transport, making it accurate. We have developed a\nnovel Physics-Informed Neural Networks (PINN)-based model to study the\nevolution of Bipolar Magnetic Regions (BMRs) using SFT in one-dimensional\nazimuthally averaged and also in two-dimensions. We demonstrate the efficiency\nand computational feasibility of our PINN-based model by comparing its\nperformance and accuracy with that of a numerical model implemented using the\nRunge-Kutta Implicit-Explicit (RK-IMEX) scheme. The mesh-independent PINN\nmethod can be used to reproduce the observed polar magnetic field with better\nflux conservation. This advancement is important for accurately reproducing\nobserved polar magnetic fields, thereby providing insights into the strength of\nfuture solar cycles. This work paves the way for more efficient and accurate\nsimulations of solar magnetic flux transport and showcases the applicability of\nPINN in solving advection-diffusion equations with a particular focus on\nheliophysics.\n","authors":["Jithu J Athalathil","Bhargav Vaidya","Sayan Kundu","Vishal Upendran","Mark C. M. Cheung"],"pdf_url":"https://arxiv.org/pdf/2409.01744v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14267v2","updated":"2024-11-20T06:50:50Z","published":"2024-05-23T07:45:48Z","title":"A Gap in Time: The Challenge of Processing Heterogeneous IoT Data in\n  Digitalized Buildings","summary":"  The increasing demand for sustainable energy solutions has driven the\nintegration of digitalized buildings into the power grid, leveraging\nInternet-of-Things (IoT) technologies to enhance energy efficiency and\noperational performance. Despite their potential, effectively utilizing IoT\npoint data within deep-learning frameworks presents significant challenges,\nprimarily due to its inherent heterogeneity. This study investigates the\ndiverse dimensions of IoT data heterogeneity in both intra-building and\ninter-building contexts, examining their implications for predictive modeling.\nA benchmarking analysis of state-of-the-art time series models highlights their\nperformance on this complex dataset. The results emphasize the critical need\nfor multi-modal data integration, domain-informed modeling, and automated data\nengineering pipelines. Additionally, the study advocates for collaborative\nefforts to establish high-quality public datasets, which are essential for\nadvancing intelligent and sustainable energy management systems in digitalized\nbuildings.\n","authors":["Xiachong Lin","Arian Prabowo","Imran Razzak","Hao Xue","Matthew Amos","Sam Behrens","Flora D. Salim"],"pdf_url":"https://arxiv.org/pdf/2405.14267v2.pdf","comment":"4 figures, 1 tables, 9 pages"},{"id":"http://arxiv.org/abs/2411.13073v1","updated":"2024-11-20T06:50:50Z","published":"2024-11-20T06:50:50Z","title":"Improving OOD Generalization of Pre-trained Encoders via Aligned\n  Embedding-Space Ensembles","summary":"  The quality of self-supervised pre-trained embeddings on out-of-distribution\n(OOD) data is poor without fine-tuning. A straightforward and simple approach\nto improving the generalization of pre-trained representation to OOD data is\nthe use of deep ensembles. However, obtaining an effective ensemble in the\nembedding space with only unlabeled data remains an unsolved problem. We first\nperform a theoretical analysis that reveals the relationship between individual\nhyperspherical embedding spaces in an ensemble. We then design a principled\nmethod to align these embedding spaces in an unsupervised manner. Experimental\nresults on the MNIST dataset show that our embedding-space ensemble method\nimproves pre-trained embedding quality on in-distribution and OOD data compared\nto single encoders.\n","authors":["Shuman Peng","Arash Khoeini","Sharan Vaswani","Martin Ester"],"pdf_url":"https://arxiv.org/pdf/2411.13073v1.pdf","comment":"Accepted at the Self-Supervised Learning Workshop and the Unifying\n  Representations in Neural Models Workshop at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.00388v2","updated":"2024-11-20T06:27:46Z","published":"2024-11-01T06:28:38Z","title":"Towards Data Valuation via Asymmetric Data Shapley","summary":"  As data emerges as a vital driver of technological and economic advancements,\na key challenge is accurately quantifying its value in algorithmic\ndecision-making. The Shapley value, a well-established concept from cooperative\ngame theory, has been widely adopted to assess the contribution of individual\ndata sources in supervised machine learning. However, its symmetry axiom\nassumes all players in the cooperative game are homogeneous, which overlooks\nthe complex structures and dependencies present in real-world datasets. To\naddress this limitation, we extend the traditional data Shapley framework to\nasymmetric data Shapley, making it flexible enough to incorporate inherent\nstructures within the datasets for structure-aware data valuation. We also\nintroduce an efficient $k$-nearest neighbor-based algorithm for its exact\ncomputation. We demonstrate the practical applicability of our framework across\nvarious machine learning tasks and data market contexts. The code is available\nat: https://github.com/xzheng01/Asymmetric-Data-Shapley.\n","authors":["Xi Zheng","Xiangyu Chang","Ruoxi Jia","Yong Tan"],"pdf_url":"https://arxiv.org/pdf/2411.00388v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13055v1","updated":"2024-11-20T06:05:11Z","published":"2024-11-20T06:05:11Z","title":"Hardware Scaling Trends and Diminishing Returns in Large-Scale\n  Distributed Training","summary":"  Dramatic increases in the capabilities of neural network models in recent\nyears are driven by scaling model size, training data, and corresponding\ncomputational resources. To develop the exceedingly large networks required in\nmodern applications, such as large language models (LLMs), model training is\ndistributed across tens of thousands of hardware accelerators (e.g. GPUs),\nrequiring orchestration of computation and communication across large computing\nclusters. In this work, we demonstrate that careful consideration of hardware\nconfiguration and parallelization strategy is critical for effective (i.e.\ncompute- and cost-efficient) scaling of model size, training data, and total\ncomputation. We conduct an extensive empirical study of the performance of\nlarge-scale LLM training workloads across model size, hardware configurations,\nand distributed parallelization strategies. We demonstrate that: (1) beyond\ncertain scales, overhead incurred from certain distributed communication\nstrategies leads parallelization strategies previously thought to be\nsub-optimal in fact become preferable; and (2) scaling the total number of\naccelerators for large model training quickly yields diminishing returns even\nwhen hardware and parallelization strategies are properly optimized, implying\npoor marginal performance per additional unit of power or GPU-hour.\n","authors":["Jared Fernandez","Luca Wehrstedt","Leonid Shamis","Mostafa Elhoushi","Kalyan Saladi","Yonatan Bisk","Emma Strubell","Jacob Kahn"],"pdf_url":"https://arxiv.org/pdf/2411.13055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.09267v5","updated":"2024-11-20T05:58:10Z","published":"2023-02-18T09:24:15Z","title":"Stochastic Approximation Approaches to Group Distributionally Robust\n  Optimization and Beyond","summary":"  This paper investigates group distributionally robust optimization (GDRO)\nwith the goal of learning a model that performs well over $m$ different\ndistributions. First, we formulate GDRO as a stochastic convex-concave\nsaddle-point problem, which is then solved by stochastic mirror descent (SMD)\nwith $m$ samples in each iteration, and attain a nearly optimal sample\ncomplexity. To reduce the number of samples required in each round from $m$ to\n1, we cast GDRO as a two-player game, where one player conducts SMD and the\nother executes an online algorithm for non-oblivious multi-armed bandits,\nmaintaining the same sample complexity. Next, we extend GDRO to address\nscenarios involving imbalanced data and heterogeneous distributions. In the\nfirst scenario, we introduce a weighted variant of GDRO, enabling\ndistribution-dependent convergence rates that rely on the number of samples\nfrom each distribution. We design two strategies to meet the sample budget: one\nintegrates non-uniform sampling into SMD, and the other employs the stochastic\nmirror-prox algorithm with mini-batches, both of which deliver faster rates for\ndistributions with more samples. In the second scenario, we propose to optimize\nthe average top-$k$ risk instead of the maximum risk, thereby mitigating the\nimpact of outlier distributions. Similar to the case of vanilla GDRO, we\ndevelop two stochastic approaches: one uses $m$ samples per iteration via SMD,\nand the other consumes $k$ samples per iteration through an online algorithm\nfor non-oblivious combinatorial semi-bandits.\n","authors":["Lijun Zhang","Haomin Bai","Peng Zhao","Tianbao Yang","Zhi-Hua Zhou"],"pdf_url":"https://arxiv.org/pdf/2302.09267v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13053v1","updated":"2024-11-20T05:57:00Z","published":"2024-11-20T05:57:00Z","title":"MEGL: Multimodal Explanation-Guided Learning","summary":"  Explaining the decision-making processes of Artificial Intelligence (AI)\nmodels is crucial for addressing their \"black box\" nature, particularly in\ntasks like image classification. Traditional eXplainable AI (XAI) methods\ntypically rely on unimodal explanations, either visual or textual, each with\ninherent limitations. Visual explanations highlight key regions but often lack\nrationale, while textual explanations provide context without spatial\ngrounding. Further, both explanation types can be inconsistent or incomplete,\nlimiting their reliability. To address these challenges, we propose a novel\nMultimodal Explanation-Guided Learning (MEGL) framework that leverages both\nvisual and textual explanations to enhance model interpretability and improve\nclassification performance. Our Saliency-Driven Textual Grounding (SDTG)\napproach integrates spatial information from visual explanations into textual\nrationales, providing spatially grounded and contextually rich explanations.\nAdditionally, we introduce Textual Supervision on Visual Explanations to align\nvisual explanations with textual rationales, even in cases where ground truth\nvisual annotations are missing. A Visual Explanation Distribution Consistency\nloss further reinforces visual coherence by aligning the generated visual\nexplanations with dataset-level patterns, enabling the model to effectively\nlearn from incomplete multimodal supervision. We validate MEGL on two new\ndatasets, Object-ME and Action-ME, for image classification with multimodal\nexplanations. Experimental results demonstrate that MEGL outperforms previous\napproaches in prediction accuracy and explanation quality across both visual\nand textual domains. Our code will be made available upon the acceptance of the\npaper.\n","authors":["Yifei Zhang","Tianxu Jiang","Bo Pan","Jingyu Wang","Guangji Bai","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2411.13053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13052v1","updated":"2024-11-20T05:56:31Z","published":"2024-11-20T05:56:31Z","title":"On-device Content-based Recommendation with Single-shot Embedding\n  Pruning: A Cooperative Game Perspective","summary":"  Content-based Recommender Systems (CRSs) play a crucial role in shaping user\nexperiences in e-commerce, online advertising, and personalized\nrecommendations. However, due to the vast amount of categorical features, the\nembedding tables used in CRS models pose a significant storage bottleneck for\nreal-world deployment, especially on resource-constrained devices. To address\nthis problem, various embedding pruning methods have been proposed, but most\nexisting ones require expensive retraining steps for each target parameter\nbudget, leading to enormous computation costs. In reality, this computation\ncost is a major hurdle in real-world applications with diverse storage\nrequirements, such as federated learning and streaming settings. In this paper,\nwe propose Shapley Value-guided Embedding Reduction (Shaver) as our response.\nWith Shaver, we view the problem from a cooperative game perspective, and\nquantify each embedding parameter's contribution with Shapley values to\nfacilitate contribution-based parameter pruning. To address the inherently high\ncomputation costs of Shapley values, we propose an efficient and unbiased\nmethod to estimate Shapley values of a CRS's embedding parameters. Moreover, in\nthe pruning stage, we put forward a field-aware codebook to mitigate the\ninformation loss in the traditional zero-out treatment. Through extensive\nexperiments on three real-world datasets, Shaver has demonstrated competitive\nperformance with lightweight recommendation models across various parameter\nbudgets. The source code is available at\nhttps://anonymous.4open.science/r/shaver-E808\n","authors":["Hung Vinh Tran","Tong Chen","Guanhua Ye","Quoc Viet Hung Nguyen","Kai Zheng","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2411.13052v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2105.03681v3","updated":"2024-11-20T05:53:38Z","published":"2021-05-08T11:43:49Z","title":"Universal Online Convex Optimization Meets Second-order Bounds","summary":"  Recently, several universal methods have been proposed for online convex\noptimization, and attain minimax rates for multiple types of convex functions\nsimultaneously. However, they need to design and optimize one surrogate loss\nfor each type of functions, making it difficult to exploit the structure of the\nproblem and utilize existing algorithms. In this paper, we propose a simple\nstrategy for universal online convex optimization, which avoids these\nlimitations. The key idea is to construct a set of experts to process the\noriginal online functions, and deploy a meta-algorithm over the linearized\nlosses to aggregate predictions from experts. Specifically, the meta-algorithm\nis required to yield a second-order bound with excess losses, so that it can\nleverage strong convexity and exponential concavity to control the meta-regret.\nIn this way, our strategy inherits the theoretical guarantee of any expert\ndesigned for strongly convex functions and exponentially concave functions, up\nto a double logarithmic factor. As a result, we can plug in off-the-shelf\nonline solvers as black-box experts to deliver problem-dependent regret bounds.\nFor general convex functions, it maintains the minimax optimality and also\nachieves a small-loss bound. Furthermore, we extend our universal strategy to\nonline composite optimization, where the loss function comprises a time-varying\nfunction and a fixed regularizer. To deal with the composite loss functions, we\nemploy a meta-algorithm based on the optimistic online learning framework,\nwhich not only possesses a second-order bound, but also can utilize estimations\nfor upcoming loss functions. With appropriate configurations, we demonstrate\nthat the additional regularizer does not contribute to the meta-regret, thus\nmaintaining the universality in the composite setting.\n","authors":["Lijun Zhang","Yibo Wang","Guanghui Wang","Jinfeng Yi","Tianbao Yang"],"pdf_url":"https://arxiv.org/pdf/2105.03681v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10115v2","updated":"2024-11-20T05:35:03Z","published":"2024-02-15T17:10:27Z","title":"Generating Visual Stimuli from EEG Recordings using Transformer-encoder\n  based EEG encoder and GAN","summary":"  In this study, we tackle a modern research challenge within the field of\nperceptual brain decoding, which revolves around synthesizing images from EEG\nsignals using an adversarial deep learning framework. The specific objective is\nto recreate images belonging to various object categories by leveraging EEG\nrecordings obtained while subjects view those images. To achieve this, we\nemploy a Transformer-encoder based EEG encoder to produce EEG encodings, which\nserve as inputs to the generator component of the GAN network. Alongside the\nadversarial loss, we also incorporate perceptual loss to enhance the quality of\nthe generated images.\n","authors":["Rahul Mishra","Arnav Bhavsar"],"pdf_url":"https://arxiv.org/pdf/2402.10115v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03127v2","updated":"2024-11-20T05:13:36Z","published":"2024-11-05T14:18:09Z","title":"Receiver-Centric Generative Semantic Communications","summary":"  This paper investigates semantic communications between a transmitter and a\nreceiver, where original data, such as videos of interest to the receiver, is\nstored at the transmitter. Although significant process has been made in\nsemantic communications, a fundamental design problem is that the semantic\ninformation is extracted based on certain criteria at the transmitter alone,\nwithout considering the receiver's specific information needs. As a result,\ncritical information of primary concern to the receiver may be lost. In such\ncases, the semantic transmission becomes meaningless to the receiver, as all\nreceived information is irrelevant to its interests. To solve this problem,\nthis paper presents a receiver-centric generative semantic communication\nsystem, where each transmission is initialized by the receiver. Specifically,\nthe receiver first sends its request for the desired semantic information to\nthe transmitter at the start of each transmission. Then, the transmitter\nextracts the required semantic information accordingly. A key challenge is how\nthe transmitter understands the receiver's requests for semantic information\nand extracts the required semantic information in a reasonable and robust\nmanner. We address this challenge by designing a well-structured framework and\nleveraging off-the-shelf generative AI products, such as GPT-4, along with\nseveral specialized tools for detection and estimation. Evaluation results\ndemonstrate the feasibility and effectiveness of the proposed new semantic\ncommunication system.\n","authors":["Xunze Liu","Yifei Sun","Zhaorui Wang","Lizhao You","Haoyuan Pan","Fangxin Wang","Shuguang Cui"],"pdf_url":"https://arxiv.org/pdf/2411.03127v2.pdf","comment":"Demo video has been made available at: https://goo.su/dUnAT"},{"id":"http://arxiv.org/abs/2404.10445v3","updated":"2024-11-20T04:51:59Z","published":"2024-04-16T10:31:06Z","title":"SparseDM: Toward Sparse Efficient Diffusion Models","summary":"  Diffusion models have been extensively used in data generation tasks and are\nrecognized as one of the best generative models. However, their time-consuming\ndeployment, long inference time, and requirements on large memory limit their\napplication on mobile devices. In this paper, we propose a method based on the\nimproved Straight-Through Estimator to improve the deployment efficiency of\ndiffusion models. Specifically, we add sparse masks to the Convolution and\nLinear layers in a pre-trained diffusion model, then use design progressive\nsparsity for model training in the fine-tuning stage, and switch the inference\nmask on and off, which supports a flexible choice of sparsity during inference\naccording to the FID and MACs requirements. Experiments on four datasets\nconducted on a state-of-the-art Transformer-based diffusion model demonstrate\nthat our method reduces MACs by $50\\%$ while increasing FID by only 1.5 on\naverage. Under other MACs conditions, the FID is also lower than 1$\\sim$137\ncompared to other methods.\n","authors":["Kafeng Wang","Jianfei Chen","He Li","Zhenpeng Mi","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2404.10445v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13029v1","updated":"2024-11-20T04:21:07Z","published":"2024-11-20T04:21:07Z","title":"Probably Approximately Precision and Recall Learning","summary":"  Precision and Recall are foundational metrics in machine learning where both\naccurate predictions and comprehensive coverage are essential, such as in\nrecommender systems and multi-label learning. In these tasks, balancing\nprecision (the proportion of relevant items among those predicted) and recall\n(the proportion of relevant items successfully predicted) is crucial. A key\nchallenge is that one-sided feedback--where only positive examples are observed\nduring training--is inherent in many practical problems. For instance, in\nrecommender systems like YouTube, training data only consists of videos that a\nuser has actively selected, while unselected items remain unseen. Despite this\nlack of negative feedback in training, avoiding undesirable recommendations at\ntest time is essential.\n  We introduce a PAC learning framework where each hypothesis is represented by\na graph, with edges indicating positive interactions, such as between users and\nitems. This framework subsumes the classical binary and multi-class PAC\nlearning models as well as multi-label learning with partial feedback, where\nonly a single random correct label per example is observed, rather than all\ncorrect labels.\n  Our work uncovers a rich statistical and algorithmic landscape, with nuanced\nboundaries on what can and cannot be learned. Notably, classical methods like\nEmpirical Risk Minimization fail in this setting, even for simple hypothesis\nclasses with only two hypotheses. To address these challenges, we develop novel\nalgorithms that learn exclusively from positive data, effectively minimizing\nboth precision and recall losses. Specifically, in the realizable setting, we\ndesign algorithms that achieve optimal sample complexity guarantees. In the\nagnostic case, we show that it is impossible to achieve additive error\nguarantees--as is standard in PAC learning--and instead obtain meaningful\nmultiplicative approximations.\n","authors":["Lee Cohen","Yishay Mansour","Shay Moran","Han Shao"],"pdf_url":"https://arxiv.org/pdf/2411.13029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13028v1","updated":"2024-11-20T04:20:17Z","published":"2024-11-20T04:20:17Z","title":"A Theory for Compressibility of Graph Transformers for Transductive\n  Learning","summary":"  Transductive tasks on graphs differ fundamentally from typical supervised\nmachine learning tasks, as the independent and identically distributed (i.i.d.)\nassumption does not hold among samples. Instead, all train/test/validation\nsamples are present during training, making them more akin to a semi-supervised\ntask. These differences make the analysis of the models substantially different\nfrom other models. Recently, Graph Transformers have significantly improved\nresults on these datasets by overcoming long-range dependency problems.\nHowever, the quadratic complexity of full Transformers has driven the community\nto explore more efficient variants, such as those with sparser attention\npatterns. While the attention matrix has been extensively discussed, the hidden\ndimension or width of the network has received less attention. In this work, we\nestablish some theoretical bounds on how and under what conditions the hidden\ndimension of these networks can be compressed. Our results apply to both sparse\nand dense variants of Graph Transformers.\n","authors":["Hamed Shirzad","Honghao Lin","Ameya Velingker","Balaji Venkatachalam","David Woodruff","Danica Sutherland"],"pdf_url":"https://arxiv.org/pdf/2411.13028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03768v3","updated":"2024-11-20T03:55:04Z","published":"2024-01-08T09:47:19Z","title":"Corn Yield Prediction Model with Deep Neural Networks for Smallholder\n  Farmer Decision Support System","summary":"  Crop yield prediction has been modeled on the assumption that there is no\ninteraction between weather and soil variables. However, this paper argues that\nan interaction exists, and it can be finely modelled using the Kendall\nCorrelation coefficient. Given the nonlinearity of the interaction between\nweather and soil variables, a deep neural network regressor (DNNR) is carefully\ndesigned with consideration to the depth, number of neurons of the hidden\nlayers, and the hyperparameters with their optimizations. Additionally, a new\nmetric, the average of absolute root squared error (ARSE) is proposed to\ncombine the strengths of root mean square error (RMSE) and mean absolute error\n(MAE). With the ARSE metric, the proposed DNNR(s), optimised random forest\nregressor (RFR) and the extreme gradient boosting regressor (XGBR) achieved\nimpressively small yield errors, 0.0172 t/ha, and 0.0243 t/ha, 0.0001 t/ha, and\n0.001 t/ha, respectively. However, the DNNR(s), with changes to the explanatory\nvariables to ensure generalizability to unforeseen data, DNNR(s) performed\nbest. Further analysis reveals that a strong interaction does exist between\nweather and soil variables. Precisely, yield is observed to increase when\nprecipitation is reduced and silt increased, and vice-versa. However, the\ndegree of decrease or increase is not quantified in this paper. Contrary to\nexisting yield models targeted towards agricultural policies and global food\nsecurity, the goal of the proposed corn yield model is to empower the\nsmallholder farmer to farm smartly and intelligently, thus the prediction model\nis integrated into a mobile application that includes education, and a\nfarmer-to-market access module.\n","authors":["Chollette Olisah","Lyndon Smith","Melvyn Smith","Lawrence Morolake","Osi Ojukwu"],"pdf_url":"https://arxiv.org/pdf/2401.03768v3.pdf","comment":"30 Pages, 11 Figures, 3 Tables"},{"id":"http://arxiv.org/abs/2411.13022v1","updated":"2024-11-20T03:53:41Z","published":"2024-11-20T03:53:41Z","title":"Training Physics-Driven Deep Learning Reconstruction without Raw Data\n  Access for Equitable Fast MRI","summary":"  Physics-driven deep learning (PD-DL) approaches have become popular for\nimproved reconstruction of fast magnetic resonance imaging (MRI) scans. Even\nthough PD-DL offers higher acceleration rates compared to existing clinical\nfast MRI techniques, their use has been limited outside specialized MRI\ncenters. One impediment for their deployment is the difficulties with\ngeneralization to pathologies or population groups that are not\nwell-represented in training sets. This has been noted in several studies, and\nfine-tuning on target populations to improve reconstruction has been suggested.\nHowever, current approaches for PD-DL training require access to raw k-space\nmeasurements, which is typically only available at specialized MRI centers that\nhave research agreements for such data access. This is especially an issue for\nrural and underserved areas, where commercial MRI scanners only provide access\nto a final reconstructed image. To tackle these challenges, we propose\nCompressibility-inspired Unsupervised Learning via Parallel Imaging Fidelity\n(CUPID) for high-quality PD-DL training, using only routine clinical\nreconstructed images exported from an MRI scanner. CUPID evaluates the goodness\nof the output with a compressibility-based approach, while ensuring that the\noutput stays consistent with the clinical parallel imaging reconstruction\nthrough well-designed perturbations. Our results show that CUPID achieves\nsimilar quality compared to well-established PD-DL training strategies that\nrequire raw k-space data access, while outperforming conventional compressed\nsensing (CS) and state-of-the-art generative methods. We also demonstrate its\neffectiveness in a zero-shot training setup for retrospectively and\nprospectively sub-sampled acquisitions, attesting to its minimal training\nburden.\n","authors":["Yaşar Utku Alçalar","Merve Gülle","Mehmet Akçakaya"],"pdf_url":"https://arxiv.org/pdf/2411.13022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13014v1","updated":"2024-11-20T03:34:31Z","published":"2024-11-20T03:34:31Z","title":"Scalable Deep Metric Learning on Attributed Graphs","summary":"  We consider the problem of constructing embeddings of large attributed graphs\nand supporting multiple downstream learning tasks. We develop a graph embedding\nmethod, which is based on extending deep metric and unbiased contrastive\nlearning techniques to 1) work with attributed graphs, 2) enabling a mini-batch\nbased approach, and 3) achieving scalability. Based on a multi-class tuplet\nloss function, we present two algorithms -- DMT for semi-supervised learning\nand DMAT-i for the unsupervised case. Analyzing our methods, we provide a\ngeneralization bound for the downstream node classification task and for the\nfirst time relate tuplet loss to contrastive learning. Through extensive\nexperiments, we show high scalability of representation construction, and in\napplying the method for three downstream tasks (node clustering, node\nclassification, and link prediction) better consistency over any single\nexisting method.\n","authors":["Xiang Li","Gagan Agrawal","Ruoming Jin","Rajiv Ramnath"],"pdf_url":"https://arxiv.org/pdf/2411.13014v1.pdf","comment":"This is the complete version of a published paper with appendix\n  including detailed proofs"},{"id":"http://arxiv.org/abs/2411.13010v1","updated":"2024-11-20T03:24:21Z","published":"2024-11-20T03:24:21Z","title":"Deriving Activation Functions via Integration","summary":"  Activation functions play a crucial role in introducing non-linearities to\ndeep neural networks. We propose a novel approach to designing activation\nfunctions by focusing on their gradients and deriving the corresponding\nfunctions through integration. Our work introduces the Expanded Integral of the\nExponential Linear Unit (xIELU), a trainable piecewise activation function\nderived by integrating trainable affine transformations applied on the ELU\nactivation function. xIELU combines two key gradient properties: a trainable\nand linearly increasing gradient for positive inputs, similar to ReLU$^2$, and\na trainable negative gradient flow for negative inputs, akin to xSiLU.\nConceptually, xIELU can be viewed as extending ReLU$^2$ to effectively handle\nnegative inputs. In experiments with 1.1B parameter Llama models trained on\n126B tokens of FineWeb Edu, xIELU achieves lower perplexity compared to both\nReLU$^2$ and SwiGLU when matched for the same compute cost and parameter count.\n","authors":["Allen Hao Huang"],"pdf_url":"https://arxiv.org/pdf/2411.13010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13009v1","updated":"2024-11-20T03:17:51Z","published":"2024-11-20T03:17:51Z","title":"LLMSteer: Improving Long-Context LLM Inference by Steering Attention on\n  Reused Contexts","summary":"  As large language models (LLMs) show impressive performance on complex tasks,\nthey still struggle with longer contextual understanding and high computational\ncosts. To balance efficiency and quality, we introduce LLMSteer, a\nfine-tuning-free framework that enhances LLMs through query-independent\nattention steering. Tested on popular LLMs and datasets, LLMSteer narrows the\nperformance gap with baselines by 65.9% and reduces the runtime delay by up to\n4.8x compared to recent attention steering methods.\n","authors":["Zhuohan Gu","Jiayi Yao","Kuntai Du","Junchen Jiang"],"pdf_url":"https://arxiv.org/pdf/2411.13009v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13008v1","updated":"2024-11-20T03:16:07Z","published":"2024-11-20T03:16:07Z","title":"Evaluating LLMs Capabilities Towards Understanding Social Dynamics","summary":"  Social media discourse involves people from different backgrounds, beliefs,\nand motives. Thus, often such discourse can devolve into toxic interactions.\nGenerative Models, such as Llama and ChatGPT, have recently exploded in\npopularity due to their capabilities in zero-shot question-answering. Because\nthese models are increasingly being used to ask questions of social\nsignificance, a crucial research question is whether they can understand social\nmedia dynamics. This work provides a critical analysis regarding generative\nLLM's ability to understand language and dynamics in social contexts,\nparticularly considering cyberbullying and anti-cyberbullying (posts aimed at\nreducing cyberbullying) interactions. Specifically, we compare and contrast the\ncapabilities of different large language models (LLMs) to understand three key\naspects of social dynamics: language, directionality, and the occurrence of\nbullying/anti-bullying messages. We found that while fine-tuned LLMs exhibit\npromising results in some social media understanding tasks (understanding\ndirectionality), they presented mixed results in others (proper paraphrasing\nand bullying/anti-bullying detection). We also found that fine-tuning and\nprompt engineering mechanisms can have positive effects in some tasks. We\nbelieve that a understanding of LLM's capabilities is crucial to design future\nmodels that can be effectively used in social applications.\n","authors":["Anique Tahir","Lu Cheng","Manuel Sandoval","Yasin N. Silva","Deborah L. Hall","Huan Liu"],"pdf_url":"https://arxiv.org/pdf/2411.13008v1.pdf","comment":"To appear in ASONAM 24 proceedings"},{"id":"http://arxiv.org/abs/2411.13004v1","updated":"2024-11-20T03:01:41Z","published":"2024-11-20T03:01:41Z","title":"MERLOT: A Distilled LLM-based Mixture-of-Experts Framework for Scalable\n  Encrypted Traffic Classification","summary":"  We present MERLOT, a scalable mixture-of-expert (MoE) based refinement of\ndistilled large language model optimized for encrypted traffic classification.\nBy applying model distillation techniques in a teacher-student paradigm,\ncompact models derived from GPT-2-base retain high classification accuracy\nwhile minimizing computational costs. These models function as specialized\nexperts in an MoE architecture, dynamically assigned via a gating network.\nUnlike generation-based methods, our approach directly classifies encrypted\ntraffic using the final decoder token with contextual feature embedding as\ninput. Experiments on 10 datasets show superior or competitive performance over\nthe state-of-the-art models while significantly reducing resource demands,\nunderscoring its effectiveness and robustness.\n","authors":["Yuxuan Chen","Rongpeng Li","Zhifeng Zhao","Honggang Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.13004v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13000v1","updated":"2024-11-20T02:53:04Z","published":"2024-11-20T02:53:04Z","title":"NCAirFL: CSI-Free Over-the-Air Federated Learning Based on Non-Coherent\n  Detection","summary":"  Over-the-air federated learning (FL), i.e., AirFL, leverages computing\nprimitively over multiple access channels. A long-standing challenge in AirFL\nis to achieve coherent signal alignment without relying on expensive channel\nestimation and feedback. This paper proposes NCAirFL, a CSI-free AirFL scheme\nbased on unbiased non-coherent detection at the edge server. By exploiting\nbinary dithering and a long-term memory based error-compensation mechanism,\nNCAirFL achieves a convergence rate of order $\\mathcal{O}(1/\\sqrt{T})$ in terms\nof the average square norm of the gradient for general non-convex and smooth\nobjectives, where $T$ is the number of communication rounds. Experiments\ndemonstrate the competitive performance of NCAirFL compared to vanilla FL with\nideal communications and to coherent transmission-based benchmarks.\n","authors":["Haifeng Wen","Nicolò Michelusi","Osvaldo Simeone","Hong Xing"],"pdf_url":"https://arxiv.org/pdf/2411.13000v1.pdf","comment":"6 pages, 2 figures, submitted for possible publication"},{"id":"http://arxiv.org/abs/2305.11367v2","updated":"2024-11-20T02:47:25Z","published":"2023-05-19T01:06:08Z","title":"Smart Pressure e-Mat for Human Sleeping Posture and Dynamic Activity\n  Recognition","summary":"  With the emphasis on healthcare, early childhood education, and fitness,\nnon-invasive measurement and recognition methods have received more attention.\nPressure sensing has been extensively studied because of its advantages of\nsimple structure, easy access, visualization application, and harmlessness.\nThis paper introduces a Smart Pressure e-Mat (SPeM) system based on\npiezoresistive material, Velostat, for human monitoring applications, including\nrecognition of sleeping postures, sports, and yoga. After a subsystem scans the\ne-mat readings and processes the signal, it generates a pressure image stream.\nDeep neural networks (DNNs) are used to fit and train the pressure image stream\nand recognize the corresponding human behavior. Four sleeping postures and 13\ndynamic activities inspired by Nintendo Switch Ring Fit Adventure (RFA) are\nused as a preliminary validation of the proposed SPeM system. The SPeM system\nachieves high accuracies in both applications, demonstrating the high accuracy\nand generalizability of the models. Compared with other pressure sensor-based\nsystems, SPeM possesses more flexible applications and commercial application\nprospects, with reliable, robust, and repeatable properties.\n","authors":["Liangqi Yuan","Yuan Wei","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2305.11367v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12995v1","updated":"2024-11-20T02:46:15Z","published":"2024-11-20T02:46:15Z","title":"Eliminating Ratio Bias for Gradient-based Simulated Parameter Estimation","summary":"  This article addresses the challenge of parameter calibration in stochastic\nmodels where the likelihood function is not analytically available. We propose\na gradient-based simulated parameter estimation framework, leveraging a\nmulti-time scale algorithm that tackles the issue of ratio bias in both maximum\nlikelihood estimation and posterior density estimation problems. Additionally,\nwe introduce a nested simulation optimization structure, providing theoretical\nanalyses including strong convergence, asymptotic normality, convergence rate,\nand budget allocation strategies for the proposed algorithm. The framework is\nfurther extended to neural network training, offering a novel perspective on\nstochastic approximation in machine learning. Numerical experiments show that\nour algorithm can improve the estimation accuracy and save computational costs.\n","authors":["Zehao Li","Yijie Peng"],"pdf_url":"https://arxiv.org/pdf/2411.12995v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12990v1","updated":"2024-11-20T02:38:24Z","published":"2024-11-20T02:38:24Z","title":"BetterBench: Assessing AI Benchmarks, Uncovering Issues, and\n  Establishing Best Practices","summary":"  AI models are increasingly prevalent in high-stakes environments,\nnecessitating thorough assessment of their capabilities and risks. Benchmarks\nare popular for measuring these attributes and for comparing model performance,\ntracking progress, and identifying weaknesses in foundation and non-foundation\nmodels. They can inform model selection for downstream tasks and influence\npolicy initiatives. However, not all benchmarks are the same: their quality\ndepends on their design and usability. In this paper, we develop an assessment\nframework considering 46 best practices across an AI benchmark's lifecycle and\nevaluate 24 AI benchmarks against it. We find that there exist large quality\ndifferences and that commonly used benchmarks suffer from significant issues.\nWe further find that most benchmarks do not report statistical significance of\ntheir results nor allow for their results to be easily replicated. To support\nbenchmark developers in aligning with best practices, we provide a checklist\nfor minimum quality assurance based on our assessment. We also develop a living\nrepository of benchmark assessments to support benchmark comparability,\naccessible at betterbench.stanford.edu.\n","authors":["Anka Reuel","Amelia Hardy","Chandler Smith","Max Lamparth","Malcolm Hardy","Mykel J. Kochenderfer"],"pdf_url":"https://arxiv.org/pdf/2411.12990v1.pdf","comment":"Accepted as a Spotlight Poster to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.10318v2","updated":"2024-11-20T02:37:27Z","published":"2024-10-14T09:24:48Z","title":"QIANets: Quantum-Integrated Adaptive Networks for Reduced Latency and\n  Improved Inference Times in CNN Models","summary":"  Convolutional neural networks (CNNs) have made significant advances in\ncomputer vision tasks, yet their high inference times and latency often limit\nreal-world applicability. While model compression techniques have gained\npopularity as solutions, they often overlook the critical balance between low\nlatency and uncompromised accuracy. By harnessing quantum-inspired pruning,\ntensor decomposition, and annealing-based matrix factorization - three\nquantum-inspired concepts - we introduce QIANets: a novel approach of\nredesigning the traditional GoogLeNet, DenseNet, and ResNet-18 model\narchitectures to process more parameters and computations whilst maintaining\nlow inference times. Despite experimental limitations, the method was tested\nand evaluated, demonstrating reductions in inference times, along with\neffective accuracy preservations.\n","authors":["Zhumazhan Balapanov","Vanessa Matvei","Olivia Holmberg","Edward Magongo","Jonathan Pei","Kevin Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.10318v2.pdf","comment":"Accepted to NeurIPS 2024 workshop on Neural Compression"},{"id":"http://arxiv.org/abs/2411.12986v1","updated":"2024-11-20T02:27:40Z","published":"2024-11-20T02:27:40Z","title":"Training Bilingual LMs with Data Constraints in the Targeted Language","summary":"  Large language models are trained on massive scrapes of the web, as required\nby current scaling laws. Most progress is made for English, given its abundance\nof high-quality pretraining data. For most other languages, however, such high\nquality pretraining data is unavailable. In this work, we study how to boost\npretrained model performance in a data constrained target language by enlisting\ndata from an auxiliary language for which high quality data is available. We\nstudy this by quantifying the performance gap between training with data in a\ndata-rich auxiliary language compared with training in the target language,\nexploring the benefits of translation systems, studying the limitations of\nmodel scaling for data constrained languages, and proposing new methods for\nupsampling data from the auxiliary language. Our results show that stronger\nauxiliary datasets result in performance gains without modification to the\nmodel or training objective for close languages, and, in particular, that\nperformance gains due to the development of more information-rich English\npretraining datasets can extend to targeted language settings with limited\ndata.\n","authors":["Skyler Seto","Maartje ter Hoeve","He Bai","Natalie Schluter","David Grangier"],"pdf_url":"https://arxiv.org/pdf/2411.12986v1.pdf","comment":"22 pages, 14 figures, 15 tables"},{"id":"http://arxiv.org/abs/2411.12103v2","updated":"2024-11-20T02:23:11Z","published":"2024-11-18T22:31:17Z","title":"Does Unlearning Truly Unlearn? A Black Box Evaluation of LLM Unlearning\n  Methods","summary":"  Large language model unlearning aims to remove harmful information that LLMs\nhave learnt to prevent their use for malicious purposes. LLMU and RMU have been\nproposed as two methods for LLM unlearning, achieving impressive results on\nunlearning benchmarks. We study in detail the efficacy of these methods by\nevaluating their impact on general model capabilities on the WMDP benchmark as\nwell as a biology benchmark we create. Our experiments show that RMU generally\nleads to better preservation of model capabilities, for similar or better\nunlearning. We further test the robustness of these methods and find that doing\n5-shot prompting or rephrasing the question in simple ways can lead to an over\nten-fold increase in accuracy on unlearning benchmarks. Finally, we show that\ntraining on unrelated data can almost completely recover pre-unlearning\nperformance, demonstrating that these methods fail at truly unlearning. The\ncode is available at: https://github.com/JaiDoshi/Knowledge-Erasure.\n","authors":["Jai Doshi","Asa Cooper Stickland"],"pdf_url":"https://arxiv.org/pdf/2411.12103v2.pdf","comment":"9 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.13953v2","updated":"2024-11-20T02:05:31Z","published":"2024-10-17T18:23:33Z","title":"On Diffusion Models for Multi-Agent Partial Observability: Shared\n  Attractors, Error Bounds, and Composite Flow","summary":"  Multiagent systems grapple with partial observability (PO), and the\ndecentralized POMDP (Dec-POMDP) model highlights the fundamental nature of this\nchallenge. Whereas recent approaches to addressing PO have appealed to deep\nlearning models, providing a rigorous understanding of how these models and\ntheir approximation errors affect agents' handling of PO and their interactions\nremain a challenge. In addressing this challenge, we investigate reconstructing\nglobal states from local action-observation histories in Dec-POMDPs using\ndiffusion models. We first find that diffusion models conditioned on local\nhistory represent possible states as stable fixed points. In collectively\nobservable (CO) Dec-POMDPs, individual diffusion models conditioned on agents'\nlocal histories share a unique fixed point corresponding to the global state,\nwhile in non-CO settings, the shared fixed points yield a distribution of\npossible states given joint history. We further find that, with deep learning\napproximation errors, fixed points can deviate from true states and the\ndeviation is negatively correlated to the Jacobian rank. Inspired by this\nlow-rank property, we bound the deviation by constructing a surrogate linear\nregression model that approximates the local behavior of diffusion models. With\nthis bound, we propose a composite diffusion process iterating over agents with\ntheoretical convergence guarantees to the true state.\n","authors":["Tonghan Wang","Heng Dong","Yanchen Jiang","David C. Parkes","Milind Tambe"],"pdf_url":"https://arxiv.org/pdf/2410.13953v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.11383v3","updated":"2024-11-20T01:59:39Z","published":"2021-10-21T18:05:40Z","title":"Finite-Time Complexity of Online Primal-Dual Natural Actor-Critic\n  Algorithm for Constrained Markov Decision Processes","summary":"  We consider a discounted cost constrained Markov decision process (CMDP)\npolicy optimization problem, in which an agent seeks to maximize a discounted\ncumulative reward subject to a number of constraints on discounted cumulative\nutilities. To solve this constrained optimization program, we study an online\nactor-critic variant of a classic primal-dual method where the gradients of\nboth the primal and dual functions are estimated using samples from a single\ntrajectory generated by the underlying time-varying Markov processes. This\nonline primal-dual natural actor-critic algorithm maintains and iteratively\nupdates three variables: a dual variable (or Lagrangian multiplier), a primal\nvariable (or actor), and a critic variable used to estimate the gradients of\nboth primal and dual variables. These variables are updated simultaneously but\non different time scales (using different step sizes) and they are all\nintertwined with each other. Our main contribution is to derive a finite-time\nanalysis for the convergence of this algorithm to the global optimum of a CMDP\nproblem. Specifically, we show that with a proper choice of step sizes the\noptimality gap and constraint violation converge to zero in expectation at a\nrate $\\mathcal{O}(1/K^{1/6})$, where K is the number of iterations. To our\nknowledge, this paper is the first to study the finite-time complexity of an\nonline primal-dual actor-critic method for solving a CMDP problem. We also\nvalidate the effectiveness of this algorithm through numerical simulations.\n","authors":["Sihan Zeng","Thinh T. Doan","Justin Romberg"],"pdf_url":"https://arxiv.org/pdf/2110.11383v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12973v1","updated":"2024-11-20T01:58:20Z","published":"2024-11-20T01:58:20Z","title":"Adaptive Process-Guided Learning: An Application in Predicting Lake DO\n  Concentrations","summary":"  This paper introduces a \\textit{Process-Guided Learning (Pril)} framework\nthat integrates physical models with recurrent neural networks (RNNs) to\nenhance the prediction of dissolved oxygen (DO) concentrations in lakes, which\nis crucial for sustaining water quality and ecosystem health. Unlike\ntraditional RNNs, which may deliver high accuracy but often lack physical\nconsistency and broad applicability, the \\textit{Pril} method incorporates\ndifferential DO equations for each lake layer, modeling it as a first-order\nlinear solution using a forward Euler scheme with a daily timestep. However,\nthis method is sensitive to numerical instabilities. When drastic fluctuations\noccur, the numerical integration is neither mass-conservative nor stable.\nEspecially during stratified conditions, exogenous fluxes into each layer cause\nsignificant within-day changes in DO concentrations. To address this challenge,\nwe further propose an \\textit{Adaptive Process-Guided Learning (April)} model,\nwhich dynamically adjusts timesteps from daily to sub-daily intervals with the\naim of mitigating the discrepancies caused by variations in entrainment fluxes.\n\\textit{April} uses a generator-discriminator architecture to identify days\nwith significant DO fluctuations and employs a multi-step Euler scheme with\nsub-daily timesteps to effectively manage these variations. We have tested our\nmethods on a wide range of lakes in the Midwestern USA, and demonstrated robust\ncapability in predicting DO concentrations even with limited training data.\nWhile primarily focused on aquatic ecosystems, this approach is broadly\napplicable to diverse scientific and engineering disciplines that utilize\nprocess-based models, such as power engineering, climate science, and\nbiomedicine.\n","authors":["Runlong Yu","Chonghao Qiu","Robert Ladwig","Paul C. Hanson","Yiqun Xie","Yanhua Li","Xiaowei Jia"],"pdf_url":"https://arxiv.org/pdf/2411.12973v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12972v1","updated":"2024-11-20T01:54:52Z","published":"2024-11-20T01:54:52Z","title":"A Foundation Model for Unified Urban Spatio-Temporal Flow Prediction","summary":"  Urban spatio-temporal flow prediction, encompassing traffic flows and crowd\nflows, is crucial for optimizing city infrastructure and managing traffic and\nemergency responses. Traditional approaches have relied on separate models\ntailored to either grid-based data, representing cities as uniform cells, or\ngraph-based data, modeling cities as networks of nodes and edges. In this\npaper, we build UniFlow, a foundational model for general urban flow prediction\nthat unifies both grid-based and graphbased data. We first design a multi-view\nspatio-temporal patching mechanism to standardize different data into a\nconsistent sequential format and then introduce a spatio-temporal transformer\narchitecture to capture complex correlations and dynamics. To leverage shared\nspatio-temporal patterns across different data types and facilitate effective\ncross-learning, we propose SpatioTemporal Memory Retrieval Augmentation\n(ST-MRA). By creating structured memory modules to store shared spatio-temporal\npatterns, ST-MRA enhances predictions through adaptive memory retrieval.\nExtensive experiments demonstrate that UniFlow outperforms existing models in\nboth grid-based and graph-based flow prediction, excelling particularly in\nscenarios with limited data availability, showcasing its superior performance\nand broad applicability. The datasets and code implementation have been\nreleased on https://github.com/YuanYuan98/UniFlow.\n","authors":["Yuan Yuan","Jingtao Ding","Chonghua Han","Depeng Jin","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2411.12972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12965v1","updated":"2024-11-20T01:40:53Z","published":"2024-11-20T01:40:53Z","title":"On adaptivity and minimax optimality of two-sided nearest neighbors","summary":"  Nearest neighbor (NN) algorithms have been extensively used for missing data\nproblems in recommender systems and sequential decision-making systems. Prior\ntheoretical analysis has established favorable guarantees for NN when the\nunderlying data is sufficiently smooth and the missingness probabilities are\nlower bounded. Here we analyze NN with non-smooth non-linear functions with\nvast amounts of missingness. In particular, we consider matrix completion\nsettings where the entries of the underlying matrix follow a latent non-linear\nfactor model, with the non-linearity belonging to a \\Holder function class that\nis less smooth than Lipschitz. Our results establish following favorable\nproperties for a suitable two-sided NN: (1) The mean squared error (MSE) of NN\nadapts to the smoothness of the non-linearity, (2) under certain regularity\nconditions, the NN error rate matches the rate obtained by an oracle equipped\nwith the knowledge of both the row and column latent factors, and finally (3)\nNN's MSE is non-trivial for a wide range of settings even when several matrix\nentries might be missing deterministically. We support our theoretical findings\nvia extensive numerical simulations and a case study with data from a mobile\nhealth study, HeartSteps.\n","authors":["Tathagata Sadhukhan","Manit Paul","Raaz Dwivedi"],"pdf_url":"https://arxiv.org/pdf/2411.12965v1.pdf","comment":"29 pages, 7 figures"},{"id":"http://arxiv.org/abs/2210.05558v3","updated":"2024-11-20T01:20:11Z","published":"2022-10-11T15:53:15Z","title":"Causal and Counterfactual Views of Missing Data Models","summary":"  It is often said that the fundamental problem of causal inference is a\nmissing data problem -- the comparison of responses to two hypothetical\ntreatment assignments is made difficult because for every experimental unit\nonly one potential response is observed. In this paper, we consider the\nimplications of the converse view: that missing data problems are a form of\ncausal inference. We make explicit how the missing data problem of recovering\nthe complete data law from the observed law can be viewed as identification of\na joint distribution over counterfactual variables corresponding to values had\nwe (possibly contrary to fact) been able to observe them. Drawing analogies\nwith causal inference, we show how identification assumptions in missing data\ncan be encoded in terms of graphical models defined over counterfactual and\nobserved variables. We review recent results in missing data identification\nfrom this viewpoint. In doing so, we note interesting similarities and\ndifferences between missing data and causal identification theories.\n","authors":["Razieh Nabi","Rohit Bhattacharya","Ilya Shpitser","James M. Robins"],"pdf_url":"https://arxiv.org/pdf/2210.05558v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.09957v3","updated":"2024-11-20T01:12:04Z","published":"2023-05-17T05:32:45Z","title":"Quantum neural networks form Gaussian processes","summary":"  It is well known that artificial neural networks initialized from independent\nand identically distributed priors converge to Gaussian processes in the limit\nof a large number of neurons per hidden layer. In this work we prove an\nanalogous result for Quantum Neural Networks (QNNs). Namely, we show that the\noutputs of certain models based on Haar random unitary or orthogonal deep QNNs\nconverge to Gaussian processes in the limit of large Hilbert space dimension\n$d$. The derivation of this result is more nuanced than in the classical case\ndue to the role played by the input states, the measurement observable, and the\nfact that the entries of unitary matrices are not independent. Then, we show\nthat the efficiency of predicting measurements at the output of a QNN using\nGaussian process regression depends on the observable's bodyness. Furthermore,\nour theorems imply that the concentration of measure phenomenon in Haar random\nQNNs is worse than previously thought, as we prove that expectation values and\ngradients concentrate as $\\mathcal{O}\\left(\\frac{1}{e^d \\sqrt{d}}\\right)$.\nFinally, we discuss how our results improve our understanding of concentration\nin $t$-designs.\n","authors":["Diego García-Martín","Martin Larocca","M. Cerezo"],"pdf_url":"https://arxiv.org/pdf/2305.09957v3.pdf","comment":"14+37 pages, 4+6 figures"},{"id":"http://arxiv.org/abs/2411.10191v2","updated":"2024-11-20T01:10:15Z","published":"2024-11-15T13:44:37Z","title":"FengWu-W2S: A deep learning model for seamless weather-to-subseasonal\n  forecast of global atmosphere","summary":"  Seamless forecasting that produces warning information at continuum\ntimescales based on only one system is a long-standing pursuit for\nweather-climate service. While the rapid advancement of deep learning has\ninduced revolutionary changes in classical forecasting field, current efforts\nare still focused on building separate AI models for weather and climate\nforecasts. To explore the seamless forecasting ability based on one AI model,\nwe propose FengWu-Weather to Subseasonal (FengWu-W2S), which builds on the\nFengWu global weather forecast model and incorporates an ocean-atmosphere-land\ncoupling structure along with a diverse perturbation strategy. FengWu-W2S can\ngenerate 6-hourly atmosphere forecasts extending up to 42 days through an\nautoregressive and seamless manner. Our hindcast results demonstrate that\nFengWu-W2S reliably predicts atmospheric conditions out to 3-6 weeks ahead,\nenhancing predictive capabilities for global surface air temperature,\nprecipitation, geopotential height and intraseasonal signals such as the\nMadden-Julian Oscillation (MJO) and North Atlantic Oscillation (NAO). Moreover,\nour ablation experiments on forecast error growth from daily to seasonal\ntimescales reveal potential pathways for developing AI-based integrated system\nfor seamless weather-climate forecasting in the future.\n","authors":["Fenghua Ling","Kang Chen","Jiye Wu","Tao Han","Jing-Jia Luo","Wanli Ouyang","Lei Bai"],"pdf_url":"https://arxiv.org/pdf/2411.10191v2.pdf","comment":"23 pages,8 figures"},{"id":"http://arxiv.org/abs/2411.12948v1","updated":"2024-11-20T00:42:40Z","published":"2024-11-20T00:42:40Z","title":"Machine learned reconstruction of tsunami dynamics from sparse\n  observations","summary":"  We investigate the use of the Senseiver, a transformer neural network\ndesigned for sparse sensing applications, to estimate full-field surface height\nmeasurements of tsunami waves from sparse observations. The model is trained on\na large ensemble of simulated data generated via a shallow water equations\nsolver, which we show to be a faithful reproduction for the underlying dynamics\nby comparison to historical events. We train the model on a dataset consisting\nof 8 tsunami simulations whose epicenters correspond to historical USGS\nearthquake records, and where the model inputs are restricted to measurements\nobtained at actively deployed buoy locations. We test the Senseiver on a\ndataset consisting of 8 simulations not included in training, demonstrating its\ncapability for extrapolation. The results show remarkable resolution of fine\nscale phase and amplitude features from the true field, provided that at least\na few of the sensors have obtained a non-zero signal. Throughout, we discuss\nwhich forecasting techniques can be improved by this method, and suggest ways\nin which the flexibility of the architecture can be leveraged to incorporate\narbitrary remote sensing data (eg. HF Radar and satellite measurements) as well\nas investigate optimal sensor placements.\n","authors":["Edward McDugald","Arvind Mohan","Darren Engwirda","Agnese Marcato","Javier Santos"],"pdf_url":"https://arxiv.org/pdf/2411.12948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12946v1","updated":"2024-11-20T00:31:23Z","published":"2024-11-20T00:31:23Z","title":"A Flexible Large Language Models Guardrail Development Methodology\n  Applied to Off-Topic Prompt Detection","summary":"  Large Language Models are prone to off-topic misuse, where users may prompt\nthese models to perform tasks beyond their intended scope. Current guardrails,\nwhich often rely on curated examples or custom classifiers, suffer from high\nfalse-positive rates, limited adaptability, and the impracticality of requiring\nreal-world data that is not available in pre-production. In this paper, we\nintroduce a flexible, data-free guardrail development methodology that\naddresses these challenges. By thoroughly defining the problem space\nqualitatively and passing this to an LLM to generate diverse prompts, we\nconstruct a synthetic dataset to benchmark and train off-topic guardrails that\noutperform heuristic approaches. Additionally, by framing the task as\nclassifying whether the user prompt is relevant with respect to the system\nprompt, our guardrails effectively generalize to other misuse categories,\nincluding jailbreak and harmful prompts. Lastly, we further contribute to the\nfield by open-sourcing both the synthetic dataset and the off-topic guardrail\nmodels, providing valuable resources for developing guardrails in\npre-production environments and supporting future research and development in\nLLM safety.\n","authors":["Gabriel Chua","Shing Yee Chan","Shaun Khoo"],"pdf_url":"https://arxiv.org/pdf/2411.12946v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2411.12943v1","updated":"2024-11-20T00:27:01Z","published":"2024-11-20T00:27:01Z","title":"Enhancing Thermal MOT: A Novel Box Association Method Leveraging Thermal\n  Identity and Motion Similarity","summary":"  Multiple Object Tracking (MOT) in thermal imaging presents unique challenges\ndue to the lack of visual features and the complexity of motion patterns. This\npaper introduces an innovative approach to improve MOT in the thermal domain by\ndeveloping a novel box association method that utilizes both thermal object\nidentity and motion similarity. Our method merges thermal feature sparsity and\ndynamic object tracking, enabling more accurate and robust MOT performance.\nAdditionally, we present a new dataset comprised of a large-scale collection of\nthermal and RGB images captured in diverse urban environments, serving as both\na benchmark for our method and a new resource for thermal imaging. We conduct\nextensive experiments to demonstrate the superiority of our approach over\nexisting methods, showing significant improvements in tracking accuracy and\nrobustness under various conditions. Our findings suggest that incorporating\nthermal identity with motion data enhances MOT performance. The newly collected\ndataset and source code is available at https://github.com/wassimea/thermalMOT\n","authors":["Wassim El Ahmar","Dhanvin Kolhatkar","Farzan Nowruzi","Robert Laganiere"],"pdf_url":"https://arxiv.org/pdf/2411.12943v1.pdf","comment":"Workshop on Towards a Complete Analysis of People, part of the\n  European Conference on Computer Vision (ECCV) 2024"},{"id":"http://arxiv.org/abs/2411.12940v1","updated":"2024-11-20T00:18:46Z","published":"2024-11-20T00:18:46Z","title":"On the relationship between Koopman operator approximations and neural\n  ordinary differential equations for data-driven time-evolution predictions","summary":"  This work explores the relationship between state space methods and Koopman\noperator-based methods for predicting the time-evolution of nonlinear dynamical\nsystems. We demonstrate that extended dynamic mode decomposition with\ndictionary learning (EDMD-DL), when combined with a state space projection, is\nequivalent to a neural network representation of the nonlinear discrete-time\nflow map on the state space. We highlight how this projection step introduces\nnonlinearity into the evolution equations, enabling significantly improved\nEDMD-DL predictions. With this projection, EDMD-DL leads to a nonlinear\ndynamical system on the state space, which can be represented in either\ndiscrete or continuous time. This system has a natural structure for neural\nnetworks, where the state is first expanded into a high dimensional feature\nspace followed by a linear mapping which represents the discrete-time map or\nthe vector field as a linear combination of these features. Inspired by these\nobservations, we implement several variations of neural ordinary differential\nequations (ODEs) and EDMD-DL, developed by combining different aspects of their\nrespective model structures and training procedures. We evaluate these methods\nusing numerical experiments on chaotic dynamics in the Lorenz system and a\nnine-mode model of turbulent shear flow, showing comparable performance across\nmethods in terms of short-time trajectory prediction, reconstruction of\nlong-time statistics, and prediction of rare events. We also show that these\nmethods provide comparable performance to a non-Markovian approach in terms of\nprediction of extreme events.\n","authors":["Jake Buzhardt","C. Ricardo Constante-Amores","Michael D. Graham"],"pdf_url":"https://arxiv.org/pdf/2411.12940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.09530v2","updated":"2024-11-20T00:10:28Z","published":"2024-05-01T15:18:01Z","title":"A community palm model","summary":"  Palm oil production has been identified as one of the major drivers of\ndeforestation for tropical countries. To meet supply chain objectives,\ncommodity producers and other stakeholders need timely information of land\ncover dynamics in their supply shed. However, such data are difficult to obtain\nfrom suppliers who may lack digital geographic representations of their supply\nsheds and production locations. Here we present a \"community model,\" a machine\nlearning model trained on pooled data sourced from many different stakeholders,\nto produce a map of palm probability at global scale. An advantage of this\nmethod is the inclusion of varied inputs, the ability to easily update the\nmodel as new training data becomes available and run the model on any year that\ninput imagery is available. Inclusion of diverse data sources into one\nprobability map can help establish a shared understanding across stakeholders\non the presence and absence of a land cover or commodity (in this case oil\npalm). The model predictors are annual composites built from publicly available\nsatellite imagery provided by Sentinel-1, Sentinel-2, and ALOS-2, and terrain\ndata from Jaxa (AW3D30) and Copernicus (GLO-30). We provide map outputs as the\nprobability of palm in a given pixel, to reflect the uncertainty of the\nunderlying state (palm or not palm). This version of this model provides global\naccuracy estimated to be 92% (at 0.5 probability threshold) on an independent\ntest set. This model, and resulting oil palm probability map products are\nuseful for accurately identifying the geographic footprint of palm cultivation.\nUsed in conjunction with timely deforestation information, this palm model is\nuseful for understanding the risk of continued oil palm plantation expansion in\nsensitive forest areas.\n","authors":["Nicholas Clinton","Andreas Vollrath","Remi D'annunzio","Desheng Liu","Henry B. Glick","Adrià Descals","Alicia Sullivan","Oliver Guinan","Jacob Abramowitz","Fred Stolle","Chris Goodman","Tanya Birch","David Quinn","Olga Danylo","Tijs Lips","Daniel Coelho","Enikoe Bihari","Bryce Cronkite-Ratcliff","Ate Poortinga","Atena Haghighattalab","Evan Notman","Michael DeWitt","Aaron Yonas","Gennadii Donchyts","Devaja Shah","David Saah","Karis Tenneson","Nguyen Hanh Quyen","Megha Verma","Andrew Wilcox"],"pdf_url":"https://arxiv.org/pdf/2405.09530v2.pdf","comment":"v03"},{"id":"http://arxiv.org/abs/2411.12935v1","updated":"2024-11-20T00:00:11Z","published":"2024-11-20T00:00:11Z","title":"Improving Low-Fidelity Models of Li-ion Batteries via Hybrid Sparse\n  Identification of Nonlinear Dynamics","summary":"  Accurate modeling of lithium ion (li-ion) batteries is essential for\nenhancing the safety, and efficiency of electric vehicles and renewable energy\nsystems. This paper presents a data-inspired approach for improving the\nfidelity of reduced-order li-ion battery models. The proposed method combines a\nGenetic Algorithm with Sequentially Thresholded Ridge Regression (GA-STRidge)\nto identify and compensate for discrepancies between a low-fidelity model (LFM)\nand data generated either from testing or a high-fidelity model (HFM). The\nhybrid model, combining physics-based and data-driven methods, is tested across\ndifferent driving cycles to demonstrate the ability to significantly reduce the\nvoltage prediction error compared to the baseline LFM, while preserving\ncomputational efficiency. The model robustness is also evaluated under various\noperating conditions, showing low prediction errors and high Pearson\ncorrelation coefficients for terminal voltage in unseen environments.\n","authors":["Samuel Filgueira da Silva","Mehmet Fatih Ozkan","Faissal El Idrissi","Prashanth Ramesh","Marcello Canova"],"pdf_url":"https://arxiv.org/pdf/2411.12935v1.pdf","comment":"6 pages"}],"Multimedia":[{"id":"http://arxiv.org/abs/2411.13536v1","updated":"2024-11-20T18:37:58Z","published":"2024-11-20T18:37:58Z","title":"Identity Preserving 3D Head Stylization with Multiview Score\n  Distillation","summary":"  3D head stylization transforms realistic facial features into artistic\nrepresentations, enhancing user engagement across gaming and virtual reality\napplications. While 3D-aware generators have made significant advancements,\nmany 3D stylization methods primarily provide near-frontal views and struggle\nto preserve the unique identities of original subjects, often resulting in\noutputs that lack diversity and individuality. This paper addresses these\nchallenges by leveraging the PanoHead model, synthesizing images from a\ncomprehensive 360-degree perspective. We propose a novel framework that employs\nnegative log-likelihood distillation (LD) to enhance identity preservation and\nimprove stylization quality. By integrating multi-view grid score and mirror\ngradients within the 3D GAN architecture and introducing a score rank weighing\ntechnique, our approach achieves substantial qualitative and quantitative\nimprovements. Our findings not only advance the state of 3D head stylization\nbut also provide valuable insights into effective distillation processes\nbetween diffusion models and GANs, focusing on the critical issue of identity\npreservation. Please visit the https://three-bee.github.io/head_stylization for\nmore visuals.\n","authors":["Bahri Batuhan Bilecen","Ahmet Berke Gokmen","Furkan Guzelant","Aysegul Dundar"],"pdf_url":"https://arxiv.org/pdf/2411.13536v1.pdf","comment":"https://three-bee.github.io/head_stylization"},{"id":"http://arxiv.org/abs/2411.13281v1","updated":"2024-11-20T12:48:34Z","published":"2024-11-20T12:48:34Z","title":"VideoAutoArena: An Automated Arena for Evaluating Large Multimodal\n  Models in Video Analysis through User Simulation","summary":"  Large multimodal models (LMMs) with advanced video analysis capabilities have\nrecently garnered significant attention. However, most evaluations rely on\ntraditional methods like multiple-choice questions in benchmarks such as\nVideoMME and LongVideoBench, which are prone to lack the depth needed to\ncapture the complex demands of real-world users. To address this limitation-and\ndue to the prohibitive cost and slow pace of human annotation for video\ntasks-we introduce VideoAutoArena, an arena-style benchmark inspired by LMSYS\nChatbot Arena's framework, designed to automatically assess LMMs' video\nanalysis abilities. VideoAutoArena utilizes user simulation to generate\nopen-ended, adaptive questions that rigorously assess model performance in\nvideo understanding. The benchmark features an automated, scalable evaluation\nframework, incorporating a modified ELO Rating System for fair and continuous\ncomparisons across multiple LMMs. To validate our automated judging system, we\nconstruct a 'gold standard' using a carefully curated subset of human\nannotations, demonstrating that our arena strongly aligns with human judgment\nwhile maintaining scalability. Additionally, we introduce a fault-driven\nevolution strategy, progressively increasing question complexity to push models\ntoward handling more challenging video analysis scenarios. Experimental results\ndemonstrate that VideoAutoArena effectively differentiates among\nstate-of-the-art LMMs, providing insights into model strengths and areas for\nimprovement. To further streamline our evaluation, we introduce VideoAutoBench\nas an auxiliary benchmark, where human annotators label winners in a subset of\nVideoAutoArena battles. We use GPT-4o as a judge to compare responses against\nthese human-validated answers. Together, VideoAutoArena and VideoAutoBench\noffer a cost-effective, and scalable framework for evaluating LMMs in\nuser-centric video analysis.\n","authors":["Ziyang Luo","Haoning Wu","Dongxu Li","Jing Ma","Mohan Kankanhalli","Junnan Li"],"pdf_url":"https://arxiv.org/pdf/2411.13281v1.pdf","comment":"Project Page: https://videoautoarena.github.io/"},{"id":"http://arxiv.org/abs/2106.15989v2","updated":"2024-11-20T07:16:16Z","published":"2021-06-30T11:30:06Z","title":"Word-level Sign Language Recognition with Multi-stream Neural Networks\n  Focusing on Local Regions and Skeletal Information","summary":"  Word-level sign language recognition (WSLR) has attracted attention because\nit is expected to overcome the communication barrier between people with speech\nimpairment and those who can hear. In the WSLR problem, a method designed for\naction recognition has achieved the state-of-the-art accuracy. Indeed, it\nsounds reasonable for an action recognition method to perform well on WSLR\nbecause sign language is regarded as an action. However, a careful evaluation\nof the tasks reveals that the tasks of action recognition and WSLR are\ninherently different. Hence, in this paper, we propose a novel WSLR method that\ntakes into account information specifically useful for the WSLR problem. We\nrealize it as a multi-stream neural network (MSNN), which consist of three\nstreams: 1) base stream, 2) local image stream, and 3) skeleton stream. Each\nstream is designed to handle different types of information. The base stream\ndeals with quick and detailed movements of the hands and body, the local image\nstream focuses on handshapes and facial expressions, and the skeleton stream\ncaptures the relative positions of the body and both hands. This approach\nallows us to combine various types of data for more comprehensive gesture\nanalysis. Experimental results on the WLASL and MS-ASL datasets show the\neffectiveness of the proposed method; it achieved an improvement of\napproximately 10\\%--15\\% in Top-1 accuracy when compared with conventional\nmethods.\n","authors":["Mizuki Maruyama","Shrey Singh","Katsufumi Inoue","Partha Pratim Roy","Masakazu Iwamura","Michifumi Yoshioka"],"pdf_url":"https://arxiv.org/pdf/2106.15989v2.pdf","comment":null}]},"2024-11-19T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.17309v2","updated":"2024-11-19T23:32:13Z","published":"2024-10-22T18:00:00Z","title":"Literature Meets Data: A Synergistic Approach to Hypothesis Generation","summary":"  AI holds promise for transforming scientific processes, including hypothesis\ngeneration. Prior work on hypothesis generation can be broadly categorized into\ntheory-driven and data-driven approaches. While both have proven effective in\ngenerating novel and plausible hypotheses, it remains an open question whether\nthey can complement each other. To address this, we develop the first method\nthat combines literature-based insights with data to perform LLM-powered\nhypothesis generation. We apply our method on five different datasets and\ndemonstrate that integrating literature and data outperforms other baselines\n(8.97\\% over few-shot, 15.75\\% over literature-based alone, and 3.37\\% over\ndata-driven alone). Additionally, we conduct the first human evaluation to\nassess the utility of LLM-generated hypotheses in assisting human\ndecision-making on two challenging tasks: deception detection and AI generated\ncontent detection. Our results show that human accuracy improves significantly\nby 7.44\\% and 14.19\\% on these tasks, respectively. These findings suggest that\nintegrating literature-based and data-driven approaches provides a\ncomprehensive and nuanced framework for hypothesis generation and could open\nnew avenues for scientific inquiry.\n","authors":["Haokun Liu","Yangqiaoyu Zhou","Mingxuan Li","Chenfei Yuan","Chenhao Tan"],"pdf_url":"https://arxiv.org/pdf/2410.17309v2.pdf","comment":"30 pages, 7 figures, code link:\n  https://github.com/ChicagoHAI/hypothesis-generation"},{"id":"http://arxiv.org/abs/2411.12925v1","updated":"2024-11-19T23:23:16Z","published":"2024-11-19T23:23:16Z","title":"Loss-to-Loss Prediction: Scaling Laws for All Datasets","summary":"  While scaling laws provide a reliable methodology for predicting train loss\nacross compute scales for a single data distribution, less is known about how\nthese predictions should change as we change the distribution. In this paper,\nwe derive a strategy for predicting one loss from another and apply it to\npredict across different pre-training datasets and from pre-training data to\ndownstream task data. Our predictions extrapolate well even at 20x the largest\nFLOP budget used to fit the curves. More precisely, we find that there are\nsimple shifted power law relationships between (1) the train losses of two\nmodels trained on two separate datasets when the models are paired by training\ncompute (train-to-train), (2) the train loss and the test loss on any\ndownstream distribution for a single model (train-to-test), and (3) the test\nlosses of two models trained on two separate train datasets (test-to-test). The\nresults hold up for pre-training datasets that differ substantially (some are\nentirely code and others have no code at all) and across a variety of\ndownstream tasks. Finally, we find that in some settings these shifted power\nlaw relationships can yield more accurate predictions than extrapolating\nsingle-dataset scaling laws.\n","authors":["David Brandfonbrener","Nikhil Anand","Nikhil Vyas","Eran Malach","Sham Kakade"],"pdf_url":"https://arxiv.org/pdf/2411.12925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12901v1","updated":"2024-11-19T22:27:53Z","published":"2024-11-19T22:27:53Z","title":"Signformer is all you need: Towards Edge AI for Sign Language","summary":"  Sign language translation, especially in gloss-free paradigm, is confronting\na dilemma of impracticality and unsustainability due to growing\nresource-intensive methodologies. Contemporary state-of-the-arts (SOTAs) have\nsignificantly hinged on pretrained sophiscated backbones such as Large Language\nModels (LLMs), embedding sources, or extensive datasets, inducing considerable\nparametric and computational inefficiency for sustainable use in real-world\nscenario. Despite their success, following this research direction undermines\nthe overarching mission of this domain to create substantial value to bridge\nhard-hearing and common populations. Committing to the prevailing trend of LLM\nand Natural Language Processing (NLP) studies, we pursue a profound essential\nchange in architecture to achieve ground-up improvements without external aid\nfrom pretrained models, prior knowledge transfer, or any NLP strategies\nconsidered not-from-scratch.\n  Introducing Signformer, a from-scratch Feather-Giant transforming the area\ntowards Edge AI that redefines extremities of performance and efficiency with\nLLM-competence and edgy-deployable compactness. In this paper, we present\nnature analysis of sign languages to inform our algorithmic design and deliver\na scalable transformer pipeline with convolution and attention novelty. We\nachieve new 2nd place on leaderboard with a parametric reduction of 467-1807x\nagainst the finests as of 2024 and outcompete almost every other methods in a\nlighter configuration of 0.57 million parameters.\n","authors":["Eta Yang"],"pdf_url":"https://arxiv.org/pdf/2411.12901v1.pdf","comment":"Official Code at: https://github.com/EtaEnding/Signformer/tree/main"},{"id":"http://arxiv.org/abs/2411.12892v1","updated":"2024-11-19T22:17:18Z","published":"2024-11-19T22:17:18Z","title":"Selective Attention: Enhancing Transformer through Principled Context\n  Control","summary":"  The attention mechanism within the transformer architecture enables the model\nto weigh and combine tokens based on their relevance to the query. While\nself-attention has enjoyed major success, it notably treats all queries $q$ in\nthe same way by applying the mapping $V^\\top\\text{softmax}(Kq)$, where $V,K$\nare the value and key embeddings respectively. In this work, we argue that this\nuniform treatment hinders the ability to control contextual sparsity and\nrelevance. As a solution, we introduce the $\\textit{Selective Self-Attention}$\n(SSA) layer that augments the softmax nonlinearity with a principled\ntemperature scaling strategy. By controlling temperature, SSA adapts the\ncontextual sparsity of the attention map to the query embedding and its\nposition in the context window. Through theory and experiments, we demonstrate\nthat this alleviates attention dilution, aids the optimization process, and\nenhances the model's ability to control softmax spikiness of individual\nqueries. We also incorporate temperature scaling for value embeddings and show\nthat it boosts the model's ability to suppress irrelevant/noisy tokens.\nNotably, SSA is a lightweight method which introduces less than 0.5% new\nparameters through a weight-sharing strategy and can be fine-tuned on existing\nLLMs. Extensive empirical evaluations demonstrate that SSA-equipped models\nachieve a noticeable and consistent accuracy improvement on language modeling\nbenchmarks.\n","authors":["Xuechen Zhang","Xiangyu Chang","Mingchen Li","Amit Roy-Chowdhury","Jiasi Chen","Samet Oymak"],"pdf_url":"https://arxiv.org/pdf/2411.12892v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.13082v2","updated":"2024-11-19T22:02:49Z","published":"2024-04-17T05:56:49Z","title":"Efficient Contextual LLM Cascades through Budget-Constrained Policy\n  Learning","summary":"  Recent successes in natural language processing have led to the proliferation\nof large language models (LLMs) by multiple providers. Each LLM offering has\ndifferent inference accuracy, monetary cost, and latency, and their accuracy\nfurther depends on the exact wording of the question (i.e., the specific\nprompt). At the same time, users often have a limit on monetary budget and\nlatency to answer all their questions, and they do not know which LLMs to\nchoose for each question to meet their accuracy and long term budget\nrequirements. To navigate this rich design space, we propose TREACLE\n($\\underline{T}$hrifty $\\underline{Rea}$soning via $\\underline{C}$ontext-Aware\n$\\underline{L}$LM and Prompt S$\\underline{e}$lection), a reinforcement learning\npolicy that jointly selects the model and prompting scheme while respecting the\nuser's monetary cost and latency constraints. TREACLE uses the problem context,\nincluding question text embeddings (reflecting the type or difficulty of a\nquery) and the response history (reflecting the consistency of previous\nresponses) to make smart decisions. Our evaluations on standard reasoning\ndatasets (GSM8K, CSQA, and LLC) with various LLMs and prompts show that TREACLE\nenables cost savings of up to 85% compared to baselines, while maintaining high\naccuracy. Importantly, it provides the user with the ability to gracefully\ntrade off accuracy for cost.\n","authors":["Xuechen Zhang","Zijian Huang","Ege Onur Taga","Carlee Joe-Wong","Samet Oymak","Jiasi Chen"],"pdf_url":"https://arxiv.org/pdf/2404.13082v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12882v1","updated":"2024-11-19T22:00:01Z","published":"2024-11-19T22:00:01Z","title":"ProSec: Fortifying Code LLMs with Proactive Security Alignment","summary":"  Recent advances in code-specific large language models (LLMs) have greatly\nenhanced code generation and refinement capabilities. However, the safety of\ncode LLMs remains under-explored, posing potential risks as insecure code\ngenerated by these models may introduce vulnerabilities into real-world\nsystems. Previous work proposes to collect security-focused instruction-tuning\ndataset from real-world vulnerabilities. It is constrained by the data sparsity\nof vulnerable code, and has limited applicability in the iterative\npost-training workflows of modern LLMs. In this paper, we propose ProSec, a\nnovel proactive security alignment approach designed to align code LLMs with\nsecure coding practices. ProSec systematically exposes the vulnerabilities in a\ncode LLM by synthesizing error-inducing coding scenarios from Common Weakness\nEnumerations (CWEs), and generates fixes to vulnerable code snippets, allowing\nthe model to learn secure practices through advanced preference learning\nobjectives. The scenarios synthesized by ProSec triggers 25 times more\nvulnerable code than a normal instruction-tuning dataset, resulting in a\nsecurity-focused alignment dataset 7 times larger than the previous work.\nExperiments show that models trained with ProSec is 29.2% to 35.5% more secure\ncompared to previous work, with a marginal negative effect of less than 2\npercentage points on model's utility.\n","authors":["Xiangzhe Xu","Zian Su","Jinyao Guo","Kaiyuan Zhang","Zhenting Wang","Xiangyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.12882v1.pdf","comment":"The first two authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2411.12865v1","updated":"2024-11-19T21:15:47Z","published":"2024-11-19T21:15:47Z","title":"AzSLD: Azerbaijani Sign Language Dataset for Fingerspelling, Word, and\n  Sentence Translation with Baseline Software","summary":"  Sign language processing technology development relies on extensive and\nreliable datasets, instructions, and ethical guidelines. We present a\ncomprehensive Azerbaijani Sign Language Dataset (AzSLD) collected from diverse\nsign language users and linguistic parameters to facilitate advancements in\nsign recognition and translation systems and support the local sign language\ncommunity. The dataset was created within the framework of a vision-based AzSL\ntranslation project. This study introduces the dataset as a summary of the\nfingerspelling alphabet and sentence- and word-level sign language datasets.\nThe dataset was collected from signers of different ages, genders, and signing\nstyles, with videos recorded from two camera angles to capture each sign in\nfull detail. This approach ensures robust training and evaluation of gesture\nrecognition models. AzSLD contains 30,000 videos, each carefully annotated with\naccurate sign labels and corresponding linguistic translations. The dataset is\naccompanied by technical documentation and source code to facilitate its use in\ntraining and testing. This dataset offers a valuable resource of labeled data\nfor researchers and developers working on sign language recognition,\ntranslation, or synthesis. Ethical guidelines were strictly followed throughout\nthe project, with all participants providing informed consent for collecting,\npublishing, and using the data.\n","authors":["Nigar Alishzade","Jamaladdin Hasanov"],"pdf_url":"https://arxiv.org/pdf/2411.12865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.09834v2","updated":"2024-11-19T21:04:38Z","published":"2024-11-14T22:54:38Z","title":"A Benchmark for Long-Form Medical Question Answering","summary":"  There is a lack of benchmarks for evaluating large language models (LLMs) in\nlong-form medical question answering (QA). Most existing medical QA evaluation\nbenchmarks focus on automatic metrics and multiple-choice questions. While\nvaluable, these benchmarks fail to fully capture or assess the complexities of\nreal-world clinical applications where LLMs are being deployed. Furthermore,\nexisting studies on evaluating long-form answer generation in medical QA are\nprimarily closed-source, lacking access to human medical expert annotations,\nwhich makes it difficult to reproduce results and enhance existing baselines.\nIn this work, we introduce a new publicly available benchmark featuring\nreal-world consumer medical questions with long-form answer evaluations\nannotated by medical doctors. We performed pairwise comparisons of responses\nfrom various open and closed-source medical and general-purpose LLMs based on\ncriteria such as correctness, helpfulness, harmfulness, and bias. Additionally,\nwe performed a comprehensive LLM-as-a-judge analysis to study the alignment\nbetween human judgments and LLMs. Our preliminary results highlight the strong\npotential of open LLMs in medical QA compared to leading closed models. Code &\nData: https://github.com/lavita-ai/medical-eval-sphere\n","authors":["Pedram Hosseini","Jessica M. Sin","Bing Ren","Bryceton G. Thomas","Elnaz Nouri","Ali Farahanchi","Saeed Hassanpour"],"pdf_url":"https://arxiv.org/pdf/2411.09834v2.pdf","comment":"AIM-FM: Advancements in Medical Foundation Models Workshop, 38th\n  Conference on Neural Information Processing Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2411.04118v2","updated":"2024-11-19T20:51:58Z","published":"2024-11-06T18:51:02Z","title":"Medical Adaptation of Large Language and Vision-Language Models: Are We\n  Making Progress?","summary":"  Several recent works seek to develop foundation models specifically for\nmedical applications, adapting general-purpose large language models (LLMs) and\nvision-language models (VLMs) via continued pretraining on publicly available\nbiomedical corpora. These works typically claim that such domain-adaptive\npretraining (DAPT) improves performance on downstream medical tasks, such as\nanswering medical licensing exam questions. In this paper, we compare seven\npublic \"medical\" LLMs and two VLMs against their corresponding base models,\narriving at a different conclusion: all medical VLMs and nearly all medical\nLLMs fail to consistently improve over their base models in the zero-/few-shot\nprompting regime for medical question-answering (QA) tasks. For instance,\nacross the tasks and model pairs we consider in the 3-shot setting, medical\nLLMs only outperform their base models in 12.1% of cases, reach a (statistical)\ntie in 49.8% of cases, and are significantly worse than their base models in\nthe remaining 38.2% of cases. Our conclusions are based on (i) comparing each\nmedical model head-to-head, directly against the corresponding base model; (ii)\noptimizing the prompts for each model separately; and (iii) accounting for\nstatistical uncertainty in comparisons. While these basic practices are not\nconsistently adopted in the literature, our ablations show that they\nsubstantially impact conclusions. Our findings suggest that state-of-the-art\ngeneral-domain models may already exhibit strong medical knowledge and\nreasoning capabilities, and offer recommendations to strengthen the conclusions\nof future studies.\n","authors":["Daniel P. Jeong","Saurabh Garg","Zachary C. Lipton","Michael Oberst"],"pdf_url":"https://arxiv.org/pdf/2411.04118v2.pdf","comment":"This version was published at EMNLP 2024 Main Conference as a Long\n  Paper (Oral). See the extended version (arXiv:2411.08870) for additional\n  results on QA tasks based on clinical notes and evaluations in the supervised\n  fine-tuning regime"},{"id":"http://arxiv.org/abs/2411.12844v1","updated":"2024-11-19T20:18:55Z","published":"2024-11-19T20:18:55Z","title":"SCOUT: A Situated and Multi-Modal Human-Robot Dialogue Corpus","summary":"  We introduce the Situated Corpus Of Understanding Transactions (SCOUT), a\nmulti-modal collection of human-robot dialogue in the task domain of\ncollaborative exploration. The corpus was constructed from multiple\nWizard-of-Oz experiments where human participants gave verbal instructions to a\nremotely-located robot to move and gather information about its surroundings.\nSCOUT contains 89,056 utterances and 310,095 words from 278 dialogues averaging\n320 utterances per dialogue. The dialogues are aligned with the multi-modal\ndata streams available during the experiments: 5,785 images and 30 maps. The\ncorpus has been annotated with Abstract Meaning Representation and Dialogue-AMR\nto identify the speaker's intent and meaning within an utterance, and with\nTransactional Units and Relations to track relationships between utterances to\nreveal patterns of the Dialogue Structure. We describe how the corpus and its\nannotations have been used to develop autonomous human-robot systems and enable\nresearch in open questions of how humans speak to robots. We release this\ncorpus to accelerate progress in autonomous, situated, human-robot dialogue,\nespecially in the context of navigation tasks where details about the\nenvironment need to be discovered.\n","authors":["Stephanie M. Lukin","Claire Bonial","Matthew Marge","Taylor Hudson","Cory J. Hayes","Kimberly A. Pollard","Anthony Baker","Ashley N. Foots","Ron Artstein","Felix Gervits","Mitchell Abrams","Cassidy Henry","Lucia Donatelli","Anton Leuski","Susan G. Hill","David Traum","Clare R. Voss"],"pdf_url":"https://arxiv.org/pdf/2411.12844v1.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2411.12843v1","updated":"2024-11-19T20:17:04Z","published":"2024-11-19T20:17:04Z","title":"Reward Modeling with Ordinal Feedback: Wisdom of the Crowd","summary":"  Learning a reward model (RM) from human preferences has been an important\ncomponent in aligning large language models (LLMs). The canonical setup of\nlearning RMs from pairwise preference data is rooted in the classic\nBradley-Terry (BT) model that accepts binary feedback, i.e., the label being\neither Response 1 is better than Response 2, or the opposite. Such a setup\ninevitably discards potentially useful samples (such as \"tied\" between the two\nresponses) and loses more fine-grained information (such as \"slightly better\").\nIn this paper, we propose a framework for learning RMs under ordinal feedback\nwhich generalizes the case of binary preference feedback to any arbitrary\ngranularity. Specifically, we first identify a marginal unbiasedness condition,\nwhich generalizes the assumption of the BT model in the existing binary\nfeedback setting. The condition validates itself via the sociological concept\nof the wisdom of the crowd. Under the condition, we develop a natural\nprobability model for pairwise preference data under ordinal feedback and\nanalyze its properties. We prove the statistical benefits of ordinal feedback\nin terms of reducing the Rademacher complexity compared to the case of binary\nfeedback. The proposed learning objective and the theory also extend to hinge\nloss and direct policy optimization (DPO). In particular, the theoretical\nanalysis may be of independent interest when applying to a seemingly unrelated\nproblem of knowledge distillation to interpret the bias-variance trade-off\ntherein. The framework also sheds light on writing guidance for human\nannotators. Our numerical experiments validate that fine-grained feedback leads\nto better reward learning for both in-distribution and out-of-distribution\nsettings. Further experiments show that incorporating a certain proportion of\nsamples with tied preference boosts RM learning.\n","authors":["Shang Liu","Yu Pan","Guanting Chen","Xiaocheng Li"],"pdf_url":"https://arxiv.org/pdf/2411.12843v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17538v3","updated":"2024-11-19T20:10:18Z","published":"2024-09-26T04:56:49Z","title":"On the Implicit Relation Between Low-Rank Adaptation and Differential\n  Privacy","summary":"  A significant approach in natural language processing involves large-scale\npre-training models on general domain data followed by their adaptation to\nspecific tasks or domains. As models grow in size, full fine-tuning all of\ntheir parameters becomes increasingly impractical. To address this, some\nmethods for low-rank task adaptation of language models have been proposed,\ne.g., LoRA and FLoRA. These methods keep the pre-trained model weights fixed\nand incorporate trainable low-rank decomposition matrices into some layers of\nthe transformer architecture, called adapters. This approach significantly\nreduces the number of trainable parameters required for downstream tasks\ncompared to full fine-tuning all parameters. In this work, we look at low-rank\nadaptation from the lens of data privacy. We show theoretically that the\nlow-rank adaptation used in LoRA and FLoRA is equivalent to injecting some\nrandom noise into the batch gradients w.r.t the adapter parameters, and we\nquantify the variance of the injected noise. By establishing a Berry-Esseen\ntype bound on the total variation distance between distribution of the injected\nnoise and a Gaussian distribution with the same variance, we show that the\ndynamics of low-rank adaptation is close to that of differentially private\nfine-tuning of the adapters. Finally, using Johnson-Lindenstrauss lemma, we\nshow that when augmented with gradient scaling, low-rank adaptation is very\nclose to performing DPSGD algorithm with a fixed noise scale to fine-tune the\nadapters. These theoretical findings suggest that unlike other existing\nfine-tuning algorithms, low-rank adaptation provides privacy w.r.t the\nfine-tuning data implicitly.\n","authors":["Saber Malekmohammadi","Golnoosh Farnadi"],"pdf_url":"https://arxiv.org/pdf/2409.17538v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12829v1","updated":"2024-11-19T19:33:54Z","published":"2024-11-19T19:33:54Z","title":"Human-Robot Dialogue Annotation for Multi-Modal Common Ground","summary":"  In this paper, we describe the development of symbolic representations\nannotated on human-robot dialogue data to make dimensions of meaning accessible\nto autonomous systems participating in collaborative, natural language\ndialogue, and to enable common ground with human partners. A particular\nchallenge for establishing common ground arises in remote dialogue (occurring\nin disaster relief or search-and-rescue tasks), where a human and robot are\nengaged in a joint navigation and exploration task of an unfamiliar\nenvironment, but where the robot cannot immediately share high quality visual\ninformation due to limited communication constraints. Engaging in a dialogue\nprovides an effective way to communicate, while on-demand or lower-quality\nvisual information can be supplemented for establishing common ground. Within\nthis paradigm, we capture propositional semantics and the illocutionary force\nof a single utterance within the dialogue through our Dialogue-AMR annotation,\nan augmentation of Abstract Meaning Representation. We then capture patterns in\nhow different utterances within and across speaker floors relate to one another\nin our development of a multi-floor Dialogue Structure annotation schema.\nFinally, we begin to annotate and analyze the ways in which the visual\nmodalities provide contextual information to the dialogue for overcoming\ndisparities in the collaborators' understanding of the environment. We conclude\nby discussing the use-cases, architectures, and systems we have implemented\nfrom our annotations that enable physical robots to autonomously engage with\nhumans in bi-directional dialogue and navigation.\n","authors":["Claire Bonial","Stephanie M. Lukin","Mitchell Abrams","Anthony Baker","Lucia Donatelli","Ashley Foots","Cory J. Hayes","Cassidy Henry","Taylor Hudson","Matthew Marge","Kimberly A. Pollard","Ron Artstein","David Traum","Clare R. Voss"],"pdf_url":"https://arxiv.org/pdf/2411.12829v1.pdf","comment":"52 pages, 14 figures"},{"id":"http://arxiv.org/abs/2411.12828v1","updated":"2024-11-19T19:33:16Z","published":"2024-11-19T19:33:16Z","title":"Probing the Capacity of Language Model Agents to Operationalize\n  Disparate Experiential Context Despite Distraction","summary":"  Large language model (LLM) agents show promise in an increasing number of\ndomains. In many proposed applications, it is expected that the agent reasons\nover accumulated experience presented in an input prompt. We propose the OEDD\n(Operationalize Experience Despite Distraction) corpus, a\nhuman-annotator-validated body of scenarios with pre-scripted agent histories\nwhere the agent must make a decision based on disparate experiential\ninformation in the presence of a distractor. We evaluate three state-of-the-art\nLLMs (GPT-3.5 Turbo, GPT-4o, and Gemini 1.5 Pro) using a minimal\nchain-of-thought prompting strategy and observe that when (1) the input context\ncontains over 1,615 tokens of historical interactions, (2) a crucially\ndecision-informing premise is the rightful conclusion over two disparate\nenvironment premises, and (3) a trivial, but distracting red herring fact\nfollows, all LLMs perform worse than random choice at selecting the better of\ntwo actions. Our code and test corpus are publicly available at:\nhttps://github.com/sonnygeorge/OEDD .\n","authors":["Sonny George","Chris Sypherd","Dylan Cashman"],"pdf_url":"https://arxiv.org/pdf/2411.12828v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12736v1","updated":"2024-11-19T18:58:03Z","published":"2024-11-19T18:58:03Z","title":"ACING: Actor-Critic for Instruction Learning in Black-Box Large Language\n  Models","summary":"  The effectiveness of Large Language Models (LLMs) in solving tasks vastly\ndepends on the quality of the instructions, which often require fine-tuning\nthrough extensive human effort. This highlights the need for automated\ninstruction optimization; however, this optimization is particularly\nchallenging when dealing with black-box LLMs, where model parameters and\ngradients remain inaccessible. We propose ACING, a task-specific prompt\noptimization approach framed as a stateless continuous-action Reinforcement\nLearning (RL) problem, known as the continuum bandit setting. ACING leverages\nan actor-critic-based method to optimize prompts, learning from\nnon-differentiable reward signals. We validate ACING by optimizing prompts for\nChatGPT on 30 instruction-based tasks. ACING consistently outperforms baseline\nmethods, achieving a median score improvement of 10 percentage points.\nFurthermore, ACING not only recovers but also surpasses human-crafted expert\ninstructions, achieving up to a 39 percentage point improvement against human\nbenchmarks.\n","authors":["Salma Kharrat","Fares Fourati","Marco Canini"],"pdf_url":"https://arxiv.org/pdf/2411.12736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12728v1","updated":"2024-11-19T18:51:23Z","published":"2024-11-19T18:51:23Z","title":"Information Theory of Meaningful Communication","summary":"  In Shannon's seminal paper, entropy of printed English, treated as a\nstationary stochastic process, was estimated to be roughly 1 bit per character.\nHowever, considered as a means of communication, language differs considerably\nfrom its printed form: (i) the units of information are not characters or even\nwords but clauses, i.e. shortest meaningful parts of speech; and (ii) what is\ntransmitted is principally the meaning of what is being said or written, while\nthe precise phrasing that was used to communicate the meaning is typically\nignored. In this study, we show that one can leverage recently developed large\nlanguage models to quantify information communicated in meaningful narratives\nin terms of bits of meaning per clause.\n","authors":["Doron Sivan","Misha Tsodyks"],"pdf_url":"https://arxiv.org/pdf/2411.12728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12720v1","updated":"2024-11-19T18:38:01Z","published":"2024-11-19T18:38:01Z","title":"Scaling laws for nonlinear dynamical models of speech","summary":"  The addition of a nonlinear restoring force to dynamical models of the speech\ngesture significantly improves the empirical accuracy of model predictions, but\nnonlinearity introduces challenges in selecting appropriate parameters and\nnumerical stability, especially when modelling variation in empirical data. We\naddress this issue by introducing simple numerical methods for parameterization\nof nonlinear task dynamic models. We first illustrate the problem and then\noutline solutions in the form of power laws that scale nonlinear stiffness\nterms. We apply the scaling laws to a cubic model and show how they facilitate\ninterpretable simulations of the nonlinear gestural dynamics underpinning\nspeech production.\n","authors":["Sam Kirkham"],"pdf_url":"https://arxiv.org/pdf/2411.12720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12719v1","updated":"2024-11-19T18:37:45Z","published":"2024-11-19T18:37:45Z","title":"Rethinking MUSHRA: Addressing Modern Challenges in Text-to-Speech\n  Evaluation","summary":"  Despite rapid advancements in TTS models, a consistent and robust human\nevaluation framework is still lacking. For example, MOS tests fail to\ndifferentiate between similar models, and CMOS's pairwise comparisons are\ntime-intensive. The MUSHRA test is a promising alternative for evaluating\nmultiple TTS systems simultaneously, but in this work we show that its reliance\non matching human reference speech unduly penalises the scores of modern TTS\nsystems that can exceed human speech quality. More specifically, we conduct a\ncomprehensive assessment of the MUSHRA test, focusing on its sensitivity to\nfactors such as rater variability, listener fatigue, and reference bias. Based\non our extensive evaluation involving 471 human listeners across Hindi and\nTamil we identify two primary shortcomings: (i) reference-matching bias, where\nraters are unduly influenced by the human reference, and (ii) judgement\nambiguity, arising from a lack of clear fine-grained guidelines. To address\nthese issues, we propose two refined variants of the MUSHRA test. The first\nvariant enables fairer ratings for synthesized samples that surpass human\nreference quality. The second variant reduces ambiguity, as indicated by the\nrelatively lower variance across raters. By combining these approaches, we\nachieve both more reliable and more fine-grained assessments. We also release\nMANGO, a massive dataset of 47,100 human ratings, the first-of-its-kind\ncollection for Indian languages, aiding in analyzing human preferences and\ndeveloping automatic metrics for evaluating TTS systems.\n","authors":["Praveen Srinivasa Varadhan","Amogh Gulati","Ashwin Sankar","Srija Anand","Anirudh Gupta","Anirudh Mukherjee","Shiva Kumar Marepally","Ankur Bhatia","Saloni Jaju","Suvrat Bhooshan","Mitesh M. Khapra"],"pdf_url":"https://arxiv.org/pdf/2411.12719v1.pdf","comment":"19 pages, 12 Figures"},{"id":"http://arxiv.org/abs/2411.12712v1","updated":"2024-11-19T18:27:25Z","published":"2024-11-19T18:27:25Z","title":"Enhancing Multi-Class Disease Classification: Neoplasms, Cardiovascular,\n  Nervous System, and Digestive Disorders Using Advanced LLMs","summary":"  In this research, we explored the improvement in terms of multi-class disease\nclassification via pre-trained language models over Medical-Abstracts-TC-Corpus\nthat spans five medical conditions. We excluded non-cancer conditions and\nexamined four specific diseases. We assessed four LLMs, BioBERT, XLNet, and\nBERT, as well as a novel base model (Last-BERT). BioBERT, which was pre-trained\non medical data, demonstrated superior performance in medical text\nclassification (97% accuracy). Surprisingly, XLNet followed closely (96%\naccuracy), demonstrating its generalizability across domains even though it was\nnot pre-trained on medical data. LastBERT, a custom model based on the lighter\nversion of BERT, also proved competitive with 87.10% accuracy (just under\nBERT's 89.33%). Our findings confirm the importance of specialized models such\nas BioBERT and also support impressions around more general solutions like\nXLNet and well-tuned transformer architectures with fewer parameters (in this\ncase, LastBERT) in medical domain tasks.\n","authors":["Ahmed Akib Jawad Karim","Muhammad Zawad Mahmud","Samiha Islam","Aznur Azam"],"pdf_url":"https://arxiv.org/pdf/2411.12712v1.pdf","comment":"7 Pages, 4 tables and 11 figures. Under review in a IEEE conference"},{"id":"http://arxiv.org/abs/2411.12703v1","updated":"2024-11-19T18:15:46Z","published":"2024-11-19T18:15:46Z","title":"Strengthening Fake News Detection: Leveraging SVM and Sophisticated Text\n  Vectorization Techniques. Defying BERT?","summary":"  The rapid spread of misinformation, particularly through online platforms,\nunderscores the urgent need for reliable detection systems. This study explores\nthe utilization of machine learning and natural language processing,\nspecifically Support Vector Machines (SVM) and BERT, to detect news that are\nfake. We employ three distinct text vectorization methods for SVM: Term\nFrequency Inverse Document Frequency (TF-IDF), Word2Vec, and Bag of Words (BoW)\nevaluating their effectiveness in distinguishing between genuine and fake news.\nAdditionally, we compare these methods against the transformer large language\nmodel, BERT. Our comprehensive approach includes detailed preprocessing steps,\nrigorous model implementation, and thorough evaluation to determine the most\neffective techniques. The results demonstrate that while BERT achieves superior\naccuracy with 99.98% and an F1-score of 0.9998, the SVM model with a linear\nkernel and BoW vectorization also performs exceptionally well, achieving 99.81%\naccuracy and an F1-score of 0.9980. These findings highlight that, despite\nBERT's superior performance, SVM models with BoW and TF-IDF vectorization\nmethods come remarkably close, offering highly competitive performance with the\nadvantage of lower computational requirements.\n","authors":["Ahmed Akib Jawad Karim","Kazi Hafiz Md Asad","Aznur Azam"],"pdf_url":"https://arxiv.org/pdf/2411.12703v1.pdf","comment":"6 pages, 3 tables and 6 Figures. Submitted to a conference"},{"id":"http://arxiv.org/abs/2406.08316v3","updated":"2024-11-19T17:49:27Z","published":"2024-06-12T15:16:40Z","title":"Is Programming by Example solved by LLMs?","summary":"  Programming-by-Examples (PBE) aims to generate an algorithm from input-output\nexamples. Such systems are practically and theoretically important: from an\nend-user perspective, they are deployed to millions of people, and from an AI\nperspective, PBE corresponds to a very general form of few-shot inductive\ninference. Given the success of Large Language Models (LLMs) in code-generation\ntasks, we investigate here the extent to which LLMs can be said to have\n\"solved\" PBE. We experiment on classic domains such as lists and strings, and\nan uncommon graphics programming domain not well represented in typical\npretraining data. We find that pretrained models are not effective at PBE, but\nthat they can be fine-tuned for much higher performance, provided the test\nproblems are in-distribution. We analyze empirically what causes these models\nto succeed and fail, and take steps toward understanding how to achieve better\nout-of-distribution generalization. Collectively these results suggest that\nLLMs make strong progress toward solving the typical suite of PBE tasks,\npotentially increasing the flexibility and applicability of PBE systems, while\nalso identifying ways in which LLMs still fall short.\n","authors":["Wen-Ding Li","Kevin Ellis"],"pdf_url":"https://arxiv.org/pdf/2406.08316v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00248v2","updated":"2024-11-19T17:46:48Z","published":"2024-10-31T22:58:08Z","title":"A Demonstration of Adaptive Collaboration of Large Language Models for\n  Medical Decision-Making","summary":"  Medical Decision-Making (MDM) is a multi-faceted process that requires\nclinicians to assess complex multi-modal patient data patient, often\ncollaboratively. Large Language Models (LLMs) promise to streamline this\nprocess by synthesizing vast medical knowledge and multi-modal health data.\nHowever, single-agent are often ill-suited for nuanced medical contexts\nrequiring adaptable, collaborative problem-solving. Our MDAgents addresses this\nneed by dynamically assigning collaboration structures to LLMs based on task\ncomplexity, mimicking real-world clinical collaboration and decision-making.\nThis framework improves diagnostic accuracy and supports adaptive responses in\ncomplex, real-world medical scenarios, making it a valuable tool for clinicians\nin various healthcare settings, and at the same time, being more efficient in\nterms of computing cost than static multi-agent decision making methods.\n","authors":["Yubin Kim","Chanwoo Park","Hyewon Jeong","Cristina Grau-Vilchez","Yik Siu Chan","Xuhai Xu","Daniel McDuff","Hyeonhoon Lee","Cynthia Breazeal","Hae Won Park"],"pdf_url":"https://arxiv.org/pdf/2411.00248v2.pdf","comment":"Under Review for ML4H 2024"},{"id":"http://arxiv.org/abs/2411.12685v1","updated":"2024-11-19T17:45:12Z","published":"2024-11-19T17:45:12Z","title":"Enhanced Sign Language Translation between American Sign Language (ASL)\n  and Indian Sign Language (ISL) Using LLMs","summary":"  We have come up with a research that hopes to provide a bridge between the\nusers of American Sign Language and the users of spoken language and Indian\nSign Language (ISL). The research enabled us to create a novel framework that\nwe have developed for Learner Systems. Leveraging art of Large models to create\nkey features including: - Real-time translation between these two sign\nlanguages in an efficient manner. Making LLM's capability available for\nseamless translations to ISL. Here is the full study showing its implementation\nin this paper. The core of the system is a sophisticated pipeline that begins\nwith reclassification and recognition of ASL gestures based on a strong Random\nForest Classifier. By recognizing the ASL, it is translated into text which can\nbe more easily processed. Highly evolved natural language NLP (Natural Language\nProcessing) techniques come in handy as they play a role in our LLM integration\nwhere you then use LLMs to be able to convert the ASL text to ISL which\nprovides you with the intent of sentence or phrase. The final step is to\nsynthesize the translated text back into ISL gestures, creating an end-to-end\ntranslation experience using RIFE-Net. This framework is tasked with key\nchallenges such as automatically dealing with gesture variability and\novercoming the linguistic differences between ASL and ISL. By automating the\ntranslation process, we hope to vastly improve accessibility for sign language\nusers. No longer will the communication gap between ASL and ISL create\nbarriers; this totally cool innovation aims to bring our communities closer\ntogether. And we believe, with full confidence in our framework, that we're\nable to apply the same principles across a wide variety of sign language\ndialects.\n","authors":["Malay Kumar","S. Sarvajit Visagan","Tanish Sarang Mahajan","Anisha Natarajan"],"pdf_url":"https://arxiv.org/pdf/2411.12685v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.00480v4","updated":"2024-11-19T17:33:22Z","published":"2021-08-01T15:43:57Z","title":"Realised Volatility Forecasting: Machine Learning via Financial Word\n  Embedding","summary":"  This study develops a financial word embedding using 15 years of business\nnews. Our results show that this specialised language model produces more\naccurate results than general word embeddings, based on a financial benchmark\nwe established. As an application, we incorporate this word embedding into a\nsimple machine learning model to enhance the HAR model for forecasting realised\nvolatility. This approach statistically and economically outperforms\nestablished econometric models. Using an explainable AI method, we also\nidentify key phrases in business news that contribute significantly to\nvolatility, offering insights into language patterns tied to market dynamics.\n","authors":["Eghbal Rahimikia","Stefan Zohren","Ser-Huang Poon"],"pdf_url":"https://arxiv.org/pdf/2108.00480v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02272v3","updated":"2024-11-19T17:29:58Z","published":"2024-11-04T17:03:55Z","title":"Combining Induction and Transduction for Abstract Reasoning","summary":"  When learning an input-output mapping from very few examples, is it better to\nfirst infer a latent function that explains the examples, or is it better to\ndirectly predict new test outputs, e.g. using a neural network? We study this\nquestion on ARC, a highly diverse dataset of abstract reasoning tasks. We train\nneural models for induction (inferring latent functions) and transduction\n(directly predicting the test output for a given test input). Our models are\ntrained on synthetic data generated by prompting LLMs to produce Python code\nspecifying a function to be inferred, plus a stochastic subroutine for\ngenerating inputs to that function. We find inductive and transductive models\nsolve very different problems, despite training on the same problems, and\ndespite sharing the same neural architecture.\n","authors":["Wen-Ding Li","Keya Hu","Carter Larsen","Yuqing Wu","Simon Alford","Caleb Woo","Spencer M. Dunn","Hao Tang","Michelangelo Naim","Dat Nguyen","Wei-Long Zheng","Zenna Tavares","Yewen Pu","Kevin Ellis"],"pdf_url":"https://arxiv.org/pdf/2411.02272v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12671v1","updated":"2024-11-19T17:23:55Z","published":"2024-11-19T17:23:55Z","title":"Neurosymbolic Graph Enrichment for Grounded World Models","summary":"  The development of artificial intelligence systems capable of understanding\nand reasoning about complex real-world scenarios is a significant challenge. In\nthis work we present a novel approach to enhance and exploit LLM reactive\ncapability to address complex problems and interpret deeply contextual\nreal-world meaning. We introduce a method and a tool for creating a multimodal,\nknowledge-augmented formal representation of meaning that combines the\nstrengths of large language models with structured semantic representations.\nOur method begins with an image input, utilizing state-of-the-art large\nlanguage models to generate a natural language description. This description is\nthen transformed into an Abstract Meaning Representation (AMR) graph, which is\nformalized and enriched with logical design patterns, and layered semantics\nderived from linguistic and factual knowledge bases. The resulting graph is\nthen fed back into the LLM to be extended with implicit knowledge activated by\ncomplex heuristic learning, including semantic implicatures, moral values,\nembodied cognition, and metaphorical representations. By bridging the gap\nbetween unstructured language models and formal semantic structures, our method\nopens new avenues for tackling intricate problems in natural language\nunderstanding and reasoning.\n","authors":["Stefano De Giorgis","Aldo Gangemi","Alessandro Russo"],"pdf_url":"https://arxiv.org/pdf/2411.12671v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10020v3","updated":"2024-11-19T17:14:57Z","published":"2024-11-15T07:54:19Z","title":"Information Extraction from Clinical Notes: Are We Ready to Switch to\n  Large Language Models?","summary":"  Backgrounds: Information extraction (IE) is critical in clinical natural\nlanguage processing (NLP). While large language models (LLMs) excel on\ngenerative tasks, their performance on extractive tasks remains debated.\nMethods: We investigated Named Entity Recognition (NER) and Relation Extraction\n(RE) using 1,588 clinical notes from four sources (UT Physicians, MTSamples,\nMIMIC-III, and i2b2). We developed an annotated corpus covering 4 clinical\nentities and 16 modifiers, and compared instruction-tuned LLaMA-2 and LLaMA-3\nagainst BiomedBERT in terms of performance, generalizability, computational\nresources, and throughput to BiomedBERT. Results: LLaMA models outperformed\nBiomedBERT across datasets. With sufficient training data, LLaMA showed modest\nimprovements (1% on NER, 1.5-3.7% on RE); improvements were larger with limited\ntraining data. On unseen i2b2 data, LLaMA-3-70B outperformed BiomedBERT by 7%\n(F1) on NER and 4% on RE. However, LLaMA models required more computing\nresources and ran up to 28 times slower. We implemented \"Kiwi,\" a clinical IE\npackage featuring both models, available at https://kiwi.clinicalnlp.org/.\nConclusion: This study is among the first to develop and evaluate a\ncomprehensive clinical IE system using open-source LLMs. Results indicate that\nLLaMA models outperform BiomedBERT for clinical NER and RE but with higher\ncomputational costs and lower throughputs. These findings highlight that\nchoosing between LLMs and traditional deep learning methods for clinical IE\napplications should remain task-specific, taking into account both performance\nmetrics and practical considerations such as available computing resources and\nthe intended use case scenarios.\n","authors":["Yan Hu","Xu Zuo","Yujia Zhou","Xueqing Peng","Jimin Huang","Vipina K. Keloth","Vincent J. Zhang","Ruey-Ling Weng","Qingyu Chen","Xiaoqian Jiang","Kirk E. Roberts","Hua Xu"],"pdf_url":"https://arxiv.org/pdf/2411.10020v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12650v1","updated":"2024-11-19T16:58:15Z","published":"2024-11-19T16:58:15Z","title":"Optimizing Airline Reservation Systems with Edge-Enabled Microservices:\n  A Framework for Real-Time Data Processing and Enhanced User Responsiveness","summary":"  The growing complexity of the operations of airline reservations requires a\nsmart solution for the adoption of novel approaches to the development of\nquick, efficient, and adaptive reservation systems. This paper outlines in\ndetail a conceptual framework for the implementation of edge computing\nmicroservices in order to address the shortcomings of traditional centralized\narchitectures. Specifically, as edge computing allows for certain activities\nsuch as seat inventory checks, booking processes and even confirmation to be\ndone nearer to the user, thus lessening the overall response time and improving\nthe performance of the system. In addition, the framework value should include\nachieving the high performance of the system such as low latency, high\nthroughput and higher user experience. The major design components include\ndeployed distributed computing microservices orchestrated by Kubernetes,\nreal-time message processing system with Kafka and its elastic scaling. Other\noperational components include Prometheus and Grafana, which are used to\nmonitor and manage resources, ensuring that all operational processes are\noptimized. Although this research focuses on a design and theoretical scheming\nof the framework, its use is foreseen to be more advantageous in facilitating a\ntransform in the provision of services in the airline industry by improving\ncustomers' satisfaction, providing infrastructure which is cheap to install and\nefficiently supporting technology changes such as artificial intelligence and\ninternet of things embedded systems. This research addresses the increasing\ndemand for new technologies with modern well-distributed and real-time-centric\nsystems and also provides a basis for future case implementation and testing.\nAs such, the proposed architecture offers a market-ready, extensible solution\nto the problems posed by existing airline reservation systems .\n","authors":["Biman Barua","M. Shamim Kaiser"],"pdf_url":"https://arxiv.org/pdf/2411.12650v1.pdf","comment":"22 pages, 11 figures"},{"id":"http://arxiv.org/abs/2411.12643v1","updated":"2024-11-19T16:54:30Z","published":"2024-11-19T16:54:30Z","title":"DLBacktrace: A Model Agnostic Explainability for any Deep Learning\n  Models","summary":"  The rapid advancement of artificial intelligence has led to increasingly\nsophisticated deep learning models, which frequently operate as opaque 'black\nboxes' with limited transparency in their decision-making processes. This lack\nof interpretability presents considerable challenges, especially in high-stakes\napplications where understanding the rationale behind a model's outputs is as\nessential as the outputs themselves. This study addresses the pressing need for\ninterpretability in AI systems, emphasizing its role in fostering trust,\nensuring accountability, and promoting responsible deployment in\nmission-critical fields. To address the interpretability challenge in deep\nlearning, we introduce DLBacktrace, an innovative technique developed by the\nAryaXAI team to illuminate model decisions across a wide array of domains,\nincluding simple Multi Layer Perceptron (MLPs), Convolutional Neural Networks\n(CNNs), Large Language Models (LLMs), Computer Vision Models, and more.\n  We provide a comprehensive overview of the DLBacktrace algorithm and present\nbenchmarking results, comparing its performance against established\ninterpretability methods, such as SHAP, LIME, GradCAM, Integrated Gradients,\nSmoothGrad, and Attention Rollout, using diverse task-based metrics. The\nproposed DLBacktrace technique is compatible with various model architectures\nbuilt in PyTorch and TensorFlow, supporting models like Llama 3.2, other NLP\narchitectures such as BERT and LSTMs, computer vision models like ResNet and\nU-Net, as well as custom deep neural network (DNN) models for tabular data.\nThis flexibility underscores DLBacktrace's adaptability and effectiveness in\nenhancing model transparency across a broad spectrum of applications. The\nlibrary is open-sourced and available at https://github.com/AryaXAI/DLBacktrace .\n","authors":["Vinay Kumar Sankarapu","Chintan Chitroda","Yashwardhan Rathore","Neeraj Kumar Singh","Pratinav Seth"],"pdf_url":"https://arxiv.org/pdf/2411.12643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11579v2","updated":"2024-11-19T16:39:57Z","published":"2024-09-17T22:06:46Z","title":"HEARTS: A Holistic Framework for Explainable, Sustainable and Robust\n  Text Stereotype Detection","summary":"  Stereotypes are generalised assumptions about societal groups, and even\nstate-of-the-art LLMs using in-context learning struggle to identify them\naccurately. Due to the subjective nature of stereotypes, where what constitutes\na stereotype can vary widely depending on cultural, social, and individual\nperspectives, robust explainability is crucial. Explainable models ensure that\nthese nuanced judgments can be understood and validated by human users,\npromoting trust and accountability. We address these challenges by introducing\nHEARTS (Holistic Framework for Explainable, Sustainable, and Robust Text\nStereotype Detection), a framework that enhances model performance, minimises\ncarbon footprint, and provides transparent, interpretable explanations. We\nestablish the Expanded Multi-Grain Stereotype Dataset (EMGSD), comprising\n57,201 labelled texts across six groups, including under-represented\ndemographics like LGBTQ+ and regional stereotypes. Ablation studies confirm\nthat BERT models fine-tuned on EMGSD outperform those trained on individual\ncomponents. We then analyse a fine-tuned, carbon-efficient ALBERT-V2 model\nusing SHAP to generate token-level importance values, ensuring alignment with\nhuman understanding, and calculate explainability confidence scores by\ncomparing SHAP and LIME outputs...\n","authors":["Theo King","Zekun Wu","Adriano Koshiyama","Emre Kazim","Philip Treleaven"],"pdf_url":"https://arxiv.org/pdf/2409.11579v2.pdf","comment":"Accepted in NeurIPS 2024 SoLaR Workshop and Safety Gen AI Workshop"},{"id":"http://arxiv.org/abs/2305.14533v2","updated":"2024-11-19T16:34:17Z","published":"2023-05-23T21:33:43Z","title":"How to Choose How to Choose Your Chatbot: A Massively Multi-System\n  MultiReference Data Set for Dialog Metric Evaluation","summary":"  We release MMSMR, a Massively Multi-System MultiReference dataset to enable\nfuture work on metrics and evaluation for dialog. Automatic metrics for\ndialogue evaluation should be robust proxies for human judgments; however, the\nverification of robustness is currently far from satisfactory. To quantify the\nrobustness correlation and understand what is necessary in a test set, we\ncreate and release an 8-reference dialog dataset by extending single-reference\nevaluation sets and introduce this new language learning conversation dataset.\nWe then train 1750 systems and evaluate them on our novel test set and the\nDailyDialog dataset. We release the novel test set, and model hyper parameters,\ninference outputs, and metric scores for each system on a variety of datasets.\n","authors":["Huda Khayrallah","Zuhaib Akhtar","Edward Cohen","Jyothir S V","João Sedoc"],"pdf_url":"https://arxiv.org/pdf/2305.14533v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12619v1","updated":"2024-11-19T16:26:19Z","published":"2024-11-19T16:26:19Z","title":"Leveraging Virtual Reality and AI Tutoring for Language Learning: A Case\n  Study of a Virtual Campus Environment with OpenAI GPT Integration with Unity\n  3D","summary":"  This paper presents a new approach to multiple language learning, with Hindi\nthe language to be learnt in our case, by using the integration of virtual\nreality environments and AI enabled tutoring systems using OpenAIs GPT api\ncalls. We have developed a scenario which has a virtual campus environment\nusing Unity which focuses on a detailed representation of our universitys\nbuildings 11th floor, where most of the cultural and technological activities\ntake place. Within this virtual environment that we have created, we have an AI\ntutor powered by OpenAI's GPT model which was called using an api which moves\naround with the user. This provided language learning support in Hindi, as GPT\nis able to take care of language translation. Our approach mainly involves\nutilising speech to text, text to text conversion and text to speech\ncapabilities to facilitate real time interaction between users and the AI tutor\nin the presence of internet. This research demonstrates the use of combining VR\ntechnology with AI tutoring for immersive language learning experiences and\nprovides interaction.\n","authors":["Adithya TG","Abhinavaram N","Gowri Srinivasa"],"pdf_url":"https://arxiv.org/pdf/2411.12619v1.pdf","comment":"5 pages, 2 tables, 8 figures"},{"id":"http://arxiv.org/abs/2411.12587v1","updated":"2024-11-19T15:55:56Z","published":"2024-11-19T15:55:56Z","title":"Whisper Finetuning on Nepali Language","summary":"  Despite the growing advancements in Automatic Speech Recognition (ASR)\nmodels, the development of robust models for underrepresented languages, such\nas Nepali, remains a challenge. This research focuses on making an exhaustive\nand generalized dataset followed by fine-tuning OpenAI's Whisper models of\ndifferent sizes to improve transcription (speech-to-text) accuracy for the\nNepali language. We leverage publicly available ASR datasets and self-recorded\ncustom datasets with a diverse range of accents, dialects, and speaking styles\nfurther enriched through augmentation. Our experimental results demonstrate\nthat fine-tuning Whisper models on our curated custom dataset substantially\nreduces the Word Error Rate (WER) across all model sizes attributed to larger\ndata variations in terms of speaker's age, gender, and sentiment, acoustic\nenvironment, dialect, denser audio segments (15-30 seconds) that are more\ncompatible with Whisper's input, and manual curation of audios and\ntranscriptions. Notably, our approach outperforms Whisper's baseline models\ntrained on Fleur's dataset, achieving WER reductions of up to 36.2% on the\nsmall and 23.8% on medium models. Furthermore, we show that data augmentation\nplays a significant role in enhancing model robustness. Our approach underlines\nthe importance of dataset quality, variation, and augmentation in the\nadaptation of state-of-the-art models to underrepresented languages for\ndeveloping accurate ASR systems.\n","authors":["Sanjay Rijal","Shital Adhikari","Manish Dahal","Manish Awale","Vaghawan Ojha"],"pdf_url":"https://arxiv.org/pdf/2411.12587v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12580v1","updated":"2024-11-19T15:47:12Z","published":"2024-11-19T15:47:12Z","title":"Procedural Knowledge in Pretraining Drives Reasoning in Large Language\n  Models","summary":"  The capabilities and limitations of Large Language Models have been sketched\nout in great detail in recent years, providing an intriguing yet conflicting\npicture. On the one hand, LLMs demonstrate a general ability to solve problems.\nOn the other hand, they show surprising reasoning gaps when compared to humans,\ncasting doubt on the robustness of their generalisation strategies. The sheer\nvolume of data used in the design of LLMs has precluded us from applying the\nmethod traditionally used to measure generalisation: train-test set separation.\nTo overcome this, we study what kind of generalisation strategies LLMs employ\nwhen performing reasoning tasks by investigating the pretraining data they rely\non. For two models of different sizes (7B and 35B) and 2.5B of their\npretraining tokens, we identify what documents influence the model outputs for\nthree simple mathematical reasoning tasks and contrast this to the data that\nare influential for answering factual questions. We find that, while the models\nrely on mostly distinct sets of data for each factual question, a document\noften has a similar influence across different reasoning questions within the\nsame task, indicating the presence of procedural knowledge. We further find\nthat the answers to factual questions often show up in the most influential\ndata. However, for reasoning questions the answers usually do not show up as\nhighly influential, nor do the answers to the intermediate reasoning steps.\nWhen we characterise the top ranked documents for the reasoning questions\nqualitatively, we confirm that the influential documents often contain\nprocedural knowledge, like demonstrating how to obtain a solution using\nformulae or code. Our findings indicate that the approach to reasoning the\nmodels use is unlike retrieval, and more like a generalisable strategy that\nsynthesises procedural knowledge from documents doing a similar form of\nreasoning.\n","authors":["Laura Ruis","Maximilian Mozes","Juhan Bae","Siddhartha Rao Kamalakara","Dwarak Talupuru","Acyr Locatelli","Robert Kirk","Tim Rocktäschel","Edward Grefenstette","Max Bartolo"],"pdf_url":"https://arxiv.org/pdf/2411.12580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12571v1","updated":"2024-11-19T15:39:51Z","published":"2024-11-19T15:39:51Z","title":"Large Language Models for Combinatorial Optimization of Design Structure\n  Matrix","summary":"  Combinatorial optimization (CO) is essential for improving efficiency and\nperformance in engineering applications. As complexity increases with larger\nproblem sizes and more intricate dependencies, identifying the optimal solution\nbecome challenging. When it comes to real-world engineering problems,\nalgorithms based on pure mathematical reasoning are limited and incapable to\ncapture the contextual nuances necessary for optimization. This study explores\nthe potential of Large Language Models (LLMs) in solving engineering CO\nproblems by leveraging their reasoning power and contextual knowledge. We\npropose a novel LLM-based framework that integrates network topology and domain\nknowledge to optimize the sequencing of Design Structure Matrix (DSM)-a common\nCO problem. Our experiments on various DSM cases demonstrate that the proposed\nmethod achieves faster convergence and higher solution quality than benchmark\nmethods. Moreover, results show that incorporating contextual domain knowledge\nsignificantly improves performance despite the choice of LLMs. These findings\nhighlight the potential of LLMs in tackling complex real-world CO problems by\ncombining semantic and mathematical reasoning. This approach paves the way for\na new paradigm in in real-world combinatorial optimization.\n","authors":["Shuo Jiang","Min Xie","Jianxi Luo"],"pdf_url":"https://arxiv.org/pdf/2411.12571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17213v5","updated":"2024-11-19T15:37:57Z","published":"2024-09-25T17:38:39Z","title":"Plurals: A System for Guiding LLMs Via Simulated Social Ensembles","summary":"  Recent debates raised concerns that language models may favor certain\nviewpoints. But what if the solution is not to aim for a 'view from nowhere'\nbut rather to leverage different viewpoints? We introduce Plurals, a system and\nPython library for pluralistic AI deliberation. Plurals consists of Agents\n(LLMs, optionally with personas) which deliberate within customizable\nStructures, with Moderators overseeing deliberation. Plurals is a generator of\nsimulated social ensembles. Plurals integrates with government datasets to\ncreate nationally representative personas, includes deliberation templates\ninspired by deliberative democracy, and allows users to customize both\ninformation-sharing structures and deliberation behavior within Structures. Six\ncase studies demonstrate fidelity to theoretical constructs and efficacy. Three\nrandomized experiments show simulated focus groups produced output resonant\nwith an online sample of the relevant audiences (chosen over zero-shot\ngeneration in 75% of trials). Plurals is both a paradigm and a concrete system\nfor pluralistic AI. The Plurals library is available at\nhttps://github.com/josh-ashkinaze/plurals and will be continually updated.\n","authors":["Joshua Ashkinaze","Emily Fry","Narendra Edara","Eric Gilbert","Ceren Budak"],"pdf_url":"https://arxiv.org/pdf/2409.17213v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12539v1","updated":"2024-11-19T14:39:29Z","published":"2024-11-19T14:39:29Z","title":"Predicting Customer Satisfaction by Replicating the Survey Response\n  Distribution","summary":"  For many call centers, customer satisfaction (CSAT) is a key performance\nindicator (KPI). However, only a fraction of customers take the CSAT survey\nafter the call, leading to a biased and inaccurate average CSAT value, and\nmissed opportunities for coaching, follow-up, and rectification. Therefore,\ncall centers can benefit from a model predicting customer satisfaction on calls\nwhere the customer did not complete the survey. Given that CSAT is a closely\nmonitored KPI, it is critical to minimize any bias in the average predicted\nCSAT (pCSAT). In this paper, we introduce a method such that predicted CSAT\n(pCSAT) scores accurately replicate the distribution of survey CSAT responses\nfor every call center with sufficient data in a live production environment.\nThe method can be applied to many multiclass classification problems to improve\nthe class balance and minimize its changes upon model updates.\n","authors":["Etienne Manderscheid","Matthias Lee"],"pdf_url":"https://arxiv.org/pdf/2411.12539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12537v1","updated":"2024-11-19T14:35:38Z","published":"2024-11-19T14:35:38Z","title":"Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues","summary":"  Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and\nDeltaNet have emerged as efficient alternatives to Transformers in large\nlanguage modeling, offering linear scaling with sequence length and improved\ntraining efficiency. However, LRNNs struggle to perform state-tracking which\nmay impair performance in tasks such as code evaluation or tracking a chess\ngame. Even parity, the simplest state-tracking task, which non-linear RNNs like\nLSTM handle effectively, cannot be solved by current LRNNs. Recently, Sarrof et\nal. (2024) demonstrated that the failure of LRNNs like Mamba to solve parity\nstems from restricting the value range of their diagonal state-transition\nmatrices to $[0, 1]$ and that incorporating negative values can resolve this\nissue. We extend this result to non-diagonal LRNNs, which have recently shown\npromise in models such as DeltaNet. We prove that finite precision LRNNs with\nstate-transition matrices having only positive eigenvalues cannot solve parity,\nwhile complex eigenvalues are needed to count modulo $3$. Notably, we also\nprove that LRNNs can learn any regular language when their state-transition\nmatrices are products of identity minus vector outer product matrices, each\nwith eigenvalues in the range $[-1, 1]$. Our empirical results confirm that\nextending the eigenvalue range of models like Mamba and DeltaNet to include\nnegative values not only enables them to solve parity but consistently improves\ntheir performance on state-tracking tasks. Furthermore, pre-training LRNNs with\nan extended eigenvalue range for language modeling achieves comparable\nperformance and stability while showing promise on code and math data. Our work\nenhances the expressivity of modern LRNNs, broadening their applicability\nwithout changing the cost of training or inference.\n","authors":["Riccardo Grazzi","Julien Siems","Jörg K. H. Franke","Arber Zela","Frank Hutter","Massimiliano Pontil"],"pdf_url":"https://arxiv.org/pdf/2411.12537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00028v2","updated":"2024-11-19T14:29:32Z","published":"2024-10-29T04:03:15Z","title":"Synergizing LLM Agents and Knowledge Graph for Socioeconomic Prediction\n  in LBSN","summary":"  The fast development of location-based social networks (LBSNs) has led to\nsignificant changes in society, resulting in popular studies of using LBSN data\nfor socioeconomic prediction, e.g., regional population and commercial activity\nestimation. Existing studies design various graphs to model heterogeneous LBSN\ndata, and further apply graph representation learning methods for socioeconomic\nprediction. However, these approaches heavily rely on heuristic ideas and\nexpertise to extract task-relevant knowledge from diverse data, which may not\nbe optimal for specific tasks. Additionally, they tend to overlook the inherent\nrelationships between different indicators, limiting the prediction accuracy.\nMotivated by the remarkable abilities of large language models (LLMs) in\ncommonsense reasoning, embedding, and multi-agent collaboration, in this work,\nwe synergize LLM agents and knowledge graph for socioeconomic prediction. We\nfirst construct a location-based knowledge graph (LBKG) to integrate\nmulti-sourced LBSN data. Then we leverage the reasoning power of LLM agent to\nidentify relevant meta-paths in the LBKG for each type of socioeconomic\nprediction task, and design a semantic-guided attention module for knowledge\nfusion with meta-paths. Moreover, we introduce a cross-task communication\nmechanism to further enhance performance by enabling knowledge sharing across\ntasks at both LLM agent and KG levels. On the one hand, the LLM agents for\ndifferent tasks collaborate to generate more diverse and comprehensive\nmeta-paths. On the other hand, the embeddings from different tasks are\nadaptively merged for better socioeconomic prediction. Experiments on two\ndatasets demonstrate the effectiveness of the synergistic design between LLM\nand KG, providing insights for information sharing across socioeconomic\nprediction tasks.\n","authors":["Zhilun Zhou","Jingyang Fan","Yu Liu","Fengli Xu","Depeng Jin","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2411.00028v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19262v3","updated":"2024-11-19T13:27:30Z","published":"2024-05-29T16:55:32Z","title":"Weak-to-Strong Search: Align Large Language Models via Searching over\n  Small Language Models","summary":"  Large language models are usually fine-tuned to align with human preferences.\nHowever, fine-tuning a large language model can be challenging. In this work,\nwe introduce $\\textit{weak-to-strong search}$, framing the alignment of a large\nlanguage model as a test-time greedy search to maximize the log-probability\ndifference between small tuned and untuned models while sampling from the\nfrozen large model. This method serves both as (1) a compute-efficient model\nup-scaling strategy that avoids directly tuning the large model and as (2) an\ninstance of weak-to-strong generalization that enhances a strong model with\nweak test-time guidance. Empirically, we demonstrate the flexibility of\nweak-to-strong search across different tasks. In controlled-sentiment\ngeneration and summarization, we use tuned and untuned $\\texttt{gpt2}$s to\nimprove the alignment of large models without additional training. Crucially,\nin a more difficult instruction-following benchmark, AlpacaEval 2.0, we show\nthat reusing off-the-shelf small models (e.g., $\\texttt{zephyr-7b-beta}$ and\nits untuned version) can improve the length-controlled win rates of both\nwhite-box and black-box large models against $\\texttt{gpt-4-turbo}$ (e.g.,\n$34.4\\% \\rightarrow 37.9\\%$ for $\\texttt{Llama-3-70B-Instruct}$ and $16.0\\%\n\\rightarrow 20.1\\%$ for $\\texttt{gpt-3.5-turbo-instruct}$), despite the small\nmodels' low win rates $\\approx 10.0\\%$.\n","authors":["Zhanhui Zhou","Zhixuan Liu","Jie Liu","Zhichen Dong","Chao Yang","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2405.19262v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.12493v1","updated":"2024-11-19T13:23:53Z","published":"2024-11-19T13:23:53Z","title":"Bias Free Sentiment Analysis","summary":"  This paper introduces the Semantic Propagation Graph Neural Network (SProp\nGNN), a machine learning sentiment analysis (SA) architecture that relies\nexclusively on syntactic structures and word-level emotional cues to predict\nemotions in text. By semantically blinding the model to information about\nspecific words, it is robust to biases such as political or gender bias that\nhave been plaguing previous machine learning-based SA systems. The SProp GNN\nshows performance superior to lexicon-based alternatives such as VADER and\nEmoAtlas on two different prediction tasks, and across two languages.\nAdditionally, it approaches the accuracy of transformer-based models while\nsignificantly reducing bias in emotion prediction tasks. By offering improved\nexplainability and reducing bias, the SProp GNN bridges the methodological gap\nbetween interpretable lexicon approaches and powerful, yet often opaque, deep\nlearning models, offering a robust tool for fair and effective emotion analysis\nin understanding human behavior through text.\n","authors":["Hubert Plisiecki"],"pdf_url":"https://arxiv.org/pdf/2411.12493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05767v2","updated":"2024-11-19T13:22:21Z","published":"2024-08-11T13:17:14Z","title":"Reference-free Hallucination Detection for Large Vision-Language Models","summary":"  Large vision-language models (LVLMs) have made significant progress in recent\nyears. While LVLMs exhibit excellent ability in language understanding,\nquestion answering, and conversations of visual inputs, they are prone to\nproducing hallucinations. While several methods are proposed to evaluate the\nhallucinations in LVLMs, most are reference-based and depend on external tools,\nwhich complicates their practical application. To assess the viability of\nalternative methods, it is critical to understand whether the reference-free\napproaches, which do not rely on any external tools, can efficiently detect\nhallucinations. Therefore, we initiate an exploratory study to demonstrate the\neffectiveness of different reference-free solutions in detecting hallucinations\nin LVLMs. In particular, we conduct an extensive study on three kinds of\ntechniques: uncertainty-based, consistency-based, and supervised uncertainty\nquantification methods on four representative LVLMs across two different tasks.\nThe empirical results show that the reference-free approaches are capable of\neffectively detecting non-factual responses in LVLMs, with the supervised\nuncertainty quantification method outperforming the others, achieving the best\nperformance across different settings.\n","authors":["Qing Li","Jiahui Geng","Chenyang Lyu","Derui Zhu","Maxim Panov","Fakhri Karray"],"pdf_url":"https://arxiv.org/pdf/2408.05767v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12484v1","updated":"2024-11-19T13:08:03Z","published":"2024-11-19T13:08:03Z","title":"Regular-pattern-sensitive CRFs for Distant Label Interactions","summary":"  Linear-chain conditional random fields (CRFs) are a common model component\nfor sequence labeling tasks when modeling the interactions between different\nlabels is important. However, the Markov assumption limits linear-chain CRFs to\nonly directly modeling interactions between adjacent labels. Weighted\nfinite-state transducers (FSTs) are a related approach which can be made to\nmodel distant label-label interactions, but exact label inference is\nintractable for these models in the general case, and the task of selecting an\nappropriate automaton structure for the desired interaction types poses a\npractical challenge. In this work, we present regular-pattern-sensitive CRFs\n(RPCRFs), a method of enriching standard linear-chain CRFs with the ability to\nlearn long-distance label interactions which occur in user-specified patterns.\nThis approach allows users to write regular-expression label patterns concisely\nspecifying which types of interactions the model should take into account,\nallowing the model to learn from data whether and in which contexts these\npatterns occur. The result can be interpreted alternatively as a CRF augmented\nwith additional, non-local potentials, or as a finite-state transducer whose\nstructure is defined by a set of easily-interpretable patterns. Critically,\nunlike the general case for FSTs (and for non-chain CRFs), exact training and\ninference are tractable for many pattern sets. In this work, we detail how a\nRPCRF can be automatically constructed from a set of user-specified patterns,\nand demonstrate the model's effectiveness on synthetic data, showing how\ndifferent types of patterns can capture different nonlocal dependency\nstructures in label sequences.\n","authors":["Sean Papay","Roman Klinger","Sebastian Pado"],"pdf_url":"https://arxiv.org/pdf/2411.12484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12483v1","updated":"2024-11-19T13:07:04Z","published":"2024-11-19T13:07:04Z","title":"Analysing Explanation-Related Interactions in Collaborative\n  Perception-Cognition-Communication-Action","summary":"  Effective communication is essential in collaborative tasks, so AI-equipped\nrobots working alongside humans need to be able to explain their behaviour in\norder to cooperate effectively and earn trust. We analyse and classify\ncommunications among human participants collaborating to complete a simulated\nemergency response task. The analysis identifies messages that relate to\nvarious kinds of interactive explanations identified in the explainable AI\nliterature. This allows us to understand what type of explanations humans\nexpect from their teammates in such settings, and thus where AI-equipped robots\nmost need explanation capabilities. We find that most explanation-related\nmessages seek clarification in the decisions or actions taken. We also confirm\nthat messages have an impact on the performance of our simulated task.\n","authors":["Marc Roig Vilamala","Jack Furby","Julian de Gortari Briseno","Mani Srivastava","Alun Preece","Carolina Fuentes Toro"],"pdf_url":"https://arxiv.org/pdf/2411.12483v1.pdf","comment":"4 pages, 3 figures, published as a Late Breaking Report in RO-MAN\n  2024"},{"id":"http://arxiv.org/abs/2411.12473v1","updated":"2024-11-19T12:55:22Z","published":"2024-11-19T12:55:22Z","title":"NMT-Obfuscator Attack: Ignore a sentence in translation with only one\n  word","summary":"  Neural Machine Translation systems are used in diverse applications due to\ntheir impressive performance. However, recent studies have shown that these\nsystems are vulnerable to carefully crafted small perturbations to their\ninputs, known as adversarial attacks. In this paper, we propose a new type of\nadversarial attack against NMT models. In this attack, we find a word to be\nadded between two sentences such that the second sentence is ignored and not\ntranslated by the NMT model. The word added between the two sentences is such\nthat the whole adversarial text is natural in the source language. This type of\nattack can be harmful in practical scenarios since the attacker can hide\nmalicious information in the automatic translation made by the target NMT\nmodel. Our experiments show that different NMT models and translation tasks are\nvulnerable to this type of attack. Our attack can successfully force the NMT\nmodels to ignore the second part of the input in the translation for more than\n50% of all cases while being able to maintain low perplexity for the whole\ninput.\n","authors":["Sahar Sadrizadeh","César Descalzo","Ljiljana Dolamic","Pascal Frossard"],"pdf_url":"https://arxiv.org/pdf/2411.12473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05232v2","updated":"2024-11-19T12:42:45Z","published":"2023-11-09T09:25:37Z","title":"A Survey on Hallucination in Large Language Models: Principles,\n  Taxonomy, Challenges, and Open Questions","summary":"  The emergence of large language models (LLMs) has marked a significant\nbreakthrough in natural language processing (NLP), fueling a paradigm shift in\ninformation acquisition. Nevertheless, LLMs are prone to hallucination,\ngenerating plausible yet nonfactual content. This phenomenon raises significant\nconcerns over the reliability of LLMs in real-world information retrieval (IR)\nsystems and has attracted intensive research to detect and mitigate such\nhallucinations. Given the open-ended general-purpose attributes inherent to\nLLMs, LLM hallucinations present distinct challenges that diverge from prior\ntask-specific models. This divergence highlights the urgency for a nuanced\nunderstanding and comprehensive overview of recent advances in LLM\nhallucinations. In this survey, we begin with an innovative taxonomy of\nhallucination in the era of LLM and then delve into the factors contributing to\nhallucinations. Subsequently, we present a thorough overview of hallucination\ndetection methods and benchmarks. Our discussion then transfers to\nrepresentative methodologies for mitigating LLM hallucinations. Additionally,\nwe delve into the current limitations faced by retrieval-augmented LLMs in\ncombating hallucinations, offering insights for developing more robust IR\nsystems. Finally, we highlight the promising research directions on LLM\nhallucinations, including hallucination in large vision-language models and\nunderstanding of knowledge boundaries in LLM hallucinations.\n","authors":["Lei Huang","Weijiang Yu","Weitao Ma","Weihong Zhong","Zhangyin Feng","Haotian Wang","Qianglong Chen","Weihua Peng","Xiaocheng Feng","Bing Qin","Ting Liu"],"pdf_url":"https://arxiv.org/pdf/2311.05232v2.pdf","comment":"Accepted by ACM Transactions on Information Systems (TOIS)"},{"id":"http://arxiv.org/abs/2402.06420v2","updated":"2024-11-19T12:41:04Z","published":"2024-02-09T14:08:23Z","title":"Findings of the First Workshop on Simulating Conversational Intelligence\n  in Chat","summary":"  The aim of the workshop was to bring together experts working on open-domain\ndialogue research. In this speedily advancing research area many challenges\nstill exist, such as learning information from conversations, and engaging in a\nrealistic and convincing simulation of human intelligence and reasoning.\nSCI-CHAT follows previous workshops on open domain dialogue but in contrast the\nfocus of the shared task is simulation of intelligent conversation as judged in\na live human evaluation. Models aim to include the ability to follow a\nchallenging topic over a multi-turn conversation, while positing, refuting and\nreasoning over arguments. The workshop included both a research track and\nshared task. The main goal of this paper is to provide an overview of the\nshared task, and an in depth analysis of the shared task results following\npresentation at the workshop. The current paper is an extension of that made\navailable prior to presentation of results at the workshop at EACL Malta\n(Graham et al., 2024). The data collected in the evaluation was made publicly\navailable to aide future research. The code was also made available for the\nsame purpose.\n","authors":["Yvette Graham","Mohammed Rameez Qureshi","Haider Khalid","Gerasimos Lampouras","Ignacio Iacobacci","Qun Liu"],"pdf_url":"https://arxiv.org/pdf/2402.06420v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04625v3","updated":"2024-11-19T12:41:04Z","published":"2024-06-07T04:19:01Z","title":"Key-Element-Informed sLLM Tuning for Document Summarization","summary":"  Remarkable advances in large language models (LLMs) have enabled high-quality\ntext summarization. However, this capability is currently accessible only\nthrough LLMs of substantial size or proprietary LLMs with usage fees. In\nresponse, smaller-scale LLMs (sLLMs) of easy accessibility and low costs have\nbeen extensively studied, yet they often suffer from missing key information\nand entities, i.e., low relevance, in particular, when input documents are\nlong. We hence propose a key-element-informed instruction tuning for\nsummarization, so-called KEITSum, which identifies key elements in documents\nand instructs sLLM to generate summaries capturing these key elements.\nExperimental results on dialogue and news datasets demonstrate that sLLM with\nKEITSum indeed provides high-quality summarization with higher relevance and\nless hallucinations, competitive to proprietary LLM.\n","authors":["Sangwon Ryu","Heejin Do","Yunsu Kim","Gary Geunbae Lee","Jungseul Ok"],"pdf_url":"https://arxiv.org/pdf/2406.04625v3.pdf","comment":"Interspeech 2024"},{"id":"http://arxiv.org/abs/2411.12460v1","updated":"2024-11-19T12:36:02Z","published":"2024-11-19T12:36:02Z","title":"Guide-to-Explain for Controllable Summarization","summary":"  Recently, large language models (LLMs) have demonstrated remarkable\nperformance in abstractive summarization tasks. However, controllable\nsummarization with LLMs remains underexplored, limiting their ability to\ngenerate summaries that align with specific user preferences. In this paper, we\nfirst investigate the capability of LLMs to control diverse attributes,\nrevealing that they encounter greater challenges with numerical attributes,\nsuch as length and extractiveness, compared to linguistic attributes. To\naddress this challenge, we propose a guide-to-explain framework (GTE) for\ncontrollable summarization. Our GTE framework enables the model to identify\nmisaligned attributes in the initial draft and guides it in explaining errors\nin the previous output. Based on this reflection, the model generates a\nwell-adjusted summary. As a result, by allowing the model to reflect on its\nmisalignment, we generate summaries that satisfy the desired attributes in\nsurprisingly fewer iterations than other iterative methods solely using LLMs.\n","authors":["Sangwon Ryu","Heejin Do","Daehee Kim","Yunsu Kim","Gary Geunbae Lee","Jungseul Ok"],"pdf_url":"https://arxiv.org/pdf/2411.12460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12458v1","updated":"2024-11-19T12:29:30Z","published":"2024-11-19T12:29:30Z","title":"Variation between Credible and Non-Credible News Across Topics","summary":"  'Fake News' continues to undermine trust in modern journalism and politics.\nDespite continued efforts to study fake news, results have been conflicting.\nPrevious attempts to analyse and combat fake news have largely focused on\ndistinguishing fake news from truth, or differentiating between its various\nsub-types (such as propaganda, satire, misinformation, etc.) This paper\nconducts a linguistic and stylistic analysis of fake news, focusing on\nvariation between various news topics. It builds on related work identifying\nfeatures from discourse and linguistics in deception detection by analysing\nfive distinct news topics: Economy, Entertainment, Health, Science, and Sports.\nThe results emphasize that linguistic features vary between credible and\ndeceptive news in each domain and highlight the importance of adapting\nclassification tasks to accommodate variety-based stylistic and linguistic\ndifferences in order to achieve better real-world performance.\n","authors":["Emilie Francis"],"pdf_url":"https://arxiv.org/pdf/2411.12458v1.pdf","comment":"9 pages, 1 figure"},{"id":"http://arxiv.org/abs/2405.04793v2","updated":"2024-11-19T10:59:30Z","published":"2024-05-08T03:57:45Z","title":"Zero-shot LLM-guided Counterfactual Generation: A Case Study on NLP\n  Model Evaluation","summary":"  With the development and proliferation of large, complex, black-box models\nfor solving many natural language processing (NLP) tasks, there is also an\nincreasing necessity of methods to stress-test these models and provide some\ndegree of interpretability or explainability. While counterfactual examples are\nuseful in this regard, automated generation of counterfactuals is a data and\nresource intensive process. such methods depend on models such as pre-trained\nlanguage models that are then fine-tuned on auxiliary, often task-specific\ndatasets, that may be infeasible to build in practice, especially for new tasks\nand data domains. Therefore, in this work we explore the possibility of\nleveraging large language models (LLMs) for zero-shot counterfactual generation\nin order to stress-test NLP models. We propose a structured pipeline to\nfacilitate this generation, and we hypothesize that the instruction-following\nand textual understanding capabilities of recent LLMs can be effectively\nleveraged for generating high quality counterfactuals in a zero-shot manner,\nwithout requiring any training or fine-tuning. Through comprehensive\nexperiments on a variety of propreitary and open-source LLMs, along with\nvarious downstream tasks in NLP, we explore the efficacy of LLMs as zero-shot\ncounterfactual generators in evaluating and explaining black-box NLP models.\n","authors":["Amrita Bhattacharjee","Raha Moraffah","Joshua Garland","Huan Liu"],"pdf_url":"https://arxiv.org/pdf/2405.04793v2.pdf","comment":"Longer version of short paper accepted at IEEE BigData 2024 (Main\n  Track)"},{"id":"http://arxiv.org/abs/2411.12405v1","updated":"2024-11-19T10:41:54Z","published":"2024-11-19T10:41:54Z","title":"Evaluating the Prompt Steerability of Large Language Models","summary":"  Building pluralistic AI requires designing models that are able to be shaped\nto represent a wide range of value systems and cultures. Achieving this\nrequires first being able to evaluate the degree to which a given model is\ncapable of reflecting various personas. To this end, we propose a benchmark for\nevaluating the steerability of model personas as a function of prompting. Our\ndesign is based on a formal definition of prompt steerability, which analyzes\nthe degree to which a model's joint behavioral distribution can be shifted from\nits baseline behavior. By defining steerability indices and inspecting how\nthese indices change as a function of steering effort, we can estimate the\nsteerability of a model across various persona dimensions and directions. Our\nbenchmark reveals that the steerability of many current models is limited --\ndue to both a skew in their baseline behavior and an asymmetry in their\nsteerability across many persona dimensions. We release an implementation of\nour benchmark at https://github.com/IBM/prompt-steering.\n","authors":["Erik Miehling","Michael Desmond","Karthikeyan Natesan Ramamurthy","Elizabeth M. Daly","Pierre Dognin","Jesus Rios","Djallel Bouneffouf","Miao Liu"],"pdf_url":"https://arxiv.org/pdf/2411.12405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17394v2","updated":"2024-11-19T10:27:37Z","published":"2024-04-26T13:14:28Z","title":"Child Speech Recognition in Human-Robot Interaction: Problem Solved?","summary":"  Automated Speech Recognition shows superhuman performance for adult English\nspeech on a range of benchmarks, but disappoints when fed children's speech.\nThis has long sat in the way of child-robot interaction. Recent evolutions in\ndata-driven speech recognition, including the availability of Transformer\narchitectures and unprecedented volumes of training data, might mean a\nbreakthrough for child speech recognition and social robot applications aimed\nat children. We revisit a study on child speech recognition from 2017 and show\nthat indeed performance has increased, with newcomer OpenAI Whisper doing\nmarkedly better than leading commercial cloud services. Performance improves\neven more in highly structured interactions when priming models with specific\nphrases. While transcription is not perfect yet, the best model recognises\n60.3% of sentences correctly barring small grammatical differences, with\nsub-second transcription time running on a local GPU, showing potential for\nusable autonomous child-robot speech interactions.\n","authors":["Ruben Janssens","Eva Verhelst","Giulio Antonio Abbo","Qiaoqiao Ren","Maria Jose Pinto Bernal","Tony Belpaeme"],"pdf_url":"https://arxiv.org/pdf/2404.17394v2.pdf","comment":"Submitted to 2024 International Conference on Social Robotics"},{"id":"http://arxiv.org/abs/2411.12395v1","updated":"2024-11-19T10:27:26Z","published":"2024-11-19T10:27:26Z","title":"Do LLMs Understand Ambiguity in Text? A Case Study in Open-world\n  Question Answering","summary":"  Ambiguity in natural language poses significant challenges to Large Language\nModels (LLMs) used for open-domain question answering. LLMs often struggle with\nthe inherent uncertainties of human communication, leading to\nmisinterpretations, miscommunications, hallucinations, and biased responses.\nThis significantly weakens their ability to be used for tasks like\nfact-checking, question answering, feature extraction, and sentiment analysis.\nUsing open-domain question answering as a test case, we compare off-the-shelf\nand few-shot LLM performance, focusing on measuring the impact of explicit\ndisambiguation strategies. We demonstrate how simple, training-free,\ntoken-level disambiguation methods may be effectively used to improve LLM\nperformance for ambiguous question answering tasks. We empirically show our\nfindings and discuss best practices and broader impacts regarding ambiguity in\nLLMs.\n","authors":["Aryan Keluskar","Amrita Bhattacharjee","Huan Liu"],"pdf_url":"https://arxiv.org/pdf/2411.12395v1.pdf","comment":"Accepted at the REU Symposium at IEEE BigData 2024"},{"id":"http://arxiv.org/abs/2410.19572v4","updated":"2024-11-19T10:00:41Z","published":"2024-10-25T14:07:53Z","title":"ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems","summary":"  Retrieval-Augmented Generation (RAG) systems using large language models\n(LLMs) often generate inaccurate responses due to the retrieval of irrelevant\nor loosely related information. Existing methods, which operate at the document\nlevel, fail to effectively filter out such content. We propose LLM-driven chunk\nfiltering, ChunkRAG, a framework that enhances RAG systems by evaluating and\nfiltering retrieved information at the chunk level. Our approach employs\nsemantic chunking to divide documents into coherent sections and utilizes\nLLM-based relevance scoring to assess each chunk's alignment with the user's\nquery. By filtering out less pertinent chunks before the generation phase, we\nsignificantly reduce hallucinations and improve factual accuracy. Experiments\nshow that our method outperforms existing RAG models, achieving higher accuracy\non tasks requiring precise information retrieval. This advancement enhances the\nreliability of RAG systems, making them particularly beneficial for\napplications like fact-checking and multi-hop reasoning.\n","authors":["Ishneet Sukhvinder Singh","Ritvik Aggarwal","Ibrahim Allahverdiyev","Muhammad Taha","Aslihan Akalin","Kevin Zhu","Sean O'Brien"],"pdf_url":"https://arxiv.org/pdf/2410.19572v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12372v1","updated":"2024-11-19T09:35:28Z","published":"2024-11-19T09:35:28Z","title":"RedPajama: an Open Dataset for Training Large Language Models","summary":"  Large language models are increasingly becoming a cornerstone technology in\nartificial intelligence, the sciences, and society as a whole, yet the optimal\nstrategies for dataset composition and filtering remain largely elusive. Many\nof the top-performing models lack transparency in their dataset curation and\nmodel development processes, posing an obstacle to the development of fully\nopen language models. In this paper, we identify three core data-related\nchallenges that must be addressed to advance open-source language models. These\ninclude (1) transparency in model development, including the data curation\nprocess, (2) access to large quantities of high-quality data, and (3)\navailability of artifacts and metadata for dataset curation and analysis. To\naddress these challenges, we release RedPajama-V1, an open reproduction of the\nLLaMA training dataset. In addition, we release RedPajama-V2, a massive\nweb-only dataset consisting of raw, unfiltered text data together with quality\nsignals and metadata. Together, the RedPajama datasets comprise over 100\ntrillion tokens spanning multiple domains and with their quality signals\nfacilitate the filtering of data, aiming to inspire the development of numerous\nnew datasets. To date, these datasets have already been used in the training of\nstrong language models used in production, such as Snowflake Arctic,\nSalesforce's XGen and AI2's OLMo. To provide insight into the quality of\nRedPajama, we present a series of analyses and ablation studies with\ndecoder-only language models with up to 1.6B parameters. Our findings\ndemonstrate how quality signals for web data can be effectively leveraged to\ncurate high-quality subsets of the dataset, underscoring the potential of\nRedPajama to advance the development of transparent and high-performing\nlanguage models at scale.\n","authors":["Maurice Weber","Daniel Fu","Quentin Anthony","Yonatan Oren","Shane Adams","Anton Alexandrov","Xiaozhong Lyu","Huu Nguyen","Xiaozhe Yao","Virginia Adams","Ben Athiwaratkun","Rahul Chalamala","Kezhen Chen","Max Ryabinin","Tri Dao","Percy Liang","Christopher Ré","Irina Rish","Ce Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.12372v1.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024) Track on Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2312.07141v3","updated":"2024-11-19T09:33:17Z","published":"2023-12-12T10:24:17Z","title":"Multilingual large language models leak human stereotypes across\n  language boundaries","summary":"  Multilingual large language models have gained prominence for their\nproficiency in processing and generating text across languages. Like their\nmonolingual counterparts, multilingual models are likely to pick up on\nstereotypes and other social biases present in their training data. In this\npaper, we study a phenomenon we term stereotype leakage, which refers to how\ntraining a model multilingually may lead to stereotypes expressed in one\nlanguage showing up in the models' behaviour in another. We propose a\nmeasurement framework for stereotype leakage and investigate its effect across\nEnglish, Russian, Chinese, and Hindi and with GPT-3.5, mT5, and mBERT. Our\nfindings show a noticeable leakage of positive, negative, and non-polar\nassociations across all languages. We find that of these models, GPT-3.5\nexhibits the most stereotype leakage, and Hindi is the most susceptible to\nleakage effects. WARNING: This paper contains model outputs which could be\noffensive in nature.\n","authors":["Yang Trista Cao","Anna Sotnikova","Jieyu Zhao","Linda X. Zou","Rachel Rudinger","Hal Daume III"],"pdf_url":"https://arxiv.org/pdf/2312.07141v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16718v2","updated":"2024-11-19T09:27:37Z","published":"2024-09-25T08:07:18Z","title":"Vision-Language Model Fine-Tuning via Simple Parameter-Efficient\n  Modification","summary":"  Recent advances in fine-tuning Vision-Language Models (VLMs) have witnessed\nthe success of prompt tuning and adapter tuning, while the classic model\nfine-tuning on inherent parameters seems to be overlooked. It is believed that\nfine-tuning the parameters of VLMs with few-shot samples corrupts the\npre-trained knowledge since fine-tuning the CLIP model even degrades\nperformance. In this paper, we revisit this viewpoint, and propose a new\nperspective: fine-tuning the specific parameters instead of all will uncover\nthe power of classic model fine-tuning on VLMs. Through our meticulous study,\nwe propose ClipFit, a simple yet effective method to fine-tune CLIP without\nintroducing any overhead of extra parameters. We demonstrate that by only\nfine-tuning the specific bias terms and normalization layers, ClipFit can\nimprove the performance of zero-shot CLIP by 7.27\\% average harmonic mean\naccuracy. Lastly, to understand how fine-tuning in CLIPFit affects the\npre-trained models, we conducted extensive experimental analyses w.r.t. changes\nin internal parameters and representations. We found that low-level text bias\nlayers and the first layer normalization layer change much more than other\nlayers. The code is available at \\url{https://github.com/minglllli/CLIPFit}.\n","authors":["Ming Li","Jike Zhong","Chenxin Li","Liuzhuozheng Li","Nie Lin","Masashi Sugiyama"],"pdf_url":"https://arxiv.org/pdf/2409.16718v2.pdf","comment":"EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2411.12357v1","updated":"2024-11-19T09:18:20Z","published":"2024-11-19T09:18:20Z","title":"A Layered Architecture for Developing and Enhancing Capabilities in\n  Large Language Model-based Software Systems","summary":"  Significant efforts has been made to expand the use of Large Language Models\n(LLMs) beyond basic language tasks. While the generalizability and versatility\nof LLMs have enabled widespread adoption, evolving demands in application\ndevelopment often exceed their native capabilities. Meeting these demands may\ninvolve a diverse set of methods, such as enhancing creativity through either\ninference temperature adjustments or creativity-provoking prompts. Selecting\nthe right approach is critical, as different methods lead to trade-offs in\nengineering complexity, scalability, and operational costs. This paper\nintroduces a layered architecture that organizes LLM software system\ndevelopment into distinct layers, each characterized by specific attributes. By\naligning capabilities with these layers, the framework encourages the\nsystematic implementation of capabilities in effective and efficient ways that\nultimately supports desired functionalities and qualities. Through practical\ncase studies, we illustrate the utility of the framework. This work offers\ndevelopers actionable insights for selecting suitable technologies in LLM-based\nsoftware system development, promoting robustness and scalability.\n","authors":["Dawen Zhang","Xiwei Xu","Chen Wang","Zhenchang Xing","Robert Mao"],"pdf_url":"https://arxiv.org/pdf/2411.12357v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06275v4","updated":"2024-11-19T09:06:33Z","published":"2023-09-12T14:36:23Z","title":"Re-Reading Improves Reasoning in Large Language Models","summary":"  To enhance the reasoning capabilities of off-the-shelf Large Language Models\n(LLMs), we introduce a simple, yet general and effective prompting method, Re2,\ni.e., \\textbf{Re}-\\textbf{Re}ading the question as input. Unlike most\nthought-eliciting prompting methods, such as Chain-of-Thought (CoT), which aim\nto elicit the reasoning process in the output, Re2 shifts the focus to the\ninput by processing questions twice, thereby enhancing the understanding\nprocess. Consequently, Re2 demonstrates strong generality and compatibility\nwith most thought-eliciting prompting methods, including CoT. Crucially, Re2\nfacilitates a \"bidirectional\" encoding in unidirectional decoder-only LLMs\nbecause the first pass could provide global information for the second pass. We\nbegin with a preliminary empirical study as the foundation of Re2, illustrating\nits potential to enable \"bidirectional\" attention mechanisms. We then evaluate\nRe2 on extensive reasoning benchmarks across 14 datasets, spanning 112\nexperiments, to validate its effectiveness and generality. Our findings\nindicate that, with the exception of a few scenarios on vanilla ChatGPT, Re2\nconsistently enhances the reasoning performance of LLMs through a simple\nre-reading strategy. Further analyses reveal Re2's adaptability, showing how it\ncan be effectively integrated with different LLMs, thought-eliciting prompting,\nand ensemble strategies. Our code is available at\n\\url{https://github.com/Tebmer/Rereading-LLM-Reasoning/}\n","authors":["Xiaohan Xu","Chongyang Tao","Tao Shen","Can Xu","Hongbo Xu","Guodong Long","Jian-guang Lou","Shuai Ma"],"pdf_url":"https://arxiv.org/pdf/2309.06275v4.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2406.05085v2","updated":"2024-11-19T08:46:34Z","published":"2024-06-07T16:59:38Z","title":"Multi-Head RAG: Solving Multi-Aspect Problems with LLMs","summary":"  Retrieval Augmented Generation (RAG) enhances the abilities of Large Language\nModels (LLMs) by enabling the retrieval of documents into the LLM context to\nprovide more accurate and relevant responses. Existing RAG solutions do not\nfocus on queries that may require fetching multiple documents with\nsubstantially different contents. Such queries occur frequently, but are\nchallenging because the embeddings of these documents may be distant in the\nembedding space, making it hard to retrieve them all. This paper introduces\nMulti-Head RAG (MRAG), a novel scheme designed to address this gap with a\nsimple yet powerful idea: leveraging activations of Transformer's multi-head\nattention layer, instead of the decoder layer, as keys for fetching\nmulti-aspect documents. The driving motivation is that different attention\nheads can learn to capture different data aspects. Harnessing the corresponding\nactivations results in embeddings that represent various facets of data items\nand queries, improving the retrieval accuracy for complex queries. We provide\nan evaluation methodology and metrics, multi-aspect datasets that we release\nonline, and real-world use cases to demonstrate MRAG's effectiveness, showing\nimprovements of up to 20% in relevance over standard RAG baselines. MRAG can be\nseamlessly integrated with existing RAG frameworks and benchmarking tools like\nRAGAS as well as different classes of data stores.\n","authors":["Maciej Besta","Ales Kubicek","Roman Niggli","Robert Gerstenberger","Lucas Weitzendorf","Mingyuan Chi","Patrick Iff","Joanna Gajda","Piotr Nyczyk","Jürgen Müller","Hubert Niewiadomski","Marcin Chrapek","Michał Podstawski","Torsten Hoefler"],"pdf_url":"https://arxiv.org/pdf/2406.05085v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02856v5","updated":"2024-11-19T08:38:55Z","published":"2024-06-05T02:12:06Z","title":"Xmodel-LM Technical Report","summary":"  We introduce Xmodel-LM, a compact and efficient 1.1B language model\npre-trained on around 2 trillion tokens. Trained on our self-built dataset\n(Xdata), which balances Chinese and English corpora based on downstream task\noptimization, Xmodel-LM exhibits remarkable performance despite its smaller\nsize. It notably surpasses existing open-source language models of similar\nscale. Our model checkpoints and code are publicly accessible on GitHub at\nhttps://github.com/XiaoduoAILab/XmodelLM.\n","authors":["Yichuan Wang","Yang Liu","Yu Yan","Qun Wang","Xucheng Huang","Ling Jiang"],"pdf_url":"https://arxiv.org/pdf/2406.02856v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01285v2","updated":"2024-11-19T08:08:38Z","published":"2024-10-02T07:14:26Z","title":"Enhancing Training Data Attribution for Large Language Models with\n  Fitting Error Consideration","summary":"  The black-box nature of large language models (LLMs) poses challenges in\ninterpreting results, impacting issues such as data intellectual property\nprotection and hallucination tracing. Training data attribution (TDA) methods\nare considered effective solutions to address these challenges. Most recent TDA\nmethods rely on influence functions, assuming the model achieves minimized\nempirical risk. However, achieving this criterion is difficult, and sourcing\naccuracy can be compromised by fitting errors during model training. In this\npaper, we introduce a novel TDA method called Debias and Denoise Attribution\n(DDA), which enhances influence functions by addressing fitting errors.\nSpecifically, the debias strategy seeks to improve the performance of influence\nfunctions by eliminating the knowledge bias present in the base model before\nfine-tuning, while the denoise strategy aims to reduce discrepancies in\ninfluence scores arising from varying degrees of fitting during the training\nprocess through smoothing techniques. Experimental results demonstrate that our\nmethod significantly outperforms existing approaches, achieving an averaged AUC\nof 91.64%. Moreover, DDA exhibits strong generality and scalability across\nvarious sources and different-scale models like LLaMA2, QWEN2, and Mistral.\n","authors":["Kangxi Wu","Liang Pang","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.01285v2.pdf","comment":"Accepted to the EMNLP 2024 main"},{"id":"http://arxiv.org/abs/2411.12307v1","updated":"2024-11-19T07:48:35Z","published":"2024-11-19T07:48:35Z","title":"Balancing Accuracy and Efficiency in Multi-Turn Intent Classification\n  for LLM-Powered Dialog Systems in Production","summary":"  Accurate multi-turn intent classification is essential for advancing\nconversational AI systems. However, challenges such as the scarcity of\ncomprehensive datasets and the complexity of contextual dependencies across\ndialogue turns hinder progress. This paper presents two novel approaches\nleveraging Large Language Models (LLMs) to enhance scalability and reduce\nlatency in production dialogue systems. First, we introduce Symbol Tuning,\nwhich simplifies intent labels to reduce task complexity and improve\nperformance in multi-turn dialogues. Second, we propose C-LARA\n(Consistency-aware, Linguistics Adaptive Retrieval Augmentation), a framework\nthat employs LLMs for data augmentation and pseudo-labeling to generate\nsynthetic multi-turn dialogues. These enriched datasets are used to fine-tune a\nsmall, efficient model suitable for deployment. Experiments conducted on\nmultilingual dialogue datasets demonstrate significant improvements in\nclassification accuracy and resource efficiency. Our methods enhance multi-turn\nintent classification accuracy by 5.09%, reduce annotation costs by 40%, and\nenable scalable deployment in low-resource multilingual industrial systems,\nhighlighting their practicality and impact.\n","authors":["Junhua Liu","Yong Keat Tan","Bin Fu","Kwan Hui Lim"],"pdf_url":"https://arxiv.org/pdf/2411.12307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15000v3","updated":"2024-11-19T07:46:16Z","published":"2024-02-22T22:28:46Z","title":"Divide-or-Conquer? Which Part Should You Distill Your LLM?","summary":"  Recent methods have demonstrated that Large Language Models (LLMs) can solve\nreasoning tasks better when they are encouraged to solve subtasks of the main\ntask first. In this paper we devise a similar strategy that breaks down\nreasoning tasks into a problem decomposition phase and a problem solving phase\nand show that the strategy is able to outperform a single stage solution.\nFurther, we hypothesize that the decomposition should be easier to distill into\na smaller model compared to the problem solving because the latter requires\nlarge amounts of domain knowledge while the former only requires learning\ngeneral problem solving strategies. We propose methods to distill these two\ncapabilities and evaluate their impact on reasoning outcomes and inference\ncost. We find that we can distill the problem decomposition phase and at the\nsame time achieve good generalization across tasks, datasets, and models.\nHowever, it is harder to distill the problem solving capability without losing\nperformance and the resulting distilled model struggles with generalization.\nThese results indicate that by using smaller, distilled problem decomposition\nmodels in combination with problem solving LLMs we can achieve reasoning with\ncost-efficient inference and local adaptation.\n","authors":["Zhuofeng Wu","He Bai","Aonan Zhang","Jiatao Gu","VG Vinod Vydiswaran","Navdeep Jaitly","Yizhe Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.15000v3.pdf","comment":"Findings of the Association for Computational Linguistics: EMNLP 2024"},{"id":"http://arxiv.org/abs/2411.12287v1","updated":"2024-11-19T07:16:48Z","published":"2024-11-19T07:16:48Z","title":"CUE-M: Contextual Understanding and Enhanced Search with Multimodal\n  Large Language Model","summary":"  The integration of Retrieval-Augmented Generation (RAG) with Multimodal Large\nLanguage Models (MLLMs) has expanded the scope of multimodal query resolution.\nHowever, current systems struggle with intent understanding, information\nretrieval, and safety filtering, limiting their effectiveness. This paper\nintroduces Contextual Understanding and Enhanced Search with MLLM (CUE-M), a\nnovel multimodal search pipeline that addresses these challenges through a\nmulti-stage framework comprising image context enrichment, intent refinement,\ncontextual query generation, external API integration, and relevance-based\nfiltering. CUE-M incorporates a robust safety framework combining image-based,\ntext-based, and multimodal classifiers, dynamically adapting to instance- and\ncategory-specific risks. Evaluations on a multimodal Q&A dataset and a public\nsafety benchmark demonstrate that CUE-M outperforms baselines in accuracy,\nknowledge integration, and safety, advancing the capabilities of multimodal\nretrieval systems.\n","authors":["Dongyoung Go","Taesun Whang","Chanhee Lee","Hwayeon Kim","Sunghoon Park","Seunghwan Ji","Dongchan Kim","Young-Bum Kim"],"pdf_url":"https://arxiv.org/pdf/2411.12287v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2411.12275v1","updated":"2024-11-19T06:55:57Z","published":"2024-11-19T06:55:57Z","title":"Building Trust: Foundations of Security, Safety and Transparency in AI","summary":"  This paper explores the rapidly evolving ecosystem of publicly available AI\nmodels, and their potential implications on the security and safety landscape.\nAs AI models become increasingly prevalent, understanding their potential risks\nand vulnerabilities is crucial. We review the current security and safety\nscenarios while highlighting challenges such as tracking issues, remediation,\nand the apparent absence of AI model lifecycle and ownership processes.\nComprehensive strategies to enhance security and safety for both model\ndevelopers and end-users are proposed. This paper aims to provide some of the\nfoundational pieces for more standardized security, safety, and transparency in\nthe development and operation of AI models and the larger open ecosystems and\ncommunities forming around them.\n","authors":["Huzaifa Sidhpurwala","Garth Mollett","Emily Fox","Mark Bestavros","Huamin Chen"],"pdf_url":"https://arxiv.org/pdf/2411.12275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12274v1","updated":"2024-11-19T06:53:54Z","published":"2024-11-19T06:53:54Z","title":"A Review on Generative AI Models for Synthetic Medical Text, Time\n  Series, and Longitudinal Data","summary":"  This paper presents the results of a novel scoping review on the practical\nmodels for generating three different types of synthetic health records (SHRs):\nmedical text, time series, and longitudinal data. The innovative aspects of the\nreview, which incorporate study objectives, data modality, and research\nmethodology of the reviewed studies, uncover the importance and the scope of\nthe topic for the digital medicine context. In total, 52 publications met the\neligibility criteria for generating medical time series (22), longitudinal data\n(17), and medical text (13). Privacy preservation was found to be the main\nresearch objective of the studied papers, along with class imbalance, data\nscarcity, and data imputation as the other objectives. The adversarial\nnetwork-based, probabilistic, and large language models exhibited superiority\nfor generating synthetic longitudinal data, time series, and medical texts,\nrespectively. Finding a reliable performance measure to quantify SHR\nre-identification risk is the major research gap of the topic.\n","authors":["Mohammad Loni","Fatemeh Poursalim","Mehdi Asadi","Arash Gharehbaghi"],"pdf_url":"https://arxiv.org/pdf/2411.12274v1.pdf","comment":"27 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.11072v2","updated":"2024-11-19T06:45:13Z","published":"2024-11-17T13:21:26Z","title":"Multilingual Large Language Models: A Systematic Survey","summary":"  This paper provides a comprehensive survey of the latest research on\nmultilingual large language models (MLLMs). MLLMs not only are able to\nunderstand and generate language across linguistic boundaries, but also\nrepresent an important advancement in artificial intelligence. We first discuss\nthe architecture and pre-training objectives of MLLMs, highlighting the key\ncomponents and methodologies that contribute to their multilingual\ncapabilities. We then discuss the construction of multilingual pre-training and\nalignment datasets, underscoring the importance of data quality and diversity\nin enhancing MLLM performance. An important focus of this survey is on the\nevaluation of MLLMs. We present a detailed taxonomy and roadmap covering the\nassessment of MLLMs' cross-lingual knowledge, reasoning, alignment with human\nvalues, safety, interpretability and specialized applications. Specifically, we\nextensively discuss multilingual evaluation benchmarks and datasets, and\nexplore the use of LLMs themselves as multilingual evaluators. To enhance MLLMs\nfrom black to white boxes, we also address the interpretability of multilingual\ncapabilities, cross-lingual transfer and language bias within these models.\nFinally, we provide a comprehensive review of real-world applications of MLLMs\nacross diverse domains, including biology, medicine, computer science,\nmathematics and law. We showcase how these models have driven innovation and\nimprovements in these specialized fields while also highlighting the challenges\nand opportunities in deploying MLLMs within diverse language communities and\napplication scenarios. We listed the paper related in this survey and publicly\navailable at https://github.com/tjunlp-lab/Awesome-Multilingual-LLMs-Papers.\n","authors":["Shaolin Zhu"," Supryadi","Shaoyang Xu","Haoran Sun","Leiyu Pan","Menglong Cui","Jiangcun Du","Renren Jin","António Branco","Deyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2411.11072v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12262v1","updated":"2024-11-19T06:21:51Z","published":"2024-11-19T06:21:51Z","title":"Low-resource Machine Translation: what for? who for? An observational\n  study on a dedicated Tetun language translation service","summary":"  The impact of machine translation (MT) on low-resource languages remains\npoorly understood. In particular, observational studies of actual usage\npatterns are scarce. Such studies could provide valuable insights into user\nneeds and behaviours, complementing survey-based methods. Here we present an\nobservational analysis of real-world MT usage for Tetun, the lingua franca of\nTimor-Leste, using server logs from a widely-used MT service with over $70,000$\nmonthly active users. Our analysis of $100,000$ translation requests reveals\npatterns that challenge assumptions based on existing corpora. We find that\nusers, many of them students on mobile devices, typically translate short texts\ninto Tetun across diverse domains including science, healthcare, and daily\nlife. This contrasts sharply with available Tetun corpora, which are dominated\nby news articles covering government and social issues. Our results suggest\nthat MT systems for languages like Tetun should prioritise translating into the\nlow-resource language, handling brief inputs effectively, and covering a wide\nrange of domains relevant to educational contexts. More broadly, this study\ndemonstrates how observational analysis can inform low-resource language\ntechnology development, by grounding research in practical community needs.\n","authors":["Raphael Merx","Hanna Suominen","Adérito José Guterres Correia","Trevor Cohn","Ekaterina Vylomova"],"pdf_url":"https://arxiv.org/pdf/2411.12262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01812v4","updated":"2024-11-19T06:14:31Z","published":"2024-09-14T02:35:29Z","title":"From Text to Multimodality: Exploring the Evolution and Impact of Large\n  Language Models in Medical Practice","summary":"  Large Language Models (LLMs) have rapidly evolved from text-based systems to\nmultimodal platforms, significantly impacting various sectors including\nhealthcare. This comprehensive review explores the progression of LLMs to\nMultimodal Large Language Models (MLLMs) and their growing influence in medical\npractice. We examine the current landscape of MLLMs in healthcare, analyzing\ntheir applications across clinical decision support, medical imaging, patient\nengagement, and research. The review highlights the unique capabilities of\nMLLMs in integrating diverse data types, such as text, images, and audio, to\nprovide more comprehensive insights into patient health. We also address the\nchallenges facing MLLM implementation, including data limitations, technical\nhurdles, and ethical considerations. By identifying key research gaps, this\npaper aims to guide future investigations in areas such as dataset development,\nmodality alignment methods, and the establishment of ethical guidelines. As\nMLLMs continue to shape the future of healthcare, understanding their potential\nand limitations is crucial for their responsible and effective integration into\nmedical practice.\n","authors":["Qian Niu","Keyu Chen","Ming Li","Pohsun Feng","Ziqian Bi","Lawrence KQ Yan","Yichao Zhang","Caitlyn Heqi Yin","Cheng Fei","Junyu Liu","Benji Peng","Tianyang Wang","Yunze Wang","Silin Chen"],"pdf_url":"https://arxiv.org/pdf/2410.01812v4.pdf","comment":"12 pages, 1 figure"},{"id":"http://arxiv.org/abs/2410.03182v2","updated":"2024-11-19T05:57:28Z","published":"2024-10-04T06:45:48Z","title":"Generating bilingual example sentences with large language models as\n  lexicography assistants","summary":"  We present a study of LLMs' performance in generating and rating example\nsentences for bilingual dictionaries across languages with varying resource\nlevels: French (high-resource), Indonesian (mid-resource), and Tetun\n(low-resource), with English as the target language. We evaluate the quality of\nLLM-generated examples against the GDEX (Good Dictionary EXample) criteria:\ntypicality, informativeness, and intelligibility. Our findings reveal that\nwhile LLMs can generate reasonably good dictionary examples, their performance\ndegrades significantly for lower-resourced languages. We also observe high\nvariability in human preferences for example quality, reflected in low\ninter-annotator agreement rates. To address this, we demonstrate that\nin-context learning can successfully align LLMs with individual annotator\npreferences. Additionally, we explore the use of pre-trained language models\nfor automated rating of examples, finding that sentence perplexity serves as a\ngood proxy for typicality and intelligibility in higher-resourced languages.\nOur study also contributes a novel dataset of 600 ratings for LLM-generated\nsentence pairs, and provides insights into the potential of LLMs in reducing\nthe cost of lexicographic work, particularly for low-resource languages.\n","authors":["Raphael Merx","Ekaterina Vylomova","Kemal Kurniawan"],"pdf_url":"https://arxiv.org/pdf/2410.03182v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12240v1","updated":"2024-11-19T05:37:17Z","published":"2024-11-19T05:37:17Z","title":"Evaluating Tokenizer Performance of Large Language Models Across\n  Official Indian Languages","summary":"  Large Language Models (LLMs) based on transformer architectures have\nrevolutionized a variety of domains, with tokenization playing a pivotal role\nin their pre-processing and fine-tuning stages. In multilingual models,\nparticularly those tailored for Indic languages, effective tokenization is\ncrucial for optimizing performance. This paper presents a comprehensive\nevaluation of tokenizers used by 12 LLMs across all 22 official languages of\nIndia, with a focus on comparing the efficiency of their tokenization\nprocesses. We employed the Normalized Sequence Length (NSL) as a key metric in\nour analysis. Our findings reveal that the SUTRA tokenizer outperforms all\nother models, including several Indic-specific models, excelling in 14\nlanguages. Notable insights include the SUTRA tokenizer's superior handling of\nIndic languages, GPT-4o's advancement over its predecessor GPT-4 in processing\nIndian languages, and the limited performance of Project Indus in certain\nlanguages. This study underscores the critical importance of developing\ntargeted tokenization strategies for multilingual and Indic-centric models,\nlaying the groundwork for future improvements in tokenizer design to enhance\nlinguistic coverage and model efficiency.\n","authors":["S. Tamang","D. J. Bora"],"pdf_url":"https://arxiv.org/pdf/2411.12240v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.11019v3","updated":"2024-11-19T05:35:02Z","published":"2023-07-20T16:46:10Z","title":"Investigating the Factual Knowledge Boundary of Large Language Models\n  with Retrieval Augmentation","summary":"  Large language models (LLMs) have shown impressive prowess in solving a wide\nrange of tasks with world knowledge. However, it remains unclear how well LLMs\nare able to perceive their factual knowledge boundaries, particularly under\nretrieval augmentation settings. In this study, we present the first analysis\non the factual knowledge boundaries of LLMs and how retrieval augmentation\naffects LLMs on open-domain question answering (QA), with a bunch of important\nfindings. Specifically, we focus on three research questions and analyze them\nby examining QA, priori judgement and posteriori judgement capabilities of\nLLMs. We show evidence that LLMs possess unwavering confidence in their\nknowledge and cannot handle the conflict between internal and external\nknowledge well. Furthermore, retrieval augmentation proves to be an effective\napproach in enhancing LLMs' awareness of knowledge boundaries. We further\nconduct thorough experiments to examine how different factors affect LLMs and\npropose a simple method to dynamically utilize supporting documents with our\njudgement strategy. Additionally, we find that the relevance between the\nsupporting documents and the questions significantly impacts LLMs' QA and\njudgemental capabilities. The code to reproduce this work is available at\nhttps://github.com/RUCAIBox/LLM-Knowledge-Boundary.\n","authors":["Ruiyang Ren","Yuhao Wang","Yingqi Qu","Wayne Xin Zhao","Jing Liu","Hao Tian","Hua Wu","Ji-Rong Wen","Haifeng Wang"],"pdf_url":"https://arxiv.org/pdf/2307.11019v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12235v1","updated":"2024-11-19T05:19:53Z","published":"2024-11-19T05:19:53Z","title":"BoolQuestions: Does Dense Retrieval Understand Boolean Logic in\n  Language?","summary":"  Dense retrieval, which aims to encode the semantic information of arbitrary\ntext into dense vector representations or embeddings, has emerged as an\neffective and efficient paradigm for text retrieval, consequently becoming an\nessential component in various natural language processing systems. These\nsystems typically focus on optimizing the embedding space by attending to the\nrelevance of text pairs, while overlooking the Boolean logic inherent in\nlanguage, which may not be captured by current training objectives. In this\nwork, we first investigate whether current retrieval systems can comprehend the\nBoolean logic implied in language. To answer this question, we formulate the\ntask of Boolean Dense Retrieval and collect a benchmark dataset, BoolQuestions,\nwhich covers complex queries containing basic Boolean logic and corresponding\nannotated passages. Through extensive experimental results on the proposed task\nand benchmark dataset, we draw the conclusion that current dense retrieval\nsystems do not fully understand Boolean logic in language, and there is a long\nway to go to improve our dense retrieval systems. Furthermore, to promote\nfurther research on enhancing the understanding of Boolean logic for language\nmodels, we explore Boolean operation on decomposed query and propose a\ncontrastive continual training method that serves as a strong baseline for the\nresearch community.\n","authors":["Zongmeng Zhang","Jinhua Zhu","Wengang Zhou","Xiang Qi","Peng Zhang","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2411.12235v1.pdf","comment":"Findings of the Association for Computational Linguistics: EMNLP 2024"},{"id":"http://arxiv.org/abs/2411.10557v2","updated":"2024-11-19T05:16:28Z","published":"2024-11-15T20:09:59Z","title":"MLAN: Language-Based Instruction Tuning Improves Zero-Shot\n  Generalization of Multimodal Large Language Models","summary":"  We present a novel instruction tuning recipe to improve the zero-shot task\ngeneralization of multimodal large language models. In contrast to existing\ninstruction tuning mechanisms that heavily rely on visual instructions, our\napproach focuses on language-based instruction tuning, offering a distinct and\nmore training efficient path for multimodal instruction tuning. We evaluate the\nperformance of the proposed approach on 9 unseen datasets across both language\nand vision modalities. Our results show that our language-only instruction\ntuning is able to significantly improve the performance of two pretrained\nmultimodal models based on Llama 2 and Vicuna on those unseen datasets.\nInterestingly, the language instruction following ability also helps unlock the\nmodels to follow vision instructions without explicit training. Compared to the\nstate of the art multimodal instruction tuning approaches that are mainly based\non visual instructions, our language-based method not only achieves superior\nperformance but also significantly enhances training efficiency. For instance,\nthe language-only instruction tuning produces competitive average performance\nacross the evaluated datasets (with even better performance on language\ndatasets) with significant training efficiency improvements (on average 4x),\nthanks to the striking reduction in the need for vision data. With a small\nnumber of visual instructions, this emerging language instruction following\nability transfers well to the unseen vision datasets, outperforming the state\nof the art with greater training efficiency.\n","authors":["Jianhong Tu","Zhuohao Ni","Nicholas Crispino","Zihao Yu","Michael Bendersky","Beliz Gunel","Ruoxi Jia","Xin Liu","Lingjuan Lyu","Dawn Song","Chenguang Wang"],"pdf_url":"https://arxiv.org/pdf/2411.10557v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12775v1","updated":"2024-11-19T05:08:00Z","published":"2024-11-19T05:08:00Z","title":"Revisiting Fake News Detection: Towards Temporality-aware Evaluation by\n  Leveraging Engagement Earliness","summary":"  Social graph-based fake news detection aims to identify news articles\ncontaining false information by utilizing social contexts, e.g., user\ninformation, tweets and comments. However, conventional methods are evaluated\nunder less realistic scenarios, where the model has access to future knowledge\non article-related and context-related data during training. In this work, we\nnewly formalize a more realistic evaluation scheme that mimics real-world\nscenarios, where the data is temporality-aware and the detection model can only\nbe trained on data collected up to a certain point in time. We show that the\ndiscriminative capabilities of conventional methods decrease sharply under this\nnew setting, and further propose DAWN, a method more applicable to such\nscenarios. Our empirical findings indicate that later engagements (e.g.,\nconsuming or reposting news) contribute more to noisy edges that link real\nnews-fake news pairs in the social graph. Motivated by this, we utilize feature\nrepresentations of engagement earliness to guide an edge weight estimator to\nsuppress the weights of such noisy edges, thereby enhancing the detection\nperformance of DAWN. Through extensive experiments, we demonstrate that DAWN\noutperforms existing fake news detection methods under real-world environments.\nThe source code is available at https://github.com/LeeJunmo/DAWN.\n","authors":["Junghoon Kim","Junmo Lee","Yeonjun In","Kanghoon Yoon","Chanyoung Park"],"pdf_url":"https://arxiv.org/pdf/2411.12775v1.pdf","comment":"WSDM 2025"},{"id":"http://arxiv.org/abs/2411.09003v2","updated":"2024-11-19T04:53:47Z","published":"2024-11-13T20:12:55Z","title":"Refusal in LLMs is an Affine Function","summary":"  We propose affine concept editing (ACE) as an approach for steering language\nmodels' behavior by intervening directly in activations. We begin with an\naffine decomposition of model activation vectors and show that prior methods\nfor steering model behavior correspond to subsets of terms of this\ndecomposition. We then provide a derivation of ACE and use it to control\nrefusal behavior on ten different models, including Llama 3 70B. ACE combines\naffine subspace projection and activation addition to reliably control the\nmodel's refusal responses across prompt types. We evaluate the results using\nLLM-based scoring on a collection of harmful and harmless prompts. Our\nexperiments demonstrate that ACE consistently achieves more precise control\nover model behavior than existing methods and generalizes to models where\ndirectional ablation via affine subspace projection alone produces incoherent\noutputs. Code for reproducing our results is available at\nhttps://github.com/EleutherAI/steering-llama3 .\n","authors":["Thomas Marshall","Adam Scherlis","Nora Belrose"],"pdf_url":"https://arxiv.org/pdf/2411.09003v2.pdf","comment":"added plots for results from additional models"},{"id":"http://arxiv.org/abs/2410.14148v3","updated":"2024-11-19T03:08:34Z","published":"2024-10-18T03:34:32Z","title":"Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in\n  Vision-Language Alignment","summary":"  The recent advancements in large language models (LLMs) and pre-trained\nvision models have accelerated the development of vision-language large models\n(VLLMs), enhancing the interaction between visual and linguistic modalities.\nDespite their notable success across various domains, VLLMs face challenges in\nmodality alignment, which can lead to issues like hallucinations and unsafe\ncontent generation. Current alignment techniques often rely on coarse feedback\nand external datasets, limiting scalability and performance. In this paper, we\npropose FiSAO (Fine-Grained Self-Alignment Optimization), a novel\nself-alignment method that utilizes the model's own visual encoder as a\nfine-grained verifier to improve vision-language alignment without the need for\nadditional data. By leveraging token-level feedback from the vision encoder,\nFiSAO significantly improves vision-language alignment, even surpassing\ntraditional preference tuning methods that require additional data. Through\nboth theoretical analysis and experimental validation, we demonstrate that\nFiSAO effectively addresses the misalignment problem in VLLMs, marking the\nfirst instance of token-level rewards being applied to such models.\n","authors":["Chenhang Cui","An Zhang","Yiyang Zhou","Zhaorun Chen","Gelei Deng","Huaxiu Yao","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.14148v3.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2411.11496v2","updated":"2024-11-19T03:01:43Z","published":"2024-11-18T11:58:07Z","title":"Safe + Safe = Unsafe? Exploring How Safe Images Can Be Exploited to\n  Jailbreak Large Vision-Language Models","summary":"  Recent advances in Large Vision-Language Models (LVLMs) have showcased strong\nreasoning abilities across multiple modalities, achieving significant\nbreakthroughs in various real-world applications. Despite this great success,\nthe safety guardrail of LVLMs may not cover the unforeseen domains introduced\nby the visual modality. Existing studies primarily focus on eliciting LVLMs to\ngenerate harmful responses via carefully crafted image-based jailbreaks\ndesigned to bypass alignment defenses. In this study, we reveal that a safe\nimage can be exploited to achieve the same jailbreak consequence when combined\nwith additional safe images and prompts. This stems from two fundamental\nproperties of LVLMs: universal reasoning capabilities and safety snowball\neffect. Building on these insights, we propose Safety Snowball Agent (SSA), a\nnovel agent-based framework leveraging agents' autonomous and tool-using\nabilities to jailbreak LVLMs. SSA operates through two principal stages: (1)\ninitial response generation, where tools generate or retrieve jailbreak images\nbased on potential harmful intents, and (2) harmful snowballing, where refined\nsubsequent prompts induce progressively harmful outputs. Our experiments\ndemonstrate that \\ours can use nearly any image to induce LVLMs to produce\nunsafe content, achieving high success jailbreaking rates against the latest\nLVLMs. Unlike prior works that exploit alignment flaws, \\ours leverages the\ninherent properties of LVLMs, presenting a profound challenge for enforcing\nsafety in generative multimodal systems. Our code is avaliable at\n\\url{https://github.com/gzcch/Safety_Snowball_Agent}.\n","authors":["Chenhang Cui","Gelei Deng","An Zhang","Jingnan Zheng","Yicong Li","Lianli Gao","Tianwei Zhang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2411.11496v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16843v2","updated":"2024-11-19T02:52:45Z","published":"2024-02-26T18:59:18Z","title":"Multi-LoRA Composition for Image Generation","summary":"  Low-Rank Adaptation (LoRA) is extensively utilized in text-to-image models\nfor the accurate rendition of specific elements like distinct characters or\nunique styles in generated images. Nonetheless, existing methods face\nchallenges in effectively composing multiple LoRAs, especially as the number of\nLoRAs to be integrated grows, thus hindering the creation of complex imagery.\nIn this paper, we study multi-LoRA composition through a decoding-centric\nperspective. We present two training-free methods: LoRA Switch, which\nalternates between different LoRAs at each denoising step, and LoRA Composite,\nwhich simultaneously incorporates all LoRAs to guide more cohesive image\nsynthesis. To evaluate the proposed approaches, we establish ComposLoRA, a new\ncomprehensive testbed as part of this research. It features a diverse range of\nLoRA categories with 480 composition sets. Utilizing an evaluation framework\nbased on GPT-4V, our findings demonstrate a clear improvement in performance\nwith our methods over the prevalent baseline, particularly evident when\nincreasing the number of LoRAs in a composition. The code, benchmarks, LoRA\nweights, and all evaluation details are available on our project website:\nhttps://maszhongming.github.io/Multi-LoRA-Composition.\n","authors":["Ming Zhong","Yelong Shen","Shuohang Wang","Yadong Lu","Yizhu Jiao","Siru Ouyang","Donghan Yu","Jiawei Han","Weizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2402.16843v2.pdf","comment":"Transactions on Machine Learning Research (TMLR), 2024"},{"id":"http://arxiv.org/abs/2411.12174v1","updated":"2024-11-19T02:39:28Z","published":"2024-11-19T02:39:28Z","title":"Just KIDDIN: Knowledge Infusion and Distillation for Detection of\n  INdecent Memes","summary":"  Toxicity identification in online multimodal environments remains a\nchallenging task due to the complexity of contextual connections across\nmodalities (e.g., textual and visual). In this paper, we propose a novel\nframework that integrates Knowledge Distillation (KD) from Large Visual\nLanguage Models (LVLMs) and knowledge infusion to enhance the performance of\ntoxicity detection in hateful memes. Our approach extracts sub-knowledge graphs\nfrom ConceptNet, a large-scale commonsense Knowledge Graph (KG) to be infused\nwithin a compact VLM framework. The relational context between toxic phrases in\ncaptions and memes, as well as visual concepts in memes enhance the model's\nreasoning capabilities. Experimental results from our study on two hate speech\nbenchmark datasets demonstrate superior performance over the state-of-the-art\nbaselines across AU-ROC, F1, and Recall with improvements of 1.1%, 7%, and 35%,\nrespectively. Given the contextual complexity of the toxicity detection task,\nour approach showcases the significance of learning from both explicit (i.e.\nKG) as well as implicit (i.e. LVLMs) contextual cues incorporated through a\nhybrid neurosymbolic approach. This is crucial for real-world applications\nwhere accurate and scalable recognition of toxic content is critical for\ncreating safer online environments.\n","authors":["Rahul Garg","Trilok Padhi","Hemang Jain","Ugur Kursuncu","Ugur Kursuncu","Ponnurangam Kumaraguru"],"pdf_url":"https://arxiv.org/pdf/2411.12174v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12157v1","updated":"2024-11-19T01:41:56Z","published":"2024-11-19T01:41:56Z","title":"A Combined Encoder and Transformer Approach for Coherent and\n  High-Quality Text Generation","summary":"  This research introduces a novel text generation model that combines BERT's\nsemantic interpretation strengths with GPT-4's generative capabilities,\nestablishing a high standard in generating coherent, contextually accurate\nlanguage. Through the combined architecture, the model enhances semantic depth\nand maintains smooth, human-like text flow, overcoming limitations seen in\nprior models. Experimental benchmarks reveal that BERT-GPT-4 surpasses\ntraditional models, including GPT-3, T5, BART, Transformer-XL, and CTRL, in key\nmetrics like Perplexity and BLEU, showcasing its superior natural language\ngeneration performance. By fully utilizing contextual information, this hybrid\nmodel generates text that is not only logically coherent but also aligns\nclosely with human language patterns, providing an advanced solution for text\ngeneration tasks. This research highlights the potential of integrating\nsemantic understanding with advanced generative models, contributing new\ninsights for NLP, and setting a foundation for broader applications of\nlarge-scale generative architectures in areas such as automated writing,\nquestion-answer systems, and adaptive conversational agents.\n","authors":["Jiajing Chen","Shuo Wang","Zhen Qi","Zhenhong Zhang","Chihang Wang","Hongye Zheng"],"pdf_url":"https://arxiv.org/pdf/2411.12157v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12156v1","updated":"2024-11-19T01:26:20Z","published":"2024-11-19T01:26:20Z","title":"HNCSE: Advancing Sentence Embeddings via Hybrid Contrastive Learning\n  with Hard Negatives","summary":"  Unsupervised sentence representation learning remains a critical challenge in\nmodern natural language processing (NLP) research. Recently, contrastive\nlearning techniques have achieved significant success in addressing this issue\nby effectively capturing textual semantics. Many such approaches prioritize the\noptimization using negative samples. In fields such as computer vision, hard\nnegative samples (samples that are close to the decision boundary and thus more\ndifficult to distinguish) have been shown to enhance representation learning.\nHowever, adapting hard negatives to contrastive sentence learning is complex\ndue to the intricate syntactic and semantic details of text. To address this\nproblem, we propose HNCSE, a novel contrastive learning framework that extends\nthe leading SimCSE approach. The hallmark of HNCSE is its innovative use of\nhard negative samples to enhance the learning of both positive and negative\nsamples, thereby achieving a deeper semantic understanding. Empirical tests on\nsemantic textual similarity and transfer task datasets validate the superiority\nof HNCSE.\n","authors":["Wenxiao Liu","Zihong Yang","Chaozhuo Li","Zijin Hong","Jianfeng Ma","Zhiquan Liu","Litian Zhang","Feiran Huang"],"pdf_url":"https://arxiv.org/pdf/2411.12156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12147v1","updated":"2024-11-19T00:50:06Z","published":"2024-11-19T00:50:06Z","title":"CoMeDi Shared Task: Models as Annotators in Lexical Semantics\n  Disagreements","summary":"  We present the results of our system for the CoMeDi Shared Task, which\npredicts majority votes (Subtask 1) and annotator disagreements (Subtask 2).\nOur approach combines model ensemble strategies with MLP-based and\nthreshold-based methods trained on pretrained language models. Treating\nindividual models as virtual annotators, we simulate the annotation process by\ndesigning aggregation measures that incorporate continuous similarity scores\nand discrete classification labels to capture both majority and disagreement.\nAdditionally, we employ anisotropy removal techniques to enhance performance.\nExperimental results demonstrate the effectiveness of our methods, particularly\nfor Subtask 2. Notably, we find that continuous similarity scores, even within\nthe same model, align better with human disagreement patterns compared to\naggregated discrete labels.\n","authors":["Zhu Liu","Zhen Hu","Ying Liu"],"pdf_url":"https://arxiv.org/pdf/2411.12147v1.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.12142v1","updated":"2024-11-19T00:44:56Z","published":"2024-11-19T00:44:56Z","title":"A Computational Method for Measuring \"Open Codes\" in Qualitative\n  Analysis","summary":"  Qualitative analysis is critical to understanding human datasets in many\nsocial science disciplines. Open coding is an inductive qualitative process\nthat identifies and interprets \"open codes\" from datasets. Yet, meeting\nmethodological expectations (such as \"as exhaustive as possible\") can be\nchallenging. While many machine learning (ML)/generative AI (GAI) studies have\nattempted to support open coding, few have systematically measured or evaluated\nGAI outcomes, increasing potential bias risks. Building on Grounded Theory and\nThematic Analysis theories, we present a computational method to measure and\nidentify potential biases from \"open codes\" systematically. Instead of\noperationalizing human expert results as the \"ground truth,\" our method is\nbuilt upon a team-based approach between human and machine coders. We\nexperiment with two HCI datasets to establish this method's reliability by 1)\ncomparing it with human analysis, and 2) analyzing its output stability. We\npresent evidence-based suggestions and example workflows for ML/GAI to support\nopen coding.\n","authors":["John Chen","Alexandros Lotsos","Lexie Zhao","Jessica Hullman","Bruce Sherin","Uri Wilensky","Michael Horn"],"pdf_url":"https://arxiv.org/pdf/2411.12142v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2411.12919v1","updated":"2024-11-19T23:17:09Z","published":"2024-11-19T23:17:09Z","title":"Enhancing Deep Learning-Driven Multi-Coil MRI Reconstruction via\n  Self-Supervised Denoising","summary":"  We examine the effect of incorporating self-supervised denoising as a\npre-processing step for training deep learning (DL) based reconstruction\nmethods on data corrupted by Gaussian noise. K-space data employed for training\nare typically multi-coil and inherently noisy. Although DL-based reconstruction\nmethods trained on fully sampled data can enable high reconstruction quality,\nobtaining large, noise-free datasets is impractical. We leverage Generalized\nStein's Unbiased Risk Estimate (GSURE) for denoising. We evaluate two DL-based\nreconstruction methods: Diffusion Probabilistic Models (DPMs) and Model-Based\nDeep Learning (MoDL). We evaluate the impact of denoising on the performance of\nthese DL-based methods in solving accelerated multi-coil magnetic resonance\nimaging (MRI) reconstruction. The experiments were carried out on T2-weighted\nbrain and fat-suppressed proton-density knee scans. We observed that\nself-supervised denoising enhances the quality and efficiency of MRI\nreconstructions across various scenarios. Specifically, employing denoised\nimages rather than noisy counterparts when training DL networks results in\nlower normalized root mean squared error (NRMSE), higher structural similarity\nindex measure (SSIM) and peak signal-to-noise ratio (PSNR) across different SNR\nlevels, including 32dB, 22dB, and 12dB for T2-weighted brain data, and 24dB,\n14dB, and 4dB for fat-suppressed knee data. Overall, we showed that denoising\nis an essential pre-processing technique capable of improving the efficacy of\nDL-based MRI reconstruction methods under diverse conditions. By refining the\nquality of input data, denoising can enable the training of more effective DL\nnetworks, potentially bypassing the need for noise-free reference MRI scans.\n","authors":["Asad Aali","Marius Arvinte","Sidharth Kumar","Yamin I. Arefeen","Jonathan I. Tamir"],"pdf_url":"https://arxiv.org/pdf/2411.12919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10951v2","updated":"2024-11-19T23:09:25Z","published":"2024-11-17T03:34:27Z","title":"TSFormer: A Robust Framework for Efficient UHD Image Restoration","summary":"  Ultra-high-definition (UHD) image restoration is vital for applications\ndemanding exceptional visual fidelity, yet existing methods often face a\ntrade-off between restoration quality and efficiency, limiting their practical\ndeployment. In this paper, we propose TSFormer, an all-in-one framework that\nintegrates \\textbf{T}rusted learning with \\textbf{S}parsification to boost both\ngeneralization capability and computational efficiency in UHD image\nrestoration. The key is that only a small amount of token movement is allowed\nwithin the model. To efficiently filter tokens, we use Min-$p$ with random\nmatrix theory to quantify the uncertainty of tokens, thereby improving the\nrobustness of the model. Our model can run a 4K image in real time (40fps) with\n3.38 M parameters. Extensive experiments demonstrate that TSFormer achieves\nstate-of-the-art restoration quality while enhancing generalization and\nreducing computational demands. In addition, our token filtering method can be\napplied to other image restoration models to effectively accelerate inference\nand maintain performance.\n","authors":["Xin Su","Chen Wu","Zhuoran Zheng"],"pdf_url":"https://arxiv.org/pdf/2411.10951v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12915v1","updated":"2024-11-19T22:59:14Z","published":"2024-11-19T22:59:14Z","title":"VILA-M3: Enhancing Vision-Language Models with Medical Expert Knowledge","summary":"  Generalist vision language models (VLMs) have made significant strides in\ncomputer vision, but they fall short in specialized fields like healthcare,\nwhere expert knowledge is essential. In traditional computer vision tasks,\ncreative or approximate answers may be acceptable, but in healthcare, precision\nis paramount.Current large multimodal models like Gemini and GPT-4o are\ninsufficient for medical tasks due to their reliance on memorized internet\nknowledge rather than the nuanced expertise required in healthcare. VLMs are\nusually trained in three stages: vision pre-training, vision-language\npre-training, and instruction fine-tuning (IFT). IFT has been typically applied\nusing a mixture of generic and healthcare data. In contrast, we propose that\nfor medical VLMs, a fourth stage of specialized IFT is necessary, which focuses\non medical data and includes information from domain expert models. Domain\nexpert models developed for medical use are crucial because they are\nspecifically trained for certain clinical tasks, e.g. to detect tumors and\nclassify abnormalities through segmentation and classification, which learn\nfine-grained features of medical data$-$features that are often too intricate\nfor a VLM to capture effectively especially in radiology. This paper introduces\na new framework, VILA-M3, for medical VLMs that utilizes domain knowledge via\nexpert models. Through our experiments, we show an improved state-of-the-art\n(SOTA) performance with an average improvement of ~9% over the prior SOTA model\nMed-Gemini and ~6% over models trained on the specific tasks. Our approach\nemphasizes the importance of domain expertise in creating precise, reliable\nVLMs for medical applications.\n","authors":["Vishwesh Nath","Wenqi Li","Dong Yang","Andriy Myronenko","Mingxin Zheng","Yao Lu","Zhijian Liu","Hongxu Yin","Yee Man Law","Yucheng Tang","Pengfei Guo","Can Zhao","Ziyue Xu","Yufan He","Greg Heinrich","Stephen Aylward","Marc Edgar","Michael Zephyr","Pavlo Molchanov","Baris Turkbey","Holger Roth","Daguang Xu"],"pdf_url":"https://arxiv.org/pdf/2411.12915v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12901v1","updated":"2024-11-19T22:27:53Z","published":"2024-11-19T22:27:53Z","title":"Signformer is all you need: Towards Edge AI for Sign Language","summary":"  Sign language translation, especially in gloss-free paradigm, is confronting\na dilemma of impracticality and unsustainability due to growing\nresource-intensive methodologies. Contemporary state-of-the-arts (SOTAs) have\nsignificantly hinged on pretrained sophiscated backbones such as Large Language\nModels (LLMs), embedding sources, or extensive datasets, inducing considerable\nparametric and computational inefficiency for sustainable use in real-world\nscenario. Despite their success, following this research direction undermines\nthe overarching mission of this domain to create substantial value to bridge\nhard-hearing and common populations. Committing to the prevailing trend of LLM\nand Natural Language Processing (NLP) studies, we pursue a profound essential\nchange in architecture to achieve ground-up improvements without external aid\nfrom pretrained models, prior knowledge transfer, or any NLP strategies\nconsidered not-from-scratch.\n  Introducing Signformer, a from-scratch Feather-Giant transforming the area\ntowards Edge AI that redefines extremities of performance and efficiency with\nLLM-competence and edgy-deployable compactness. In this paper, we present\nnature analysis of sign languages to inform our algorithmic design and deliver\na scalable transformer pipeline with convolution and attention novelty. We\nachieve new 2nd place on leaderboard with a parametric reduction of 467-1807x\nagainst the finests as of 2024 and outcompete almost every other methods in a\nlighter configuration of 0.57 million parameters.\n","authors":["Eta Yang"],"pdf_url":"https://arxiv.org/pdf/2411.12901v1.pdf","comment":"Official Code at: https://github.com/EtaEnding/Signformer/tree/main"},{"id":"http://arxiv.org/abs/2411.12897v1","updated":"2024-11-19T22:25:26Z","published":"2024-11-19T22:25:26Z","title":"Tree Species Classification using Machine Learning and 3D Tomographic\n  SAR -- a case study in Northern Europe","summary":"  Tree species classification plays an important role in nature conservation,\nforest inventories, forest management, and the protection of endangered\nspecies. Over the past four decades, remote sensing technologies have been\nextensively utilized for tree species classification, with Synthetic Aperture\nRadar (SAR) emerging as a key technique. In this study, we employed TomoSense,\na 3D tomographic dataset, which utilizes a stack of single-look complex (SLC)\nimages, a byproduct of SAR, captured at different incidence angles to generate\na three-dimensional representation of the terrain. Our research focuses on\nevaluating multiple tabular machine-learning models using the height\ninformation derived from the tomographic image intensities to classify eight\ndistinct tree species. The SLC data and tomographic imagery were analyzed\nacross different polarimetric configurations and geosplit configurations. We\ninvestigated the impact of these variations on classification accuracy,\ncomparing the performance of various tabular machine-learning models and\noptimizing them using Bayesian optimization. Additionally, we incorporated a\nproxy for actual tree height using point cloud data from Light Detection and\nRanging (LiDAR) to provide height statistics associated with the model's\npredictions. This comparison offers insights into the reliability of\ntomographic data in predicting tree species classification based on height.\n","authors":["Colverd Grace","Schade Laura","Takami Jumpei","Bot Karol","Gallego Joseph"],"pdf_url":"https://arxiv.org/pdf/2411.12897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05430v2","updated":"2024-11-19T22:11:40Z","published":"2023-12-09T01:45:16Z","title":"FT2TF: First-Person Statement Text-To-Talking Face Generation","summary":"  Talking face generation has gained immense popularity in the computer vision\ncommunity, with various applications including AR, VR, teleconferencing,\ndigital assistants, and avatars. Traditional methods are mainly audio-driven,\nwhich have to deal with the inevitable resource-intensive nature of audio\nstorage and processing. To address such a challenge, we propose FT2TF -\nFirst-Person Statement Text-To-Talking Face Generation, a novel one-stage\nend-to-end pipeline for talking face generation driven by first-person\nstatement text. Different from previous work, our model only leverages visual\nand textual information without any other sources (e.g., audio/landmark/pose)\nduring inference. Extensive experiments are conducted on LRS2 and LRS3\ndatasets, and results on multi-dimensional evaluation metrics are reported.\nBoth quantitative and qualitative results showcase that FT2TF outperforms\nexisting relevant methods and reaches the state-of-the-art. This achievement\nhighlights our model's capability to bridge first-person statements and dynamic\nface generation, providing insightful guidance for future work.\n","authors":["Xingjian Diao","Ming Cheng","Wayner Barrios","SouYoung Jin"],"pdf_url":"https://arxiv.org/pdf/2312.05430v2.pdf","comment":"Accepted at WACV 2025"},{"id":"http://arxiv.org/abs/2403.09327v2","updated":"2024-11-19T21:51:34Z","published":"2024-03-14T12:17:07Z","title":"Perspective-Equivariance for Unsupervised Imaging with Camera Geometry","summary":"  Ill-posed image reconstruction problems appear in many scenarios such as\nremote sensing, where obtaining high quality images is crucial for\nenvironmental monitoring, disaster management and urban planning. Deep learning\nhas seen great success in overcoming the limitations of traditional methods.\nHowever, these inverse problems rarely come with ground truth data,\nhighlighting the importance of unsupervised learning from partial and noisy\nmeasurements alone. We propose perspective-equivariant imaging (EI), a\nframework that leverages classical projective camera geometry in optical\nimaging systems, such as satellites or handheld cameras, to recover information\nlost in ill-posed camera imaging problems. We show that our much richer\nnon-linear class of group transforms, derived from camera geometry, generalises\nprevious EI work and is an excellent prior for satellite and urban image data.\nPerspective-EI achieves state-of-the-art results in multispectral\npansharpening, outperforming other unsupervised methods in the literature. Code\nat https://github.com/Andrewwango/perspective-equivariant-imaging.\n","authors":["Andrew Wang","Mike Davies"],"pdf_url":"https://arxiv.org/pdf/2403.09327v2.pdf","comment":"ECCV camera-ready"},{"id":"http://arxiv.org/abs/2411.12874v1","updated":"2024-11-19T21:42:57Z","published":"2024-11-19T21:42:57Z","title":"Residual Vision Transformer (ResViT) Based Self-Supervised Learning\n  Model for Brain Tumor Classification","summary":"  Deep learning has proven very promising for interpreting MRI in brain tumor\ndiagnosis. However, deep learning models suffer from a scarcity of brain MRI\ndatasets for effective training. Self-supervised learning (SSL) models provide\ndata-efficient and remarkable solutions to limited dataset problems. Therefore,\nthis paper introduces a generative SSL model for brain tumor classification in\ntwo stages. The first stage is designed to pre-train a Residual Vision\nTransformer (ResViT) model for MRI synthesis as a pretext task. The second\nstage includes fine-tuning a ResViT-based classifier model as a downstream\ntask. Accordingly, we aim to leverage local features via CNN and global\nfeatures via ViT, employing a hybrid CNN-transformer architecture for ResViT in\npretext and downstream tasks. Moreover, synthetic MRI images are utilized to\nbalance the training set. The proposed model performs on public BraTs 2023,\nFigshare, and Kaggle datasets. Furthermore, we compare the proposed model with\nvarious deep learning models, including A-UNet, ResNet-9, pix2pix, pGAN for MRI\nsynthesis, and ConvNeXtTiny, ResNet101, DenseNet12, Residual CNN, ViT for\nclassification. According to the results, the proposed model pretraining on the\nMRI dataset is superior compared to the pretraining on the ImageNet dataset.\nOverall, the proposed model attains the highest accuracy, achieving 90.56% on\nthe BraTs dataset with T1 sequence, 98.53% on the Figshare, and 98.47% on the\nKaggle brain tumor datasets. As a result, the proposed model demonstrates a\nrobust, effective, and successful approach to handling insufficient dataset\nchallenges in MRI analysis by incorporating SSL, fine-tuning, data\naugmentation, and combining CNN and ViT.\n","authors":["Meryem Altin Karagoz","O. Ufuk Nalbantoglu","Geoffrey C. Fox"],"pdf_url":"https://arxiv.org/pdf/2411.12874v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12872v1","updated":"2024-11-19T21:34:50Z","published":"2024-11-19T21:34:50Z","title":"From Text to Pose to Image: Improving Diffusion Model Control and\n  Quality","summary":"  In the last two years, text-to-image diffusion models have become extremely\npopular. As their quality and usage increase, a major concern has been the need\nfor better output control. In addition to prompt engineering, one effective\nmethod to improve the controllability of diffusion models has been to condition\nthem on additional modalities such as image style, depth map, or keypoints.\nThis forms the basis of ControlNets or Adapters. When attempting to apply these\nmethods to control human poses in outputs of text-to-image diffusion models,\ntwo main challenges have arisen. The first challenge is generating poses\nfollowing a wide range of semantic text descriptions, for which previous\nmethods involved searching for a pose within a dataset of (caption, pose)\npairs. The second challenge is conditioning image generation on a specified\npose while keeping both high aesthetic and high pose fidelity. In this article,\nwe fix these two main issues by introducing a text-to-pose (T2P) generative\nmodel alongside a new sampling algorithm, and a new pose adapter that\nincorporates more pose keypoints for higher pose fidelity. Together, these two\nnew state-of-the-art models enable, for the first time, a generative\ntext-to-pose-to-image framework for higher pose control in diffusion models. We\nrelease all models and the code used for the experiments at\nhttps://github.com/clement-bonnet/text-to-pose.\n","authors":["Clément Bonnett","Ariel N. Lee","Franck Wertel","Antoine Tamano","Tanguy Cizain","Pablo Ducru"],"pdf_url":"https://arxiv.org/pdf/2411.12872v1.pdf","comment":"Published at the NeurIPS 2024 Workshop on Compositional Learning:\n  Perspectives, Methods, and Paths Forward"},{"id":"http://arxiv.org/abs/2410.04680v3","updated":"2024-11-19T21:23:47Z","published":"2024-10-07T01:24:39Z","title":"Next Best Sense: Guiding Vision and Touch with FisherRF for 3D Gaussian\n  Splatting","summary":"  We propose a framework for active next best view and touch selection for\nrobotic manipulators using 3D Gaussian Splatting (3DGS). 3DGS is emerging as a\nuseful explicit 3D scene representation for robotics, as it has the ability to\nrepresent scenes in a both photorealistic and geometrically accurate manner.\nHowever, in real-world, online robotic scenes where the number of views is\nlimited given efficiency requirements, random view selection for 3DGS becomes\nimpractical as views are often overlapping and redundant. We address this issue\nby proposing an end-to-end online training and active view selection pipeline,\nwhich enhances the performance of 3DGS in few-view robotics settings. We first\nelevate the performance of few-shot 3DGS with a novel semantic depth alignment\nmethod using Segment Anything Model 2 (SAM2) that we supplement with Pearson\ndepth and surface normal loss to improve color and depth reconstruction of\nreal-world scenes. We then extend FisherRF, a next-best-view selection method\nfor 3DGS, to select views and touch poses based on depth uncertainty. We\nperform online view selection on a real robot system during live 3DGS training.\nWe motivate our improvements to few-shot GS scenes, and extend depth-based\nFisherRF to them, where we demonstrate both qualitative and quantitative\nimprovements on challenging robot scenes. For more information, please see our\nproject page at https://arm.stanford.edu/next-best-sense.\n","authors":["Matthew Strong","Boshu Lei","Aiden Swann","Wen Jiang","Kostas Daniilidis","Monroe Kennedy III"],"pdf_url":"https://arxiv.org/pdf/2410.04680v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10082v3","updated":"2024-11-19T21:19:43Z","published":"2024-06-14T14:36:54Z","title":"Whisper-Flamingo: Integrating Visual Features into Whisper for\n  Audio-Visual Speech Recognition and Translation","summary":"  Audio-Visual Speech Recognition (AVSR) uses lip-based video to improve\nperformance in noise. Since videos are harder to obtain than audio, the video\ntraining data of AVSR models is usually limited to a few thousand hours. In\ncontrast, speech models such as Whisper are trained with hundreds of thousands\nof hours of data, and thus learn a better speech-to-text decoder. The huge\ntraining data difference motivates us to adapt Whisper to handle video inputs.\nInspired by Flamingo which injects visual features into language models, we\npropose Whisper-Flamingo which integrates visual features into the Whisper\nspeech recognition and translation model with gated cross attention. Our models\nachieve state-of-the-art ASR WER (0.68%) and AVSR WER (0.76%) on LRS3, and\nstate-of-the-art ASR WER (1.3%) and AVSR WER (1.4%) on LRS2. Audio-visual\nWhisper-Flamingo outperforms audio-only Whisper on English speech recognition\nand En-X translation for 6 languages in noisy conditions. Moreover,\nWhisper-Flamingo is versatile and conducts all of these tasks using one set of\nparameters, while prior methods are trained separately on each language.\n","authors":["Andrew Rouditchenko","Yuan Gong","Samuel Thomas","Leonid Karlinsky","Hilde Kuehne","Rogerio Feris","James Glass"],"pdf_url":"https://arxiv.org/pdf/2406.10082v3.pdf","comment":"Interspeech 2024. V3: Added results on LRS2. Code at\n  https://github.com/roudimit/whisper-flamingo"},{"id":"http://arxiv.org/abs/2411.12846v1","updated":"2024-11-19T20:31:38Z","published":"2024-11-19T20:31:38Z","title":"Towards Fairness in AI for Melanoma Detection: Systemic Review and\n  Recommendations","summary":"  Early and accurate melanoma detection is crucial for improving patient\noutcomes. Recent advancements in artificial intelligence AI have shown promise\nin this area, but the technologys effectiveness across diverse skin tones\nremains a critical challenge. This study conducts a systematic review and\npreliminary analysis of AI based melanoma detection research published between\n2013 and 2024, focusing on deep learning methodologies, datasets, and skin tone\nrepresentation. Our findings indicate that while AI can enhance melanoma\ndetection, there is a significant bias towards lighter skin tones. To address\nthis, we propose including skin hue in addition to skin tone as represented by\nthe LOreal Color Chart Map for a more comprehensive skin tone assessment\ntechnique. This research highlights the need for diverse datasets and robust\nevaluation metrics to develop AI models that are equitable and effective for\nall patients. By adopting best practices outlined in a PRISMA Equity framework\ntailored for healthcare and melanoma detection, we can work towards reducing\ndisparities in melanoma outcomes.\n","authors":["Laura N Montoya","Jennafer Shae Roberts","Belen Sanchez Hidalgo"],"pdf_url":"https://arxiv.org/pdf/2411.12846v1.pdf","comment":"22 pages, 4 figures, 7 tables,accepted for publication in Future of\n  Information and Communication Conference (FICC) 2025, whose proceedings will\n  be published in 'Lecture Notes in Networks and Systems' by Springer Nature"},{"id":"http://arxiv.org/abs/2411.12841v1","updated":"2024-11-19T20:10:28Z","published":"2024-11-19T20:10:28Z","title":"Data-to-Model Distillation: Data-Efficient Learning Framework","summary":"  Dataset distillation aims to distill the knowledge of a large-scale real\ndataset into small yet informative synthetic data such that a model trained on\nit performs as well as a model trained on the full dataset. Despite recent\nprogress, existing dataset distillation methods often struggle with\ncomputational efficiency, scalability to complex high-resolution datasets, and\ngeneralizability to deep architectures. These approaches typically require\nretraining when the distillation ratio changes, as knowledge is embedded in raw\npixels. In this paper, we propose a novel framework called Data-to-Model\nDistillation (D2M) to distill the real dataset's knowledge into the learnable\nparameters of a pre-trained generative model by aligning rich representations\nextracted from real and generated images. The learned generative model can then\nproduce informative training images for different distillation ratios and deep\narchitectures. Extensive experiments on 15 datasets of varying resolutions show\nD2M's superior performance, re-distillation efficiency, and cross-architecture\ngeneralizability. Our method effectively scales up to high-resolution 128x128\nImageNet-1K. Furthermore, we verify D2M's practical benefits for downstream\napplications in neural architecture search.\n","authors":["Ahmad Sajedi","Samir Khaki","Lucy Z. Liu","Ehsan Amjadian","Yuri A. Lawryshyn","Konstantinos N. Plataniotis"],"pdf_url":"https://arxiv.org/pdf/2411.12841v1.pdf","comment":"Accepted in the 18th European Conference on Computer Vision (ECCV\n  2024), Milan, Italy, September 29 October 4, 2024"},{"id":"http://arxiv.org/abs/2410.19774v2","updated":"2024-11-19T19:56:20Z","published":"2024-10-14T01:35:41Z","title":"Copula-Linked Parallel ICA: A Method for Coupling Structural and\n  Functional MRI brain Networks","summary":"  Different brain imaging modalities offer unique insights into brain function\nand structure. Combining them enhances our understanding of neural mechanisms.\nPrior multimodal studies fusing functional MRI (fMRI) and structural MRI (sMRI)\nhave shown the benefits of this approach. Since sMRI lacks temporal data,\nexisting fusion methods often compress fMRI temporal information into summary\nmeasures, sacrificing rich temporal dynamics. Motivated by the observation that\ncovarying networks are identified in both sMRI and resting-state fMRI, we\ndeveloped a novel fusion method, by combining deep learning frameworks, copulas\nand independent component analysis (ICA), named copula linked parallel ICA\n(CLiP-ICA). This method estimates independent sources for each modality and\nlinks the spatial sources of fMRI and sMRI using a copula-based model for more\nflexible integration of temporal and spatial data. We tested CLiP-ICA using\ndata from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Our results\nshowed that CLiP-ICA effectively captures both strongly and weakly linked sMRI\nand fMRI networks, including the cerebellum, sensorimotor, visual, cognitive\ncontrol, and default mode networks. It revealed more meaningful components and\nfewer artifacts, addressing the long-standing issue of optimal model order in\nICA. CLiP-ICA also detected complex functional connectivity patterns across\nstages of cognitive decline, with cognitively normal subjects generally showing\nhigher connectivity in sensorimotor and visual networks compared to patients\nwith Alzheimer, along with patterns suggesting potential compensatory\nmechanisms.\n","authors":["Oktay Agcaoglu","Rogers F. Silva","Deniz Alacam","Sergey Plis","Tulay Adali","Vince Calhoun"],"pdf_url":"https://arxiv.org/pdf/2410.19774v2.pdf","comment":"25 pages, 10 figures, journal article"},{"id":"http://arxiv.org/abs/2411.12833v1","updated":"2024-11-19T19:39:42Z","published":"2024-11-19T19:39:42Z","title":"Efficient Medicinal Image Transmission and Resolution Enhancement via\n  GAN","summary":"  While X-ray imaging is indispensable in medical diagnostics, it inherently\ncarries with it those noises and limitations on resolution that mask the\ndetails necessary for diagnosis. B/W X-ray images require a careful balance\nbetween noise suppression and high-detail preservation to ensure clarity in\nsoft-tissue structures and bone edges. While traditional methods, such as CNNs\nand early super-resolution models like ESRGAN, have enhanced image resolution,\nthey often perform poorly regarding high-frequency detail preservation and\nnoise control for B/W imaging. We are going to present one efficient approach\nthat improves the quality of an image with the optimization of network\ntransmission in the following paper. The pre-processing of X-ray images into\nlow-resolution files by Real-ESRGAN, a version of ESRGAN elucidated and\nimproved, helps reduce the server load and transmission bandwidth.\nLower-resolution images are upscaled at the receiving end using Real-ESRGAN,\nfine-tuned for real-world image degradation. The model integrates\nResidual-in-Residual Dense Blocks with perceptual and adversarial loss\nfunctions for high-quality upscaled images with low noise. We further fine-tune\nReal-ESRGAN by adapting it to the specific B/W noise and contrast\ncharacteristics. This suppresses noise artifacts without compromising detail.\nThe comparative evaluation conducted shows that our approach achieves superior\nnoise reduction and detail clarity compared to state-of-the-art CNN-based and\nESRGAN models, apart from reducing network bandwidth requirements. These\nbenefits are confirmed both by quantitative metrics, including Peak\nSignal-to-Noise Ratio and Structural Similarity Index, and by qualitative\nassessments, which indicate the potential of Real-ESRGAN for diagnostic-quality\nX-ray imaging and for efficient medical data transmission.\n","authors":["Rishabh Kumar Sharma","Mukund Sharma","Pushkar Sharma","Jeetashree Aparjeeta"],"pdf_url":"https://arxiv.org/pdf/2411.12833v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12832v1","updated":"2024-11-19T19:36:18Z","published":"2024-11-19T19:36:18Z","title":"HyperGAN-CLIP: A Unified Framework for Domain Adaptation, Image\n  Synthesis and Manipulation","summary":"  Generative Adversarial Networks (GANs), particularly StyleGAN and its\nvariants, have demonstrated remarkable capabilities in generating highly\nrealistic images. Despite their success, adapting these models to diverse tasks\nsuch as domain adaptation, reference-guided synthesis, and text-guided\nmanipulation with limited training data remains challenging. Towards this end,\nin this study, we present a novel framework that significantly extends the\ncapabilities of a pre-trained StyleGAN by integrating CLIP space via\nhypernetworks. This integration allows dynamic adaptation of StyleGAN to new\ndomains defined by reference images or textual descriptions. Additionally, we\nintroduce a CLIP-guided discriminator that enhances the alignment between\ngenerated images and target domains, ensuring superior image quality. Our\napproach demonstrates unprecedented flexibility, enabling text-guided image\nmanipulation without the need for text-specific training data and facilitating\nseamless style transfer. Comprehensive qualitative and quantitative evaluations\nconfirm the robustness and superior performance of our framework compared to\nexisting methods.\n","authors":["Abdul Basit Anees","Ahmet Canberk Baykal","Muhammed Burak Kizil","Duygu Ceylan","Erkut Erdem","Aykut Erdem"],"pdf_url":"https://arxiv.org/pdf/2411.12832v1.pdf","comment":"Accepted for publication in SIGGRAPH Asia 2024. Project Website:\n  https://cyberiada.github.io/HyperGAN-CLIP/"},{"id":"http://arxiv.org/abs/2411.12831v1","updated":"2024-11-19T19:35:28Z","published":"2024-11-19T19:35:28Z","title":"Towards motion from video diffusion models","summary":"  Text-conditioned video diffusion models have emerged as a powerful tool in\nthe realm of video generation and editing. But their ability to capture the\nnuances of human movement remains under-explored. Indeed the ability of these\nmodels to faithfully model an array of text prompts can lead to a wide host of\napplications in human and character animation. In this work, we take initial\nsteps to investigate whether these models can effectively guide the synthesis\nof realistic human body animations. Specifically we propose to synthesize human\nmotion by deforming an SMPL-X body representation guided by Score distillation\nsampling (SDS) calculated using a video diffusion model. By analyzing the\nfidelity of the resulting animations, we gain insights into the extent to which\nwe can obtain motion using publicly available text-to-video diffusion models\nusing SDS. Our findings shed light on the potential and limitations of these\nmodels for generating diverse and plausible human motions, paving the way for\nfurther research in this exciting area.\n","authors":["Paul Janson","Tiberiu Popa","Eugene Belilovsky"],"pdf_url":"https://arxiv.org/pdf/2411.12831v1.pdf","comment":"Accepted at ECCV 2024 Workshop :Foundation Models for 3D Humans"},{"id":"http://arxiv.org/abs/2403.08125v2","updated":"2024-11-19T19:26:23Z","published":"2024-03-12T23:27:30Z","title":"Q-SLAM: Quadric Representations for Monocular SLAM","summary":"  In this paper, we reimagine volumetric representations through the lens of\nquadrics. We posit that rigid scene components can be effectively decomposed\ninto quadric surfaces. Leveraging this assumption, we reshape the volumetric\nrepresentations with million of cubes by several quadric planes, which results\nin more accurate and efficient modeling of 3D scenes in SLAM contexts. First,\nwe use the quadric assumption to rectify noisy depth estimations from RGB\ninputs. This step significantly improves depth estimation accuracy, and allows\nus to efficiently sample ray points around quadric planes instead of the entire\nvolume space in previous NeRF-SLAM systems. Second, we introduce a novel\nquadric-decomposed transformer to aggregate information across quadrics. The\nquadric semantics are not only explicitly used for depth correction and scene\ndecomposition, but also serve as an implicit supervision signal for the mapping\nnetwork. Through rigorous experimental evaluation, our method exhibits superior\nperformance over other approaches relying on estimated depth, and achieves\ncomparable accuracy to methods utilizing ground truth depth on both synthetic\nand real-world datasets.\n","authors":["Chensheng Peng","Chenfeng Xu","Yue Wang","Mingyu Ding","Heng Yang","Masayoshi Tomizuka","Kurt Keutzer","Marco Pavone","Wei Zhan"],"pdf_url":"https://arxiv.org/pdf/2403.08125v2.pdf","comment":"Conference on Robot Learning (CoRL 2024)"},{"id":"http://arxiv.org/abs/2409.13576v2","updated":"2024-11-19T19:26:07Z","published":"2024-09-20T15:24:26Z","title":"Region Prompt Tuning: Fine-grained Scene Text Detection Utilizing Region\n  Text Prompt","summary":"  Recent advancements in prompt tuning have successfully adapted large-scale\nmodels like Contrastive Language-Image Pre-trained (CLIP) for downstream tasks\nsuch as scene text detection. Typically, text prompt complements the text\nencoder's input, focusing on global features while neglecting fine-grained\ndetails, leading to fine-grained text being ignored in task of scene text\ndetection. In this paper, we propose the region prompt tuning (RPT) method for\nfine-grained scene text detection, where region text prompt proposed would help\nfocus on fine-grained features. Region prompt tuning method decomposes region\ntext prompt into individual characters and splits visual feature map into\nregion visual tokens, creating a one-to-one correspondence between characters\nand tokens. This allows a character matches the local features of a token,\nthereby avoiding the omission of detailed features and fine-grained text. To\nachieve this, we introduce a sharing position embedding to link each character\nwith its corresponding token and employ a bidirectional distance loss to align\neach region text prompt character with the target ``text''. To refine the\ninformation at fine-grained level, we implement character-token level\ninteractions before and after encoding. Our proposed method combines a general\nscore map from the image-text process with a region score map derived from\ncharacter-token matching, producing a final score map that could balance the\nglobal and local features and be fed into DBNet to detect the text. Experiments\non benchmarks like ICDAR2015, TotalText, and CTW1500 demonstrate RPT impressive\nperformance, underscoring its effectiveness for scene text detection.\n","authors":["Xingtao Lin","Heqian Qiu","Lanxiao Wang","Ruihang Wang","Linfeng Xu","Hongliang Li"],"pdf_url":"https://arxiv.org/pdf/2409.13576v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09467v2","updated":"2024-11-19T19:20:37Z","published":"2024-10-12T10:14:11Z","title":"Enhancing Single Image to 3D Generation using Gaussian Splatting and\n  Hybrid Diffusion Priors","summary":"  3D object generation from a single image involves estimating the full 3D\ngeometry and texture of unseen views from an unposed RGB image captured in the\nwild. Accurately reconstructing an object's complete 3D structure and texture\nhas numerous applications in real-world scenarios, including robotic\nmanipulation, grasping, 3D scene understanding, and AR/VR. Recent advancements\nin 3D object generation have introduced techniques that reconstruct an object's\n3D shape and texture by optimizing the efficient representation of Gaussian\nSplatting, guided by pre-trained 2D or 3D diffusion models. However, a notable\ndisparity exists between the training datasets of these models, leading to\ndistinct differences in their outputs. While 2D models generate highly detailed\nvisuals, they lack cross-view consistency in geometry and texture. In contrast,\n3D models ensure consistency across different views but often result in overly\nsmooth textures. We propose bridging the gap between 2D and 3D diffusion models\nto address this limitation by integrating a two-stage frequency-based\ndistillation loss with Gaussian Splatting. Specifically, we leverage geometric\npriors in the low-frequency spectrum from a 3D diffusion model to maintain\nconsistent geometry and use a 2D diffusion model to refine the fidelity and\ntexture in the high-frequency spectrum of the generated 3D structure, resulting\nin more detailed and fine-grained outcomes. Our approach enhances geometric\nconsistency and visual quality, outperforming the current SOTA. Additionally,\nwe demonstrate the easy adaptability of our method for efficient object pose\nestimation and tracking.\n","authors":["Hritam Basak","Hadi Tabatabaee","Shreekant Gayaka","Ming-Feng Li","Xin Yang","Cheng-Hao Kuo","Arnie Sen","Min Sun","Zhaozheng Yin"],"pdf_url":"https://arxiv.org/pdf/2410.09467v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12817v1","updated":"2024-11-19T19:10:12Z","published":"2024-11-19T19:10:12Z","title":"What Makes a Good Dataset for Knowledge Distillation?","summary":"  Knowledge distillation (KD) has been a popular and effective method for model\ncompression. One important assumption of KD is that the teacher's original\ndataset will also be available when training the student. However, in\nsituations such as continual learning and distilling large models trained on\ncompany-withheld datasets, having access to the original data may not always be\npossible. This leads practitioners towards utilizing other sources of\nsupplemental data, which could yield mixed results. One must then ask: \"what\nmakes a good dataset for transferring knowledge from teacher to student?\" Many\nwould assume that only real in-domain imagery is viable, but is that the only\noption? In this work, we explore multiple possible surrogate distillation\ndatasets and demonstrate that many different datasets, even unnatural synthetic\nimagery, can serve as a suitable alternative in KD. From examining these\nalternative datasets, we identify and present various criteria describing what\nmakes a good dataset for distillation. Source code will be available in the\nfuture.\n","authors":["Logan Frank","Jim Davis"],"pdf_url":"https://arxiv.org/pdf/2411.12817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12814v1","updated":"2024-11-19T19:06:29Z","published":"2024-11-19T19:06:29Z","title":"Interactive Medical Image Segmentation: A Benchmark Dataset and Baseline","summary":"  Interactive Medical Image Segmentation (IMIS) has long been constrained by\nthe limited availability of large-scale, diverse, and densely annotated\ndatasets, which hinders model generalization and consistent evaluation across\ndifferent models. In this paper, we introduce the IMed-361M benchmark dataset,\na significant advancement in general IMIS research. First, we collect and\nstandardize over 6.4 million medical images and their corresponding ground\ntruth masks from multiple data sources. Then, leveraging the strong object\nrecognition capabilities of a vision foundational model, we automatically\ngenerated dense interactive masks for each image and ensured their quality\nthrough rigorous quality control and granularity management. Unlike previous\ndatasets, which are limited by specific modalities or sparse annotations,\nIMed-361M spans 14 modalities and 204 segmentation targets, totaling 361\nmillion masks-an average of 56 masks per image. Finally, we developed an IMIS\nbaseline network on this dataset that supports high-quality mask generation\nthrough interactive inputs, including clicks, bounding boxes, text prompts, and\ntheir combinations. We evaluate its performance on medical image segmentation\ntasks from multiple perspectives, demonstrating superior accuracy and\nscalability compared to existing interactive segmentation models. To facilitate\nresearch on foundational models in medical computer vision, we release the\nIMed-361M and model at https://github.com/uni-medical/IMIS-Bench.\n","authors":["Junlong Cheng","Bin Fu","Jin Ye","Guoan Wang","Tianbin Li","Haoyu Wang","Ruoyu Li","He Yao","Junren Chen","JingWen Li","Yanzhou Su","Min Zhu","Junjun He"],"pdf_url":"https://arxiv.org/pdf/2411.12814v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12811v1","updated":"2024-11-19T19:04:31Z","published":"2024-11-19T19:04:31Z","title":"Stylecodes: Encoding Stylistic Information For Image Generation","summary":"  Diffusion models excel in image generation, but controlling them remains a\nchallenge. We focus on the problem of style-conditioned image generation.\nAlthough example images work, they are cumbersome: srefs (style-reference\ncodes) from MidJourney solve this issue by expressing a specific image style in\na short numeric code. These have seen widespread adoption throughout social\nmedia due to both their ease of sharing and the fact they allow using an image\nfor style control, without having to post the source images themselves.\nHowever, users are not able to generate srefs from their own images, nor is the\nunderlying training procedure public. We propose StyleCodes: an open-source and\nopen-research style encoder architecture and training procedure to express\nimage style as a 20-symbol base64 code. Our experiments show that our encoding\nresults in minimal loss in quality compared to traditional image-to-style\ntechniques.\n","authors":["Ciara Rowles"],"pdf_url":"https://arxiv.org/pdf/2411.12811v1.pdf","comment":"code: https://github.com/CiaraStrawberry/stylecodes project page:\n  https://ciarastrawberry.github.io/stylecodes.github.io/. arXiv admin note:\n  substantial text overlap with arXiv:2408.03209"},{"id":"http://arxiv.org/abs/2410.07908v4","updated":"2024-11-19T19:03:07Z","published":"2024-10-10T13:36:49Z","title":"ONCOPILOT: A Promptable CT Foundation Model For Solid Tumor Evaluation","summary":"  Carcinogenesis is a proteiform phenomenon, with tumors emerging in various\nlocations and displaying complex, diverse shapes. At the crucial intersection\nof research and clinical practice, it demands precise and flexible assessment.\nHowever, current biomarkers, such as RECIST 1.1's long and short axis\nmeasurements, fall short of capturing this complexity, offering an approximate\nestimate of tumor burden and a simplistic representation of a more intricate\nprocess. Additionally, existing supervised AI models face challenges in\naddressing the variability in tumor presentations, limiting their clinical\nutility. These limitations arise from the scarcity of annotations and the\nmodels' focus on narrowly defined tasks.\n  To address these challenges, we developed ONCOPILOT, an interactive\nradiological foundation model trained on approximately 7,500 CT scans covering\nthe whole body, from both normal anatomy and a wide range of oncological cases.\nONCOPILOT performs 3D tumor segmentation using visual prompts like point-click\nand bounding boxes, outperforming state-of-the-art models (e.g., nnUnet) and\nachieving radiologist-level accuracy in RECIST 1.1 measurements. The key\nadvantage of this foundation model is its ability to surpass state-of-the-art\nperformance while keeping the radiologist in the loop, a capability that\nprevious models could not achieve. When radiologists interactively refine the\nsegmentations, accuracy improves further. ONCOPILOT also accelerates\nmeasurement processes and reduces inter-reader variability, facilitating\nvolumetric analysis and unlocking new biomarkers for deeper insights.\n  This AI assistant is expected to enhance the precision of RECIST 1.1\nmeasurements, unlock the potential of volumetric biomarkers, and improve\npatient stratification and clinical care, while seamlessly integrating into the\nradiological workflow.\n","authors":["Léo Machado","Hélène Philippe","Élodie Ferreres","Julien Khlaut","Julie Dupuis","Korentin Le Floch","Denis Habip Gatenyo","Pascal Roux","Jules Grégory","Maxime Ronot","Corentin Dancette","Tom Boeken","Daniel Tordjman","Pierre Manceron","Paul Hérent"],"pdf_url":"https://arxiv.org/pdf/2410.07908v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11844v2","updated":"2024-11-19T18:59:42Z","published":"2024-11-18T18:59:31Z","title":"Generative World Explorer","summary":"  Planning with partial observation is a central challenge in embodied AI. A\nmajority of prior works have tackled this challenge by developing agents that\nphysically explore their environment to update their beliefs about the world\nstate. In contrast, humans can $\\textit{imagine}$ unseen parts of the world\nthrough a mental exploration and $\\textit{revise}$ their beliefs with imagined\nobservations. Such updated beliefs can allow them to make more informed\ndecisions, without necessitating the physical exploration of the world at all\ntimes. To achieve this human-like ability, we introduce the $\\textit{Generative\nWorld Explorer (Genex)}$, an egocentric world exploration framework that allows\nan agent to mentally explore a large-scale 3D world (e.g., urban scenes) and\nacquire imagined observations to update its belief. This updated belief will\nthen help the agent to make a more informed decision at the current step. To\ntrain $\\textit{Genex}$, we create a synthetic urban scene dataset, Genex-DB.\nOur experimental results demonstrate that (1) $\\textit{Genex}$ can generate\nhigh-quality and consistent observations during long-horizon exploration of a\nlarge virtual physical world and (2) the beliefs updated with the generated\nobservations can inform an existing decision-making model (e.g., an LLM agent)\nto make better plans.\n","authors":["Taiming Lu","Tianmin Shu","Alan Yuille","Daniel Khashabi","Jieneng Chen"],"pdf_url":"https://arxiv.org/pdf/2411.11844v2.pdf","comment":"Website: generative-world-explorer.github.io"},{"id":"http://arxiv.org/abs/2411.12724v1","updated":"2024-11-19T18:45:16Z","published":"2024-11-19T18:45:16Z","title":"Heuristic-Free Multi-Teacher Learning","summary":"  We introduce Teacher2Task, a novel framework for multi-teacher learning that\neliminates the need for manual aggregation heuristics. Existing multi-teacher\nmethods typically rely on such heuristics to combine predictions from multiple\nteachers, often resulting in sub-optimal aggregated labels and the propagation\nof aggregation errors. Teacher2Task addresses these limitations by introducing\nteacher-specific input tokens and reformulating the training process. Instead\nof relying on aggregated labels, the framework transforms the training data,\nconsisting of ground truth labels and annotations from N teachers, into N+1\ndistinct tasks: N auxiliary tasks that predict the labeling styles of the N\nindividual teachers, and one primary task that focuses on the ground truth\nlabels. This approach, drawing upon principles from multiple learning\nparadigms, demonstrates strong empirical results across a range of\narchitectures, modalities, and tasks.\n","authors":["Huy Thong Nguyen","En-Hung Chu","Lenord Melvix","Jazon Jiao","Chunglin Wen","Benjamin Louie"],"pdf_url":"https://arxiv.org/pdf/2411.12724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12713v1","updated":"2024-11-19T18:27:31Z","published":"2024-11-19T18:27:31Z","title":"CATCH: Complementary Adaptive Token-level Contrastive Decoding to\n  Mitigate Hallucinations in LVLMs","summary":"  Large Vision-Language Model (LVLM) systems have demonstrated impressive\nvision-language reasoning capabilities but suffer from pervasive and severe\nhallucination issues, posing significant risks in critical domains such as\nhealthcare and autonomous systems. Despite previous efforts to mitigate\nhallucinations, a persistent issue remains: visual defect from vision-language\nmisalignment, creating a bottleneck in visual processing capacity. To address\nthis challenge, we develop Complementary Adaptive Token-level Contrastive\nDecoding to Mitigate Hallucinations in LVLMs (CATCH), based on the Information\nBottleneck theory. CATCH introduces Complementary Visual Decoupling (CVD) for\nvisual information separation, Non-Visual Screening (NVS) for hallucination\ndetection, and Adaptive Token-level Contrastive Decoding (ATCD) for\nhallucination mitigation. CATCH addresses issues related to visual defects that\ncause diminished fine-grained feature perception and cumulative hallucinations\nin open-ended scenarios. It is applicable to various visual question-answering\ntasks without requiring any specific data or prior knowledge, and generalizes\nrobustly to new tasks without additional training, opening new possibilities\nfor advancing LVLM in various challenging applications.\n","authors":["Zhehan Kan","Ce Zhang","Zihan Liao","Yapeng Tian","Wenming Yang","Junyuan Xiao","Xu Li","Dongmei Jiang","Yaowei Wang","Qingmin Liao"],"pdf_url":"https://arxiv.org/pdf/2411.12713v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12707v1","updated":"2024-11-19T18:22:25Z","published":"2024-11-19T18:22:25Z","title":"Barttender: An approachable & interpretable way to compare medical\n  imaging and non-imaging data","summary":"  Imaging-based deep learning has transformed healthcare research, yet its\nclinical adoption remains limited due to challenges in comparing imaging models\nwith traditional non-imaging and tabular data. To bridge this gap, we introduce\nBarttender, an interpretable framework that uses deep learning for the direct\ncomparison of the utility of imaging versus non-imaging tabular data for tasks\nlike disease prediction.\n  Barttender converts non-imaging tabular features, such as scalar data from\nelectronic health records, into grayscale bars, facilitating an interpretable\nand scalable deep learning based modeling of both data modalities. Our\nframework allows researchers to evaluate differences in utility through\nperformance measures, as well as local (sample-level) and global\n(population-level) explanations. We introduce a novel measure to define global\nfeature importances for image-based deep learning models, which we call gIoU.\nExperiments on the CheXpert and MIMIC datasets with chest X-rays and scalar\ndata from electronic health records show that Barttender performs comparably to\ntraditional methods and offers enhanced explainability using deep learning\nmodels.\n","authors":["Ayush Singla","Shakson Isaac","Chirag J. Patel"],"pdf_url":"https://arxiv.org/pdf/2411.12707v1.pdf","comment":"Accepted to the Proceedings Track at Machine Learning for Health\n  (ML4H 2024) conference, held on December 15-16, 2024 in Vancouver, Canada"},{"id":"http://arxiv.org/abs/2411.12593v1","updated":"2024-11-19T18:04:13Z","published":"2024-11-19T18:04:13Z","title":"AdaCM$^2$: On Understanding Extremely Long-Term Video with Adaptive\n  Cross-Modality Memory Reduction","summary":"  The advancements in large language models (LLMs) have propelled the\nimprovement of video understanding tasks by incorporating LLMs with visual\nmodels. However, most existing LLM-based models (e.g., VideoLLaMA, VideoChat)\nare constrained to processing short-duration videos. Recent attempts to\nunderstand long-term videos by extracting and compressing visual features into\na fixed memory size. Nevertheless, those methods leverage only visual modality\nto merge video tokens and overlook the correlation between visual and textual\nqueries, leading to difficulties in effectively handling complex\nquestion-answering tasks. To address the challenges of long videos and complex\nprompts, we propose AdaCM$^2$, which, for the first time, introduces an\nadaptive cross-modality memory reduction approach to video-text alignment in an\nauto-regressive manner on video streams. Our extensive experiments on various\nvideo understanding tasks, such as video captioning, video question answering,\nand video classification, demonstrate that AdaCM$^2$ achieves state-of-the-art\nperformance across multiple datasets while significantly reducing memory usage.\nNotably, it achieves a 4.5% improvement across multiple tasks in the LVU\ndataset with a GPU memory consumption reduction of up to 65%.\n","authors":["Yuanbin Man","Ying Huang","Chengming Zhang","Bingzhe Li","Wei Niu","Miao Yin"],"pdf_url":"https://arxiv.org/pdf/2411.12593v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2411.12921v1","updated":"2024-11-19T23:19:46Z","published":"2024-11-19T23:19:46Z","title":"A Comparative Study of Text Retrieval Models on DaReCzech","summary":"  This article presents a comprehensive evaluation of 7 off-the-shelf document\nretrieval models: Splade, Plaid, Plaid-X, SimCSE, Contriever, OpenAI ADA and\nGemma2 chosen to determine their performance on the Czech retrieval dataset\nDaReCzech. The primary objective of our experiments is to estimate the quality\nof modern retrieval approaches in the Czech language. Our analyses include\nretrieval quality, speed, and memory footprint. Secondly, we analyze whether it\nis better to use the model directly in Czech text, or to use machine\ntranslation into English, followed by retrieval in English. Our experiments\nidentify the most effective option for Czech information retrieval. The\nfindings revealed notable performance differences among the models, with\nGemma22 achieving the highest precision and recall, while Contriever performing\npoorly. Conclusively, SPLADE and PLAID models offered a balance of efficiency\nand performance.\n","authors":["Jakub Stetina","Martin Fajcik","Michal Stefanik","Michal Hradis"],"pdf_url":"https://arxiv.org/pdf/2411.12921v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12880v1","updated":"2024-11-19T21:57:22Z","published":"2024-11-19T21:57:22Z","title":"Advancing Large Language Models for Spatiotemporal and Semantic\n  Association Mining of Similar Environmental Events","summary":"  Retrieval and recommendation are two essential tasks in modern search tools.\nThis paper introduces a novel retrieval-reranking framework leveraging Large\nLanguage Models (LLMs) to enhance the spatiotemporal and semantic associated\nmining and recommendation of relevant unusual climate and environmental events\ndescribed in news articles and web posts. This framework uses advanced natural\nlanguage processing techniques to address the limitations of traditional manual\ncuration methods in terms of high labor cost and lack of scalability.\nSpecifically, we explore an optimized solution to employ cutting-edge embedding\nmodels for semantically analyzing spatiotemporal events (news) and propose a\nGeo-Time Re-ranking (GT-R) strategy that integrates multi-faceted criteria\nincluding spatial proximity, temporal association, semantic similarity, and\ncategory-instructed similarity to rank and identify similar spatiotemporal\nevents. We apply the proposed framework to a dataset of four thousand Local\nEnvironmental Observer (LEO) Network events, achieving top performance in\nrecommending similar events among multiple cutting-edge dense retrieval models.\nThe search and recommendation pipeline can be applied to a wide range of\nsimilar data search tasks dealing with geospatial and temporal data. We hope\nthat by linking relevant events, we can better aid the general public to gain\nan enhanced understanding of climate change and its impact on different\ncommunities.\n","authors":["Yuanyuan Tian","Wenwen Li","Lei Hu","Xiao Chen","Michael Brook","Michael Brubaker","Fan Zhang","Anna K. Liljedahl"],"pdf_url":"https://arxiv.org/pdf/2411.12880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12649v1","updated":"2024-11-19T16:58:03Z","published":"2024-11-19T16:58:03Z","title":"PseudoSeer: a Search Engine for Pseudocode","summary":"  A novel pseudocode search engine is designed to facilitate efficient\nretrieval and search of academic papers containing pseudocode. By leveraging\nElasticsearch, the system enables users to search across various facets of a\npaper, such as the title, abstract, author information, and LaTeX code\nsnippets, while supporting advanced features like combined facet searches and\nexact-match queries for more targeted results. A description of the data\nacquisition process is provided, with arXiv as the primary data source, along\nwith methods for data extraction and text-based indexing, highlighting how\ndifferent data elements are stored and optimized for search. A weighted\nBM25-based ranking algorithm is used by the search engine, and factors\nconsidered when prioritizing search results for both single and combined facet\nsearches are described. We explain how each facet is weighted in a combined\nsearch. Several search engine results pages are displayed. Finally, there is a\nbrief overview of future work and potential evaluation methodology for\nassessing the effectiveness and performance of the search engine is described.\n","authors":["Levent Toksoz","Mukund Srinath","Gang Tan","C. Lee Giles"],"pdf_url":"https://arxiv.org/pdf/2411.12649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12173v2","updated":"2024-11-19T13:56:14Z","published":"2024-08-22T07:41:33Z","title":"Hardware Acceleration for Knowledge Graph Processing: Challenges &\n  Recent Developments","summary":"  Knowledge graphs (KGs) have achieved significant attention in recent years,\nparticularly in the area of the Semantic Web as well as gaining popularity in\nother application domains such as data mining and search engines.\nSimultaneously, there has been enormous progress in the development of\ndifferent types of heterogeneous hardware, impacting the way KGs are processed.\nThe aim of this paper is to provide a systematic literature review of knowledge\ngraph hardware acceleration. For this, we present a classification of the\nprimary areas in knowledge graph technology that harnesses different hardware\nunits for accelerating certain knowledge graph functionalities. We then\nextensively describe respective works, focusing on how KG related schemes\nharness modern hardware accelerators. Based on our review, we identify various\nresearch gaps and future exploratory directions that are anticipated to be of\nsignificant value both for academics and industry practitioners.\n","authors":["Maciej Besta","Robert Gerstenberger","Patrick Iff","Pournima Sonawane","Juan Gómez Luna","Raghavendra Kanakagiri","Rui Min","Grzegorz Kwaśniewski","Onur Mutlu","Torsten Hoefler","Raja Appuswamy","Aidan O Mahony"],"pdf_url":"https://arxiv.org/pdf/2408.12173v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12441v1","updated":"2024-11-19T12:04:02Z","published":"2024-11-19T12:04:02Z","title":"Towards Unifying Feature Interaction Models for Click-Through Rate\n  Prediction","summary":"  Modeling feature interactions plays a crucial role in accurately predicting\nclick-through rates (CTR) in advertising systems. To capture the intricate\npatterns of interaction, many existing models employ matrix-factorization\ntechniques to represent features as lower-dimensional embedding vectors,\nenabling the modeling of interactions as products between these embeddings. In\nthis paper, we propose a general framework called IPA to systematically unify\nthese models. Our framework comprises three key components: the Interaction\nFunction, which facilitates feature interaction; the Layer Pooling, which\nconstructs higher-level interaction layers; and the Layer Aggregator, which\ncombines the outputs of all layers to serve as input for the subsequent\nclassifier. We demonstrate that most existing models can be categorized within\nour framework by making specific choices for these three components. Through\nextensive experiments and a dimensional collapse analysis, we evaluate the\nperformance of these choices. Furthermore, by leveraging the most powerful\ncomponents within our framework, we introduce a novel model that achieves\ncompetitive results compared to state-of-the-art CTR models. PFL gets\nsignificant GMV lift during online A/B test in Tencent's advertising platform\nand has been deployed as the production model in several primary scenarios.\n","authors":["Yu Kang","Junwei Pan","Jipeng Jin","Shudong Huang","Xiaofeng Gao","Lei Xiao"],"pdf_url":"https://arxiv.org/pdf/2411.12441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12354v1","updated":"2024-11-19T09:16:25Z","published":"2024-11-19T09:16:25Z","title":"Scalable and Effective Negative Sample Generation for Hyperedge\n  Prediction","summary":"  Hyperedge prediction is crucial in hypergraph analysis for understanding\ncomplex multi-entity interactions in various web-based applications, including\nsocial networks and e-commerce systems. Traditional methods often face\ndifficulties in generating high-quality negative samples due to the imbalance\nbetween positive and negative instances. To address this, we present the\nScalable and Effective Negative Sample Generation for Hyperedge Prediction\n(SEHP) framework, which utilizes diffusion models to tackle these challenges.\nSEHP employs a boundary-aware loss function that iteratively refines negative\nsamples, moving them closer to decision boundaries to improve classification\nperformance. SEHP samples positive instances to form sub-hypergraphs for\nscalable batch processing. By using structural information from sub-hypergraphs\nas conditions within the diffusion process, SEHP effectively captures global\npatterns. To enhance efficiency, our approach operates directly in latent\nspace, avoiding the need for discrete ID generation and resulting in\nsignificant speed improvements while preserving accuracy. Extensive experiments\nshow that SEHP outperforms existing methods in accuracy, efficiency, and\nscalability, representing a substantial advancement in hyperedge prediction\ntechniques. Our code is available here.\n","authors":["Shilin Qu","Weiqing Wang","Yuan-Fang Li","Quoc Viet Hung Nguyen","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2411.12354v1.pdf","comment":"11"},{"id":"http://arxiv.org/abs/2406.05085v2","updated":"2024-11-19T08:46:34Z","published":"2024-06-07T16:59:38Z","title":"Multi-Head RAG: Solving Multi-Aspect Problems with LLMs","summary":"  Retrieval Augmented Generation (RAG) enhances the abilities of Large Language\nModels (LLMs) by enabling the retrieval of documents into the LLM context to\nprovide more accurate and relevant responses. Existing RAG solutions do not\nfocus on queries that may require fetching multiple documents with\nsubstantially different contents. Such queries occur frequently, but are\nchallenging because the embeddings of these documents may be distant in the\nembedding space, making it hard to retrieve them all. This paper introduces\nMulti-Head RAG (MRAG), a novel scheme designed to address this gap with a\nsimple yet powerful idea: leveraging activations of Transformer's multi-head\nattention layer, instead of the decoder layer, as keys for fetching\nmulti-aspect documents. The driving motivation is that different attention\nheads can learn to capture different data aspects. Harnessing the corresponding\nactivations results in embeddings that represent various facets of data items\nand queries, improving the retrieval accuracy for complex queries. We provide\nan evaluation methodology and metrics, multi-aspect datasets that we release\nonline, and real-world use cases to demonstrate MRAG's effectiveness, showing\nimprovements of up to 20% in relevance over standard RAG baselines. MRAG can be\nseamlessly integrated with existing RAG frameworks and benchmarking tools like\nRAGAS as well as different classes of data stores.\n","authors":["Maciej Besta","Ales Kubicek","Roman Niggli","Robert Gerstenberger","Lucas Weitzendorf","Mingyuan Chi","Patrick Iff","Joanna Gajda","Piotr Nyczyk","Jürgen Müller","Hubert Niewiadomski","Marcin Chrapek","Michał Podstawski","Torsten Hoefler"],"pdf_url":"https://arxiv.org/pdf/2406.05085v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12307v1","updated":"2024-11-19T07:48:35Z","published":"2024-11-19T07:48:35Z","title":"Balancing Accuracy and Efficiency in Multi-Turn Intent Classification\n  for LLM-Powered Dialog Systems in Production","summary":"  Accurate multi-turn intent classification is essential for advancing\nconversational AI systems. However, challenges such as the scarcity of\ncomprehensive datasets and the complexity of contextual dependencies across\ndialogue turns hinder progress. This paper presents two novel approaches\nleveraging Large Language Models (LLMs) to enhance scalability and reduce\nlatency in production dialogue systems. First, we introduce Symbol Tuning,\nwhich simplifies intent labels to reduce task complexity and improve\nperformance in multi-turn dialogues. Second, we propose C-LARA\n(Consistency-aware, Linguistics Adaptive Retrieval Augmentation), a framework\nthat employs LLMs for data augmentation and pseudo-labeling to generate\nsynthetic multi-turn dialogues. These enriched datasets are used to fine-tune a\nsmall, efficient model suitable for deployment. Experiments conducted on\nmultilingual dialogue datasets demonstrate significant improvements in\nclassification accuracy and resource efficiency. Our methods enhance multi-turn\nintent classification accuracy by 5.09%, reduce annotation costs by 40%, and\nenable scalable deployment in low-resource multilingual industrial systems,\nhighlighting their practicality and impact.\n","authors":["Junhua Liu","Yong Keat Tan","Bin Fu","Kwan Hui Lim"],"pdf_url":"https://arxiv.org/pdf/2411.12307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12295v1","updated":"2024-11-19T07:30:07Z","published":"2024-11-19T07:30:07Z","title":"Consistency Regularization for Complementary Clothing Recommendations","summary":"  This paper reports on the development of a Consistency Regularized model for\nBayesian Personalized Ranking (CR-BPR), addressing to the drawbacks in existing\ncomplementary clothing recommendation methods, namely limited consistency and\nbiased learning caused by diverse feature scale of multi-modal data. Compared\nto other product types, fashion preferences are inherently subjective and more\npersonal, and fashion are often presented, not by individual clothing product,\nbut with other complementary product(s) in a well coordinated fashion outfit.\nCurrent complementary-product recommendation studies primarily focus on user\npreference and product matching, this study further emphasizes the consistency\nobserved in user-product interactions as well as product-product interactions,\nin the specific context of clothing matching. Most traditional approaches often\nunderplayed the impact of existing wardrobe items on future matching choices,\nresulting in less effective preference prediction models. Moreover, many\nmulti-modal information based models overlook the limitations arising from\nvarious feature scales being involved. To address these gaps, the CR-BPR model\nintegrates collaborative filtering techniques to incorporate both user\npreference and product matching modeling, with a unique focus on consistency\nregularization for each aspect. Additionally, the incorporation of a feature\nscaling process further addresses the imbalances caused by different feature\nscales, ensuring that the model can effectively handle multi-modal data without\nbeing skewed by any particular type of feature. The effectiveness of the CR-BPR\nmodel was validated through detailed analysis involving two benchmark datasets.\nThe results confirmed that the proposed approach significantly outperforms\nexisting models.\n","authors":["Shuiying Liao","P. Y. Mok","Li Li"],"pdf_url":"https://arxiv.org/pdf/2411.12295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.11019v3","updated":"2024-11-19T05:35:02Z","published":"2023-07-20T16:46:10Z","title":"Investigating the Factual Knowledge Boundary of Large Language Models\n  with Retrieval Augmentation","summary":"  Large language models (LLMs) have shown impressive prowess in solving a wide\nrange of tasks with world knowledge. However, it remains unclear how well LLMs\nare able to perceive their factual knowledge boundaries, particularly under\nretrieval augmentation settings. In this study, we present the first analysis\non the factual knowledge boundaries of LLMs and how retrieval augmentation\naffects LLMs on open-domain question answering (QA), with a bunch of important\nfindings. Specifically, we focus on three research questions and analyze them\nby examining QA, priori judgement and posteriori judgement capabilities of\nLLMs. We show evidence that LLMs possess unwavering confidence in their\nknowledge and cannot handle the conflict between internal and external\nknowledge well. Furthermore, retrieval augmentation proves to be an effective\napproach in enhancing LLMs' awareness of knowledge boundaries. We further\nconduct thorough experiments to examine how different factors affect LLMs and\npropose a simple method to dynamically utilize supporting documents with our\njudgement strategy. Additionally, we find that the relevance between the\nsupporting documents and the questions significantly impacts LLMs' QA and\njudgemental capabilities. The code to reproduce this work is available at\nhttps://github.com/RUCAIBox/LLM-Knowledge-Boundary.\n","authors":["Ruiyang Ren","Yuhao Wang","Yingqi Qu","Wayne Xin Zhao","Jing Liu","Hao Tian","Hua Wu","Ji-Rong Wen","Haifeng Wang"],"pdf_url":"https://arxiv.org/pdf/2307.11019v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12235v1","updated":"2024-11-19T05:19:53Z","published":"2024-11-19T05:19:53Z","title":"BoolQuestions: Does Dense Retrieval Understand Boolean Logic in\n  Language?","summary":"  Dense retrieval, which aims to encode the semantic information of arbitrary\ntext into dense vector representations or embeddings, has emerged as an\neffective and efficient paradigm for text retrieval, consequently becoming an\nessential component in various natural language processing systems. These\nsystems typically focus on optimizing the embedding space by attending to the\nrelevance of text pairs, while overlooking the Boolean logic inherent in\nlanguage, which may not be captured by current training objectives. In this\nwork, we first investigate whether current retrieval systems can comprehend the\nBoolean logic implied in language. To answer this question, we formulate the\ntask of Boolean Dense Retrieval and collect a benchmark dataset, BoolQuestions,\nwhich covers complex queries containing basic Boolean logic and corresponding\nannotated passages. Through extensive experimental results on the proposed task\nand benchmark dataset, we draw the conclusion that current dense retrieval\nsystems do not fully understand Boolean logic in language, and there is a long\nway to go to improve our dense retrieval systems. Furthermore, to promote\nfurther research on enhancing the understanding of Boolean logic for language\nmodels, we explore Boolean operation on decomposed query and propose a\ncontrastive continual training method that serves as a strong baseline for the\nresearch community.\n","authors":["Zongmeng Zhang","Jinhua Zhu","Wengang Zhou","Xiang Qi","Peng Zhang","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2411.12235v1.pdf","comment":"Findings of the Association for Computational Linguistics: EMNLP 2024"},{"id":"http://arxiv.org/abs/2411.12229v1","updated":"2024-11-19T04:51:08Z","published":"2024-11-19T04:51:08Z","title":"SymphonyQG: Towards Symphonious Integration of Quantization and Graph\n  for Approximate Nearest Neighbor Search","summary":"  Approximate nearest neighbor (ANN) search in high-dimensional Euclidean space\nhas a broad range of applications. Among existing ANN algorithms, graph-based\nmethods have shown superior performance in terms of the time-accuracy\ntrade-off. However, they face performance bottlenecks due to the random memory\naccesses caused by the searching process on the graph indices and the costs of\ncomputing exact distances to guide the searching process. To relieve the\nbottlenecks, a recent method named NGT-QG makes an attempt by integrating\nquantization and graph. It (1) replicates and stores the quantization codes of\na vertex's neighbors compactly so that they can be accessed sequentially, and\n(2) uses a SIMD-based implementation named FastScan to efficiently estimate\ndistances based on the quantization codes in batch for guiding the searching\nprocess. While NGT-QG achieves promising improvements over the vanilla\ngraph-based methods, it has not fully unleashed the potential of integrating\nquantization and graph. For instance, it entails a re-ranking step to compute\nexact distances at the end, which introduces extra random memory accesses; its\ngraph structure is not jointly designed considering the in-batch nature of\nFastScan, which causes wastes of computation in searching. In this work,\nfollowing NGT-QG, we present a new method named SymphonyQG, which achieves more\nsymphonious integration of quantization and graph (e.g., it avoids the explicit\nre-ranking step and refines the graph structure to be more aligned with\nFastScan). Based on extensive experiments on real-world datasets, SymphonyQG\nestablishes the new state-of-the-art in terms of the time-accuracy trade-off.\n","authors":["Yutong Gou","Jianyang Gao","Yuexuan Xu","Cheng Long"],"pdf_url":"https://arxiv.org/pdf/2411.12229v1.pdf","comment":"The paper has been accepted by SIGMOD 2025"},{"id":"http://arxiv.org/abs/2411.12205v1","updated":"2024-11-19T03:48:48Z","published":"2024-11-19T03:48:48Z","title":"Sparser Training for On-Device Recommendation Systems","summary":"  Recommender systems often rely on large embedding tables that map users and\nitems to dense vectors of uniform size, leading to substantial memory\nconsumption and inefficiencies. This is particularly problematic in\nmemory-constrained environments like mobile and Web of Things (WoT)\napplications, where scalability and real-time performance are critical. Various\nresearch efforts have sought to address these issues. Although embedding\npruning methods utilizing Dynamic Sparse Training (DST) stand out due to their\nlow training and inference costs, consistent sparsity, and end-to-end\ndifferentiability, they face key challenges. Firstly, they typically\ninitializes the mask matrix, which is used to prune redundant parameters, with\nrandom uniform sparse initialization. This strategy often results in suboptimal\nperformance as it creates unstructured and inefficient connections. Secondly,\nthey tend to favor the users/items sampled in the single batch immediately\nbefore weight exploration when they reactivate pruned parameters with large\ngradient magnitudes, which does not necessarily improve the overall\nperformance. Thirdly, while they use sparse weights during forward passes, they\nstill need to compute dense gradients during backward passes. In this paper, we\npropose SparseRec, an lightweight embedding method based on DST, to address\nthese issues. Specifically, SparseRec initializes the mask matrix using\nNonnegative Matrix Factorization. It accumulates gradients to identify the\ninactive parameters that can better improve the model performance after\nactivation. Furthermore, it avoids dense gradients during backpropagation by\nsampling a subset of important vectors. Gradients are calculated only for\nparameters in this subset, thus maintaining sparsity during training in both\nforward and backward passes.\n","authors":["Yunke Qu","Liang Qu","Tong Chen","Xiangyu Zhao","Jianxin Li","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2411.12205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12179v1","updated":"2024-11-19T02:45:17Z","published":"2024-11-19T02:45:17Z","title":"Multi-Grained Preference Enhanced Transformer for Multi-Behavior\n  Sequential Recommendation","summary":"  Sequential recommendation (SR) aims to predict the next purchasing item\naccording to users' dynamic preference learned from their historical user-item\ninteractions. To improve the performance of recommendation, learning dynamic\nheterogeneous cross-type behavior dependencies is indispensable for recommender\nsystem. However, there still exists some challenges in Multi-Behavior\nSequential Recommendation (MBSR). On the one hand, existing methods only model\nheterogeneous multi-behavior dependencies at behavior-level or item-level, and\nmodelling interaction-level dependencies is still a challenge. On the other\nhand, the dynamic multi-grained behavior-aware preference is hard to capture in\ninteraction sequences, which reflects interaction-aware sequential pattern. To\ntackle these challenges, we propose a Multi-Grained Preference enhanced\nTransformer framework (M-GPT). First, M-GPT constructs a interaction-level\ngraph of historical cross-typed interactions in a sequence. Then graph\nconvolution is performed to derive interaction-level multi-behavior dependency\nrepresentation repeatedly, in which the complex correlation between historical\ncross-typed interactions at specific orders can be well learned. Secondly, a\nnovel multi-scale transformer architecture equipped with multi-grained user\npreference extraction is proposed to encode the interaction-aware sequential\npattern enhanced by capturing temporal behavior-aware multi-grained preference\n. Experiments on the real-world datasets indicate that our method M-GPT\nconsistently outperforms various state-of-the-art recommendation methods.\n","authors":["Chuan He","Yongchao Liu","Qiang Li","Weiqiang Wang","Xin Fu","Xinyi Fu","Chuntao Hong","Xinwei Yao"],"pdf_url":"https://arxiv.org/pdf/2411.12179v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2409.10825v2","updated":"2024-11-19T01:51:37Z","published":"2024-09-17T01:37:57Z","title":"Unveiling and Mitigating Bias in Large Language Model Recommendations: A\n  Path to Fairness","summary":"  Large Language Model (LLM)-based recommendation systems provide more\ncomprehensive recommendations than traditional systems by deeply analyzing\ncontent and user behavior. However, these systems often exhibit biases,\nfavoring mainstream content while marginalizing non-traditional options due to\nskewed training data. This study investigates the intricate relationship\nbetween bias and LLM-based recommendation systems, with a focus on music, song,\nand book recommendations across diverse demographic and cultural groups.\nThrough a comprehensive analysis conducted over different LLM-models, this\npaper evaluates the impact of bias on recommendation outcomes. Our findings\nhighlight that biases are not only deeply embedded but also widely pervasive\nacross these systems, emphasizing the substantial and widespread nature of the\nissue. Moreover, contextual information, such as socioeconomic status, further\namplify these biases, demonstrating the complexity and depth of the challenges\nfaced in creating fair recommendations across different groups.\n","authors":["Shahnewaz Karim Sakib","Anindya Bijoy Das"],"pdf_url":"https://arxiv.org/pdf/2409.10825v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.05508v2","updated":"2024-11-19T01:15:44Z","published":"2024-05-09T02:37:53Z","title":"Redefining Information Retrieval of Structured Database via Large\n  Language Models","summary":"  Retrieval augmentation is critical when Language Models (LMs) exploit\nnon-parametric knowledge related to the query through external knowledge bases\nbefore reasoning. The retrieved information is incorporated into LMs as context\nalongside the query, enhancing the reliability of responses towards factual\nquestions. Prior researches in retrieval augmentation typically follow a\nretriever-generator paradigm. In this context, traditional retrievers encounter\nchallenges in precisely and seamlessly extracting query-relevant information\nfrom knowledge bases. To address this issue, this paper introduces a novel\nretrieval augmentation framework called ChatLR that primarily employs the\npowerful semantic understanding ability of Large Language Models (LLMs) as\nretrievers to achieve precise and concise information retrieval. Additionally,\nwe construct an LLM-based search and question answering system tailored for the\nfinancial domain by fine-tuning LLM on two tasks including Text2API and API-ID\nrecognition. Experimental results demonstrate the effectiveness of ChatLR in\naddressing user queries, achieving an overall information retrieval accuracy\nexceeding 98.8\\%.\n","authors":["Mingzhu Wang","Yuzhe Zhang","Qihang Zhao","Junyi Yang","Hong Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.05508v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2010.13442v7","updated":"2024-11-19T20:12:32Z","published":"2020-10-26T09:27:39Z","title":"Refl-Spanners: A Purely Regular Approach to Non-Regular Core Spanners","summary":"  The regular spanners (characterised by vset-automata) are closed under the\nalgebraic operations of union, join and projection, and have desirable\nalgorithmic properties. The core spanners (introduced by Fagin, Kimelfeld,\nReiss, and Vansummeren (PODS 2013, JACM 2015) as a formalisation of the core\nfunctionality of the query language AQL used in IBM's SystemT) additionally\nneed string-equality selections and it has been shown by Freydenberger and\nHolldack (ICDT 2016, Theory of Computing Systems 2018) that this leads to high\ncomplexity and even undecidability of the typical problems in static analysis\nand query evaluation. We propose an alternative approach to core spanners: by\nincorporating the string-equality selections directly into the regular language\nthat represents the underlying regular spanner (instead of treating it as an\nalgebraic operation on the table extracted by the regular spanner), we obtain a\nfragment of core spanners that, while having slightly weaker expressive power\nthan the full class of core spanners, arguably still covers the intuitive\napplications of string-equality selections for information extraction and has\nmuch better upper complexity bounds of the typical problems in static analysis\nand query evaluation.\n","authors":["Markus L. Schmid","Nicole Schweikardt"],"pdf_url":"https://arxiv.org/pdf/2010.13442v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15186v1","updated":"2024-11-19T05:46:20Z","published":"2024-11-19T05:46:20Z","title":"Preliminary Evaluation of the Test-Time Training Layers in\n  Recommendation System (Student Abstract)","summary":"  This paper explores the application and effectiveness of Test-Time Training\n(TTT) layers in improving the performance of recommendation systems. We\ndeveloped a model, TTT4Rec, utilizing TTT-Linear as the feature extraction\nlayer. Our tests across multiple datasets indicate that TTT4Rec, as a base\nmodel, performs comparably or even surpasses other baseline models in similar\nenvironments.\n","authors":["Tianyu Zhan","Zheqi Lv","Shengyu Zhang","Jiwei Li"],"pdf_url":"https://arxiv.org/pdf/2411.15186v1.pdf","comment":"To be published in AAAI-25 Student Abstract and Poster Program"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2308.09301v2","updated":"2024-11-19T23:57:24Z","published":"2023-08-18T04:49:45Z","title":"Automata Learning from Preference and Equivalence Queries","summary":"  Active automata learning from membership and equivalence queries is a\nfoundational problem with numerous applications. We propose a novel variant of\nthe active automata learning problem: actively learn finite automata using\npreference queries -- i.e., queries about the relative position of two\nsequences in a total order -- instead of membership queries. Our solution is\nREMAP, a novel algorithm which leverages a symbolic observation table along\nwith unification and constraint solving to navigate a space of symbolic\nhypotheses (each representing a set of automata), and uses\nsatisfiability-solving to construct a concrete automaton from a symbolic\nhypothesis. REMAP is guaranteed to correctly infer the minimal automaton with\npolynomial query complexity under exact equivalence queries, and achieves\nPAC-identification ($\\varepsilon$-approximate, with high probability) of the\nminimal automaton using sampling-based equivalence queries. Our empirical\nevaluations of REMAP on the task of learning reward machines for two\nreinforcement learning domains indicate REMAP scales to large automata and is\neffective at learning correct automata from consistent teachers, under both\nexact and sampling-based equivalence queries.\n","authors":["Eric Hsiung","Joydeep Biswas","Swarat Chaudhuri"],"pdf_url":"https://arxiv.org/pdf/2308.09301v2.pdf","comment":"29 pages, 12 figures"},{"id":"http://arxiv.org/abs/2411.12930v1","updated":"2024-11-19T23:43:25Z","published":"2024-11-19T23:43:25Z","title":"LEDRO: LLM-Enhanced Design Space Reduction and Optimization for Analog\n  Circuits","summary":"  Traditional approaches for designing analog circuits are time-consuming and\nrequire significant human expertise. Existing automation efforts using methods\nlike Bayesian Optimization (BO) and Reinforcement Learning (RL) are sub-optimal\nand costly to generalize across different topologies and technology nodes. In\nour work, we introduce a novel approach, LEDRO, utilizing Large Language Models\n(LLMs) in conjunction with optimization techniques to iteratively refine the\ndesign space for analog circuit sizing. LEDRO is highly generalizable compared\nto other RL and BO baselines, eliminating the need for design annotation or\nmodel training for different topologies or technology nodes. We conduct a\ncomprehensive evaluation of our proposed framework and baseline on 22 different\nOp-Amp topologies across four FinFET technology nodes. Results demonstrate the\nsuperior performance of LEDRO as it outperforms our best baseline by an average\nof 13% FoM improvement with 2.15x speed-up on low complexity Op-Amps and 48%\nFoM improvement with 1.7x speed-up on high complexity Op-Amps. This highlights\nLEDRO's effective performance, efficiency, and generalizability.\n","authors":["Dimple Vijay Kochar","Hanrui Wang","Anantha Chandrakasan","Xin Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.12930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17309v2","updated":"2024-11-19T23:32:13Z","published":"2024-10-22T18:00:00Z","title":"Literature Meets Data: A Synergistic Approach to Hypothesis Generation","summary":"  AI holds promise for transforming scientific processes, including hypothesis\ngeneration. Prior work on hypothesis generation can be broadly categorized into\ntheory-driven and data-driven approaches. While both have proven effective in\ngenerating novel and plausible hypotheses, it remains an open question whether\nthey can complement each other. To address this, we develop the first method\nthat combines literature-based insights with data to perform LLM-powered\nhypothesis generation. We apply our method on five different datasets and\ndemonstrate that integrating literature and data outperforms other baselines\n(8.97\\% over few-shot, 15.75\\% over literature-based alone, and 3.37\\% over\ndata-driven alone). Additionally, we conduct the first human evaluation to\nassess the utility of LLM-generated hypotheses in assisting human\ndecision-making on two challenging tasks: deception detection and AI generated\ncontent detection. Our results show that human accuracy improves significantly\nby 7.44\\% and 14.19\\% on these tasks, respectively. These findings suggest that\nintegrating literature-based and data-driven approaches provides a\ncomprehensive and nuanced framework for hypothesis generation and could open\nnew avenues for scientific inquiry.\n","authors":["Haokun Liu","Yangqiaoyu Zhou","Mingxuan Li","Chenfei Yuan","Chenhao Tan"],"pdf_url":"https://arxiv.org/pdf/2410.17309v2.pdf","comment":"30 pages, 7 figures, code link:\n  https://github.com/ChicagoHAI/hypothesis-generation"},{"id":"http://arxiv.org/abs/2411.12925v1","updated":"2024-11-19T23:23:16Z","published":"2024-11-19T23:23:16Z","title":"Loss-to-Loss Prediction: Scaling Laws for All Datasets","summary":"  While scaling laws provide a reliable methodology for predicting train loss\nacross compute scales for a single data distribution, less is known about how\nthese predictions should change as we change the distribution. In this paper,\nwe derive a strategy for predicting one loss from another and apply it to\npredict across different pre-training datasets and from pre-training data to\ndownstream task data. Our predictions extrapolate well even at 20x the largest\nFLOP budget used to fit the curves. More precisely, we find that there are\nsimple shifted power law relationships between (1) the train losses of two\nmodels trained on two separate datasets when the models are paired by training\ncompute (train-to-train), (2) the train loss and the test loss on any\ndownstream distribution for a single model (train-to-test), and (3) the test\nlosses of two models trained on two separate train datasets (test-to-test). The\nresults hold up for pre-training datasets that differ substantially (some are\nentirely code and others have no code at all) and across a variety of\ndownstream tasks. Finally, we find that in some settings these shifted power\nlaw relationships can yield more accurate predictions than extrapolating\nsingle-dataset scaling laws.\n","authors":["David Brandfonbrener","Nikhil Anand","Nikhil Vyas","Eran Malach","Sham Kakade"],"pdf_url":"https://arxiv.org/pdf/2411.12925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12924v1","updated":"2024-11-19T23:22:33Z","published":"2024-11-19T23:22:33Z","title":"Human-In-the-Loop Software Development Agents","summary":"  Recently, Large Language Models (LLMs)-based multi-agent paradigms for\nsoftware engineering are introduced to automatically resolve software\ndevelopment tasks (e.g., from a given issue to source code). However, existing\nwork is evaluated based on historical benchmark datasets, does not consider\nhuman feedback at each stage of the automated software development process, and\nhas not been deployed in practice. In this paper, we introduce a\nHuman-in-the-loop LLM-based Agents framework (HULA) for software development\nthat allows software engineers to refine and guide LLMs when generating coding\nplans and source code for a given task. We design, implement, and deploy the\nHULA framework into Atlassian JIRA for internal uses. Through a multi-stage\nevaluation of the HULA framework, Atlassian software engineers perceive that\nHULA can minimize the overall development time and effort, especially in\ninitiating a coding plan and writing code for straightforward tasks. On the\nother hand, challenges around code quality are raised to be solved in some\ncases. We draw lessons learned and discuss opportunities for future work, which\nwill pave the way for the advancement of LLM-based agents in software\ndevelopment.\n","authors":["Wannita Takerngsaksiri","Jirat Pasuksmit","Patanamon Thongtanunam","Chakkrit Tantithamthavorn","Ruixiong Zhang","Fan Jiang","Jing Li","Evan Cook","Kun Chen","Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2411.12924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12919v1","updated":"2024-11-19T23:17:09Z","published":"2024-11-19T23:17:09Z","title":"Enhancing Deep Learning-Driven Multi-Coil MRI Reconstruction via\n  Self-Supervised Denoising","summary":"  We examine the effect of incorporating self-supervised denoising as a\npre-processing step for training deep learning (DL) based reconstruction\nmethods on data corrupted by Gaussian noise. K-space data employed for training\nare typically multi-coil and inherently noisy. Although DL-based reconstruction\nmethods trained on fully sampled data can enable high reconstruction quality,\nobtaining large, noise-free datasets is impractical. We leverage Generalized\nStein's Unbiased Risk Estimate (GSURE) for denoising. We evaluate two DL-based\nreconstruction methods: Diffusion Probabilistic Models (DPMs) and Model-Based\nDeep Learning (MoDL). We evaluate the impact of denoising on the performance of\nthese DL-based methods in solving accelerated multi-coil magnetic resonance\nimaging (MRI) reconstruction. The experiments were carried out on T2-weighted\nbrain and fat-suppressed proton-density knee scans. We observed that\nself-supervised denoising enhances the quality and efficiency of MRI\nreconstructions across various scenarios. Specifically, employing denoised\nimages rather than noisy counterparts when training DL networks results in\nlower normalized root mean squared error (NRMSE), higher structural similarity\nindex measure (SSIM) and peak signal-to-noise ratio (PSNR) across different SNR\nlevels, including 32dB, 22dB, and 12dB for T2-weighted brain data, and 24dB,\n14dB, and 4dB for fat-suppressed knee data. Overall, we showed that denoising\nis an essential pre-processing technique capable of improving the efficacy of\nDL-based MRI reconstruction methods under diverse conditions. By refining the\nquality of input data, denoising can enable the training of more effective DL\nnetworks, potentially bypassing the need for noise-free reference MRI scans.\n","authors":["Asad Aali","Marius Arvinte","Sidharth Kumar","Yamin I. Arefeen","Jonathan I. Tamir"],"pdf_url":"https://arxiv.org/pdf/2411.12919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12914v1","updated":"2024-11-19T22:57:40Z","published":"2024-11-19T22:57:40Z","title":"Trojan Cleansing with Neural Collapse","summary":"  Trojan attacks are sophisticated training-time attacks on neural networks\nthat embed backdoor triggers which force the network to produce a specific\noutput on any input which includes the trigger. With the increasing relevance\nof deep networks which are too large to train with personal resources and which\nare trained on data too large to thoroughly audit, these training-time attacks\npose a significant risk. In this work, we connect trojan attacks to Neural\nCollapse, a phenomenon wherein the final feature representations of\nover-parameterized neural networks converge to a simple geometric structure. We\nprovide experimental evidence that trojan attacks disrupt this convergence for\na variety of datasets and architectures. We then use this disruption to design\na lightweight, broadly generalizable mechanism for cleansing trojan attacks\nfrom a wide variety of different network architectures and experimentally\ndemonstrate its efficacy.\n","authors":["Xihe Gu","Greg Fields","Yaman Jandali","Tara Javidi","Farinaz Koushanfar"],"pdf_url":"https://arxiv.org/pdf/2411.12914v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12913v1","updated":"2024-11-19T22:57:38Z","published":"2024-11-19T22:57:38Z","title":"MLDGG: Meta-Learning for Domain Generalization on Graphs","summary":"  Domain generalization on graphs aims to develop models with robust\ngeneralization capabilities, ensuring effective performance on the testing set\ndespite disparities between testing and training distributions. However,\nexisting methods often rely on static encoders directly applied to the target\ndomain, constraining its flexible adaptability. In contrast to conventional\nmethodologies, which concentrate on developing specific generalized models, our\nframework, MLDGG, endeavors to achieve adaptable generalization across diverse\ndomains by integrating cross-multi-domain meta-learning with structure learning\nand semantic identification. Initially, it introduces a generalized structure\nlearner to mitigate the adverse effects of task-unrelated edges, enhancing the\ncomprehensiveness of representations learned by Graph Neural Networks (GNNs)\nwhile capturing shared structural information across domains. Subsequently, a\nrepresentation learner is designed to disentangle domain-invariant semantic and\ndomain-specific variation information in node embedding by leveraging causal\nreasoning for semantic identification, further enhancing generalization. In the\ncontext of meta-learning, meta-parameters for both learners are optimized to\nfacilitate knowledge transfer and enable effective adaptation to graphs through\nfine-tuning within the target domains, where target graphs are inaccessible\nduring training. Our empirical results demonstrate that MLDGG surpasses\nbaseline methods, showcasing its effectiveness in three different distribution\nshift settings.\n","authors":["Qin Tian","Chen Zhao","Minglai Shao","Wenjun Wang","Yujie Lin","Dong Li"],"pdf_url":"https://arxiv.org/pdf/2411.12913v1.pdf","comment":"Accepted in KDD 2025 (research track)"},{"id":"http://arxiv.org/abs/2411.12901v1","updated":"2024-11-19T22:27:53Z","published":"2024-11-19T22:27:53Z","title":"Signformer is all you need: Towards Edge AI for Sign Language","summary":"  Sign language translation, especially in gloss-free paradigm, is confronting\na dilemma of impracticality and unsustainability due to growing\nresource-intensive methodologies. Contemporary state-of-the-arts (SOTAs) have\nsignificantly hinged on pretrained sophiscated backbones such as Large Language\nModels (LLMs), embedding sources, or extensive datasets, inducing considerable\nparametric and computational inefficiency for sustainable use in real-world\nscenario. Despite their success, following this research direction undermines\nthe overarching mission of this domain to create substantial value to bridge\nhard-hearing and common populations. Committing to the prevailing trend of LLM\nand Natural Language Processing (NLP) studies, we pursue a profound essential\nchange in architecture to achieve ground-up improvements without external aid\nfrom pretrained models, prior knowledge transfer, or any NLP strategies\nconsidered not-from-scratch.\n  Introducing Signformer, a from-scratch Feather-Giant transforming the area\ntowards Edge AI that redefines extremities of performance and efficiency with\nLLM-competence and edgy-deployable compactness. In this paper, we present\nnature analysis of sign languages to inform our algorithmic design and deliver\na scalable transformer pipeline with convolution and attention novelty. We\nachieve new 2nd place on leaderboard with a parametric reduction of 467-1807x\nagainst the finests as of 2024 and outcompete almost every other methods in a\nlighter configuration of 0.57 million parameters.\n","authors":["Eta Yang"],"pdf_url":"https://arxiv.org/pdf/2411.12901v1.pdf","comment":"Official Code at: https://github.com/EtaEnding/Signformer/tree/main"},{"id":"http://arxiv.org/abs/2411.12898v1","updated":"2024-11-19T22:26:42Z","published":"2024-11-19T22:26:42Z","title":"Problem-dependent convergence bounds for randomized linear gradient\n  compression","summary":"  In distributed optimization, the communication of model updates can be a\nperformance bottleneck. Consequently, gradient compression has been proposed as\na means of increasing optimization throughput. In general, due to information\nloss, compression introduces a penalty on the number of iterations needed to\nreach a solution. In this work, we investigate how the iteration penalty\ndepends on the interaction between compression and problem structure, in the\ncontext of non-convex stochastic optimization. We focus on linear compression\nschemes, where compression and decompression can be modeled as multiplication\nwith a random matrix. We consider several distributions of matrices, among them\nrandom orthogonal matrices and matrices with random Gaussian entries. We find\nthat in each case, the impact of compression on convergence can be quantified\nin terms of the norm of the Hessian of the objective, using a norm defined by\nthe compression scheme. The analysis reveals that in certain cases, compression\nperformance is related to low-rank structure or other spectral properties of\nthe problem. In these cases, our bounds predict that the penalty introduced by\ncompression is significantly reduced compared to worst-case bounds that only\nconsider the compression level, ignoring problem data. We verify the\ntheoretical findings on several optimization problems, including fine-tuning an\nimage classification model.\n","authors":["Thomas Flynn","Patrick Johnstone","Shinjae Yoo"],"pdf_url":"https://arxiv.org/pdf/2411.12898v1.pdf","comment":"15 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.12897v1","updated":"2024-11-19T22:25:26Z","published":"2024-11-19T22:25:26Z","title":"Tree Species Classification using Machine Learning and 3D Tomographic\n  SAR -- a case study in Northern Europe","summary":"  Tree species classification plays an important role in nature conservation,\nforest inventories, forest management, and the protection of endangered\nspecies. Over the past four decades, remote sensing technologies have been\nextensively utilized for tree species classification, with Synthetic Aperture\nRadar (SAR) emerging as a key technique. In this study, we employed TomoSense,\na 3D tomographic dataset, which utilizes a stack of single-look complex (SLC)\nimages, a byproduct of SAR, captured at different incidence angles to generate\na three-dimensional representation of the terrain. Our research focuses on\nevaluating multiple tabular machine-learning models using the height\ninformation derived from the tomographic image intensities to classify eight\ndistinct tree species. The SLC data and tomographic imagery were analyzed\nacross different polarimetric configurations and geosplit configurations. We\ninvestigated the impact of these variations on classification accuracy,\ncomparing the performance of various tabular machine-learning models and\noptimizing them using Bayesian optimization. Additionally, we incorporated a\nproxy for actual tree height using point cloud data from Light Detection and\nRanging (LiDAR) to provide height statistics associated with the model's\npredictions. This comparison offers insights into the reliability of\ntomographic data in predicting tree species classification based on height.\n","authors":["Colverd Grace","Schade Laura","Takami Jumpei","Bot Karol","Gallego Joseph"],"pdf_url":"https://arxiv.org/pdf/2411.12897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14107v2","updated":"2024-11-19T22:19:12Z","published":"2024-10-18T01:26:04Z","title":"Transfer Learning on Transformers for Building Energy Consumption\n  Forecasting -- A Comparative Study","summary":"  This study investigates the application of Transfer Learning (TL) on\nTransformer architectures to enhance building energy consumption forecasting.\nTransformers are a relatively new deep learning architecture, which has served\nas the foundation for groundbreaking technologies such as ChatGPT. While TL has\nbeen studied in the past, prior studies considered either one data-centric TL\nstrategy or used older deep learning models such as Recurrent Neural Networks\nor Convolutional Neural Networks. Here, we carry out an extensive empirical\nstudy on six different data-centric TL strategies and analyse their performance\nunder varying feature spaces. In addition to the vanilla Transformer\narchitecture, we also experiment with Informer and PatchTST, specifically\ndesigned for time series forecasting. We use 16 datasets from the Building Data\nGenome Project 2 to create building energy consumption forecasting models.\nExperimental results reveal that while TL is generally beneficial, especially\nwhen the target domain has no data, careful selection of the exact TL strategy\nshould be made to gain the maximum benefit. This decision largely depends on\nthe feature space properties such as the recorded weather features. We also\nnote that PatchTST outperforms the other two Transformer variants (vanilla\nTransformer and Informer). Our findings advance the building energy consumption\nforecasting using advanced approaches like TL and Transformer architectures.\n","authors":["Robert Spencer","Surangika Ranathunga","Mikael Boulic"," Andries","van Heerden","Teo Susnjak"],"pdf_url":"https://arxiv.org/pdf/2410.14107v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12892v1","updated":"2024-11-19T22:17:18Z","published":"2024-11-19T22:17:18Z","title":"Selective Attention: Enhancing Transformer through Principled Context\n  Control","summary":"  The attention mechanism within the transformer architecture enables the model\nto weigh and combine tokens based on their relevance to the query. While\nself-attention has enjoyed major success, it notably treats all queries $q$ in\nthe same way by applying the mapping $V^\\top\\text{softmax}(Kq)$, where $V,K$\nare the value and key embeddings respectively. In this work, we argue that this\nuniform treatment hinders the ability to control contextual sparsity and\nrelevance. As a solution, we introduce the $\\textit{Selective Self-Attention}$\n(SSA) layer that augments the softmax nonlinearity with a principled\ntemperature scaling strategy. By controlling temperature, SSA adapts the\ncontextual sparsity of the attention map to the query embedding and its\nposition in the context window. Through theory and experiments, we demonstrate\nthat this alleviates attention dilution, aids the optimization process, and\nenhances the model's ability to control softmax spikiness of individual\nqueries. We also incorporate temperature scaling for value embeddings and show\nthat it boosts the model's ability to suppress irrelevant/noisy tokens.\nNotably, SSA is a lightweight method which introduces less than 0.5% new\nparameters through a weight-sharing strategy and can be fine-tuned on existing\nLLMs. Extensive empirical evaluations demonstrate that SSA-equipped models\nachieve a noticeable and consistent accuracy improvement on language modeling\nbenchmarks.\n","authors":["Xuechen Zhang","Xiangyu Chang","Mingchen Li","Amit Roy-Chowdhury","Jiasi Chen","Samet Oymak"],"pdf_url":"https://arxiv.org/pdf/2411.12892v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12886v1","updated":"2024-11-19T22:08:26Z","published":"2024-11-19T22:08:26Z","title":"NPGPT: Natural Product-Like Compound Generation with GPT-based Chemical\n  Language Models","summary":"  Natural products are substances produced by organisms in nature and often\npossess biological activity and structural diversity. Drug development based on\nnatural products has been common for many years. However, the intricate\nstructures of these compounds present challenges in terms of structure\ndetermination and synthesis, particularly compared to the efficiency of\nhigh-throughput screening of synthetic compounds. In recent years, deep\nlearning-based methods have been applied to the generation of molecules. In\nthis study, we trained chemical language models on a natural product dataset\nand generated natural product-like compounds. The results showed that the\ndistribution of the compounds generated was similar to that of natural\nproducts. We also evaluated the effectiveness of the generated compounds as\ndrug candidates. Our method can be used to explore the vast chemical space and\nreduce the time and cost of drug discovery of natural products.\n","authors":["Koh Sakano","Kairi Furui","Masahito Ohue"],"pdf_url":"https://arxiv.org/pdf/2411.12886v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.13082v2","updated":"2024-11-19T22:02:49Z","published":"2024-04-17T05:56:49Z","title":"Efficient Contextual LLM Cascades through Budget-Constrained Policy\n  Learning","summary":"  Recent successes in natural language processing have led to the proliferation\nof large language models (LLMs) by multiple providers. Each LLM offering has\ndifferent inference accuracy, monetary cost, and latency, and their accuracy\nfurther depends on the exact wording of the question (i.e., the specific\nprompt). At the same time, users often have a limit on monetary budget and\nlatency to answer all their questions, and they do not know which LLMs to\nchoose for each question to meet their accuracy and long term budget\nrequirements. To navigate this rich design space, we propose TREACLE\n($\\underline{T}$hrifty $\\underline{Rea}$soning via $\\underline{C}$ontext-Aware\n$\\underline{L}$LM and Prompt S$\\underline{e}$lection), a reinforcement learning\npolicy that jointly selects the model and prompting scheme while respecting the\nuser's monetary cost and latency constraints. TREACLE uses the problem context,\nincluding question text embeddings (reflecting the type or difficulty of a\nquery) and the response history (reflecting the consistency of previous\nresponses) to make smart decisions. Our evaluations on standard reasoning\ndatasets (GSM8K, CSQA, and LLC) with various LLMs and prompts show that TREACLE\nenables cost savings of up to 85% compared to baselines, while maintaining high\naccuracy. Importantly, it provides the user with the ability to gracefully\ntrade off accuracy for cost.\n","authors":["Xuechen Zhang","Zijian Huang","Ege Onur Taga","Carlee Joe-Wong","Samet Oymak","Jiasi Chen"],"pdf_url":"https://arxiv.org/pdf/2404.13082v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12878v1","updated":"2024-11-19T21:53:06Z","published":"2024-11-19T21:53:06Z","title":"Local Anti-Concentration Class: Logarithmic Regret for Greedy Linear\n  Contextual Bandit","summary":"  We study the performance guarantees of exploration-free greedy algorithms for\nthe linear contextual bandit problem. We introduce a novel condition, named the\n\\textit{Local Anti-Concentration} (LAC) condition, which enables a greedy\nbandit algorithm to achieve provable efficiency. We show that the LAC condition\nis satisfied by a broad class of distributions, including Gaussian,\nexponential, uniform, Cauchy, and Student's~$t$ distributions, along with other\nexponential family distributions and their truncated variants. This\nsignificantly expands the class of distributions under which greedy algorithms\ncan perform efficiently. Under our proposed LAC condition, we prove that the\ncumulative expected regret of the greedy algorithm for the linear contextual\nbandit is bounded by $O(\\operatorname{poly} \\log T)$. Our results establish the\nwidest range of distributions known to date that allow a sublinear regret bound\nfor greedy algorithms, further achieving a sharp poly-logarithmic regret.\n","authors":["Seok-Jin Kim","Min-hwan Oh"],"pdf_url":"https://arxiv.org/pdf/2411.12878v1.pdf","comment":"NeurIPS2024"},{"id":"http://arxiv.org/abs/2409.14622v4","updated":"2024-11-19T21:44:26Z","published":"2024-09-22T23:18:06Z","title":"LatentQGAN: A Hybrid QGAN with Classical Convolutional Autoencoder","summary":"  Quantum machine learning consists in taking advantage of quantum computations\nto generate classical data. A potential application of quantum machine learning\nis to harness the power of quantum computers for generating classical data, a\nprocess essential to a multitude of applications such as enriching training\ndatasets, anomaly detection, and risk management in finance. Given the success\nof Generative Adversarial Networks in classical image generation, the\ndevelopment of its quantum versions has been actively conducted. However,\nexisting implementations on quantum computers often face significant\nchallenges, such as scalability and training convergence issues. To address\nthese issues, we propose LatentQGAN, a novel quantum model that uses a hybrid\nquantum-classical GAN coupled with an autoencoder. Although it was initially\ndesigned for image generation, the LatentQGAN approach holds potential for\nbroader application across various practical data generation tasks.\nExperimental outcomes on both classical simulators and noisy intermediate scale\nquantum computers have demonstrated significant performance enhancements over\nexisting quantum methods, alongside a significant reduction in quantum\nresources overhead.\n","authors":["Alexis Vieloszynski","Soumaya Cherkaoui","Ola Ahmad","Jean-Frédéric Laprade","Oliver Nahman-Lévesque","Abdallah Aaraba","Shengrui Wang"],"pdf_url":"https://arxiv.org/pdf/2409.14622v4.pdf","comment":"This paper was accepted for publication on the 10th IEEE World Forum\n  on Internet of Things (IEEE WFIoT2024), in the session SS - QIoT-1: Special\n  Session - Quantum Internet of Things (QIoT)-1, November 10th, from 14:00 to\n  15:30 EST"},{"id":"http://arxiv.org/abs/2411.12876v1","updated":"2024-11-19T21:44:21Z","published":"2024-11-19T21:44:21Z","title":"Puppet-CNN: Input-Adaptive Convolutional Neural Networks with Model\n  Compression using Ordinary Differential Equation","summary":"  Convolutional Neural Network (CNN) has been applied to more and more\nscenarios due to its excellent performance in many machine learning tasks,\nespecially with deep and complex structures. However, as the network goes\ndeeper, more parameters need to be stored and optimized. Besides, almost all\ncommon CNN models adopt \"train-and-use\" strategy where the structure is\npre-defined and the kernel parameters are fixed after the training with the\nsame structure and set of parameters used for all data without considering the\ncontent complexity. In this paper, we propose a new CNN framework, named as\n$\\textit{Puppet-CNN}$, which contains two modules: a $\\textit{puppet module}$\nand a $\\textit{puppeteer module}$. The puppet module is a CNN model used to\nactually process the input data just like other works, but its depth and\nkernels are generated by the puppeteer module (realized with Ordinary\nDifferential Equation (ODE)) based on the input complexity each time. By\nrecurrently generating kernel parameters in the puppet module, we can take\nadvantage of the dependence among kernels of different convolutional layers to\nsignificantly reduce the size of CNN model by only storing and training the\nparameters of the much smaller puppeteer ODE module. Through experiments on\nseveral datasets, our method has proven to be superior than the traditional\nCNNs on both performance and efficiency. The model size can be reduced more\nthan 10 times.\n","authors":["Yucheng Xing","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2411.12876v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12874v1","updated":"2024-11-19T21:42:57Z","published":"2024-11-19T21:42:57Z","title":"Residual Vision Transformer (ResViT) Based Self-Supervised Learning\n  Model for Brain Tumor Classification","summary":"  Deep learning has proven very promising for interpreting MRI in brain tumor\ndiagnosis. However, deep learning models suffer from a scarcity of brain MRI\ndatasets for effective training. Self-supervised learning (SSL) models provide\ndata-efficient and remarkable solutions to limited dataset problems. Therefore,\nthis paper introduces a generative SSL model for brain tumor classification in\ntwo stages. The first stage is designed to pre-train a Residual Vision\nTransformer (ResViT) model for MRI synthesis as a pretext task. The second\nstage includes fine-tuning a ResViT-based classifier model as a downstream\ntask. Accordingly, we aim to leverage local features via CNN and global\nfeatures via ViT, employing a hybrid CNN-transformer architecture for ResViT in\npretext and downstream tasks. Moreover, synthetic MRI images are utilized to\nbalance the training set. The proposed model performs on public BraTs 2023,\nFigshare, and Kaggle datasets. Furthermore, we compare the proposed model with\nvarious deep learning models, including A-UNet, ResNet-9, pix2pix, pGAN for MRI\nsynthesis, and ConvNeXtTiny, ResNet101, DenseNet12, Residual CNN, ViT for\nclassification. According to the results, the proposed model pretraining on the\nMRI dataset is superior compared to the pretraining on the ImageNet dataset.\nOverall, the proposed model attains the highest accuracy, achieving 90.56% on\nthe BraTs dataset with T1 sequence, 98.53% on the Figshare, and 98.47% on the\nKaggle brain tumor datasets. As a result, the proposed model demonstrates a\nrobust, effective, and successful approach to handling insufficient dataset\nchallenges in MRI analysis by incorporating SSL, fine-tuning, data\naugmentation, and combining CNN and ViT.\n","authors":["Meryem Altin Karagoz","O. Ufuk Nalbantoglu","Geoffrey C. Fox"],"pdf_url":"https://arxiv.org/pdf/2411.12874v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12873v1","updated":"2024-11-19T21:36:04Z","published":"2024-11-19T21:36:04Z","title":"Tensor-Based Foundations of Ordinary Least Squares and Neural Network\n  Regression Models","summary":"  This article introduces a novel approach to the mathematical development of\nOrdinary Least Squares and Neural Network regression models, diverging from\ntraditional methods in current Machine Learning literature. By leveraging\nTensor Analysis and fundamental matrix computations, the theoretical\nfoundations of both models are meticulously detailed and extended to their\ncomplete algorithmic forms. The study culminates in the presentation of three\nalgorithms, including a streamlined version of the Backpropagation Algorithm\nfor Neural Networks, illustrating the benefits of this new mathematical\napproach.\n","authors":["Roberto Dias Algarte"],"pdf_url":"https://arxiv.org/pdf/2411.12873v1.pdf","comment":"16 pages, 3 algorithms"},{"id":"http://arxiv.org/abs/2411.12872v1","updated":"2024-11-19T21:34:50Z","published":"2024-11-19T21:34:50Z","title":"From Text to Pose to Image: Improving Diffusion Model Control and\n  Quality","summary":"  In the last two years, text-to-image diffusion models have become extremely\npopular. As their quality and usage increase, a major concern has been the need\nfor better output control. In addition to prompt engineering, one effective\nmethod to improve the controllability of diffusion models has been to condition\nthem on additional modalities such as image style, depth map, or keypoints.\nThis forms the basis of ControlNets or Adapters. When attempting to apply these\nmethods to control human poses in outputs of text-to-image diffusion models,\ntwo main challenges have arisen. The first challenge is generating poses\nfollowing a wide range of semantic text descriptions, for which previous\nmethods involved searching for a pose within a dataset of (caption, pose)\npairs. The second challenge is conditioning image generation on a specified\npose while keeping both high aesthetic and high pose fidelity. In this article,\nwe fix these two main issues by introducing a text-to-pose (T2P) generative\nmodel alongside a new sampling algorithm, and a new pose adapter that\nincorporates more pose keypoints for higher pose fidelity. Together, these two\nnew state-of-the-art models enable, for the first time, a generative\ntext-to-pose-to-image framework for higher pose control in diffusion models. We\nrelease all models and the code used for the experiments at\nhttps://github.com/clement-bonnet/text-to-pose.\n","authors":["Clément Bonnett","Ariel N. Lee","Franck Wertel","Antoine Tamano","Tanguy Cizain","Pablo Ducru"],"pdf_url":"https://arxiv.org/pdf/2411.12872v1.pdf","comment":"Published at the NeurIPS 2024 Workshop on Compositional Learning:\n  Perspectives, Methods, and Paths Forward"},{"id":"http://arxiv.org/abs/2411.10982v2","updated":"2024-11-19T21:20:47Z","published":"2024-11-17T06:37:54Z","title":"Towards a framework on tabular synthetic data generation: a minimalist\n  approach: theory, use cases, and limitations","summary":"  We propose and study a minimalist approach towards synthetic tabular data\ngeneration. The model consists of a minimalistic unsupervised SparsePCA encoder\n(with contingent clustering step or log transformation to handle nonlinearity)\nand XGboost decoder which is SOTA for structured data regression and\nclassification tasks. We study and contrast the methodologies with\n(variational) autoencoders in several toy low dimensional scenarios to derive\nnecessary intuitions. The framework is applied to high dimensional simulated\ncredit scoring data which parallels real-life financial applications. We\napplied the method to robustness testing to demonstrate practical use cases.\nThe case study result suggests that the method provides an alternative to raw\nand quantile perturbation for model robustness testing. We show that the method\nis simplistic, guarantees interpretability all the way through, does not\nrequire extra tuning and provide unique benefits.\n","authors":["Yueyang Shen","Agus Sudjianto","Arun Prakash R","Anwesha Bhattacharyya","Maorong Rao","Yaqun Wang","Joel Vaughan","Nengfeng Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.10982v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17616v2","updated":"2024-11-19T21:08:20Z","published":"2024-07-24T20:06:12Z","title":"Pretraining a Neural Operator in Lower Dimensions","summary":"  There has recently been increasing attention towards developing foundational\nneural Partial Differential Equation (PDE) solvers and neural operators through\nlarge-scale pretraining. However, unlike vision and language models that make\nuse of abundant and inexpensive (unlabeled) data for pretraining, these neural\nsolvers usually rely on simulated PDE data, which can be costly to obtain,\nespecially for high-dimensional PDEs. In this work, we aim to Pretrain neural\nPDE solvers on Lower Dimensional PDEs (PreLowD) where data collection is the\nleast expensive. We evaluated the effectiveness of this pretraining strategy in\nsimilar PDEs in higher dimensions. We use the Factorized Fourier Neural\nOperator (FFNO) due to having the necessary flexibility to be applied to PDE\ndata of arbitrary spatial dimensions and reuse trained parameters in lower\ndimensions. In addition, our work sheds light on the effect of the fine-tuning\nconfiguration to make the most of this pretraining strategy. Code is available\nat https://github.com/BaratiLab/PreLowD.\n","authors":["AmirPouya Hemmasian","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2407.17616v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2411.12907v1","updated":"2024-11-19T22:51:31Z","published":"2024-11-19T22:51:31Z","title":"Narrative Information Theory","summary":"  We propose an information-theoretic framework to measure narratives,\nproviding a formalism to understand pivotal moments, cliffhangers, and plot\ntwists. This approach offers creatives and AI researchers tools to analyse and\nbenchmark human- and AI-created stories. We illustrate our method in TV shows,\nshowing its ability to quantify narrative complexity and emotional dynamics\nacross genres. We discuss applications in media and in human-in-the-loop\ngenerative AI storytelling.\n","authors":["Lion Schulz","Miguel Patrício","Daan Odijk"],"pdf_url":"https://arxiv.org/pdf/2411.12907v1.pdf","comment":"To be published in NeurIPS 2024 Workshop on Creativity & Generative\n  AI. 7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.12825v1","updated":"2024-11-19T19:22:24Z","published":"2024-11-19T19:22:24Z","title":"TopoCode: Topologically Informed Error Detection and Correction in\n  Communication Systems","summary":"  Traditional error detection and correction codes focus on bit-level fidelity,\nwhich is insufficient for emerging technologies like eXtended Reality (XR) and\nholographic communications requiring high-data-rate, low-latency systems.\nBit-level metrics cannot comprehensively evaluate Quality-of-Service (QoS) in\nthese scenarios. This letter proposes TopoCode which leverages Topological Data\nAnalysis (TDA) and persistent homology to encode topological information for\nmessage-level error detection and correction. It introduces minimal redundancy\nwhile enabling effective data reconstruction, especially in low Signal-to-Noise\nRatio (SNR) conditions. TopoCode offers a promising approach to meet the\ndemands of next-generation communication systems prioritizing semantic accuracy\nand message-level integrity.\n","authors":["Hongzhi Guo"],"pdf_url":"https://arxiv.org/pdf/2411.12825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12293v1","updated":"2024-11-19T07:26:30Z","published":"2024-11-19T07:26:30Z","title":"Generative Timelines for Instructed Visual Assembly","summary":"  The objective of this work is to manipulate visual timelines (e.g. a video)\nthrough natural language instructions, making complex timeline editing tasks\naccessible to non-expert or potentially even disabled users. We call this task\nInstructed visual assembly. This task is challenging as it requires (i)\nidentifying relevant visual content in the input timeline as well as retrieving\nrelevant visual content in a given input (video) collection, (ii) understanding\nthe input natural language instruction, and (iii) performing the desired edits\nof the input visual timeline to produce an output timeline. To address these\nchallenges, we propose the Timeline Assembler, a generative model trained to\nperform instructed visual assembly tasks. The contributions of this work are\nthree-fold. First, we develop a large multimodal language model, which is\ndesigned to process visual content, compactly represent timelines and\naccurately interpret timeline editing instructions. Second, we introduce a\nnovel method for automatically generating datasets for visual assembly tasks,\nenabling efficient training of our model without the need for human-labeled\ndata. Third, we validate our approach by creating two novel datasets for image\nand video assembly, demonstrating that the Timeline Assembler substantially\noutperforms established baseline models, including the recent GPT-4o, in\naccurately executing complex assembly instructions across various real-world\ninspired scenarios.\n","authors":["Alejandro Pardo","Jui-Hsien Wang","Bernard Ghanem","Josef Sivic","Bryan Russell","Fabian Caba Heilbron"],"pdf_url":"https://arxiv.org/pdf/2411.12293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12776v1","updated":"2024-11-19T07:18:38Z","published":"2024-11-19T07:18:38Z","title":"Cross-Layer Encrypted Semantic Communication Framework for Panoramic\n  Video Transmission","summary":"  In this paper, we propose a cross-layer encrypted semantic communication\n(CLESC) framework for panoramic video transmission, incorporating feature\nextraction, encoding, encryption, cyclic redundancy check (CRC), and\nretransmission processes to achieve compatibility between semantic\ncommunication and traditional communication systems. Additionally, we propose\nan adaptive cross-layer transmission mechanism that dynamically adjusts CRC,\nchannel coding, and retransmission schemes based on the importance of semantic\ninformation. This ensures that important information is prioritized under poor\ntransmission conditions. To verify the aforementioned framework, we also design\nan end-to-end adaptive panoramic video semantic transmission (APVST) network\nthat leverages a deep joint source-channel coding (Deep JSCC) structure and\nattention mechanism, integrated with a latitude adaptive module that\nfacilitates adaptive semantic feature extraction and variable-length encoding\nof panoramic videos. The proposed CLESC is also applicable to the transmission\nof other modal data. Simulation results demonstrate that the proposed CLESC\neffectively achieves compatibility and adaptation between semantic\ncommunication and traditional communication systems, improving both\ntransmission efficiency and channel adaptability. Compared to traditional\ncross-layer transmission schemes, the CLESC framework can reduce bandwidth\nconsumption by 85% while showing significant advantages under low\nsignal-to-noise ratio (SNR) conditions.\n","authors":["Haixiao Gao","Mengying Sun","Xiaodong Xu","Bingxuan Xu","Shujun Han","Bizhu Wang","Sheng Jiang","Chen Dong","Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.12776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16334v3","updated":"2024-11-19T06:12:47Z","published":"2023-10-25T03:30:37Z","title":"Structured Multi-Track Accompaniment Arrangement via Style Prior\n  Modelling","summary":"  In the realm of music AI, arranging rich and structured multi-track\naccompaniments from a simple lead sheet presents significant challenges. Such\nchallenges include maintaining track cohesion, ensuring long-term coherence,\nand optimizing computational efficiency. In this paper, we introduce a novel\nsystem that leverages prior modelling over disentangled style factors to\naddress these challenges. Our method presents a two-stage process: initially, a\npiano arrangement is derived from the lead sheet by retrieving piano texture\nstyles; subsequently, a multi-track orchestration is generated by infusing\norchestral function styles into the piano arrangement. Our key design is the\nuse of vector quantization and a unique multi-stream Transformer to model the\nlong-term flow of the orchestration style, which enables flexible,\ncontrollable, and structured music generation. Experiments show that by\nfactorizing the arrangement task into interpretable sub-stages, our approach\nenhances generative capacity while improving efficiency. Additionally, our\nsystem supports a variety of music genres and provides style control at\ndifferent composition hierarchies. We further show that our system achieves\nsuperior coherence, structure, and overall arrangement quality compared to\nexisting baselines.\n","authors":["Jingwei Zhao","Gus Xia","Ziyu Wang","Ye Wang"],"pdf_url":"https://arxiv.org/pdf/2310.16334v3.pdf","comment":"Accepted by NeurIPS 2024; significance test updated with Bonferroni\n  correction"},{"id":"http://arxiv.org/abs/2411.12197v1","updated":"2024-11-19T03:29:18Z","published":"2024-11-19T03:29:18Z","title":"MTFusion: Reconstructing Any 3D Object from Single Image Using\n  Multi-word Textual Inversion","summary":"  Reconstructing 3D models from single-view images is a long-standing problem\nin computer vision. The latest advances for single-image 3D reconstruction\nextract a textual description from the input image and further utilize it to\nsynthesize 3D models. However, existing methods focus on capturing a single key\nattribute of the image (e.g., object type, artistic style) and fail to consider\nthe multi-perspective information required for accurate 3D reconstruction, such\nas object shape and material properties. Besides, the reliance on Neural\nRadiance Fields hinders their ability to reconstruct intricate surfaces and\ntexture details. In this work, we propose MTFusion, which leverages both image\ndata and textual descriptions for high-fidelity 3D reconstruction. Our approach\nconsists of two stages. First, we adopt a novel multi-word textual inversion\ntechnique to extract a detailed text description capturing the image's\ncharacteristics. Then, we use this description and the image to generate a 3D\nmodel with FlexiCubes. Additionally, MTFusion enhances FlexiCubes by employing\na special decoder network for Signed Distance Functions, leading to faster\ntraining and finer surface representation. Extensive evaluations demonstrate\nthat our MTFusion surpasses existing image-to-3D methods on a wide range of\nsynthetic and real-world images. Furthermore, the ablation study proves the\neffectiveness of our network designs.\n","authors":["Yu Liu","Ruowei Wang","Jiaqi Li","Zixiang Xu","Qijun Zhao"],"pdf_url":"https://arxiv.org/pdf/2411.12197v1.pdf","comment":"PRCV 2024"}]},"2024-11-18T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2411.12118v1","updated":"2024-11-18T23:12:13Z","published":"2024-11-18T23:12:13Z","title":"Mechanism and Emergence of Stacked Attention Heads in Multi-Layer\n  Transformers","summary":"  In this paper, I introduce the retrieval problem, a simple reasoning task\nthat can be solved only by transformers with a minimum number of layers. The\ntask has an adjustable difficulty that can further increase the required number\nof layers to any arbitrary value. I demonstrate that large language models can\nsolve the task under different prompting formulations without any fine-tuning.\nTo understand how transformers solve the retrieval problem, I train several\ntransformers on a minimal formulation. I find that successful learning occurs\nonly under the presence of an implicit curriculum. I uncover the learned\nmechanisms by studying the attention maps in the trained transformers. I also\nstudy the training process, uncovering that attention heads always emerge in a\nspecific sequence.\n","authors":["Tiberiu Musat"],"pdf_url":"https://arxiv.org/pdf/2411.12118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12074v1","updated":"2024-11-18T21:36:44Z","published":"2024-11-18T21:36:44Z","title":"Mitigating Gender Bias in Contextual Word Embeddings","summary":"  Word embeddings have been shown to produce remarkable results in tackling a\nvast majority of NLP related tasks. Unfortunately, word embeddings also capture\nthe stereotypical biases that are prevalent in society, affecting the\npredictive performance of the embeddings when used in downstream tasks. While\nvarious techniques have been proposed \\cite{bolukbasi2016man, zhao2018learning}\nand criticized\\cite{gonen2019lipstick} for static embeddings, very little work\nhas focused on mitigating bias in contextual embeddings. In this paper, we\npropose a novel objective function for MLM(Masked-Language Modeling) which\nlargely mitigates the gender bias in contextual embeddings and also preserves\nthe performance for downstream tasks. Since previous works on measuring bias in\ncontextual embeddings lack in normative reasoning, we also propose novel\nevaluation metrics that are straight-forward and aligned with our motivations\nin debiasing. We also propose new methods for debiasing static embeddings and\nprovide empirical proof via extensive analysis and experiments, as to why the\nmain source of bias in static embeddings stems from the presence of\nstereotypical names rather than gendered words themselves. All experiments and\nembeddings studied are in English, unless otherwise\nspecified.\\citep{bender2011achieving}.\n","authors":["Navya Yarrabelly","Vinay Damodaran","Feng-Guang Su"],"pdf_url":"https://arxiv.org/pdf/2411.12074v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12056v1","updated":"2024-11-18T20:54:17Z","published":"2024-11-18T20:54:17Z","title":"Benchmarking pre-trained text embedding models in aligning built asset\n  information","summary":"  Accurate mapping of the built asset information to established data\nclassification systems and taxonomies is crucial for effective asset\nmanagement, whether for compliance at project handover or ad-hoc data\nintegration scenarios. Due to the complex nature of built asset data, which\npredominantly comprises technical text elements, this process remains largely\nmanual and reliant on domain expert input. Recent breakthroughs in contextual\ntext representation learning (text embedding), particularly through pre-trained\nlarge language models, offer promising approaches that can facilitate the\nautomation of cross-mapping of the built asset data. However, no comprehensive\nevaluation has yet been conducted to assess these models' ability to\neffectively represent the complex semantics specific to built asset technical\nterminology. This study presents a comparative benchmark of state-of-the-art\ntext embedding models to evaluate their effectiveness in aligning built asset\ninformation with domain-specific technical concepts. Our proposed datasets are\nderived from two renowned built asset data classification dictionaries. The\nresults of our benchmarking across six proposed datasets, covering three tasks\nof clustering, retrieval, and reranking, highlight the need for future research\non domain adaptation techniques. The benchmarking resources are published as an\nopen-source library, which will be maintained and extended to support future\nevaluations in this field.\n","authors":["Mehrzad Shahinmoghadam","Ali Motamedi"],"pdf_url":"https://arxiv.org/pdf/2411.12056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05981v4","updated":"2024-11-18T20:18:32Z","published":"2024-06-10T02:47:55Z","title":"ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training\n  Multiplication-Less Reparameterization","summary":"  Large language models (LLMs) have shown impressive performance on language\ntasks but face challenges when deployed on resource-constrained devices due to\ntheir extensive parameters and reliance on dense multiplications, resulting in\nhigh memory demands and latency bottlenecks. Shift-and-add reparameterization\noffers a promising solution by replacing costly multiplications with\nhardware-friendly primitives in both the attention and multi-layer perceptron\n(MLP) layers of an LLM. However, current reparameterization techniques require\ntraining from scratch or full parameter fine-tuning to restore accuracy, which\nis resource-intensive for LLMs. To address this, we propose accelerating\npretrained LLMs through post-training shift-and-add reparameterization,\ncreating efficient multiplication-free models, dubbed ShiftAddLLM.\nSpecifically, we quantize each weight matrix into binary matrices paired with\ngroup-wise scaling factors. The associated multiplications are reparameterized\ninto (1) shifts between activations and scaling factors and (2) queries and\nadds according to the binary matrices. To reduce accuracy loss, we present a\nmulti-objective optimization method to minimize both weight and output\nactivation reparameterization errors. Additionally, based on varying\nsensitivity across layers to reparameterization, we develop an automated bit\nallocation strategy to further reduce memory usage and latency. Experiments on\nfive LLM families and eight tasks consistently validate the effectiveness of\nShiftAddLLM, achieving average perplexity improvements of 5.6 and 22.7 points\nat comparable or lower latency compared to the most competitive quantized LLMs\nat 3 and 2 bits, respectively, and more than 80% memory and energy reductions\nover the original LLMs. Codes and models are available at\nhttps://github.com/GATECH-EIC/ShiftAddLLM.\n","authors":["Haoran You","Yipin Guo","Yichao Fu","Wei Zhou","Huihong Shi","Xiaofan Zhang","Souvik Kundu","Amir Yazdanbakhsh","Yingyan Celine Lin"],"pdf_url":"https://arxiv.org/pdf/2406.05981v4.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.12000v1","updated":"2024-11-18T19:36:26Z","published":"2024-11-18T19:36:26Z","title":"ByteScience: Bridging Unstructured Scientific Literature and Structured\n  Data with Auto Fine-tuned Large Language Model in Token Granularity","summary":"  Natural Language Processing (NLP) is widely used to supply summarization\nability from long context to structured information. However, extracting\nstructured knowledge from scientific text by NLP models remains a challenge\nbecause of its domain-specific nature to complex data preprocessing and the\ngranularity of multi-layered device-level information. To address this, we\nintroduce ByteScience, a non-profit cloud-based auto fine-tuned Large Language\nModel (LLM) platform, which is designed to extract structured scientific data\nand synthesize new scientific knowledge from vast scientific corpora. The\nplatform capitalizes on DARWIN, an open-source, fine-tuned LLM dedicated to\nnatural science. The platform was built on Amazon Web Services (AWS) and\nprovides an automated, user-friendly workflow for custom model development and\ndata extraction. The platform achieves remarkable accuracy with only a small\namount of well-annotated articles. This innovative tool streamlines the\ntransition from the science literature to structured knowledge and data and\nbenefits the advancements in natural informatics.\n","authors":["Tong Xie","Hanzhi Zhang","Shaozhou Wang","Yuwei Wan","Imran Razzak","Chunyu Kit","Wenjie Zhangand Bram Hoex"],"pdf_url":"https://arxiv.org/pdf/2411.12000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11984v1","updated":"2024-11-18T19:14:36Z","published":"2024-11-18T19:14:36Z","title":"Understanding Chain-of-Thought in LLMs through Information Theory","summary":"  Large Language Models (LLMs) have shown impressive performance in complex\nreasoning tasks through Chain-of-Thought (CoT) reasoning, allowing models to\nbreak down problems into manageable sub-tasks. However, existing CoT evaluation\ntechniques either require annotated CoT data or fall short in accurately\nassessing intermediate reasoning steps, leading to high rates of false\npositives. In this paper, we formalize CoT reasoning in LLMs through an\ninformation-theoretic lens. Specifically, our framework quantifies the\n`information gain' at each reasoning step, enabling the identification of\nfailure modes in LLMs without the need for expensive annotated datasets. We\ndemonstrate the efficacy of our approach through extensive experiments on toy\nand GSM-8K data, where it significantly outperforms existing outcome-based\nmethods by providing more accurate insights into model performance on\nindividual tasks.\n","authors":["Jean-Francois Ton","Muhammad Faaiz Taufiq","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2411.11984v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11843v1","updated":"2024-11-18T18:59:15Z","published":"2024-11-18T18:59:15Z","title":"Bi-Mamba: Towards Accurate 1-Bit State Space Models","summary":"  The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs.\n","authors":["Shengkun Tang","Liqun Ma","Haonan Li","Mingjie Sun","Zhiqiang Shen"],"pdf_url":"https://arxiv.org/pdf/2411.11843v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11829v1","updated":"2024-11-18T18:48:13Z","published":"2024-11-18T18:48:13Z","title":"Tackling prediction tasks in relational databases with LLMs","summary":"  Though large language models (LLMs) have demonstrated exceptional performance\nacross numerous problems, their application to predictive tasks in relational\ndatabases remains largely unexplored. In this work, we address the notion that\nLLMs cannot yield satisfactory results on relational databases due to their\ninterconnected tables, complex relationships, and heterogeneous data types.\nUsing the recently introduced RelBench benchmark, we demonstrate that even a\nstraightforward application of LLMs achieves competitive performance on these\ntasks. These findings establish LLMs as a promising new baseline for ML on\nrelational databases and encourage further research in this direction.\n","authors":["Marek Wydmuch","Łukasz Borchmann","Filip Graliński"],"pdf_url":"https://arxiv.org/pdf/2411.11829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22587v2","updated":"2024-11-18T18:42:44Z","published":"2024-10-29T23:00:05Z","title":"Toxicity of the Commons: Curating Open-Source Pre-Training Data","summary":"  Open-source large language models are becoming increasingly available and\npopular among researchers and practitioners. While significant progress has\nbeen made on open-weight models, open training data is a practice yet to be\nadopted by the leading open-weight models creators. At the same time, there\nresearchers are working to make language models safer. We propose a data\ncuration pipeline to reduce harmful outputs by models trained on public domain\ndata. There are unique challenges to working with public domain data, as these\nsources differ from web text in both form and content. Many sources are\nhistorical documents and are the result of Optical Character Recognition (OCR).\nConsequently, current state-of-the-art approaches to toxicity filtering are\noften infeasible or inappropriate for open data models. In this paper, we\nintroduce a new fully open-source pipeline for open-data toxicity filtering.\nOur contributions are threefold. We create a custom training dataset,\nToxicCommons, which is composed of texts which have been classified across five\ndifferent dimensions (racial/origin-based, gender/sex-based, religious,\nability-based discrimination, and violence). We use this dataset to train a\ncustom classifier, Celadon, that can be used to detect toxic content in open\ndata more efficiently at a larger scale. Finally, we describe the balanced\napproach to content filtration that optimizes safety filtering with respect to\nthe filtered data available for training.\n","authors":["Catherine Arnett","Eliot Jones","Ivan P. Yamshchikov","Pierre-Carl Langlais"],"pdf_url":"https://arxiv.org/pdf/2410.22587v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00024v2","updated":"2024-11-18T18:41:08Z","published":"2024-10-28T22:30:06Z","title":"A Perspective for Adapting Generalist AI to Specialized Medical AI\n  Applications and Their Challenges","summary":"  The integration of Large Language Models (LLMs) into medical applications has\nsparked widespread interest across the healthcare industry, from drug discovery\nand development to clinical decision support, assisting telemedicine, medical\ndevices, and healthcare insurance applications. This perspective paper aims to\ndiscuss the inner workings of building LLM-powered medical AI applications and\nintroduces a comprehensive framework for their development. We review existing\nliterature and outline the unique challenges of applying LLMs in specialized\nmedical contexts. Additionally, we introduce a three-step framework to organize\nmedical LLM research activities: 1) Modeling: breaking down complex medical\nworkflows into manageable steps for developing medical-specific models; 2)\nOptimization: optimizing the model performance with crafted prompts and\nintegrating external knowledge and tools, and 3) System engineering:\ndecomposing complex tasks into subtasks and leveraging human expertise for\nbuilding medical AI applications. Furthermore, we offer a detailed use case\nplaybook that describes various LLM-powered medical AI applications, such as\noptimizing clinical trial design, enhancing clinical decision support, and\nadvancing medical imaging analysis. Finally, we discuss various challenges and\nconsiderations for building medical AI applications with LLMs, such as handling\nhallucination issues, data ownership and compliance, privacy, intellectual\nproperty considerations, compute cost, sustainability issues, and responsible\nAI requirements.\n","authors":["Zifeng Wang","Hanyin Wang","Benjamin Danek","Ying Li","Christina Mack","Hoifung Poon","Yajuan Wang","Pranav Rajpurkar","Jimeng Sun"],"pdf_url":"https://arxiv.org/pdf/2411.00024v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.04254v2","updated":"2024-11-18T18:35:06Z","published":"2024-04-05T17:58:52Z","title":"Watermark-based Detection and Attribution of AI-Generated Content","summary":"  Several companies have deployed watermark-based detection to identify\nAI-generated content. However, attribution--the ability to trace back to the\nuser of a generative AI (GenAI) service who created a given piece of\nAI-generated content--remains largely unexplored despite its growing\nimportance. In this work, we aim to bridge this gap by conducting the first\nsystematic study on watermark-based, user-level attribution of AI-generated\ncontent. Our key idea is to assign a unique watermark to each user of the GenAI\nservice and embed this watermark into the AI-generated content created by that\nuser. Attribution is then performed by identifying the user whose watermark\nbest matches the one extracted from the given content. This approach, however,\nfaces a key challenge: How should watermarks be selected for users to maximize\nattribution performance? To address the challenge, we first theoretically\nderive lower bounds on detection and attribution performance through rigorous\nprobabilistic analysis for any given set of user watermarks. Then, we select\nwatermarks for users to maximize these lower bounds, thereby optimizing\ndetection and attribution performance. Our theoretical and empirical results\nshow that watermark-based attribution inherits both the accuracy and\n(non-)robustness properties of the underlying watermark. Specifically,\nattribution remains highly accurate when the watermarked AI-generated content\nis either not post-processed or subjected to common post-processing such as\nJPEG compression, as well as black-box adversarial post-processing with limited\nquery budgets.\n","authors":["Zhengyuan Jiang","Moyang Guo","Yuepeng Hu","Neil Zhenqiang Gong"],"pdf_url":"https://arxiv.org/pdf/2404.04254v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11770v1","updated":"2024-11-18T17:50:34Z","published":"2024-11-18T17:50:34Z","title":"CNMBert: A Model For Hanyu Pinyin Abbreviation to Character Conversion\n  Task","summary":"  The task of converting Hanyu Pinyin abbreviations to Chinese characters\nrepresents a significant branch within the domain of Chinese Spelling\nCorrection (CSC). This task is typically one of text-length alignment, however,\ndue to the limited informational content in pinyin abbreviations, achieving\naccurate conversion is challenging. In this paper, we propose CNMBert which\nstands for zh-CN Pinyin Multi-mask Bert Model as a solution to this issue.\nCNMBert surpasses few-shot GPT models, achieving a 59.63% MRR on a\n10,424-sample Hanyu Pinyin abbreviation test dataset.\n","authors":["Zishuo Feng","Feng Cao"],"pdf_url":"https://arxiv.org/pdf/2411.11770v1.pdf","comment":"9 pages, 2figures"},{"id":"http://arxiv.org/abs/2411.11767v1","updated":"2024-11-18T17:46:32Z","published":"2024-11-18T17:46:32Z","title":"Drowning in Documents: Consequences of Scaling Reranker Inference","summary":"  Rerankers, typically cross-encoders, are often used to re-score the documents\nretrieved by cheaper initial IR systems. This is because, though expensive,\nrerankers are assumed to be more effective. We challenge this assumption by\nmeasuring reranker performance for full retrieval, not just re-scoring\nfirst-stage retrieval. Our experiments reveal a surprising trend: the best\nexisting rerankers provide diminishing returns when scoring progressively more\ndocuments and actually degrade quality beyond a certain limit. In fact, in this\nsetting, rerankers can frequently assign high scores to documents with no\nlexical or semantic overlap with the query. We hope that our findings will spur\nfuture research to improve reranking.\n","authors":["Mathew Jacob","Erik Lindgren","Matei Zaharia","Michael Carbin","Omar Khattab","Andrew Drozdov"],"pdf_url":"https://arxiv.org/pdf/2411.11767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11758v1","updated":"2024-11-18T17:37:10Z","published":"2024-11-18T17:37:10Z","title":"The Power of Many: Multi-Agent Multimodal Models for Cultural Image\n  Captioning","summary":"  Large Multimodal Models (LMMs) exhibit impressive performance across various\nmultimodal tasks. However, their effectiveness in cross-cultural contexts\nremains limited due to the predominantly Western-centric nature of most data\nand models. Conversely, multi-agent models have shown significant capability in\nsolving complex tasks. Our study evaluates the collective performance of LMMs\nin a multi-agent interaction setting for the novel task of cultural image\ncaptioning. Our contributions are as follows: (1) We introduce MosAIC, a\nMulti-Agent framework to enhance cross-cultural Image Captioning using LMMs\nwith distinct cultural personas; (2) We provide a dataset of culturally\nenriched image captions in English for images from China, India, and Romania\nacross three datasets: GeoDE, GD-VCR, CVQA; (3) We propose a culture-adaptable\nmetric for evaluating cultural information within image captions; and (4) We\nshow that the multi-agent interaction outperforms single-agent models across\ndifferent metrics, and offer valuable insights for future research. Our dataset\nand models can be accessed at https://github.com/MichiganNLP/MosAIC.\n","authors":["Longju Bai","Angana Borah","Oana Ignat","Rada Mihalcea"],"pdf_url":"https://arxiv.org/pdf/2411.11758v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06153v2","updated":"2024-11-18T17:25:15Z","published":"2024-10-08T15:52:42Z","title":"AgentSquare: Automatic LLM Agent Search in Modular Design Space","summary":"  Recent advancements in Large Language Models (LLMs) have led to a rapid\ngrowth of agentic systems capable of handling a wide range of complex tasks.\nHowever, current research largely relies on manual, task-specific design,\nlimiting their adaptability to novel tasks. In this paper, we introduce a new\nresearch problem: Modularized LLM Agent Search (MoLAS). We propose a modular\ndesign space that abstracts existing LLM agent designs into four fundamental\nmodules with uniform IO interface: Planning, Reasoning, Tool Use, and Memory.\nBuilding on this design space, we present a novel LLM agent search framework\ncalled AgentSquare, which introduces two core mechanisms, i.e., module\nevolution and recombination, to efficiently search for optimized LLM agents. To\nfurther accelerate the process, we design a performance predictor that uses\nin-context surrogate models to skip unpromising agent designs. Extensive\nexperiments across six benchmarks, covering the diverse scenarios of web,\nembodied, tool use and game applications, show that AgentSquare substantially\noutperforms hand-crafted agents, achieving an average performance gain of 17.2%\nagainst best-known human designs. Moreover, AgentSquare can generate\ninterpretable design insights, enabling a deeper understanding of agentic\narchitecture and its impact on task performance. We believe that the modular\ndesign space and AgentSquare search framework offer a platform for fully\nexploiting the potential of prior successful designs and consolidating the\ncollective efforts of research community. Code repo is available at\nhttps://github.com/tsinghua-fib-lab/AgentSquare.\n","authors":["Yu Shang","Yu Li","Keyu Zhao","Likai Ma","Jiahe Liu","Fengli Xu","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2410.06153v2.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2411.11736v1","updated":"2024-11-18T17:03:30Z","published":"2024-11-18T17:03:30Z","title":"Advacheck at GenAI Detection Task 1: AI Detection Powered by\n  Domain-Aware Multi-Tasking","summary":"  The paper describes a system designed by Advacheck team to recognise\nmachine-generated and human-written texts in the monolingual subtask of GenAI\nDetection Task 1 competition. Our developed system is a multi-task architecture\nwith shared Transformer Encoder between several classification heads. One head\nis responsible for binary classification between human-written and\nmachine-generated texts, while the other heads are auxiliary multiclass\nclassifiers for texts of different domains from particular datasets. As\nmulticlass heads were trained to distinguish the domains presented in the data,\nthey provide a better understanding of the samples. This approach led us to\nachieve the first place in the official ranking with 83.07% macro F1-score on\nthe test set and bypass the baseline by 10%. We further study obtained system\nthrough ablation, error and representation analyses, finding that multi-task\nlearning outperforms single-task mode and simultaneous tasks form a cluster\nstructure in embeddings space.\n","authors":["German Gritsai","Anastasia Voznyuk","Ildar Khabutdinov","Andrey Grabovoy"],"pdf_url":"https://arxiv.org/pdf/2411.11736v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2411.12121v1","updated":"2024-11-18T23:18:55Z","published":"2024-11-18T23:18:55Z","title":"Metamorphic Evaluation of ChatGPT as a Recommender System","summary":"  With the rise of Large Language Models (LLMs) such as ChatGPT, researchers\nhave been working on how to utilize the LLMs for better recommendations.\nHowever, although LLMs exhibit black-box and probabilistic characteristics\n(meaning their internal working is not visible), the evaluation framework used\nfor assessing these LLM-based recommender systems (RS) are the same as those\nused for traditional recommender systems. To address this gap, we introduce the\nmetamorphic testing for the evaluation of GPT-based RS. This testing technique\ninvolves defining of metamorphic relations (MRs) between the inputs and\nchecking if the relationship has been satisfied in the outputs. Specifically,\nwe examined the MRs from both RS and LLMs perspectives, including rating\nmultiplication/shifting in RS and adding spaces/randomness in the LLMs prompt\nvia prompt perturbation. Similarity metrics (e.g. Kendall tau and Ranking\nBiased Overlap(RBO)) are deployed to measure whether the relationship has been\nsatisfied in the outputs of MRs. The experiment results on MovieLens dataset\nwith GPT3.5 show that lower similarity are obtained in terms of Kendall $\\tau$\nand RBO, which concludes that there is a need of a comprehensive evaluation of\nthe LLM-based RS in addition to the existing evaluation metrics used for\ntraditional recommender systems.\n","authors":["Madhurima Khirbat","Yongli Ren","Pablo Castells","Mark Sanderson"],"pdf_url":"https://arxiv.org/pdf/2411.12121v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12092v1","updated":"2024-11-18T22:04:41Z","published":"2024-11-18T22:04:41Z","title":"Preprocessing for lessening the influence of eye artifacts in eeg\n  analysis","summary":"  We dealt with the problem of artifacts in eeg signals in relation to the\nusage of lengthy trials. Specifically, we considered eye artifacts found in eeg\nsignals,their influence in the analysis of the data and alternatives to\ndiminish their impact on later studies of brain activity on lengthy tasks. We\nproposed a scheme of partial rejection on independent signal components,\nprovidesd a method to extract eeg signal components with diministhed influence\nof eye artifacts, and assess the importance of using artifact free signal\nexcerpts to extract signal components in order to analyze brain activity in a\nmusical context.\n","authors":["Alejandro Villena","Lorenzo J. Tardon","Isabel Barbancho","Ana M. Barbancho","Elvira Brattico","Niels T. Haumann"],"pdf_url":"https://arxiv.org/pdf/2411.12092v1.pdf","comment":"16 pages, journal article"},{"id":"http://arxiv.org/abs/2411.12064v1","updated":"2024-11-18T21:10:14Z","published":"2024-11-18T21:10:14Z","title":"TSPRank: Bridging Pairwise and Listwise Methods with a Bilinear\n  Travelling Salesman Model","summary":"  Traditional Learning-To-Rank (LETOR) approaches, including pairwise methods\nlike RankNet and LambdaMART, often fall short by solely focusing on pairwise\ncomparisons, leading to sub-optimal global rankings. Conversely, deep learning\nbased listwise methods, while aiming to optimise entire lists, require complex\ntuning and yield only marginal improvements over robust pairwise models. To\novercome these limitations, we introduce Travelling Salesman Problem Rank\n(TSPRank), a hybrid pairwise-listwise ranking method. TSPRank reframes the\nranking problem as a Travelling Salesman Problem (TSP), a well-known\ncombinatorial optimisation challenge that has been extensively studied for its\nnumerous solution algorithms and applications. This approach enables the\nmodelling of pairwise relationships and leverages combinatorial optimisation to\ndetermine the listwise ranking. This approach can be directly integrated as an\nadditional component into embeddings generated by existing backbone models to\nenhance ranking performance. Our extensive experiments across three backbone\nmodels on diverse tasks, including stock ranking, information retrieval, and\nhistorical events ordering, demonstrate that TSPRank significantly outperforms\nboth pure pairwise and listwise methods. Our qualitative analysis reveals that\nTSPRank's main advantage over existing methods is its ability to harness global\ninformation better while ranking. TSPRank's robustness and superior performance\nacross different domains highlight its potential as a versatile and effective\nLETOR solution. The code and preprocessed data are available at\nhttps://github.com/waylonli/TSPRank-KDD2025.\n","authors":["Weixian Waylon Li","Yftah Ziser","Yifei Xie","Shay B. Cohen","Tiejun Ma"],"pdf_url":"https://arxiv.org/pdf/2411.12064v1.pdf","comment":"Accepted to ACM SIGKDD 2025 Research Track"},{"id":"http://arxiv.org/abs/2411.12056v1","updated":"2024-11-18T20:54:17Z","published":"2024-11-18T20:54:17Z","title":"Benchmarking pre-trained text embedding models in aligning built asset\n  information","summary":"  Accurate mapping of the built asset information to established data\nclassification systems and taxonomies is crucial for effective asset\nmanagement, whether for compliance at project handover or ad-hoc data\nintegration scenarios. Due to the complex nature of built asset data, which\npredominantly comprises technical text elements, this process remains largely\nmanual and reliant on domain expert input. Recent breakthroughs in contextual\ntext representation learning (text embedding), particularly through pre-trained\nlarge language models, offer promising approaches that can facilitate the\nautomation of cross-mapping of the built asset data. However, no comprehensive\nevaluation has yet been conducted to assess these models' ability to\neffectively represent the complex semantics specific to built asset technical\nterminology. This study presents a comparative benchmark of state-of-the-art\ntext embedding models to evaluate their effectiveness in aligning built asset\ninformation with domain-specific technical concepts. Our proposed datasets are\nderived from two renowned built asset data classification dictionaries. The\nresults of our benchmarking across six proposed datasets, covering three tasks\nof clustering, retrieval, and reranking, highlight the need for future research\non domain adaptation techniques. The benchmarking resources are published as an\nopen-source library, which will be maintained and extended to support future\nevaluations in this field.\n","authors":["Mehrzad Shahinmoghadam","Ali Motamedi"],"pdf_url":"https://arxiv.org/pdf/2411.12056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11767v1","updated":"2024-11-18T17:46:32Z","published":"2024-11-18T17:46:32Z","title":"Drowning in Documents: Consequences of Scaling Reranker Inference","summary":"  Rerankers, typically cross-encoders, are often used to re-score the documents\nretrieved by cheaper initial IR systems. This is because, though expensive,\nrerankers are assumed to be more effective. We challenge this assumption by\nmeasuring reranker performance for full retrieval, not just re-scoring\nfirst-stage retrieval. Our experiments reveal a surprising trend: the best\nexisting rerankers provide diminishing returns when scoring progressively more\ndocuments and actually degrade quality beyond a certain limit. In fact, in this\nsetting, rerankers can frequently assign high scores to documents with no\nlexical or semantic overlap with the query. We hope that our findings will spur\nfuture research to improve reranking.\n","authors":["Mathew Jacob","Erik Lindgren","Matei Zaharia","Michael Carbin","Omar Khattab","Andrew Drozdov"],"pdf_url":"https://arxiv.org/pdf/2411.11767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11739v1","updated":"2024-11-18T17:08:35Z","published":"2024-11-18T17:08:35Z","title":"QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou","summary":"  In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels.\n","authors":["Xinchen Luo","Jiangxia Cao","Tianyu Sun","Jinkai Yu","Rui Huang","Wei Yuan","Hezheng Lin","Yichen Zheng","Shiyao Wang","Qigen Hu","Changqing Qiu","Jiaqi Zhang","Xu Zhang","Zhiheng Yan","Jingming Zhang","Simin Zhang","Mingxing Wen","Zhaojie Liu","Kun Gai","Guorui Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.11739v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2411.11692v1","updated":"2024-11-18T16:13:49Z","published":"2024-11-18T16:13:49Z","title":"Do Captioning Metrics Reflect Music Semantic Alignment?","summary":"  Music captioning has emerged as a promising task, fueled by the advent of\nadvanced language generation models. However, the evaluation of music\ncaptioning relies heavily on traditional metrics such as BLEU, METEOR, and\nROUGE which were developed for other domains, without proper justification for\ntheir use in this new field. We present cases where traditional metrics are\nvulnerable to syntactic changes, and show they do not correlate well with human\njudgments. By addressing these issues, we aim to emphasize the need for a\ncritical reevaluation of how music captions are assessed.\n","authors":["Jinwoo Lee","Kyogu Lee"],"pdf_url":"https://arxiv.org/pdf/2411.11692v1.pdf","comment":"International Society for Music Information Retrieval (ISMIR) 2024,\n  Late Breaking Demo (LBD)"},{"id":"http://arxiv.org/abs/2411.11677v1","updated":"2024-11-18T15:57:14Z","published":"2024-11-18T15:57:14Z","title":"Few-shot Model Extraction Attacks against Sequential Recommender Systems","summary":"  Among adversarial attacks against sequential recommender systems, model\nextraction attacks represent a method to attack sequential recommendation\nmodels without prior knowledge. Existing research has primarily concentrated on\nthe adversary's execution of black-box attacks through data-free model\nextraction. However, a significant gap remains in the literature concerning the\ndevelopment of surrogate models by adversaries with access to few-shot raw data\n(10\\% even less). That is, the challenge of how to construct a surrogate model\nwith high functional similarity within the context of few-shot data scenarios\nremains an issue that requires resolution.This study addresses this gap by\nintroducing a novel few-shot model extraction framework against sequential\nrecommenders, which is designed to construct a superior surrogate model with\nthe utilization of few-shot data. The proposed few-shot model extraction\nframework is comprised of two components: an autoregressive augmentation\ngeneration strategy and a bidirectional repair loss-facilitated model\ndistillation procedure. Specifically, to generate synthetic data that closely\napproximate the distribution of raw data, autoregressive augmentation\ngeneration strategy integrates a probabilistic interaction sampler to extract\ninherent dependencies and a synthesis determinant signal module to characterize\nuser behavioral patterns. Subsequently, bidirectional repair loss, which target\nthe discrepancies between the recommendation lists, is designed as auxiliary\nloss to rectify erroneous predictions from surrogate models, transferring\nknowledge from the victim model to the surrogate model effectively. Experiments\non three datasets show that the proposed few-shot model extraction framework\nyields superior surrogate models.\n","authors":["Hui Zhang","Fu Liu"],"pdf_url":"https://arxiv.org/pdf/2411.11677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11508v1","updated":"2024-11-18T12:12:47Z","published":"2024-11-18T12:12:47Z","title":"Collaborative Contrastive Network for Click-Through Rate Prediction","summary":"  E-commerce platforms provide entrances for customers to enter mini-apps to\nmeet their specific shopping needs. At the entrance of a mini-app, a trigger\nitem recommended based on customers' historical preferences, is displayed to\nattract customers to enter the mini-app. Existing Click-Through Rate (CTR)\nprediction approaches have two significant weaknesses: (i) A portion of\ncustomer entries is driven by their interest in the mini-app itself rather than\nthe trigger item. In such cases, approaches highly hinging on the trigger item\ntend to recommend similar items, thus misunderstanding the customers' real\nintention; (ii) Approaches that consider customers' intention toward mini-apps,\nrequire the regular existence of mini-apps for customers to cultivate routine\nshopping habits, making such approaches less robust for mini-apps that are\navailable for only short periods (1 or 3 days) in Explosive Promotional\nScenarios (EPS), such as the Black Friday and China's Double 11 Shopping\nCarnival. To address the above-mentioned issues, we introduce a more general\nand robust CTR prediction approach, dubbed Collaborative Contrastive Network\n(CCN). Given a user, CCN learns to identify two item clusters that can\nrepresent the user's interests and disinterests, via leveraging the\ncollaborative relationship of co-click/co-non-click or the non-collaborative\nrelationship of mono-click as the supervision signal for contrastive learning.\nThis paradigm does not need to explicitly estimate user's binary entry\nintention and avoids amplifying the impact of the trigger item. Online A/B\ntesting on large-scale real-world data demonstrates that CCN sets a new\nstate-of-the-art performance on Taobao, boosting CTR by 12.3% and order volume\nby 12.7%.\n","authors":["Chen Gao","Zixin Zhao","Sihao Hu","Lv Shao","Tong Liu"],"pdf_url":"https://arxiv.org/pdf/2411.11508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11502v1","updated":"2024-11-18T12:02:24Z","published":"2024-11-18T12:02:24Z","title":"All-domain Moveline Evolution Network for Click-Through Rate Prediction","summary":"  E-commerce app users exhibit behaviors that are inherently logically\nconsistent. A series of multi-scenario user behaviors interconnect to form the\nscene-level all-domain user moveline, which ultimately reveals the user's true\nintention. Traditional CTR prediction methods typically focus on the item-level\ninteraction between the target item and the historically interacted items.\nHowever, the scene-level interaction between the target item and the user\nmoveline remains underexplored. There are two challenges when modeling the\ninteraction with preceding all-domain user moveline: (i) Heterogeneity between\nitems and scenes: Unlike traditional user behavior sequences that utilize items\nas carriers, the user moveline utilizes scenes as carriers. The heterogeneity\nbetween items and scenes complicates the process of aligning interactions\nwithin a unified representation space. (ii) Temporal misalignment of linked\nscene-level and item-level behaviors: In the preceding user moveline with a\nfixed sampling length, certain critical scene-level behaviors are closely\nlinked to subsequent item-level behaviors. However, it is impossible to\nestablish a complete temporal alignment that clearly identifies which specific\nscene-level behaviors correspond to which item-level behaviors. To address\nthese challenges and pioneer modeling user intent from the perspective of the\nall-domain moveline, we propose All-domain Moveline Evolution Network (AMEN).\nAMEN not only transfers interactions between items and scenes to homogeneous\nrepresentation spaces, but also introduces a Temporal Sequential Pairwise (TSP)\nmechanism to understand the nuanced associations between scene-level and\nitem-level behaviors, ensuring that the all-domain user moveline differentially\ninfluences CTR predictions for user's favored and unfavored items. Online A/B\ntesting demonstrates that our method achieves a +11.6% increase in CTCVR.\n","authors":["Chen Gao","Zixin Zhao","Lv Shao","Tong Liu"],"pdf_url":"https://arxiv.org/pdf/2411.11502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.05649v2","updated":"2024-11-18T08:04:14Z","published":"2024-11-08T15:45:33Z","title":"Harnessing High-Level Song Descriptors towards Natural Language-Based\n  Music Recommendation","summary":"  Recommender systems relying on Language Models (LMs) have gained popularity\nin assisting users to navigate large catalogs. LMs often exploit item\nhigh-level descriptors, i.e. categories or consumption contexts, from training\ndata or user preferences. This has been proven effective in domains like movies\nor products. However, in the music domain, understanding how effectively LMs\nutilize song descriptors for natural language-based music recommendation is\nrelatively limited. In this paper, we assess LMs effectiveness in recommending\nsongs based on user natural language descriptions and items with descriptors\nlike genres, moods, and listening contexts. We formulate the recommendation\ntask as a dense retrieval problem and assess LMs as they become increasingly\nfamiliar with data pertinent to the task and domain. Our findings reveal\nimproved performance as LMs are fine-tuned for general language similarity,\ninformation retrieval, and mapping longer descriptions to shorter, high-level\ndescriptors in music.\n","authors":["Elena V. Epure","Gabriel Meseguer-Brocal","Darius Afchar","Romain Hennequin"],"pdf_url":"https://arxiv.org/pdf/2411.05649v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10918v2","updated":"2024-11-18T06:50:30Z","published":"2024-05-17T17:09:45Z","title":"A Framework for Leveraging Partially-Labeled Data for Product\n  Attribute-Value Identification","summary":"  In the e-commerce domain, the accurate extraction of attribute-value pairs\n(e.g., Brand: Apple) from product titles and user search queries is crucial for\nenhancing search and recommendation systems. A major challenge with neural\nmodels for this task is the lack of high-quality training data, as the\nannotations for attribute-value pairs in the available datasets are often\nincomplete. To address this, we introduce GenToC, a model designed for training\ndirectly with partially-labeled data, eliminating the necessity for a fully\nannotated dataset. GenToC employs a marker-augmented generative model to\nidentify potential attributes, followed by a token classification model that\ndetermines the associated values for each attribute. GenToC outperforms\nexisting state-of-the-art models, exhibiting upto 56.3% increase in the number\nof accurate extractions. Furthermore, we utilize GenToC to regenerate the\ntraining dataset to expand attribute-value annotations. This bootstrapping\nsubstantially improves the data quality for training other standard NER models,\nwhich are typically faster but less capable in handling partially-labeled data,\nenabling them to achieve comparable performance to GenToC. Our results\ndemonstrate GenToC's unique ability to learn from a limited set of\npartially-labeled data and improve the training of more efficient models,\nadvancing the automated extraction of attribute-value pairs. Finally, our model\nhas been successfully integrated into IndiaMART, India's largest B2B e-commerce\nplatform, achieving a significant increase of 20.2% in the number of correctly\nidentified attribute-value pairs over the existing deployed system while\nachieving a high precision of 89.5%.\n","authors":["D. Subhalingam","Keshav Kolluru"," Mausam","Saurabh Singal"],"pdf_url":"https://arxiv.org/pdf/2405.10918v2.pdf","comment":"Accepted to KDD 2025 ADS Track"},{"id":"http://arxiv.org/abs/2409.19979v3","updated":"2024-11-18T06:28:01Z","published":"2024-09-30T06:07:12Z","title":"Enhancing High-order Interaction Awareness in LLM-based Recommender\n  Model","summary":"  Large language models (LLMs) have demonstrated prominent reasoning\ncapabilities in recommendation tasks by transforming them into text-generation\ntasks. However, existing approaches either disregard or ineffectively model the\nuser-item high-order interactions. To this end, this paper presents an enhanced\nLLM-based recommender (ELMRec). We enhance whole-word embeddings to\nsubstantially enhance LLMs' interpretation of graph-constructed interactions\nfor recommendations, without requiring graph pre-training. This finding may\ninspire endeavors to incorporate rich knowledge graphs into LLM-based\nrecommenders via whole-word embedding. We also found that LLMs often recommend\nitems based on users' earlier interactions rather than recent ones, and present\na reranking solution. Our ELMRec outperforms state-of-the-art (SOTA) methods in\nboth direct and sequential recommendations.\n","authors":["Xinfeng Wang","Jin Cui","Fumiyo Fukumoto","Yoshimi Suzuki"],"pdf_url":"https://arxiv.org/pdf/2409.19979v3.pdf","comment":"Long paper accepted to EMNLP 2024 Main. 16 pages"},{"id":"http://arxiv.org/abs/2407.19682v2","updated":"2024-11-18T06:25:14Z","published":"2024-07-29T03:54:00Z","title":"GradCraft: Elevating Multi-task Recommendations through Holistic\n  Gradient Crafting","summary":"  Recommender systems require the simultaneous optimization of multiple\nobjectives to accurately model user interests, necessitating the application of\nmulti-task learning methods. However, existing multi-task learning methods in\nrecommendations overlook the specific characteristics of recommendation\nscenarios, falling short in achieving proper gradient balance. To address this\nchallenge, we set the target of multi-task learning as attaining the\nappropriate magnitude balance and the global direction balance, and propose an\ninnovative methodology named GradCraft in response. GradCraft dynamically\nadjusts gradient magnitudes to align with the maximum gradient norm, mitigating\ninterference from gradient magnitudes for subsequent manipulation. It then\nemploys projections to eliminate gradient conflicts in directions while\nconsidering all conflicting tasks simultaneously, theoretically guaranteeing\nthe global resolution of direction conflicts. GradCraft ensures the concurrent\nachievement of appropriate magnitude balance and global direction balance,\naligning with the inherent characteristics of recommendation scenarios. Both\noffline and online experiments attest to the efficacy of GradCraft in enhancing\nmulti-task performance in recommendations. The source code for GradCraft can be\naccessed at https://github.com/baiyimeng/GradCraft.\n","authors":["Yimeng Bai","Yang Zhang","Fuli Feng","Jing Lu","Xiaoxue Zang","Chenyi Lei","Yang Song"],"pdf_url":"https://arxiv.org/pdf/2407.19682v2.pdf","comment":"Accepted by KDD'24"},{"id":"http://arxiv.org/abs/2406.10806v2","updated":"2024-11-18T02:19:02Z","published":"2024-06-16T05:17:56Z","title":"ptt5-v2: A Closer Look at Continued Pretraining of T5 Models for the\n  Portuguese Language","summary":"  Despite advancements in Natural Language Processing (NLP) and the growing\navailability of pretrained models, the English language remains the primary\nfocus of model development. Continued pretraining on language-specific corpora\nprovides a practical solution for adapting models to other languages. However,\nthe impact of different pretraining settings on downstream tasks remains\nunderexplored. This work introduces $\\texttt{ptt5-v2}$, investigating the\ncontinued pretraining of T5 models for Portuguese. We first develop a baseline\nset of settings and pretrain models with sizes up to 3B parameters. Finetuning\non three Portuguese downstream tasks (assin2 STS, assin2 RTE, and TweetSentBR)\nyields SOTA results on the latter two. We then explore the effects of different\npretraining configurations, including pretraining data quality, optimization\nstrategies, and multi-epoch pretraining. Perhaps surprisingly, their impact\nremains subtle compared to our baseline. We release $\\texttt{ptt5-v2}$\npretrained checkpoints and their MonoT5-based finetuned $\\texttt{MonoPTT5}$\nrerankers on HuggingFace in their respective collections at\n\\url{https://huggingface.co/unicamp-dl}.\n","authors":["Marcos Piau","Roberto Lotufo","Rodrigo Nogueira"],"pdf_url":"https://arxiv.org/pdf/2406.10806v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11240v1","updated":"2024-11-18T02:15:29Z","published":"2024-11-18T02:15:29Z","title":"Controlling Diversity at Inference: Guiding Diffusion Recommender Models\n  with Targeted Category Preferences","summary":"  Diversity control is an important task to alleviate bias amplification and\nfilter bubble problems. The desired degree of diversity may fluctuate based on\nusers' daily moods or business strategies. However, existing methods for\ncontrolling diversity often lack flexibility, as diversity is decided during\ntraining and cannot be easily modified during inference. We propose\n\\textbf{D3Rec} (\\underline{D}isentangled \\underline{D}iffusion model for\n\\underline{D}iversified \\underline{Rec}ommendation), an end-to-end method that\ncontrols the accuracy-diversity trade-off at inference. D3Rec meets our three\ndesiderata by (1) generating recommendations based on category preferences, (2)\ncontrolling category preferences during the inference phase, and (3) adapting\nto arbitrary targeted category preferences. In the forward process, D3Rec\nremoves category preferences lurking in user interactions by adding noises.\nThen, in the reverse process, D3Rec generates recommendations through denoising\nsteps while reflecting desired category preferences. Extensive experiments on\nreal-world and synthetic datasets validate the effectiveness of D3Rec in\ncontrolling diversity at inference.\n","authors":["Gwangseok Han","Wonbin Kweon","Minsoo Kim","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2411.11240v1.pdf","comment":"KDD 2025"},{"id":"http://arxiv.org/abs/2402.07440v3","updated":"2024-11-18T01:52:49Z","published":"2024-02-12T06:43:52Z","title":"Benchmarking and Building Long-Context Retrieval Models with LoCo and\n  M2-BERT","summary":"  Retrieval pipelines-an integral component of many machine learning\nsystems-perform poorly in domains where documents are long (e.g., 10K tokens or\nmore) and where identifying the relevant document requires synthesizing\ninformation across the entire text. Developing long-context retrieval encoders\nsuitable for these domains raises three challenges: (1) how to evaluate\nlong-context retrieval performance, (2) how to pretrain a base language model\nto represent both short contexts (corresponding to queries) and long contexts\n(corresponding to documents), and (3) how to fine-tune this model for retrieval\nunder the batch size limitations imposed by GPU memory constraints. To address\nthese challenges, we first introduce LoCoV1, a novel 12 task benchmark\nconstructed to measure long-context retrieval where chunking is not possible or\nnot effective. We next present the M2-BERT retrieval encoder, an 80M parameter\nstate-space encoder model built from the Monarch Mixer architecture, capable of\nscaling to documents up to 32K tokens long. We describe a pretraining data\nmixture which allows this encoder to process both short and long context\nsequences, and a finetuning approach that adapts this base model to retrieval\nwith only single-sample batches. Finally, we validate the M2-BERT retrieval\nencoder on LoCoV1, finding that it outperforms competitive Transformer-based\nmodels by at least 23.3 points, despite containing upwards of 90x fewer\nparameters.\n","authors":["Jon Saad-Falcon","Daniel Y. Fu","Simran Arora","Neel Guha","Christopher Ré"],"pdf_url":"https://arxiv.org/pdf/2402.07440v3.pdf","comment":"International Conference on Machine Learning (ICML) 2024"},{"id":"http://arxiv.org/abs/2411.11225v1","updated":"2024-11-18T01:30:34Z","published":"2024-11-18T01:30:34Z","title":"Online Item Cold-Start Recommendation with Popularity-Aware\n  Meta-Learning","summary":"  With the rise of e-commerce and short videos, online recommender systems that\ncan capture users' interests and update new items in real-time play an\nincreasingly important role. In both online and offline recommendation, the\ncold-start problem due to interaction sparsity has been affecting the\nrecommendation effect of cold-start items, which is also known as the long-tail\nproblem of item distribution. Many cold-start scheme based on fine-tuning or\nknowledge transferring shows excellent performance on offline recommendation.\nYet, these schemes are infeasible for online recommendation on streaming data\npipelines due to different training method, computational overhead and time\nconstraints.\n  Inspired by the above questions, we propose a model-agnostic recommendation\nalgorithm called Popularity-Aware Meta-learning (PAM), to address the item\ncold-start problem under streaming data settings. PAM divides the incoming data\ninto different meta-learning tasks by predefined item popularity thresholds.\nThe model can distinguish and reweight behavior-related features and\ncontent-related features in each task based on their different roles in\ndifferent popularity levels, thus adapting to recommendations for cold-start\nsamples. These task-fixing design significantly reduces additional computation\nand storage costs compared to offline methods. Furthermore, PAM also introduced\ndata augmentation and an additional self-supervised loss specifically designed\nfor low-popularity tasks, leveraging insights from high-popularity samples.\nThis approach effectively mitigates the issue of inadequate supervision due to\nthe scarcity of cold-start samples. Experimental results across multiple public\ndatasets demonstrate the superiority of our approach over other baseline\nmethods in addressing cold-start challenges in online streaming data scenarios.\n","authors":["Yunze Luo","Yuezihan Jiang","Yinjie Jiang","Gaode Chen","Jingchi Wang","Kaigui Bian","Peiyi Li","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.11225v1.pdf","comment":"11 pages, 4 figures, to be published in KDD '25"},{"id":"http://arxiv.org/abs/1607.00223v2","updated":"2024-11-18T07:36:40Z","published":"2016-07-01T12:45:43Z","title":"Memory Based Collaborative Filtering with Lucene","summary":"  Memory Based Collaborative Filtering is a widely used approach to provide\nrecommendations. It exploits similarities between ratings across a population\nof users by forming a weighted vote to predict unobserved ratings. Bespoke\nsolutions are frequently adopted to deal with the problem of high quality\nrecommendations on large data sets. A disadvantage of this approach, however,\nis the loss of generality and flexibility of the general collaborative\nfiltering systems. In this paper, we have developed a methodology that allows\none to build a scalable and effective collaborative filtering system on top of\na conventional full-text search engine such as Apache Lucene.\n","authors":["Claudio Gennaro"],"pdf_url":"https://arxiv.org/pdf/1607.00223v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14466v1","updated":"2024-11-18T14:05:43Z","published":"2024-11-18T14:05:43Z","title":"Learning to Ask: Conversational Product Search via Representation\n  Learning","summary":"  Online shopping platforms, such as Amazon and AliExpress, are increasingly\nprevalent in society, helping customers purchase products conveniently. With\nrecent progress in natural language processing, researchers and practitioners\nshift their focus from traditional product search to conversational product\nsearch. Conversational product search enables user-machine conversations and\nthrough them collects explicit user feedback that allows to actively clarify\nthe users' product preferences. Therefore, prospective research on an\nintelligent shopping assistant via conversations is indispensable. Existing\npublications on conversational product search either model conversations\nindependently from users, queries, and products or lead to a vocabulary\nmismatch. In this work, we propose a new conversational product search model,\nConvPS, to assist users in locating desirable items. The model is first trained\nto jointly learn the semantic representations of user, query, item, and\nconversation via a unified generative framework. After learning these\nrepresentations, they are integrated to retrieve the target items in the latent\nsemantic space. Meanwhile, we propose a set of greedy and explore-exploit\nstrategies to learn to ask the user a sequence of high-performance questions\nfor conversations. Our proposed ConvPS model can naturally integrate the\nrepresentation learning of the user, query, item, and conversation into a\nunified generative framework, which provides a promising avenue for\nconstructing accurate and robust conversational product search systems that are\nflexible and adaptive. Experimental results demonstrate that our ConvPS model\nsignificantly outperforms state-of-the-art baselines.\n","authors":["Jie Zou","Jimmy Xiangji Huang","Zhaochun Ren","Evangelos Kanoulas"],"pdf_url":"https://arxiv.org/pdf/2411.14466v1.pdf","comment":"Accepted by ACM TOIS"}],"Multimedia":[{"id":"http://arxiv.org/abs/2411.12072v1","updated":"2024-11-18T21:32:49Z","published":"2024-11-18T21:32:49Z","title":"Zoomed In, Diffused Out: Towards Local Degradation-Aware Multi-Diffusion\n  for Extreme Image Super-Resolution","summary":"  Large-scale, pre-trained Text-to-Image (T2I) diffusion models have gained\nsignificant popularity in image generation tasks and have shown unexpected\npotential in image Super-Resolution (SR). However, most existing T2I diffusion\nmodels are trained with a resolution limit of 512x512, making scaling beyond\nthis resolution an unresolved but necessary challenge for image SR. In this\nwork, we introduce a novel approach that, for the first time, enables these\nmodels to generate 2K, 4K, and even 8K images without any additional training.\nOur method leverages MultiDiffusion, which distributes the generation across\nmultiple diffusion paths to ensure global coherence at larger scales, and local\ndegradation-aware prompt extraction, which guides the T2I model to reconstruct\nfine local structures according to its low-resolution input. These innovations\nunlock higher resolutions, allowing T2I diffusion models to be applied to image\nSR tasks without limitation on resolution.\n","authors":["Brian B. Moser","Stanislav Frolov","Tobias C. Nauen","Federico Raue","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2411.12072v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12008v1","updated":"2024-11-18T19:48:18Z","published":"2024-11-18T19:48:18Z","title":"Compression of Higher Order Ambisonics with Multichannel RVQGAN","summary":"  A multichannel extension to the RVQGAN neural coding method is proposed, and\nrealized for data-driven compression of third-order Ambisonics audio. The\ninput- and output layers of the generator and discriminator models are modified\nto accept multiple (16) channels without increasing the model bitrate. We also\npropose a loss function for accounting for spatial perception in immersive\nreproduction, and transfer learning from single-channel models. Listening test\nresults with 7.1.4 immersive playback show that the proposed extension is\nsuitable for coding scene-based, 16-channel Ambisonics content with good\nquality at 16 kbit/s.\n","authors":["Toni Hirvonen","Mahmoud Namazi"],"pdf_url":"https://arxiv.org/pdf/2411.12008v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.04796v2","updated":"2024-11-18T18:20:38Z","published":"2023-02-09T17:43:33Z","title":"BASICS: Broad quality Assessment of Static point clouds In Compression\n  Scenarios","summary":"  Point clouds have become increasingly prevalent in representing 3D scenes\nwithin virtual environments, alongside 3D meshes. Their ease of capture has\nfacilitated a wide array of applications on mobile devices, from smartphones to\nautonomous vehicles. Notably, point cloud compression has reached an advanced\nstage and has been standardized. However, the availability of quality\nassessment datasets, which are essential for developing improved objective\nquality metrics, remains limited. In this paper, we introduce BASICS, a\nlarge-scale quality assessment dataset tailored for static point clouds. The\nBASICS dataset comprises 75 unique point clouds, each compressed with four\ndifferent algorithms including a learning-based method, resulting in the\nevaluation of nearly 1500 point clouds by 3500 unique participants.\nFurthermore, we conduct a comprehensive analysis of the gathered data,\nbenchmark existing point cloud quality assessment metrics and identify their\nlimitations. By publicly releasing the BASICS dataset, we lay the foundation\nfor addressing these limitations and fostering the development of more precise\nquality metrics.\n","authors":["Ali Ak","Emin Zerman","Maurice Quach","Aladine Chetouani","Aljosa Smolic","Giuseppe Valenzise","Patrick Le Callet"],"pdf_url":"https://arxiv.org/pdf/2302.04796v2.pdf","comment":"Published in IEEE TMM, 14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2411.11688v1","updated":"2024-11-18T16:11:25Z","published":"2024-11-18T16:11:25Z","title":"Conceptwm: A Diffusion Model Watermark for Concept Protection","summary":"  The personalization techniques of diffusion models succeed in generating\nspecific concepts but also pose threats to copyright protection and illegal\nuse. Model Watermarking is an effective method to prevent the unauthorized use\nof subject-driven or style-driven image generation, safeguarding concept\ncopyrights. However, under the goal of concept-oriented protection, current\nwatermarking schemes typically add watermarks to all images rather than\napplying them in a refined manner targeted at specific concepts. Additionally,\nthe personalization techniques of diffusion models can easily remove\nwatermarks. Existing watermarking methods struggle to achieve fine-grained\nwatermark embedding with a few images of specific concept and prevent removal\nof watermarks through personalized fine-tuning. Therefore, we introduce a novel\nconcept-oriented watermarking framework that seamlessly embeds imperceptible\nwatermarks into the concept of diffusion models. We conduct extensive\nexperiments and ablation studies to verify our framework. Our code is available\nat https://anonymous.4open.science/r/Conceptwm-4EB3/.\n","authors":["Liangqi Lei","Keke Gai","Jing Yu","Liehuang Zhu","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2411.11688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13360v2","updated":"2024-11-18T15:35:14Z","published":"2024-10-17T09:10:26Z","title":"Retrieval-Augmented Personalization for Multimodal Large Language Models","summary":"  The development of large language models (LLMs) has significantly enhanced\nthe capabilities of multimodal LLMs (MLLMs) as general assistants. However,\nlack of user-specific knowledge still restricts their application in human's\ndaily life. In this paper, we introduce the Retrieval Augmented Personalization\n(RAP) framework for MLLMs' personalization. Starting from a general MLLM, we\nturn it into a personalized assistant in three steps. (a) Remember: We design a\nkey-value database to store user-related information, e.g., user's name, avatar\nand other attributes. (b) Retrieve: When the user initiates a conversation, RAP\nwill retrieve relevant information from the database using a multimodal\nretriever. (c) Generate: The input query and retrieved concepts' information\nare fed into MLLMs to generate personalized, knowledge-augmented responses.\nUnlike previous methods, RAP allows real-time concept editing via updating the\nexternal database. To further improve generation quality and alignment with\nuser-specific information, we design a pipeline for data collection and create\na specialized dataset for personalized training of MLLMs. Based on the dataset,\nwe train a series of MLLMs as personalized multimodal assistants. By\npretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual\nconcepts without additional finetuning. Our models demonstrate outstanding\nflexibility and generation quality across a variety of tasks, such as\npersonalized image captioning, question answering and visual recognition. The\ncode, data and models are available at https://github.com/Hoar012/RAP-MLLM.\n","authors":["Haoran Hao","Jiaming Han","Changsheng Li","Yu-Feng Li","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2410.13360v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07899v2","updated":"2024-11-18T07:13:24Z","published":"2024-11-12T16:12:51Z","title":"Rendering-Oriented 3D Point Cloud Attribute Compression using Sparse\n  Tensor-based Transformer","summary":"  The evolution of 3D visualization techniques has fundamentally transformed\nhow we interact with digital content. At the forefront of this change is point\ncloud technology, offering an immersive experience that surpasses traditional\n2D representations. However, the massive data size of point clouds presents\nsignificant challenges in data compression. Current methods for lossy point\ncloud attribute compression (PCAC) generally focus on reconstructing the\noriginal point clouds with minimal error. However, for point cloud\nvisualization scenarios, the reconstructed point clouds with distortion still\nneed to undergo a complex rendering process, which affects the final\nuser-perceived quality. In this paper, we propose an end-to-end deep learning\nframework that seamlessly integrates PCAC with differentiable rendering,\ndenoted as rendering-oriented PCAC (RO-PCAC), directly targeting the quality of\nrendered multiview images for viewing. In a differentiable manner, the impact\nof the rendering process on the reconstructed point clouds is taken into\naccount. Moreover, we characterize point clouds as sparse tensors and propose a\nsparse tensor-based transformer, called SP-Trans. By aligning with the local\ndensity of the point cloud and utilizing an enhanced local attention mechanism,\nSP-Trans captures the intricate relationships within the point cloud, further\nimproving feature analysis and synthesis within the framework. Extensive\nexperiments demonstrate that the proposed RO-PCAC achieves state-of-the-art\ncompression performance, compared to existing reconstruction-oriented methods,\nincluding traditional, learning-based, and hybrid methods.\n","authors":["Xiao Huo","Junhui Hou","Shuai Wan","Fuzheng Yang"],"pdf_url":"https://arxiv.org/pdf/2411.07899v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11278v1","updated":"2024-11-18T04:35:20Z","published":"2024-11-18T04:35:20Z","title":"Towards Open-Vocabulary Audio-Visual Event Localization","summary":"  The Audio-Visual Event Localization (AVEL) task aims to temporally locate and\nclassify video events that are both audible and visible. Most research in this\nfield assumes a closed-set setting, which restricts these models' ability to\nhandle test data containing event categories absent (unseen) during training.\nRecently, a few studies have explored AVEL in an open-set setting, enabling the\nrecognition of unseen events as ``unknown'', but without providing\ncategory-specific semantics. In this paper, we advance the field by introducing\nthe Open-Vocabulary Audio-Visual Event Localization (OV-AVEL) problem, which\nrequires localizing audio-visual events and predicting explicit categories for\nboth seen and unseen data at inference. To address this new task, we propose\nthe OV-AVEBench dataset, comprising 24,800 videos across 67 real-life\naudio-visual scenes (seen:unseen = 46:21), each with manual segment-level\nannotation. We also establish three evaluation metrics for this task. Moreover,\nwe investigate two baseline approaches, one training-free and one using a\nfurther fine-tuning paradigm. Specifically, we utilize the unified multimodal\nspace from the pretrained ImageBind model to extract audio, visual, and textual\n(event classes) features. The training-free baseline then determines\npredictions by comparing the consistency of audio-text and visual-text feature\nsimilarities. The fine-tuning baseline incorporates lightweight temporal layers\nto encode temporal relations within the audio and visual modalities, using\nOV-AVEBench training data for model fine-tuning. We evaluate these baselines on\nthe proposed OV-AVEBench dataset and discuss potential directions for future\nwork in this new field.\n","authors":["Jinxing Zhou","Dan Guo","Ruohao Guo","Yuxin Mao","Jingjing Hu","Yiran Zhong","Xiaojun Chang","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2411.11278v1.pdf","comment":"Project page: https://github.com/jasongief/OV-AVEL"},{"id":"http://arxiv.org/abs/2411.11222v1","updated":"2024-11-18T01:19:37Z","published":"2024-11-18T01:19:37Z","title":"The Sound of Water: Inferring Physical Properties from Pouring Liquids","summary":"  We study the connection between audio-visual observations and the underlying\nphysics of a mundane yet intriguing everyday activity: pouring liquids. Given\nonly the sound of liquid pouring into a container, our objective is to\nautomatically infer physical properties such as the liquid level, the shape and\nsize of the container, the pouring rate and the time to fill. To this end, we:\n(i) show in theory that these properties can be determined from the fundamental\nfrequency (pitch); (ii) train a pitch detection model with supervision from\nsimulated data and visual data with a physics-inspired objective; (iii)\nintroduce a new large dataset of real pouring videos for a systematic study;\n(iv) show that the trained model can indeed infer these physical properties for\nreal data; and finally, (v) we demonstrate strong generalization to various\ncontainer shapes, other datasets, and in-the-wild YouTube videos. Our work\npresents a keen understanding of a narrow yet rich problem at the intersection\nof acoustics, physics, and learning. It opens up applications to enhance\nmultisensory perception in robotic pouring.\n","authors":["Piyush Bagad","Makarand Tapaswi","Cees G. M. Snoek","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2411.11222v1.pdf","comment":"25 pages, 17 figures. Project page at\n  https://bpiyush.github.io/pouring-water-website"}]},"2024-11-21T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2411.14405v1","updated":"2024-11-21T18:37:33Z","published":"2024-11-21T18:37:33Z","title":"Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions","summary":"  Currently OpenAI o1 has sparked a surge of interest in the study of large\nreasoning models (LRM). Building on this momentum, Marco-o1 not only focuses on\ndisciplines with standard answers, such as mathematics, physics, and coding --\nwhich are well-suited for reinforcement learning (RL) -- but also places\ngreater emphasis on open-ended resolutions. We aim to address the question:\n\"Can the o1 model effectively generalize to broader domains where clear\nstandards are absent and rewards are challenging to quantify?\" Marco-o1 is\npowered by Chain-of-Thought (CoT) fine-tuning, Monte Carlo Tree Search (MCTS),\nreflection mechanisms, and innovative reasoning strategies -- optimized for\ncomplex real-world problem-solving tasks.\n","authors":["Yu Zhao","Huifeng Yin","Bo Zeng","Hao Wang","Tianqi Shi","Chenyang Lyu","Longyue Wang","Weihua Luo","Kaifu Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14398v1","updated":"2024-11-21T18:27:25Z","published":"2024-11-21T18:27:25Z","title":"Lightweight Safety Guardrails Using Fine-tuned BERT Embeddings","summary":"  With the recent proliferation of large language models (LLMs), enterprises\nhave been able to rapidly develop proof-of-concepts and prototypes. As a\nresult, there is a growing need to implement robust guardrails that monitor,\nquantize and control an LLM's behavior, ensuring that the use is reliable,\nsafe, accurate and also aligned with the users' expectations. Previous\napproaches for filtering out inappropriate user prompts or system outputs, such\nas LlamaGuard and OpenAI's MOD API, have achieved significant success by\nfine-tuning existing LLMs. However, using fine-tuned LLMs as guardrails\nintroduces increased latency and higher maintenance costs, which may not be\npractical or scalable for cost-efficient deployments. We take a different\napproach, focusing on fine-tuning a lightweight architecture: Sentence-BERT.\nThis method reduces the model size from LlamaGuard's 7 billion parameters to\napproximately 67 million, while maintaining comparable performance on the AEGIS\nsafety benchmark.\n","authors":["Aaron Zheng","Mansi Rana","Andreas Stolcke"],"pdf_url":"https://arxiv.org/pdf/2411.14398v1.pdf","comment":"To appear in Proceedings of COLING 2025"},{"id":"http://arxiv.org/abs/2411.14393v1","updated":"2024-11-21T18:25:19Z","published":"2024-11-21T18:25:19Z","title":"POS-tagging to highlight the skeletal structure of sentences","summary":"  This study presents the development of a part-of-speech (POS) tagging model\nto extract the skeletal structure of sentences using transfer learning with the\nBERT architecture for token classification. The model, fine-tuned on Russian\ntext, demonstrating its effectiveness. The approach offers potential\napplications in enhancing natural language processing tasks, such as improving\nmachine translation.\n  Keywords: part of speech tagging, morphological analysis, natural language\nprocessing, BERT.\n","authors":["Grigorii Churakov"],"pdf_url":"https://arxiv.org/pdf/2411.14393v1.pdf","comment":"in Russian language. Conference: Automated control systems and\n  information technologies https://asuit.pstu.ru/ Section: IT and automated\n  systems"},{"id":"http://arxiv.org/abs/2410.16162v2","updated":"2024-11-21T18:05:04Z","published":"2024-10-21T16:26:09Z","title":"Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models\n  Elicits Generalization to Composite Spatial Reasoning","summary":"  Vision language models (VLMs) have demonstrated impressive performance across\na wide range of downstream tasks. However, their proficiency in spatial\nreasoning remains limited, despite its crucial role in tasks involving\nnavigation and interaction with physical environments. Specifically, most of\nthese tasks rely on the core spatial reasoning capabilities in two-dimensional\n(2D) environments, and our evaluation reveals that state-of-the-art VLMs\nfrequently generate implausible and incorrect responses to composite spatial\nreasoning problems, including simple pathfinding tasks that humans can solve\neffortlessly at a glance. To address this, we explore an effective approach to\nenhance 2D spatial reasoning within VLMs by training the model solely on basic\nspatial capabilities. We begin by disentangling the key components of 2D\nspatial reasoning: direction comprehension, distance estimation, and\nlocalization. Our central hypothesis is that mastering these basic spatial\ncapabilities can significantly enhance a model's performance on composite\nspatial tasks requiring advanced spatial understanding and combinatorial\nproblem-solving, with generalized improvements in visual-spatial tasks. To\ninvestigate this hypothesis, we introduce Sparkle, a framework that fine-tunes\nVLMs on these three basic spatial capabilities by synthetic data generation and\ntargeted supervision to form an instruction dataset for each capability. Our\nexperiments demonstrate that VLMs fine-tuned with Sparkle achieve significant\nperformance gains, not only in the basic tasks themselves but also in\ngeneralizing to composite and out-of-distribution spatial reasoning tasks.\nThese findings underscore the effectiveness of mastering basic spatial\ncapabilities in enhancing composite spatial problem-solving, offering insights\ninto systematic strategies for improving VLMs' spatial reasoning capabilities.\n","authors":["Yihong Tang","Ao Qu","Zhaokai Wang","Dingyi Zhuang","Zhaofeng Wu","Wei Ma","Shenhao Wang","Yunhan Zheng","Zhan Zhao","Jinhua Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.16162v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14512v2","updated":"2024-11-21T17:48:25Z","published":"2024-08-25T04:32:45Z","title":"LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with\n  LLM Token Embeddings","summary":"  Zero-shot graph machine learning, especially with graph neural networks\n(GNNs), has garnered significant interest due to the challenge of scarce\nlabeled data. While methods like self-supervised learning and graph prompt\nlearning have been extensively explored, they often rely on fine-tuning with\ntask-specific labels, limiting their effectiveness in zero-shot scenarios.\nInspired by the zero-shot capabilities of instruction-fine-tuned large language\nmodels (LLMs), we introduce a novel framework named Token Embedding-Aligned\nGraph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and\ncross-task zero-shot learners for graph machine learning. Concretely, we\npretrain a GNN, aligning its representations with token embeddings of an LLM.\nWe then train a linear projector that transforms the GNN's representations into\na fixed number of graph token embeddings without tuning the LLM. A unified\ninstruction is designed for various graph tasks at different levels, such as\nnode classification (node-level) and link prediction (edge-level). These design\nchoices collectively enhance our method's effectiveness in zero-shot learning,\nsetting it apart from existing methods. Experiments show that our graph token\nembeddings help the LLM predictor achieve state-of-the-art performance on\nunseen datasets and tasks compared to other methods using LLMs as predictors.\n","authors":["Duo Wang","Yuan Zuo","Fengzhi Li","Junjie Wu"],"pdf_url":"https://arxiv.org/pdf/2408.14512v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14343v1","updated":"2024-11-21T17:41:08Z","published":"2024-11-21T17:41:08Z","title":"UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptation of LLMs\n  on Low-Resource Languages","summary":"  Large language models (LLMs) under-perform on low-resource languages due to\nlimited training data. We present a method to efficiently collect text data for\nlow-resource languages from the entire Common Crawl corpus. Our approach,\nUnifiedCrawl, filters and extracts common crawl using minimal compute\nresources, yielding mono-lingual datasets much larger than previously available\nsources. We demonstrate that leveraging this data to fine-tuning multilingual\nLLMs via efficient adapter methods (QLoRA) significantly boosts performance on\nthe low-resource language, while minimizing VRAM usage. Our experiments show\nlarge improvements in language modeling perplexity and an increase in few-shot\nprompting scores. Our work and released source code provide an affordable\napproach to improve LLMs for low-resource languages using consumer hardware.\nOur source code is available here at\nhttps://github.com/bethelmelesse/unifiedcrawl.\n","authors":["Bethel Melesse Tessema","Akhil Kedia","Tae-Sun Chung"],"pdf_url":"https://arxiv.org/pdf/2411.14343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14318v1","updated":"2024-11-21T17:10:02Z","published":"2024-11-21T17:10:02Z","title":"Velocitune: A Velocity-based Dynamic Domain Reweighting Method for\n  Continual Pre-training","summary":"  It is well-known that a diverse corpus is critical for training large\nlanguage models, which are typically constructed from a mixture of various\ndomains. In general, previous efforts resort to sampling training data from\ndifferent domains with static proportions, as well as adjusting data\nproportions during training. However, few methods have addressed the\ncomplexities of domain-adaptive continual pre-training. To fill this gap, we\npropose Velocitune, a novel framework dynamically assesses learning velocity\nand adjusts data proportions accordingly, favoring slower-learning domains\nwhile shunning faster-learning ones, which is guided by a scaling law to\nindicate the desired learning goal for each domain with less associated cost.\nTo evaluate the effectiveness of Velocitune, we conduct experiments in a\nreasoning-focused dataset with CodeLlama, as well as in a corpus specialised\nfor system command generation with Llama3 and Mistral. Velocitune achieves\nperformance gains in both math and code reasoning tasks and command-line\ngeneration benchmarks. Further analysis reveals that key factors driving\nVelocitune's effectiveness include target loss prediction and data ordering.\n","authors":["Zheheng Luo","Xin Zhang","Xiao Liu","Haoling Li","Yeyun Gong","Chen Qi","Peng Cheng"],"pdf_url":"https://arxiv.org/pdf/2411.14318v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2411.13009v2","updated":"2024-11-21T16:49:51Z","published":"2024-11-20T03:17:51Z","title":"LLMSteer: Improving Long-Context LLM Inference by Steering Attention on\n  Reused Contexts","summary":"  As large language models (LLMs) show impressive performance on complex tasks,\nthey still struggle with longer contextual understanding and high computational\ncosts. To balance efficiency and quality, we introduce LLMSteer, a\nfine-tuning-free framework that enhances LLMs through query-independent\nattention steering. Tested on popular LLMs and datasets, LLMSteer narrows the\nperformance gap with baselines by 65.9% and reduces the runtime delay by up to\n4.8x compared to recent attention steering methods.\n","authors":["Zhuohan Gu","Jiayi Yao","Kuntai Du","Junchen Jiang"],"pdf_url":"https://arxiv.org/pdf/2411.13009v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16520v2","updated":"2024-11-21T16:43:06Z","published":"2024-10-21T21:21:29Z","title":"AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context","summary":"  As our understanding of autism and ableism continues to increase, so does our\nunderstanding of ableist language towards autistic people. Such language poses\na significant challenge in NLP research due to its subtle and context-dependent\nnature. Yet, detecting anti-autistic ableist language remains underexplored,\nwith existing NLP tools often failing to capture its nuanced expressions. We\npresent AUTALIC, the first benchmark dataset dedicated to the detection of\nanti-autistic ableist language in context, addressing a significant gap in the\nfield. The dataset comprises 2,400 autism-related sentences collected from\nReddit, accompanied by surrounding context, and is annotated by trained experts\nwith backgrounds in neurodiversity. Our comprehensive evaluation reveals that\ncurrent language models, including state-of-the-art LLMs, struggle to reliably\nidentify anti-autistic ableism and align with human judgments, underscoring\ntheir limitations in this domain. We publicly release AUTALIC along with the\nindividual annotations which serve as a valuable resource to researchers\nworking on ableism, neurodiversity, and also studying disagreements in\nannotation tasks. This dataset serves as a crucial step towards developing more\ninclusive and context-aware NLP systems that better reflect diverse\nperspectives.\n","authors":["Naba Rizvi","Harper Strickland","Daniel Gitelman","Tristan Cooper","Alexis Morales-Flores","Michael Golden","Aekta Kallepalli","Akshat Alurkar","Haaset Owens","Saleha Ahmedi","Isha Khirwadkar","Imani Munyaka","Nedjma Ousidhoum"],"pdf_url":"https://arxiv.org/pdf/2410.16520v2.pdf","comment":"9 pages, 5 figures, 7 tables"},{"id":"http://arxiv.org/abs/2411.14279v1","updated":"2024-11-21T16:33:30Z","published":"2024-11-21T16:33:30Z","title":"Looking Beyond Text: Reducing Language bias in Large Vision-Language\n  Models via Multimodal Dual-Attention and Soft-Image Guidance","summary":"  Large vision-language models (LVLMs) have achieved impressive results in\nvarious vision-language tasks. However, despite showing promising performance,\nLVLMs suffer from hallucinations caused by language bias, leading to diminished\nfocus on images and ineffective visual comprehension. We identify two primary\nreasons for this bias: 1. Different scales of training data between the\npretraining stage of LLM and multimodal alignment stage. 2. The learned\ninference bias due to short-term dependency of text data. Therefore, we propose\nLACING, a systemic framework designed to address the language bias of LVLMs\nwith muLtimodal duAl-attention meChanIsm (MDA) aNd soft-image Guidance (IFG).\nSpecifically, MDA introduces a parallel dual-attention mechanism that enhances\nthe integration of visual inputs across the model. IFG introduces a learnable\nsoft visual prompt during training and inference to replace visual inputs,\ndesigned to compel LVLMs to prioritize text inputs. Then, IFG further proposes\na novel decoding strategy using the soft visual prompt to mitigate the model's\nover-reliance on adjacent text inputs. Comprehensive experiments demonstrate\nthat our method effectively debiases LVLMs from their language bias, enhancing\nvisual comprehension and reducing hallucinations without requiring additional\ntraining resources or data. The code and model are available at\n[lacing-lvlm.github.io](https://lacing-lvlm.github.io).\n","authors":["Haozhe Zhao","Shuzheng Si","Liang Chen","Yichi Zhang","Maosong Sun","Mingjia Zhang","Baobao Chang"],"pdf_url":"https://arxiv.org/pdf/2411.14279v1.pdf","comment":"19 pages, 12 figures"},{"id":"http://arxiv.org/abs/2411.14272v1","updated":"2024-11-21T16:28:32Z","published":"2024-11-21T16:28:32Z","title":"Efficient Aspect-Based Summarization of Climate Change Reports with\n  Small Language Models","summary":"  The use of Natural Language Processing (NLP) for helping decision-makers with\nClimate Change action has recently been highlighted as a use case aligning with\na broader drive towards NLP technologies for social good. In this context,\nAspect-Based Summarization (ABS) systems that extract and summarize relevant\ninformation are particularly useful as they provide stakeholders with a\nconvenient way of finding relevant information in expert-curated reports. In\nthis work, we release a new dataset for ABS of Climate Change reports and we\nemploy different Large Language Models (LLMs) and so-called Small Language\nModels (SLMs) to tackle this problem in an unsupervised way. Considering the\nproblem at hand, we also show how SLMs are not significantly worse for the\nproblem while leading to reduced carbon footprint; we do so by applying for the\nfirst time an existing framework considering both energy efficiency and task\nperformance to the evaluation of zero-shot generative models for ABS. Overall,\nour results show that modern language models, both big and small, can\neffectively tackle ABS for Climate Change reports but more research is needed\nwhen we frame the problem as a Retrieval Augmented Generation (RAG) problem and\nour work and dataset will help foster efforts in this direction.\n","authors":["Iacopo Ghinassi","Leonardo Catalano","Tommaso Colella"],"pdf_url":"https://arxiv.org/pdf/2411.14272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11585v3","updated":"2024-11-21T16:28:03Z","published":"2024-03-18T08:58:47Z","title":"Linguacodus: A Synergistic Framework for Transformative Code Generation\n  in Machine Learning Pipelines","summary":"  In the ever-evolving landscape of machine learning, seamless translation of\nnatural language descriptions into executable code remains a formidable\nchallenge. This paper introduces Linguacodus, an innovative framework designed\nto tackle this challenge by deploying a dynamic pipeline that iteratively\ntransforms natural language task descriptions into code through high-level\ndata-shaping instructions. The core of Linguacodus is a fine-tuned large\nlanguage model (LLM), empowered to evaluate diverse solutions for various\nproblems and select the most fitting one for a given task. This paper details\nthe fine-tuning process, and sheds light on how natural language descriptions\ncan be translated into functional code. Linguacodus represents a substantial\nleap towards automated code generation, effectively bridging the gap between\ntask descriptions and executable code. It holds great promise for advancing\nmachine learning applications across diverse domains. Additionally, we propose\nan algorithm capable of transforming a natural description of an ML task into\ncode with minimal human interaction. In extensive experiments on a vast machine\nlearning code dataset originating from Kaggle, we showcase the effectiveness of\nLinguacodus. The investigations highlight its potential applications across\ndiverse domains, emphasizing its impact on applied machine learning in various\nscientific fields.\n","authors":["Ekaterina Trofimova","Emil Sataev","Andrey E. Ustyuzhanin"],"pdf_url":"https://arxiv.org/pdf/2403.11585v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21271v2","updated":"2024-11-21T16:12:34Z","published":"2024-10-28T17:59:03Z","title":"EoRA: Training-free Compensation for Compressed LLM with Eigenspace\n  Low-Rank Approximation","summary":"  In this work, we re-formulate the model compression problem into the\ncustomized compensation problem: Given a compressed model, we aim to introduce\nresidual low-rank paths to compensate for compression errors under customized\nrequirements from users (e.g., tasks, compression ratios), resulting in greater\nflexibility in adjusting overall capacity without being constrained by specific\ncompression formats. However, naively applying SVD to derive residual paths\ncauses suboptimal utilization of the low-rank representation capacity. Instead,\nwe propose Training-free Eigenspace Low-Rank Approximation (EoRA), a method\nthat directly minimizes compression-induced errors without requiring\ngradient-based training, achieving fast optimization in minutes using a small\namount of calibration data. EoRA projects compression errors into the\neigenspace of input activations, leveraging eigenvalues to effectively\nprioritize the reconstruction of high-importance error components. Moreover,\nEoRA can be seamlessly integrated with fine-tuning and quantization to further\nimprove effectiveness and efficiency. EoRA consistently outperforms previous\nmethods in compensating errors for compressed LLaMA2/3 models on various tasks,\nsuch as language generation, commonsense reasoning, and math reasoning tasks\n(e.g., 31.31%/12.88% and 9.69% improvements on ARC-Easy/ARC-Challenge and\nMathQA when compensating LLaMA3-8B that is quantized to 4-bit and pruned to 2:4\nsparsity). EoRA offers a scalable, training-free solution to compensate for\ncompression errors, making it a powerful tool to deploy LLMs in various\ncapacity and efficiency requirements.\n","authors":["Shih-Yang Liu","Huck Yang","Chien-Yi Wang","Nai Chit Fung","Hongxu Yin","Charbel Sakr","Saurav Muralidharan","Kwang-Ting Cheng","Jan Kautz","Yu-Chiang Frank Wang","Pavlo Molchanov","Min-Hung Chen"],"pdf_url":"https://arxiv.org/pdf/2410.21271v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14258v1","updated":"2024-11-21T16:09:05Z","published":"2024-11-21T16:09:05Z","title":"Knowledge Graphs, Large Language Models, and Hallucinations: An NLP\n  Perspective","summary":"  Large Language Models (LLMs) have revolutionized Natural Language Processing\n(NLP) based applications including automated text generation, question\nanswering, chatbots, and others. However, they face a significant challenge:\nhallucinations, where models produce plausible-sounding but factually incorrect\nresponses. This undermines trust and limits the applicability of LLMs in\ndifferent domains. Knowledge Graphs (KGs), on the other hand, provide a\nstructured collection of interconnected facts represented as entities (nodes)\nand their relationships (edges). In recent research, KGs have been leveraged to\nprovide context that can fill gaps in an LLM understanding of certain topics\noffering a promising approach to mitigate hallucinations in LLMs, enhancing\ntheir reliability and accuracy while benefiting from their wide applicability.\nNonetheless, it is still a very active area of research with various unresolved\nopen problems. In this paper, we discuss these open challenges covering\nstate-of-the-art datasets and benchmarks as well as methods for knowledge\nintegration and evaluating hallucinations. In our discussion, we consider the\ncurrent use of KGs in LLM systems and identify future directions within each of\nthese challenges.\n","authors":["Ernests Lavrinovics","Russa Biswas","Johannes Bjerva","Katja Hose"],"pdf_url":"https://arxiv.org/pdf/2411.14258v1.pdf","comment":"7 pages, 2 Figures, 1 Table"},{"id":"http://arxiv.org/abs/2411.05930v2","updated":"2024-11-21T16:06:05Z","published":"2024-11-08T19:31:19Z","title":"BERTrend: Neural Topic Modeling for Emerging Trends Detection","summary":"  Detecting and tracking emerging trends and weak signals in large, evolving\ntext corpora is vital for applications such as monitoring scientific\nliterature, managing brand reputation, surveilling critical infrastructure and\nmore generally to any kind of text-based event detection. Existing solutions\noften fail to capture the nuanced context or dynamically track evolving\npatterns over time. BERTrend, a novel method, addresses these limitations using\nneural topic modeling in an online setting. It introduces a new metric to\nquantify topic popularity over time by considering both the number of documents\nand update frequency. This metric classifies topics as noise, weak, or strong\nsignals, flagging emerging, rapidly growing topics for further investigation.\nExperimentation on two large real-world datasets demonstrates BERTrend's\nability to accurately detect and track meaningful weak signals while filtering\nout noise, offering a comprehensive solution for monitoring emerging trends in\nlarge-scale, evolving text corpora. The method can also be used for\nretrospective analysis of past events. In addition, the use of Large Language\nModels together with BERTrend offers efficient means for the interpretability\nof trends of events.\n","authors":["Allaa Boutaleb","Jerome Picault","Guillaume Grosjean"],"pdf_url":"https://arxiv.org/pdf/2411.05930v2.pdf","comment":"17 pages, 12 figures, FuturED 2024: Workshop on Future of Event\n  Detection (CoLocated with EMNLP 2024)"},{"id":"http://arxiv.org/abs/2411.14257v1","updated":"2024-11-21T16:05:58Z","published":"2024-11-21T16:05:58Z","title":"Do I Know This Entity? Knowledge Awareness and Hallucinations in\n  Language Models","summary":"  Hallucinations in large language models are a widespread problem, yet the\nmechanisms behind whether models will hallucinate are poorly understood,\nlimiting our ability to solve this problem. Using sparse autoencoders as an\ninterpretability tool, we discover that a key part of these mechanisms is\nentity recognition, where the model detects if an entity is one it can recall\nfacts about. Sparse autoencoders uncover meaningful directions in the\nrepresentation space, these detect whether the model recognizes an entity, e.g.\ndetecting it doesn't know about an athlete or a movie. This suggests that\nmodels can have self-knowledge: internal representations about their own\ncapabilities. These directions are causally relevant: capable of steering the\nmodel to refuse to answer questions about known entities, or to hallucinate\nattributes of unknown entities when it would otherwise refuse. We demonstrate\nthat despite the sparse autoencoders being trained on the base model, these\ndirections have a causal effect on the chat model's refusal behavior,\nsuggesting that chat finetuning has repurposed this existing mechanism.\nFurthermore, we provide an initial exploration into the mechanistic role of\nthese directions in the model, finding that they disrupt the attention of\ndownstream heads that typically move entity attributes to the final token.\n","authors":["Javier Ferrando","Oscar Obeso","Senthooran Rajamanoharan","Neel Nanda"],"pdf_url":"https://arxiv.org/pdf/2411.14257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14252v1","updated":"2024-11-21T15:59:29Z","published":"2024-11-21T15:59:29Z","title":"Intent-Aware Dialogue Generation and Multi-Task Contrastive Learning for\n  Multi-Turn Intent Classification","summary":"  Generating large-scale, domain-specific, multilingual multi-turn dialogue\ndatasets remains a significant hurdle for training effective Multi-Turn Intent\nClassification models in chatbot systems. In this paper, we introduce\nChain-of-Intent, a novel mechanism that combines Hidden Markov Models with\nLarge Language Models (LLMs) to generate contextually aware, intent-driven\nconversations through self-play. By extracting domain-specific knowledge from\ne-commerce chat logs, we estimate conversation turns and intent transitions,\nwhich guide the generation of coherent dialogues. Leveraging LLMs to enhance\nemission probabilities, our approach produces natural and contextually\nconsistent questions and answers. We also propose MINT-CL, a framework for\nmulti-turn intent classification using multi-task contrastive learning,\nimproving classification accuracy without the need for extensive annotated\ndata. Evaluations show that our methods outperform baselines in dialogue\nquality and intent classification accuracy, especially in multilingual\nsettings, while significantly reducing data generation efforts. Furthermore, we\nrelease MINT-E, a multilingual, intent-aware multi-turn e-commerce dialogue\ncorpus to support future research in this area.\n","authors":["Junhua Liu","Yong Keat Tan","Bin Fu","Kwan Hui Lim"],"pdf_url":"https://arxiv.org/pdf/2411.14252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14251v1","updated":"2024-11-21T15:57:02Z","published":"2024-11-21T15:57:02Z","title":"Natural Language Reinforcement Learning","summary":"  Reinforcement Learning (RL) mathematically formulates decision-making with\nMarkov Decision Process (MDP). With MDPs, researchers have achieved remarkable\nbreakthroughs across various domains, including games, robotics, and language\nmodels. This paper seeks a new possibility, Natural Language Reinforcement\nLearning (NLRL), by extending traditional MDP to natural language-based\nrepresentation space. Specifically, NLRL innovatively redefines RL principles,\nincluding task objectives, policy, value function, Bellman equation, and policy\niteration, into their language counterparts. With recent advancements in large\nlanguage models (LLMs), NLRL can be practically implemented to achieve RL-like\npolicy and value improvement by either pure prompting or gradient-based\ntraining. Experiments over Maze, Breakthrough, and Tic-Tac-Toe games\ndemonstrate the effectiveness, efficiency, and interpretability of the NLRL\nframework among diverse use cases. Our code will be released at\nhttps://github.com/waterhorse1/Natural-language-RL.\n","authors":["Xidong Feng","Ziyu Wan","Haotian Fu","Bo Liu","Mengyue Yang","Girish A. Koushik","Zhiyuan Hu","Ying Wen","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2411.14251v1.pdf","comment":"Extension of arXiv:2402.07157"},{"id":"http://arxiv.org/abs/2411.14215v1","updated":"2024-11-21T15:25:08Z","published":"2024-11-21T15:25:08Z","title":"Evaluating the Robustness of Analogical Reasoning in Large Language\n  Models","summary":"  LLMs have performed well on several reasoning benchmarks, including ones that\ntest analogical reasoning abilities. However, there is debate on the extent to\nwhich they are performing general abstract reasoning versus employing\nnon-robust processes, e.g., that overly rely on similarity to pre-training\ndata. Here we investigate the robustness of analogy-making abilities previously\nclaimed for LLMs on three of four domains studied by Webb, Holyoak, and Lu\n(2023): letter-string analogies, digit matrices, and story analogies. For each\ndomain we test humans and GPT models on robustness to variants of the original\nanalogy problems that test the same abstract reasoning abilities but are likely\ndissimilar from tasks in the pre-training data. The performance of a system\nthat uses robust abstract reasoning should not decline substantially on these\nvariants.\n  On simple letter-string analogies, we find that while the performance of\nhumans remains high for two types of variants we tested, the GPT models'\nperformance declines sharply. This pattern is less pronounced as the complexity\nof these problems is increased, as both humans and GPT models perform poorly on\nboth the original and variant problems requiring more complex analogies. On\ndigit-matrix problems, we find a similar pattern but only on one out of the two\ntypes of variants we tested. On story-based analogy problems, we find that,\nunlike humans, the performance of GPT models are susceptible to answer-order\neffects, and that GPT models also may be more sensitive than humans to\nparaphrasing.\n  This work provides evidence that LLMs often lack the robustness of zero-shot\nhuman analogy-making, exhibiting brittleness on most of the variations we\ntested. More generally, this work points to the importance of carefully\nevaluating AI systems not only for accuracy but also robustness when testing\ntheir cognitive capabilities.\n","authors":["Martha Lewis","Melanie Mitchell"],"pdf_url":"https://arxiv.org/pdf/2411.14215v1.pdf","comment":"31 pages, 13 figures. arXiv admin note: text overlap with\n  arXiv:2402.08955"},{"id":"http://arxiv.org/abs/2411.14199v1","updated":"2024-11-21T15:07:42Z","published":"2024-11-21T15:07:42Z","title":"OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented\n  LMs","summary":"  Scientific progress depends on researchers' ability to synthesize the growing\nbody of literature. Can large language models (LMs) assist scientists in this\ntask? We introduce OpenScholar, a specialized retrieval-augmented LM that\nanswers scientific queries by identifying relevant passages from 45 million\nopen-access papers and synthesizing citation-backed responses. To evaluate\nOpenScholar, we develop ScholarQABench, the first large-scale multi-domain\nbenchmark for literature search, comprising 2,967 expert-written queries and\n208 long-form answers across computer science, physics, neuroscience, and\nbiomedicine. On ScholarQABench, OpenScholar-8B outperforms GPT-4o by 5% and\nPaperQA2 by 7% in correctness, despite being a smaller, open model. While GPT4o\nhallucinates citations 78 to 90% of the time, OpenScholar achieves citation\naccuracy on par with human experts. OpenScholar's datastore, retriever, and\nself-feedback inference loop also improves off-the-shelf LMs: for instance,\nOpenScholar-GPT4o improves GPT-4o's correctness by 12%. In human evaluations,\nexperts preferred OpenScholar-8B and OpenScholar-GPT4o responses over\nexpert-written ones 51% and 70% of the time, respectively, compared to GPT4o's\n32%. We open-source all of our code, models, datastore, data and a public demo.\n","authors":["Akari Asai","Jacqueline He","Rulin Shao","Weijia Shi","Amanpreet Singh","Joseph Chee Chang","Kyle Lo","Luca Soldaini","Sergey Feldman","Mike D'arcy","David Wadden","Matt Latzke","Minyang Tian","Pan Ji","Shengyan Liu","Hao Tong","Bohao Wu","Yanyu Xiong","Luke Zettlemoyer","Graham Neubig","Dan Weld","Doug Downey","Wen-tau Yih","Pang Wei Koh","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2411.14199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14198v1","updated":"2024-11-21T15:06:51Z","published":"2024-11-21T15:06:51Z","title":"Why do language models perform worse for morphologically complex\n  languages?","summary":"  Language models perform differently across languages. It has been previously\nsuggested that morphological typology may explain some of this variability\n(Cotterell et al., 2018). We replicate previous analyses and find additional\nnew evidence for a performance gap between agglutinative and fusional\nlanguages, where fusional languages, such as English, tend to have better\nlanguage modeling performance than morphologically more complex languages like\nTurkish. We then propose and test three possible causes for this performance\ngap: morphological alignment of tokenizers, tokenization quality, and\ndisparities in dataset sizes and measurement. To test the morphological\nalignment hypothesis, we present MorphScore, a tokenizer evaluation metric, and\nsupporting datasets for 22 languages. We find some evidence that tokenization\nquality explains the performance gap, but none for the role of morphological\nalignment. Instead we find that the performance gap is most reduced when\ntraining datasets are of equivalent size across language types, but only when\nscaled according to the so-called \"byte-premium\" -- the different encoding\nefficiencies of different languages and orthographies. These results suggest\nthat no language is harder or easier for a language model to learn on the basis\nof its morphological typology. Differences in performance can be attributed to\ndisparities in dataset size. These results bear on ongoing efforts to improve\nperformance for low-performing and under-resourced languages.\n","authors":["Catherine Arnett","Benjamin K. Bergen"],"pdf_url":"https://arxiv.org/pdf/2411.14198v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2406.04289v4","updated":"2024-11-21T14:27:01Z","published":"2024-06-06T17:34:24Z","title":"What Languages are Easy to Language-Model? A Perspective from Learning\n  Probabilistic Regular Languages","summary":"  What can large language models learn? By definition, language models (LM) are\ndistributions over strings. Therefore, an intuitive way of addressing the above\nquestion is to formalize it as a matter of learnability of classes of\ndistributions over strings. While prior work in this direction focused on\nassessing the theoretical limits, in contrast, we seek to understand the\nempirical learnability. Unlike prior empirical work, we evaluate neural LMs on\ntheir home turf-learning probabilistic languages-rather than as classifiers of\nformal languages. In particular, we investigate the learnability of regular LMs\n(RLMs) by RNN and Transformer LMs. We empirically test the learnability of RLMs\nas a function of various complexity parameters of the RLM and the hidden state\nsize of the neural LM. We find that the RLM rank, which corresponds to the size\nof linear space spanned by the logits of its conditional distributions, and the\nexpected length of sampled strings are strong and significant predictors of\nlearnability for both RNNs and Transformers. Several other predictors also\nreach significance, but with differing patterns between RNNs and Transformers.\n","authors":["Nadav Borenstein","Anej Svete","Robin Chan","Josef Valvoda","Franz Nowak","Isabelle Augenstein","Eleanor Chodroff","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2406.04289v4.pdf","comment":"Accepted to ACL 2024"},{"id":"http://arxiv.org/abs/2411.14137v1","updated":"2024-11-21T14:01:42Z","published":"2024-11-21T14:01:42Z","title":"Visual Contexts Clarify Ambiguous Expressions: A Benchmark Dataset","summary":"  The ability to perform complex reasoning across multimodal inputs is\nessential for models to effectively interact with humans in real-world\nscenarios. Advancements in vision-language models have significantly improved\nperformance on tasks that require processing explicit and direct textual\ninputs, such as Visual Question Answering (VQA) and Visual Grounding (VG).\nHowever, less attention has been given to improving the model capabilities to\ncomprehend nuanced and ambiguous forms of communication. This presents a\ncritical challenge, as human language in real-world interactions often convey\nhidden intentions that rely on context for accurate interpretation. To address\nthis gap, we propose VAGUE, a multimodal benchmark comprising 3.9K indirect\nhuman utterances paired with corresponding scenes. Additionally, we contribute\na model-based pipeline for generating prompt-solution pairs from input images.\nOur work aims to delve deeper into the ability of models to understand indirect\ncommunication and seek to contribute to the development of models capable of\nmore refined and human-like interactions. Extensive evaluation on multiple VLMs\nreveals that mainstream models still struggle with indirect communication when\nrequired to perform complex linguistic and visual reasoning. We release our\ncode and data at https://github.com/Hazel-Heejeong-Nam/VAGUE.git.\n","authors":["Heejeong Nam","Jinwoo Ahn"],"pdf_url":"https://arxiv.org/pdf/2411.14137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14121v1","updated":"2024-11-21T13:45:40Z","published":"2024-11-21T13:45:40Z","title":"Learning from \"Silly\" Questions Improves Large Language Models, But Only\n  Slightly","summary":"  Constructing high-quality Supervised Fine-Tuning (SFT) datasets is critical\nfor the training of large language models (LLMs). Recent studies have shown\nthat using data from a specific source, Ruozhiba, a Chinese website where users\nask \"silly\" questions to better understand certain topics, can lead to better\nfine-tuning performance. This paper aims to explore some hidden factors: the\npotential interpretations of its success and a large-scale evaluation of the\nperformance. First, we leverage GPT-4 to analyze the successful cases of\nRuozhiba questions from the perspective of education, psychology, and cognitive\nscience, deriving a set of explanatory rules. Then, we construct fine-tuning\ndatasets by applying these rules to the MMLU training set. Surprisingly, our\nresults indicate that rules can significantly improve model performance in\ncertain tasks, while potentially diminishing performance on others. For\nexample, SFT data generated following the \"Counterintuitive Thinking\" rule can\nachieve approximately a 5% improvement on the \"Global Facts\" task, whereas the\n\"Blurring the Conceptual Boundaries\" rule leads to a performance drop of 6.14%\non the \"Econometrics\" task. In addition, for specific tasks, different rules\ntend to have a consistent impact on model performance. This suggests that the\ndifferences between the extracted rules are not as significant, and the\neffectiveness of the rules is relatively consistent across tasks. Our research\nhighlights the importance of considering task diversity and rule applicability\nwhen constructing SFT datasets to achieve more comprehensive performance\nimprovements.\n","authors":["Tingyuan Zhu","Shudong Liu","Yidong Wang","Derek F. Wong","Han Yu","Takahiro Shinozaki","Jindong Wang"],"pdf_url":"https://arxiv.org/pdf/2411.14121v1.pdf","comment":"27 pages, 14 figures"},{"id":"http://arxiv.org/abs/2411.14103v1","updated":"2024-11-21T13:09:36Z","published":"2024-11-21T13:09:36Z","title":"Lost in Inference: Rediscovering the Role of Natural Language Inference\n  for Large Language Models","summary":"  In the recent past, a popular way of evaluating natural language\nunderstanding (NLU), was to consider a model's ability to perform natural\nlanguage inference (NLI) tasks. In this paper, we investigate if NLI tasks,\nthat are rarely used for LLM evaluation, can still be informative for\nevaluating LLMs. Focusing on five different NLI benchmarks across six models of\ndifferent scales, we investigate if they are able to discriminate models of\ndifferent size and quality and how their accuracies develop during training.\nFurthermore, we investigate the extent to which the softmax distributions of\nmodels align with human distributions in cases where statements are ambiguous\nor vague. Overall, our results paint a positive picture for the NLI tasks: we\nfind that they are able to discriminate well between models at various stages\nof training, yet are not (all) saturated. Furthermore, we find that while the\nsimilarity of model distributions with human label distributions increases with\nscale, it is still much higher than the similarity between two populations of\nhumans, making it a potentially interesting statistic to consider.\n","authors":["Lovish Madaan","David Esiobu","Pontus Stenetorp","Barbara Plank","Dieuwke Hupkes"],"pdf_url":"https://arxiv.org/pdf/2411.14103v1.pdf","comment":"preprint, 13 pages"},{"id":"http://arxiv.org/abs/2210.04359v3","updated":"2024-11-21T13:06:53Z","published":"2022-10-09T22:02:58Z","title":"Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years\n  of German Parliamentary Debates","summary":"  Solidarity is a crucial concept to understand social relations in societies.\nIn this paper, we explore fine-grained solidarity frames to study solidarity\ntowards women and migrants in German parliamentary debates between 1867 and\n2022. Using 2,864 manually annotated text snippets (with a cost exceeding 18k\nEuro), we evaluate large language models (LLMs) like Llama 3, GPT-3.5, and\nGPT-4. We find that GPT-4 outperforms other LLMs, approaching human annotation\nquality. Using GPT-4, we automatically annotate more than 18k further instances\n(with a cost of around 500 Euro) across 155 years and find that solidarity with\nmigrants outweighs anti-solidarity but that frequencies and solidarity types\nshift over time. Most importantly, group-based notions of (anti-)solidarity\nfade in favor of compassionate solidarity, focusing on the vulnerability of\nmigrant groups, and exchange-based anti-solidarity, focusing on the lack of\n(economic) contribution. Our study highlights the interplay of historical\nevents, socio-economic needs, and political ideologies in shaping migration\ndiscourse and social cohesion. We also show that powerful LLMs, if carefully\nprompted, can be cost-effective alternatives to human annotation for hard\nsocial scientific tasks.\n","authors":["Aida Kostikova","Benjamin Paassen","Dominik Beese","Ole Pütz","Gregor Wiedemann","Steffen Eger"],"pdf_url":"https://arxiv.org/pdf/2210.04359v3.pdf","comment":"EMNLP 2024 (Main Conference) Camera-Ready Version"},{"id":"http://arxiv.org/abs/2411.14100v1","updated":"2024-11-21T13:05:18Z","published":"2024-11-21T13:05:18Z","title":"BEST-STD: Bidirectional Mamba-Enhanced Speech Tokenization for Spoken\n  Term Detection","summary":"  Spoken term detection (STD) is often hindered by reliance on frame-level\nfeatures and the computationally intensive DTW-based template matching,\nlimiting its practicality. To address these challenges, we propose a novel\napproach that encodes speech into discrete, speaker-agnostic semantic tokens.\nThis facilitates fast retrieval using text-based search algorithms and\neffectively handles out-of-vocabulary terms. Our approach focuses on generating\nconsistent token sequences across varying utterances of the same term. We also\npropose a bidirectional state space modeling within the Mamba encoder, trained\nin a self-supervised learning framework, to learn contextual frame-level\nfeatures that are further encoded into discrete tokens. Our analysis shows that\nour speech tokens exhibit greater speaker invariance than those from existing\ntokenizers, making them more suitable for STD tasks. Empirical evaluation on\nLibriSpeech and TIMIT databases indicates that our method outperforms existing\nSTD baselines while being more efficient.\n","authors":["Anup Singh","Kris Demuynck","Vipul Arora"],"pdf_url":"https://arxiv.org/pdf/2411.14100v1.pdf","comment":"Submitted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2411.14073v1","updated":"2024-11-21T12:38:23Z","published":"2024-11-21T12:38:23Z","title":"Meaning at the Planck scale? Contextualized word embeddings for doing\n  history, philosophy, and sociology of science","summary":"  This paper explores the potential of contextualized word embeddings (CWEs) as\na new tool in the history, philosophy, and sociology of science (HPSS) for\nstudying contextual and evolving meanings of scientific concepts. Using the\nterm \"Planck\" as a test case, I evaluate five BERT-based models with varying\ndegrees of domain-specific pretraining, including my custom model\nAstro-HEP-BERT, trained on the Astro-HEP Corpus, a dataset containing 21.84\nmillion paragraphs from 600,000 articles in astrophysics and high-energy\nphysics. For this analysis, I compiled two labeled datasets: (1) the\nAstro-HEP-Planck Corpus, consisting of 2,900 labeled occurrences of \"Planck\"\nsampled from 1,500 paragraphs in the Astro-HEP Corpus, and (2) a\nphysics-related Wikipedia dataset comprising 1,186 labeled occurrences of\n\"Planck\" across 885 paragraphs. Results demonstrate that the domain-adapted\nmodels outperform the general-purpose ones in disambiguating the target term,\npredicting its known meanings, and generating high-quality sense clusters, as\nmeasured by a novel purity indicator I developed. Additionally, this approach\nreveals semantic shifts in the target term over three decades in the unlabeled\nAstro-HEP Corpus, highlighting the emergence of the Planck space mission as a\ndominant sense. The study underscores the importance of domain-specific\npretraining for analyzing scientific language and demonstrates the\ncost-effectiveness of adapting pretrained models for HPSS research. By offering\na scalable and transferable method for modeling the meanings of scientific\nconcepts, CWEs open up new avenues for investigating the socio-historical\ndynamics of scientific discourses.\n","authors":["Arno Simons"],"pdf_url":"https://arxiv.org/pdf/2411.14073v1.pdf","comment":"18 pages, 7 figures (1 in the Supplement)"},{"id":"http://arxiv.org/abs/2411.14072v1","updated":"2024-11-21T12:36:19Z","published":"2024-11-21T12:36:19Z","title":"The Master-Slave Encoder Model for Improving Patent Text Summarization:\n  A New Approach to Combining Specifications and Claims","summary":"  In order to solve the problem of insufficient generation quality caused by\ntraditional patent text abstract generation models only originating from patent\nspecifications, the problem of new terminology OOV caused by rapid patent\nupdates, and the problem of information redundancy caused by insufficient\nconsideration of the high professionalism, accuracy, and uniqueness of patent\ntexts, we proposes a patent text abstract generation model (MSEA) based on a\nmaster-slave encoder architecture; Firstly, the MSEA model designs a\nmaster-slave encoder, which combines the instructions in the patent text with\nthe claims as input, and fully explores the characteristics and details between\nthe two through the master-slave encoder; Then, the model enhances the\nconsideration of new technical terms in the input sequence based on the pointer\nnetwork, and further enhances the correlation with the input text by re\nweighing the \"remembered\" and \"for-gotten\" parts of the input sequence from the\nencoder; Finally, an enhanced repetition suppression mechanism for patent text\nwas introduced to ensure accurate and non redundant abstracts generated. On a\npublicly available patent text dataset, compared to the state-of-the-art model,\nImproved Multi-Head Attention Mechanism (IMHAM), the MSEA model achieves an\nimprovement of 0.006, 0.005, and 0.005 in Rouge-1, Rouge-2, and Rouge-L scores,\nrespectively. MSEA leverages the characteristics of patent texts to effectively\nenhance the quality of patent text generation, demonstrating its advancement\nand effectiveness in the experiments.\n","authors":["Shu Zhou","Xin Wang","Zhengda Zhou","Haohan Yi","Xuhui Zheng","Hao Wan"],"pdf_url":"https://arxiv.org/pdf/2411.14072v1.pdf","comment":"25pages, 1 figure"},{"id":"http://arxiv.org/abs/2410.14729v2","updated":"2024-11-21T12:17:29Z","published":"2024-10-16T07:13:35Z","title":"Is Less More? Exploring Token Condensation as Training-free Adaptation\n  for CLIP","summary":"  Contrastive language-image pre-training (CLIP) has shown remarkable\ngeneralization ability in image classification. However, CLIP sometimes\nencounters performance drops on downstream datasets during zero-shot inference.\nTest-time adaptation methods attempt to mitigate this by adjusting\nnormalization layers or tuning context prompts with large batch sizes and\nextensive augmentations; yet, these methods are computationally intensive. This\nraises an important question: Is there a training-free approach that can\nefficiently address CLIP's performance drop in such cases? To explore this, we\nbenchmark token condensation techniques, originally designed to enhance the\nefficiency of vision transformers, on CLIP zero-shot inference tasks. We\nobserve that although token condensation may compromise in-domain accuracy, it\nsurprisingly enhances CLIP's performance on certain cross-dataset benchmarks.\nThis motivates two key inquiries: (1) Can token condensation serve as a\n\"free-lunch\" solution for CLIP zero-shot inference? (2) What criteria should\nguide condensation -- how can essential tokens be identified and redundant ones\neliminated? To address these questions, we propose Token Condensation as\nAdaptation (TCA), a training-free adaptation method for CLIP by pruning\nclass-irrelevant visual tokens while merging class-ambiguous tokens. As the\nfirst approach for CLIP's token efficiency, TCA demonstrates superior\nperformance across cross-dataset tasks, achieving up to a 21.4\\% improvement\nover the strongest baseline while reducing GFLOPs by 12.2\\% to 48.9\\%, with\nminimized hyperparameter dependency.\n","authors":["Zixin Wang","Dong Gong","Sen Wang","Zi Huang","Yadan Luo"],"pdf_url":"https://arxiv.org/pdf/2410.14729v2.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2406.12907v3","updated":"2024-11-21T12:17:22Z","published":"2024-06-12T13:30:48Z","title":"Reconciling Kaplan and Chinchilla Scaling Laws","summary":"  Kaplan et al. [2020] (`Kaplan') and Hoffmann et al. [2022] (`Chinchilla')\nstudied the scaling behavior of transformers trained on next-token language\nprediction. These studies produced different estimates for how the number of\nparameters ($N$) and training tokens ($D$) should be set to achieve the lowest\npossible loss for a given compute budget ($C$). Kaplan: $N_\\text{optimal}\n\\propto C^{0.73}$, Chinchilla: $N_\\text{optimal} \\propto C^{0.50}$. This paper\nfinds that much of this discrepancy can be attributed to Kaplan counting\nnon-embedding rather than total parameters, combined with their analysis being\nperformed at small scale. Simulating the Chinchilla study under these\nconditions produces biased scaling coefficients close to Kaplan's. Hence, this\npaper reaffirms Chinchilla's scaling coefficients, by explaining the primary\ncause of Kaplan's original overestimation. As a second contribution, the paper\nexplains differences in the reported relationships between loss and compute.\nThese findings lead us to recommend that future scaling studies use total\nparameters and compute.\n","authors":["Tim Pearce","Jinyeop Song"],"pdf_url":"https://arxiv.org/pdf/2406.12907v3.pdf","comment":"Published in TMLR 2024"},{"id":"http://arxiv.org/abs/2411.14062v1","updated":"2024-11-21T12:16:16Z","published":"2024-11-21T12:16:16Z","title":"MMGenBench: Evaluating the Limits of LMMs from the Text-to-Image\n  Generation Perspective","summary":"  Large Multimodal Models (LMMs) have demonstrated remarkable capabilities.\nWhile existing benchmarks for evaluating LMMs mainly focus on image\ncomprehension, few works evaluate them from the image generation perspective.\nTo address this issue, we propose a straightforward automated evaluation\npipeline. Specifically, this pipeline requires LMMs to generate an image-prompt\nfrom a given input image. Subsequently, it employs text-to-image generative\nmodels to create a new image based on these generated prompts. Finally, we\nevaluate the performance of LMMs by comparing the original image with the\ngenerated one. Furthermore, we introduce MMGenBench-Test, a comprehensive\nbenchmark developed to evaluate LMMs across 13 distinct image patterns, and\nMMGenBench-Domain, targeting the performance evaluation of LMMs within the\ngenerative image domain. A thorough evaluation involving over 50 popular LMMs\ndemonstrates the effectiveness and reliability in both the pipeline and\nbenchmark. Our observations indicate that numerous LMMs excelling in existing\nbenchmarks fail to adequately complete the basic tasks, related to image\nunderstanding and description. This finding highlights the substantial\npotential for performance improvement in current LMMs and suggests avenues for\nfuture model optimization. Concurrently, our pipeline facilitates the efficient\nassessment of LMMs performance across diverse domains by using solely image\ninputs.\n","authors":["Hailang Huang","Yong Wang","Zixuan Huang","Huaqiu Li","Tongwen Huang","Xiangxiang Chu","Richong Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14062v1.pdf","comment":"This project is available at: https://github.com/lerogo/MMGenBench"},{"id":"http://arxiv.org/abs/2411.02193v2","updated":"2024-11-21T12:10:54Z","published":"2024-11-04T15:46:20Z","title":"Improving Steering Vectors by Targeting Sparse Autoencoder Features","summary":"  To control the behavior of language models, steering methods attempt to\nensure that outputs of the model satisfy specific pre-defined properties.\nAdding steering vectors to the model is a promising method of model control\nthat is easier than finetuning, and may be more robust than prompting. However,\nit can be difficult to anticipate the effects of steering vectors produced by\nmethods such as CAA [Panickssery et al., 2024] or the direct use of SAE latents\n[Templeton et al., 2024]. In our work, we address this issue by using SAEs to\nmeasure the effects of steering vectors, giving us a method that can be used to\nunderstand the causal effect of any steering vector intervention. We use this\nmethod for measuring causal effects to develop an improved steering method,\nSAE-Targeted Steering (SAE-TS), which finds steering vectors to target specific\nSAE features while minimizing unintended side effects. We show that overall,\nSAE-TS balances steering effects with coherence better than CAA and SAE feature\nsteering, when evaluated on a range of tasks.\n","authors":["Sviatoslav Chalnev","Matthew Siu","Arthur Conmy"],"pdf_url":"https://arxiv.org/pdf/2411.02193v2.pdf","comment":"8 maintext pages and 9 appendix pages"},{"id":"http://arxiv.org/abs/2411.14055v1","updated":"2024-11-21T12:02:39Z","published":"2024-11-21T12:02:39Z","title":"DRPruning: Efficient Large Language Model Pruning through\n  Distributionally Robust Optimization","summary":"  Large language models (LLMs) deliver impressive results but face challenges\nfrom increasing model sizes and computational costs. Structured pruning reduces\nmodel size and speeds up inference but often causes uneven degradation across\ndomains, leading to biased performance. To address this, we propose DRPruning,\nwhich incorporates distributionally robust optimization to restore balanced\nperformance across domains, along with further improvements to enhance\nrobustness. Experiments in monolingual and multilingual settings show that our\nmethod surpasses similarly sized models in pruning and continued pretraining\nover perplexity, downstream tasks, and instruction tuning. We further provide\nanalysis demonstrating the robustness of our method towards various domains and\ndistribution shifts. Furthermore, our method automatically determines optimal\nreference losses and data ratios, suggesting potential for broader\napplications. Our code is available at https://github.com/hexuandeng/DRPruning.\n","authors":["Hexuan Deng","Wenxiang Jiao","Xuebo Liu","Min Zhang","Zhaopeng Tu"],"pdf_url":"https://arxiv.org/pdf/2411.14055v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2411.03962v2","updated":"2024-11-21T12:00:23Z","published":"2024-11-06T14:51:02Z","title":"How Does A Text Preprocessing Pipeline Affect Ontology Syntactic\n  Matching?","summary":"  The generic text preprocessing pipeline, comprising Tokenisation,\nNormalisation, Stop Words Removal, and Stemming/Lemmatisation, has been\nimplemented in many ontology matching (OM) systems. However, the lack of\nstandardisation in text preprocessing creates diversity in mapping results. In\nthis paper, we investigate the effect of the text preprocessing pipeline on OM\ntasks at syntactic levels. Our experiments on 8 Ontology Alignment Evaluation\nInitiative (OAEI) track repositories with 49 distinct alignments indicate: (1)\nTokenisation and Normalisation are currently more effective than Stop Words\nRemoval and Stemming/Lemmatisation; and (2) The selection of Lemmatisation and\nStemming is task-specific. We recommend standalone Lemmatisation or Stemming\nwith post-hoc corrections. We find that (3) Porter Stemmer and Snowball Stemmer\nperform better than Lancaster Stemmer; and that (4) Part-of-Speech (POS)\nTagging does not help Lemmatisation. To repair less effective Stop Words\nRemoval and Stemming/Lemmatisation used in OM tasks, we propose a novel\ncontext-based pipeline repair approach that significantly improves matching\ncorrectness and overall matching performance. We also discuss the use of text\npreprocessing pipeline in the new era of large language models (LLMs).\n","authors":["Zhangcheng Qiang","Kerry Taylor","Weiqing Wang"],"pdf_url":"https://arxiv.org/pdf/2411.03962v2.pdf","comment":"13 pages, 26 figures, 4 tables"},{"id":"http://arxiv.org/abs/2411.14054v1","updated":"2024-11-21T11:59:13Z","published":"2024-11-21T11:59:13Z","title":"FunctionChat-Bench: Comprehensive Evaluation of Language Models'\n  Generative Capabilities in Korean Tool-use Dialogs","summary":"  This study investigates language models' generative capabilities in tool-use\ndialogs. We categorize the models' outputs in tool-use dialogs into four\ndistinct types: Tool Call, Answer Completion, Slot Question, and Relevance\nDetection, which serve as aspects for evaluation. We introduce\nFunctionChat-Bench, comprising 700 evaluation items and automated assessment\nprograms. Using this benchmark, we evaluate several language models that\nsupport function calling. Our findings indicate that while language models may\nexhibit high accuracy in single-turn Tool Call scenarios, this does not\nnecessarily translate to superior generative performance in multi-turn\nenvironments. We argue that the capabilities required for function calling\nextend beyond generating tool call messages; they must also effectively\ngenerate conversational messages that engage the user.\n","authors":["Shinbok Lee","Gaeun Seo","Daniel Lee","Byeongil Ko","Sunghee Jung","Myeongcheol Shin"],"pdf_url":"https://arxiv.org/pdf/2411.14054v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2411.14042v1","updated":"2024-11-21T11:44:23Z","published":"2024-11-21T11:44:23Z","title":"Forecasting Future International Events: A Reliable Dataset for\n  Text-Based Event Modeling","summary":"  Predicting future international events from textual information, such as news\narticles, has tremendous potential for applications in global policy, strategic\ndecision-making, and geopolitics. However, existing datasets available for this\ntask are often limited in quality, hindering the progress of related research.\nIn this paper, we introduce WORLDREP (WORLD Relationship and Event Prediction),\na novel dataset designed to address these limitations by leveraging the\nadvanced reasoning capabilities of large-language models (LLMs). Our dataset\nfeatures high-quality scoring labels generated through advanced prompt modeling\nand rigorously validated by domain experts in political science. We showcase\nthe quality and utility of WORLDREP for real-world event prediction tasks,\ndemonstrating its effectiveness through extensive experiments and analysis.\nFurthermore, we publicly release our dataset along with the full automation\nsource code for data collection, labeling, and benchmarking, aiming to support\nand advance research in text-based event prediction.\n","authors":["Daehoon Gwak","Junwoo Park","Minho Park","Chaehun Park","Hyunchan Lee","Edward Choi","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2411.14042v1.pdf","comment":"EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2411.11581v2","updated":"2024-11-21T11:27:34Z","published":"2024-11-18T13:57:35Z","title":"OASIS: Open Agents Social Interaction Simulations on One Million Agents","summary":"  There has been a growing interest in enhancing rule-based agent-based models\n(ABMs) for social media platforms (i.e., X, Reddit) with more realistic large\nlanguage model (LLM) agents, thereby allowing for a more nuanced study of\ncomplex systems. As a result, several LLM-based ABMs have been proposed in the\npast year. While they hold promise, each simulator is specifically designed to\nstudy a particular scenario, making it time-consuming and resource-intensive to\nexplore other phenomena using the same ABM. Additionally, these models simulate\nonly a limited number of agents, whereas real-world social media platforms\ninvolve millions of users. To this end, we propose OASIS, a generalizable and\nscalable social media simulator. OASIS is designed based on real-world social\nmedia platforms, incorporating dynamically updated environments (i.e., dynamic\nsocial networks and post information), diverse action spaces (i.e., following,\ncommenting), and recommendation systems (i.e., interest-based and\nhot-score-based). Additionally, OASIS supports large-scale user simulations,\ncapable of modeling up to one million users. With these features, OASIS can be\neasily extended to different social media platforms to study large-scale group\nphenomena and behaviors. We replicate various social phenomena, including\ninformation spreading, group polarization, and herd effects across X and Reddit\nplatforms. Moreover, we provide observations of social phenomena at different\nagent group scales. We observe that the larger agent group scale leads to more\nenhanced group dynamics and more diverse and helpful agents' opinions. These\nfindings demonstrate OASIS's potential as a powerful tool for studying complex\nsystems in digital environments.\n","authors":["Ziyi Yang","Zaibin Zhang","Zirui Zheng","Yuxian Jiang","Ziyue Gan","Zhiyu Wang","Zijian Ling","Jinsong Chen","Martz Ma","Bowen Dong","Prateek Gupta","Shuyue Hu","Zhenfei Yin","Guohao Li","Xu Jia","Lijun Wang","Bernard Ghanem","Huchuan Lu","Wanli Ouyang","Yu Qiao","Philip Torr","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2411.11581v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14012v1","updated":"2024-11-21T10:54:35Z","published":"2024-11-21T10:54:35Z","title":"Logic Augmented Generation","summary":"  Semantic Knowledge Graphs (SKG) face challenges with scalability,\nflexibility, contextual understanding, and handling unstructured or ambiguous\ninformation. However, they offer formal and structured knowledge enabling\nhighly interpretable and reliable results by means of reasoning and querying.\nLarge Language Models (LLMs) overcome those limitations making them suitable in\nopen-ended tasks and unstructured environments. Nevertheless, LLMs are neither\ninterpretable nor reliable. To solve the dichotomy between LLMs and SKGs we\nenvision Logic Augmented Generation (LAG) that combines the benefits of the two\nworlds. LAG uses LLMs as Reactive Continuous Knowledge Graphs that can generate\npotentially infinite relations and tacit knowledge on-demand. SKGs are key for\ninjecting a discrete heuristic dimension with clear logical and factual\nboundaries. We exemplify LAG in two tasks of collective intelligence, i.e.,\nmedical diagnostics and climate projections. Understanding the properties and\nlimitations of LAG, which are still mostly unknown, is of utmost importance for\nenabling a variety of tasks involving tacit knowledge in order to provide\ninterpretable and effective results.\n","authors":["Aldo Gangemi","Andrea Giovanni Nuzzolese"],"pdf_url":"https://arxiv.org/pdf/2411.14012v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2405.15145v3","updated":"2024-11-21T10:52:29Z","published":"2024-05-24T01:49:02Z","title":"CulturePark: Boosting Cross-cultural Understanding in Large Language\n  Models","summary":"  Cultural bias is pervasive in many large language models (LLMs), largely due\nto the deficiency of data representative of different cultures. Typically,\ncultural datasets and benchmarks are constructed either by extracting subsets\nof existing datasets or by aggregating from platforms such as Wikipedia and\nsocial media. However, these approaches are highly dependent on real-world data\nand human annotations, making them costly and difficult to scale. Inspired by\ncognitive theories on social communication, this paper introduces CulturePark,\nan LLM-powered multi-agent communication framework for cultural data\ncollection. CulturePark simulates cross-cultural human communication with\nLLM-based agents playing roles in different cultures. It generates high-quality\ncross-cultural dialogues encapsulating human beliefs, norms, and customs. Using\nCulturePark, we generated 41,000 cultural samples to fine-tune eight\nculture-specific LLMs. We evaluated these models across three downstream tasks:\ncontent moderation, cultural alignment, and cultural education. Results show\nthat for content moderation, our GPT-3.5-based models either match or\noutperform GPT-4 on datasets. Regarding cultural alignment, our models surpass\nGPT-4 on Hofstede's VSM 13 framework. Furthermore, for cultural education of\nhuman participants, our models demonstrate superior outcomes in both learning\nefficacy and user experience compared to GPT-4. CulturePark proves an important\nstep in addressing cultural bias and advancing the democratization of AI,\nhighlighting the critical role of culturally inclusive data in model training.\nCode is released at https://github.com/Scarelette/CulturePark.\n","authors":["Cheng Li","Damien Teney","Linyi Yang","Qingsong Wen","Xing Xie","Jindong Wang"],"pdf_url":"https://arxiv.org/pdf/2405.15145v3.pdf","comment":"NeurIPS 2024; Code is released at\n  https://github.com/Scarelette/CulturePark. arXiv admin note: substantial text\n  overlap with arXiv:2402.10946"},{"id":"http://arxiv.org/abs/2406.06371v5","updated":"2024-11-21T10:45:39Z","published":"2024-06-10T15:32:42Z","title":"mHuBERT-147: A Compact Multilingual HuBERT Model","summary":"  We present mHuBERT-147, the first general-purpose massively multilingual\nHuBERT speech representation model trained on 90K hours of clean, open-license\ndata. To scale up the multi-iteration HuBERT approach, we use faiss-based\nclustering, achieving 5.2x faster label assignment than the original method. We\nalso apply a new multilingual batching up-sampling strategy, leveraging both\nlanguage and dataset diversity. After 3 training iterations, our compact 95M\nparameter mHuBERT-147 outperforms larger models trained on substantially more\ndata. We rank second and first on the ML-SUPERB 10min and 1h leaderboards, with\nSOTA scores for 3 tasks. Across ASR/LID tasks, our model consistently surpasses\nXLS-R (300M params; 436K hours) and demonstrates strong competitiveness against\nthe much larger MMS (1B params; 491K hours). Our findings indicate that\nmHuBERT-147 is a promising model for multilingual speech tasks, offering an\nunprecedented balance between high performance and parameter efficiency.\n","authors":["Marcely Zanon Boito","Vivek Iyer","Nikolaos Lagos","Laurent Besacier","Ioan Calapodescu"],"pdf_url":"https://arxiv.org/pdf/2406.06371v5.pdf","comment":"Extended version of the Interspeech 2024 paper of same name"},{"id":"http://arxiv.org/abs/2303.08032v3","updated":"2024-11-21T09:46:44Z","published":"2023-03-14T16:11:47Z","title":"Verifying the Robustness of Automatic Credibility Assessment","summary":"  Text classification methods have been widely investigated as a way to detect\ncontent of low credibility: fake news, social media bots, propaganda, etc.\nQuite accurate models (likely based on deep neural networks) help in moderating\npublic electronic platforms and often cause content creators to face rejection\nof their submissions or removal of already published texts. Having the\nincentive to evade further detection, content creators try to come up with a\nslightly modified version of the text (known as an attack with an adversarial\nexample) that exploit the weaknesses of classifiers and result in a different\noutput. Here we systematically test the robustness of common text classifiers\nagainst available attacking techniques and discover that, indeed,\nmeaning-preserving changes in input text can mislead the models. The approaches\nwe test focus on finding vulnerable spans in text and replacing individual\ncharacters or words, taking into account the similarity between the original\nand replacement content. We also introduce BODEGA: a benchmark for testing both\nvictim models and attack methods on four misinformation detection tasks in an\nevaluation framework designed to simulate real use-cases of content moderation.\nThe attacked tasks include (1) fact checking and detection of (2) hyperpartisan\nnews, (3) propaganda and (4) rumours. Our experimental results show that modern\nlarge language models are often more vulnerable to attacks than previous,\nsmaller solutions, e.g. attacks on GEMMA being up to 27\\% more successful than\nthose on BERT. Finally, we manually analyse a subset adversarial examples and\ncheck what kinds of modifications are used in successful attacks.\n","authors":["Piotr Przybyła","Alexander Shvets","Horacio Saggion"],"pdf_url":"https://arxiv.org/pdf/2303.08032v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00774v3","updated":"2024-11-21T09:19:28Z","published":"2024-11-01T17:59:51Z","title":"Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model\n  with Frozen LLM","summary":"  Rapidly developing large language models (LLMs) have brought tremendous\nintelligent applications. Especially, the GPT-4o's excellent duplex speech\ninteraction ability has brought impressive experience to users. Researchers\nhave recently proposed several multi-modal LLMs in this direction that can\nachieve user-agent speech-to-speech conversations. This paper proposes a novel\nspeech-text multimodal LLM architecture called Freeze-Omni. Our main\ncontribution is that the speech input and output modalities can be easily\nconnected to a textual LLM while keeping the LLM's parameters frozen throughout\nthe training process. We design a three-stage training strategy for modeling\nboth the speech input and output, enabling Freeze-Omni to obtain\nspeech-to-speech conversation ability using text-speech paired data (such as\nASR and TTS data) and only 60,000 multi-round text Q&A data on 8 GPUs.\nMoreover, we can effectively ensure that the intelligence of the Freeze-Omni in\nthe speech modality is at the same level compared with that in the text\nmodality of its backbone LLM, while achieving low latency end-to-end spoken\nresponse. In addition, we also designed a method to achieve duplex dialogue\nability through multi-task training, giving Freeze-Omni a more natural style of\ndialogue ability between users and agents. In summary, Freeze-Omni holds great\npotential to conduct speech-to-speech dialogue based on a multimodal LLM under\nthe condition of a frozen LLM, avoiding the catastrophic forgetting problem\ncaused by limited data and training resources.\n","authors":["Xiong Wang","Yangze Li","Chaoyou Fu","Yunhang Shen","Lei Xie","Ke Li","Xing Sun","Long Ma"],"pdf_url":"https://arxiv.org/pdf/2411.00774v3.pdf","comment":"Project Page: https://freeze-omni.github.io/"},{"id":"http://arxiv.org/abs/2411.13958v1","updated":"2024-11-21T09:13:12Z","published":"2024-11-21T09:13:12Z","title":"Sentiment Analysis of Economic Text: A Lexicon-Based Approach","summary":"  We propose an Economic Lexicon (EL) specifically designed for textual\napplications in economics. We construct the dictionary with two important\ncharacteristics: 1) to have a wide coverage of terms used in documents\ndiscussing economic concepts, and 2) to provide a human-annotated sentiment\nscore in the range [-1,1]. We illustrate the use of the EL in the context of a\nsimple sentiment measure and consider several applications in economics. The\ncomparison to other lexicons shows that the EL is superior due to its wider\ncoverage of domain relevant terms and its more accurate categorization of the\nword sentiment.\n","authors":["Luca Barbaglia","Sergio Consoli","Sebastiano Manzan","Luca Tiozzo Pezzoli","Elisa Tosetti"],"pdf_url":"https://arxiv.org/pdf/2411.13958v1.pdf","comment":"37 pages, 9 figures, 6 tables, in press"},{"id":"http://arxiv.org/abs/2310.08944v2","updated":"2024-11-21T08:50:56Z","published":"2023-10-13T08:19:31Z","title":"A Confidence-based Acquisition Model for Self-supervised Active Learning\n  and Label Correction","summary":"  Supervised neural approaches are hindered by their dependence on large,\nmeticulously annotated datasets, a requirement that is particularly cumbersome\nfor sequential tasks. The quality of annotations tends to deteriorate with the\ntransition from expert-based to crowd-sourced labelling. To address these\nchallenges, we present CAMEL (Confidence-based Acquisition Model for Efficient\nself-supervised active Learning), a pool-based active learning framework\ntailored to sequential multi-output problems. CAMEL possesses two core\nfeatures: (1) it requires expert annotators to label only a fraction of a\nchosen sequence, and (2) it facilitates self-supervision for the remainder of\nthe sequence. By deploying a label correction mechanism, CAMEL can also be\nutilised for data cleaning. We evaluate CAMEL on two sequential tasks, with a\nspecial emphasis on dialogue belief tracking, a task plagued by the constraints\nof limited and noisy datasets. Our experiments demonstrate that CAMEL\nsignificantly outperforms the baselines in terms of efficiency. Furthermore,\nthe data corrections suggested by our method contribute to an overall\nimprovement in the quality of the resulting datasets.\n","authors":["Carel van Niekerk","Christian Geishauser","Michael Heck","Shutong Feng","Hsien-chin Lin","Nurul Lubis","Benjamin Ruppik","Renato Vukovic","Milica Gašić"],"pdf_url":"https://arxiv.org/pdf/2310.08944v2.pdf","comment":"Accepted at TACL"},{"id":"http://arxiv.org/abs/2402.17497v2","updated":"2024-11-21T08:44:20Z","published":"2024-02-27T13:22:51Z","title":"REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain\n  Question Answering","summary":"  Considering the limited internal parametric knowledge, retrieval-augmented\ngeneration (RAG) has been widely used to extend the knowledge scope of large\nlanguage models (LLMs). Despite the extensive efforts on RAG research, in\nexisting methods, LLMs cannot precisely assess the relevance of retrieved\ndocuments, thus likely leading to misleading or even incorrect utilization of\nexternal knowledge (eg., retrieved documents). To address this issue, in this\npaper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for\nopen-domain question answering (QA). As the key motivation, we aim to enhance\nthe self-awareness regarding the reliability of external knowledge for LLMs, so\nas to adaptively utilize external knowledge in RAG systems. Specially, we\ndevelop a novel architecture for LLM-based RAG systems, by incorporating a\nspecially designed assessment module that precisely assesses the relevance of\nretrieved documents. Furthermore, we propose an improved training method based\non bi-granularity relevance fusion and noise-resistant training. By combining\nthe improvements in both architecture and training, our proposed REAR can\nbetter utilize external knowledge by effectively perceiving the relevance of\nretrieved documents. Experiments on four open-domain QA tasks show that REAR\nsignificantly outperforms previous a number of competitive RAG approaches. Our\ncodes can be accessed at https://github.com/RUCAIBox/REAR.\n","authors":["Yuhao Wang","Ruiyang Ren","Junyi Li","Wayne Xin Zhao","Jing Liu","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2402.17497v2.pdf","comment":"Accepted to EMNLP 2024 Main Conference. Published on ACL Anthology:\n  https://aclanthology.org/2024.emnlp-main.321.pdf"},{"id":"http://arxiv.org/abs/2411.13904v1","updated":"2024-11-21T07:30:02Z","published":"2024-11-21T07:30:02Z","title":"Towards Full Delegation: Designing Ideal Agentic Behaviors for Travel\n  Planning","summary":"  How are LLM-based agents used in the future? While many of the existing work\non agents has focused on improving the performance of a specific family of\nobjective and challenging tasks, in this work, we take a different perspective\nby thinking about full delegation: agents take over humans' routine\ndecision-making processes and are trusted by humans to find solutions that fit\npeople's personalized needs and are adaptive to ever-changing context. In order\nto achieve such a goal, the behavior of the agents, i.e., agentic behaviors,\nshould be evaluated not only on their achievements (i.e., outcome evaluation),\nbut also how they achieved that (i.e., procedure evaluation). For this, we\npropose APEC Agent Constitution, a list of criteria that an agent should follow\nfor good agentic behaviors, including Accuracy, Proactivity, Efficiency and\nCredibility. To verify whether APEC aligns with human preferences, we develop\nAPEC-Travel, a travel planning agent that proactively extracts hidden\npersonalized needs via multi-round dialog with travelers. APEC-Travel is\nconstructed purely from synthetic data generated by Llama3.1-405B-Instruct with\na diverse set of travelers' persona to simulate rich distribution of dialogs.\nIteratively fine-tuned to follow APEC Agent Constitution, APEC-Travel surpasses\nbaselines by 20.7% on rule-based metrics and 9.1% on LLM-as-a-Judge scores\nacross the constitution axes.\n","authors":["Song Jiang","Da JU","Andrew Cohen","Sasha Mitts","Aaron Foss","Justine T Kao","Xian Li","Yuandong Tian"],"pdf_url":"https://arxiv.org/pdf/2411.13904v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13902v1","updated":"2024-11-21T07:28:07Z","published":"2024-11-21T07:28:07Z","title":"PIORS: Personalized Intelligent Outpatient Reception based on Large\n  Language Model with Multi-Agents Medical Scenario Simulation","summary":"  In China, receptionist nurses face overwhelming workloads in outpatient\nsettings, limiting their time and attention for each patient and ultimately\nreducing service quality. In this paper, we present the Personalized\nIntelligent Outpatient Reception System (PIORS). This system integrates an\nLLM-based reception nurse and a collaboration between LLM and hospital\ninformation system (HIS) into real outpatient reception setting, aiming to\ndeliver personalized, high-quality, and efficient reception services.\nAdditionally, to enhance the performance of LLMs in real-world healthcare\nscenarios, we propose a medical conversational data generation framework named\nService Flow aware Medical Scenario Simulation (SFMSS), aiming to adapt the LLM\nto the real-world environments and PIORS settings. We evaluate the\neffectiveness of PIORS and SFMSS through automatic and human assessments\ninvolving 15 users and 15 clinical experts. The results demonstrate that\nPIORS-Nurse outperforms all baselines, including the current state-of-the-art\nmodel GPT-4o, and aligns with human preferences and clinical needs. Further\ndetails and demo can be found at https://github.com/FudanDISC/PIORS\n","authors":["Zhijie Bao","Qingyun Liu","Ying Guo","Zhengqiang Ye","Jun Shen","Shirong Xie","Jiajie Peng","Xuanjing Huang","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2411.13902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02884v2","updated":"2024-11-21T07:07:59Z","published":"2024-10-03T18:12:29Z","title":"LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level\n  Mathematical Reasoning","summary":"  This paper presents an advanced mathematical problem-solving framework,\nLLaMA-Berry, for enhancing the mathematical reasoning ability of Large Language\nModels (LLMs). The framework combines Monte Carlo Tree Search (MCTS) with\niterative Self-Refine to optimize the reasoning path and utilizes a pairwise\nreward model to evaluate different paths globally. By leveraging the\nself-critic and rewriting capabilities of LLMs, Self-Refine applied to MCTS\n(SR-MCTS) overcomes the inefficiencies and limitations of conventional\nstep-wise and greedy search algorithms by fostering a more efficient\nexploration of solution spaces. Pairwise Preference Reward Model~(PPRM),\ninspired by Reinforcement Learning from Human Feedback (RLHF), is then used to\nmodel pairwise preferences between solutions, utilizing an Enhanced Borda Count\n(EBC) method to synthesize these preferences into a global ranking score to\nfind better answers. This approach addresses the challenges of scoring\nvariability and non-independent distributions in mathematical reasoning tasks.\nThe framework has been tested on general and advanced benchmarks, showing\nsuperior performance in terms of search efficiency and problem-solving\ncapability compared to existing methods like ToT and rStar, particularly in\ncomplex Olympiad-level benchmarks, including GPQA, AIME24 and AMC23.\n","authors":["Di Zhang","Jianbo Wu","Jingdi Lei","Tong Che","Jiatong Li","Tong Xie","Xiaoshui Huang","Shufei Zhang","Marco Pavone","Yuqiang Li","Wanli Ouyang","Dongzhan Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.02884v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17304v3","updated":"2024-11-21T07:03:33Z","published":"2024-02-27T08:27:15Z","title":"Probing Multimodal Large Language Models for Global and Local Semantic\n  Representations","summary":"  The advancement of Multimodal Large Language Models (MLLMs) has greatly\naccelerated the development of applications in understanding integrated texts\nand images. Recent works leverage image-caption datasets to train MLLMs,\nachieving state-of-the-art performance on image-to-text tasks. However, there\nare few studies exploring which layers of MLLMs make the most effort to the\nglobal image information, which plays vital roles in multimodal comprehension\nand generation. In this study, we find that the intermediate layers of models\ncan encode more global semantic information, whose representation vectors\nperform better on visual-language entailment tasks, rather than the topmost\nlayers. We further probe models regarding local semantic representations\nthrough object recognition tasks. We find that the topmost layers may\nexcessively focus on local information, leading to a diminished ability to\nencode global information. Our code and data are released via\nhttps://github.com/kobayashikanna01/probing_MLLM_rep.\n","authors":["Mingxu Tao","Quzhe Huang","Kun Xu","Liwei Chen","Yansong Feng","Dongyan Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.17304v3.pdf","comment":"Accepted by LREC-COLING 2024 as a short paper. ACL Anthology URL:\n  [https://aclanthology.org/2024.lrec-main.1142/]"},{"id":"http://arxiv.org/abs/2402.04838v5","updated":"2024-11-21T06:52:02Z","published":"2024-02-07T13:39:38Z","title":"PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity\n  Recognition","summary":"  In this study, we aim to reduce generation latency for Named Entity\nRecognition (NER) with Large Language Models (LLMs). The main cause of high\nlatency in LLMs is the sequential decoding process, which autoregressively\ngenerates all labels and mentions for NER, significantly increase the sequence\nlength. To this end, we introduce Parallel Decoding in LLM for NE}\n(PaDeLLM-NER), a approach that integrates seamlessly into existing generative\nmodel frameworks without necessitating additional modules or architectural\nmodifications. PaDeLLM-NER allows for the simultaneous decoding of all\nmentions, thereby reducing generation latency. Experiments reveal that\nPaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times\nfaster than the autoregressive approach for both English and Chinese.\nSimultaneously it maintains the quality of predictions as evidenced by the\nperformance that is on par with the state-of-the-art across various datasets.\n","authors":["Jinghui Lu","Ziwei Yang","Yanjie Wang","Xuejing Liu","Brian Mac Namee","Can Huang"],"pdf_url":"https://arxiv.org/pdf/2402.04838v5.pdf","comment":"Accepted to Neurips2024"},{"id":"http://arxiv.org/abs/2403.03163v2","updated":"2024-11-21T06:18:07Z","published":"2024-03-05T17:56:27Z","title":"Design2Code: Benchmarking Multimodal Code Generation for Automated\n  Front-End Engineering","summary":"  Generative AI has made rapid advancements in recent years, achieving\nunprecedented capabilities in multimodal understanding and code generation.\nThis can enable a new paradigm of front-end development in which multimodal\nlarge language models (MLLMs) directly convert visual designs into code\nimplementations. In this work, we construct Design2Code - the first real-world\nbenchmark for this task. Specifically, we manually curate 484 diverse\nreal-world webpages as test cases and develop a set of automatic evaluation\nmetrics to assess how well current multimodal LLMs can generate the code\nimplementations that directly render into the given reference webpages, given\nthe screenshots as input. We also complement automatic metrics with\ncomprehensive human evaluations to validate the performance ranking. To\nrigorously benchmark MLLMs, we test various multimodal prompting methods on\nfrontier models such as GPT-4o, GPT-4V, Gemini, and Claude. Our fine-grained\nbreak-down metrics indicate that models mostly lag in recalling visual elements\nfrom the input webpages and generating correct layout designs.\n","authors":["Chenglei Si","Yanzhe Zhang","Ryan Li","Zhengyuan Yang","Ruibo Liu","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2403.03163v2.pdf","comment":"The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2407.13891v2","updated":"2024-11-21T06:07:13Z","published":"2024-07-18T20:31:07Z","title":"High Risk of Political Bias in Black Box Emotion Inference Models","summary":"  This paper investigates the presence of political bias in emotion inference\nmodels used for sentiment analysis (SA) in social science research. Machine\nlearning models often reflect biases in their training data, impacting the\nvalidity of their outcomes. While previous research has highlighted gender and\nrace biases, our study focuses on political bias - an underexplored yet\npervasive issue that can skew the interpretation of text data across a wide\narray of studies. We conducted a bias audit on a Polish sentiment analysis\nmodel developed in our lab. By analyzing valence predictions for names and\nsentences involving Polish politicians, we uncovered systematic differences\ninfluenced by political affiliations. Our findings indicate that annotations by\nhuman raters propagate political biases into the model's predictions. To\nmitigate this, we pruned the training dataset of texts mentioning these\npoliticians and observed a reduction in bias, though not its complete\nelimination. Given the significant implications of political bias in SA, our\nstudy emphasizes caution in employing these models for social science research.\nWe recommend a critical examination of SA results and propose using\nlexicon-based systems as a more ideologically neutral alternative. This paper\nunderscores the necessity for ongoing scrutiny and methodological adjustments\nto ensure the reliability and impartiality of the use of machine learning in\nacademic and applied contexts.\n","authors":["Hubert Plisiecki","Paweł Lenartowicz","Maria Flakus","Artur Pokropek"],"pdf_url":"https://arxiv.org/pdf/2407.13891v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13868v1","updated":"2024-11-21T06:06:04Z","published":"2024-11-21T06:06:04Z","title":"Robust Detection of Watermarks for Large Language Models Under Human\n  Edits","summary":"  Watermarking has offered an effective approach to distinguishing text\ngenerated by large language models (LLMs) from human-written text. However, the\npervasive presence of human edits on LLM-generated text dilutes watermark\nsignals, thereby significantly degrading detection performance of existing\nmethods. In this paper, by modeling human edits through mixture model\ndetection, we introduce a new method in the form of a truncated goodness-of-fit\ntest for detecting watermarked text under human edits, which we refer to as\nTr-GoF. We prove that the Tr-GoF test achieves optimality in robust detection\nof the Gumbel-max watermark in a certain asymptotic regime of substantial text\nmodifications and vanishing watermark signals. Importantly, Tr-GoF achieves\nthis optimality \\textit{adaptively} as it does not require precise knowledge of\nhuman edit levels or probabilistic specifications of the LLMs, in contrast to\nthe optimal but impractical (Neyman--Pearson) likelihood ratio test. Moreover,\nwe establish that the Tr-GoF test attains the highest detection efficiency rate\nin a certain regime of moderate text modifications. In stark contrast, we show\nthat sum-based detection rules, as employed by existing methods, fail to\nachieve optimal robustness in both regimes because the additive nature of their\nstatistics is less resilient to edit-induced noise. Finally, we demonstrate the\ncompetitive and sometimes superior empirical performance of the Tr-GoF test on\nboth synthetic data and open-source LLMs in the OPT and LLaMA families.\n","authors":["Xiang Li","Feng Ruan","Huiyuan Wang","Qi Long","Weijie J. Su"],"pdf_url":"https://arxiv.org/pdf/2411.13868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13865v1","updated":"2024-11-21T06:01:47Z","published":"2024-11-21T06:01:47Z","title":"HARec: Hyperbolic Graph-LLM Alignment for Exploration and Exploitation\n  in Recommender Systems","summary":"  Modern recommendation systems often create information cocoons, limiting\nusers' exposure to diverse content. To enhance user experience, a crucial\nchallenge is developing systems that can balance content exploration and\nexploitation, allowing users to adjust their recommendation preferences.\nIntuitively, this balance can be achieved through a tree-structured\nrepresentation, where depth search facilitates exploitation and breadth search\nenables exploration. However, current works face two challenges to achieve this\ntarget: (1) Euclidean methods fail to fully capture hierarchical structures and\nlack flexibility in balancing exploration-exploitation, while (2) hyperbolic\napproaches, despite better hierarchical modeling, suffer from insufficient\nsemantic alignment due to their reliance on Euclidean text encoders. To address\nthese challenges, we propose HARec, a hyperbolic representation learning\nframework that jointly aligns user-item collaborative information with textual\ndescriptions in hyperbolic space. Our framework introduces two key technique\nnovelty: (1) a hierarchical-aware graph-llm alignment mechanism that enables\nbetter hierarchical representation, and (2) a hyperbolic hierarchical tree\nstructure that facilitates user-adjustable exploration-exploitation trade-offs.\nExtensive experiments demonstrate that HARec consistently outperforms both\nEuclidean and hyperbolic baselines, achieving up to 5.49% improvement in\nutility metrics and 11.39% increase in diversity metrics.\n","authors":["Qiyao Ma","Menglin Yang","Mingxuan Ju","Tong Zhao","Neil Shah","Rex Ying"],"pdf_url":"https://arxiv.org/pdf/2411.13865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11053v3","updated":"2024-11-21T06:01:03Z","published":"2024-11-17T12:31:04Z","title":"SRA-MCTS: Self-driven Reasoning Augmentation with Monte Carlo Tree\n  Search for Code Generation","summary":"  Large language models demonstrate exceptional performance in simple code\ngeneration tasks but still face challenges in tackling complex problems. These\nchallenges may stem from insufficient reasoning and problem decomposition\ncapabilities. To address this issue, we propose a reasoning-augmented data\ngeneration process, SRA-MCTS, which guides the model to autonomously generate\nhigh-quality intermediate reasoning paths. This creates a positive feedback\nloop, enabling continuous improvement. Our method operates entirely through the\nmodel itself without requiring additional supervision. By synthesizing natural\nlanguage reasoning paths and translating them into executable code, the\napproach ensures analytical accuracy and enhances the success rate in solving\ncomplex tasks. Experimental results show that, even without additional\nsupervisory signals, our method achieves performance improvements across\ndifferent model scales, demonstrating the significant potential of\nself-improvement in small models. Furthermore, the method remains robust when\ntraditional Chain-of-Thought (CoT) approaches exhibit performance degradation,\nwith notable improvements observed in diversity metrics such as pass@10. We\nencourage further exploration of reasoning processes within training data to\nenhance the ability of language models to address complex problems.\n","authors":["Bin Xu","Yiguan Lin","Yinghao Li","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2411.11053v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13504v2","updated":"2024-11-21T05:19:56Z","published":"2024-11-20T17:55:38Z","title":"Disentangling Memory and Reasoning Ability in Large Language Models","summary":"  Large Language Models (LLMs) have demonstrated strong performance in handling\ncomplex tasks requiring both extensive knowledge and reasoning abilities.\nHowever, the existing LLM inference pipeline operates as an opaque process\nwithout explicit separation between knowledge retrieval and reasoning steps,\nmaking the model's decision-making process unclear and disorganized. This\nambiguity can lead to issues such as hallucinations and knowledge forgetting,\nwhich significantly impact the reliability of LLMs in high-stakes domains. In\nthis paper, we propose a new inference paradigm that decomposes the complex\ninference process into two distinct and clear actions: (1) memory recall: which\nretrieves relevant knowledge, and (2) reasoning: which performs logical steps\nbased on the recalled knowledge. To facilitate this decomposition, we introduce\ntwo special tokens memory and reason, guiding the model to distinguish between\nsteps that require knowledge retrieval and those that involve reasoning. Our\nexperiment results show that this decomposition not only improves model\nperformance but also enhances the interpretability of the inference process,\nenabling users to identify sources of error and refine model responses\neffectively. The code is available at\nhttps://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning.\n","authors":["Mingyu Jin","Weidi Luo","Sitao Cheng","Xinyi Wang","Wenyue Hua","Ruixiang Tang","William Yang Wang","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.13504v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08109v2","updated":"2024-11-21T04:39:13Z","published":"2024-10-10T16:56:05Z","title":"A Closer Look at Machine Unlearning for Large Language Models","summary":"  Large language models (LLMs) may memorize sensitive or copyrighted content,\nraising privacy and legal concerns. Due to the high cost of retraining from\nscratch, researchers attempt to employ machine unlearning to remove specific\ncontent from LLMs while preserving the overall performance. In this paper, we\ndiscuss several issues in machine unlearning for LLMs and provide our insights\non possible approaches. To address the issue of inadequate evaluation of model\noutputs after unlearning, we introduce three additional metrics to evaluate\ntoken diversity, sentence semantics, and factual correctness. We then\ncategorize unlearning methods into untargeted and targeted, and discuss their\nissues respectively. Specifically, the behavior that untargeted unlearning\nattempts to approximate is unpredictable and may involve hallucinations, and\nexisting regularization is insufficient for targeted unlearning. To alleviate\nthese issues, we propose using the objective of maximizing entropy (ME) for\nuntargeted unlearning and incorporate answer preservation (AP) loss as\nregularization for targeted unlearning. Experimental results across three\nscenarios, i.e., fictitious unlearning, continual unlearning, and real-world\nunlearning, demonstrate the effectiveness of our approaches. The code is\navailable at https://github.com/sail-sg/closer-look-LLM-unlearning.\n","authors":["Xiaojian Yuan","Tianyu Pang","Chao Du","Kejiang Chen","Weiming Zhang","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2410.08109v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13826v1","updated":"2024-11-21T04:23:17Z","published":"2024-11-21T04:23:17Z","title":"Interactive and Expressive Code-Augmented Planning with Large Language\n  Models","summary":"  Large Language Models (LLMs) demonstrate strong abilities in common-sense\nreasoning and interactive decision-making, but often struggle with complex,\nlong-horizon planning tasks. Recent techniques have sought to structure LLM\noutputs using control flow and other code-adjacent techniques to improve\nplanning performance. These techniques include using variables (to track\nimportant information) and functions (to divide complex tasks into smaller\nre-usable sub-tasks). However, purely code-based approaches can be error-prone\nand insufficient for handling ambiguous or unstructured data. To address these\nchallenges, we propose REPL-Plan, an LLM planning approach that is fully\ncode-expressive (it can utilize all the benefits of code) while also being\ndynamic (it can flexibly adapt from errors and use the LLM for fuzzy\nsituations). In REPL-Plan, an LLM solves tasks by interacting with a\nRead-Eval-Print Loop (REPL), which iteratively executes and evaluates code,\nsimilar to language shells or interactive code notebooks, allowing the model to\nflexibly correct errors and handle tasks dynamically. We demonstrate that\nREPL-Plan achieves strong results across various planning domains compared to\nprevious methods.\n","authors":["Anthony Z. Liu","Xinhe Wang","Jacob Sansom","Yao Fu","Jongwook Choi","Sungryull Sohn","Jaekyeom Kim","Honglak Lee"],"pdf_url":"https://arxiv.org/pdf/2411.13826v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13820v1","updated":"2024-11-21T03:52:41Z","published":"2024-11-21T03:52:41Z","title":"InstCache: A Predictive Cache for LLM Serving","summary":"  Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB.\n","authors":["Longwei Zou","Tingfeng Liu","Kai Chen","Jiangang Kong","Yangdong Deng"],"pdf_url":"https://arxiv.org/pdf/2411.13820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13802v1","updated":"2024-11-21T03:05:38Z","published":"2024-11-21T03:05:38Z","title":"SemiKong: Curating, Training, and Evaluating A Semiconductor\n  Industry-Specific Large Language Model","summary":"  Large Language Models (LLMs) have demonstrated the potential to address some\nissues within the semiconductor industry. However, they are often\ngeneral-purpose models that lack the specialized knowledge needed to tackle the\nunique challenges of this sector, such as the intricate physics and chemistry\nof semiconductor devices and processes. SemiKong, the first industry-specific\nLLM for the semiconductor domain, provides a foundation that can be used to\ndevelop tailored proprietary models. With SemiKong 1.0, we aim to develop a\nfoundational model capable of understanding etching problems at an expert\nlevel. Our key contributions include (a) curating a comprehensive corpus of\nsemiconductor-related texts, (b) creating a foundational model with in-depth\nsemiconductor knowledge, and (c) introducing a framework for integrating expert\nknowledge, thereby advancing the evaluation process of domain-specific AI\nmodels. Through fine-tuning a pre-trained LLM using our curated dataset, we\nhave shown that SemiKong outperforms larger, general-purpose LLMs in various\nsemiconductor manufacturing and design tasks. Our extensive experiments\nunderscore the importance of developing domain-specific LLMs as a foundation\nfor company- or tool-specific proprietary models, paving the way for further\nresearch and applications in the semiconductor domain. Code and dataset will be\navailable at https://github.com/aitomatic/semikong\n","authors":["Christopher Nguyen","William Nguyen","Atsushi Suzuki","Daisuke Oku","Hong An Phan","Sang Dinh","Zooey Nguyen","Anh Ha","Shruti Raghavan","Huy Vo","Thang Nguyen","Lan Nguyen","Yoshikuni Hirayama"],"pdf_url":"https://arxiv.org/pdf/2411.13802v1.pdf","comment":"On-going work"},{"id":"http://arxiv.org/abs/2411.13800v1","updated":"2024-11-21T02:58:23Z","published":"2024-11-21T02:58:23Z","title":"Explaining GPT-4's Schema of Depression Using Machine Behavior Analysis","summary":"  Use of large language models such as ChatGPT (GPT-4) for mental health\nsupport has grown rapidly, emerging as a promising route to assess and help\npeople with mood disorders, like depression. However, we have a limited\nunderstanding of GPT-4's schema of mental disorders, that is, how it internally\nassociates and interprets symptoms. In this work, we leveraged contemporary\nmeasurement theory to decode how GPT-4 interrelates depressive symptoms to\ninform both clinical utility and theoretical understanding. We found GPT-4's\nassessment of depression: (a) had high overall convergent validity (r = .71\nwith self-report on 955 samples, and r = .81 with experts judgments on 209\nsamples); (b) had moderately high internal consistency (symptom\ninter-correlates r = .23 to .78 ) that largely aligned with literature and\nself-report; except that GPT-4 (c) underemphasized suicidality's -- and\noveremphasized psychomotor's -- relationship with other symptoms, and (d) had\nsymptom inference patterns that suggest nuanced hypotheses (e.g. sleep and\nfatigue are influenced by most other symptoms while feelings of\nworthlessness/guilt is mostly influenced by depressed mood).\n","authors":["Adithya V Ganesan","Vasudha Varadarajan","Yash Kumar Lal","Veerle C. Eijsbroek","Katarina Kjell","Oscar N. E. Kjell","Tanuja Dhanasekaran","Elizabeth C. Stade","Johannes C. Eichstaedt","Ryan L. Boyd","H. Andrew Schwartz","Lucie Flek"],"pdf_url":"https://arxiv.org/pdf/2411.13800v1.pdf","comment":"21 pages, 3 tables, 6 figures, 1 supplementary table, 83 references"},{"id":"http://arxiv.org/abs/2409.08435v4","updated":"2024-11-21T02:31:15Z","published":"2024-09-13T00:03:19Z","title":"When Context Leads but Parametric Memory Follows in Large Language\n  Models","summary":"  Large language models (LLMs) have demonstrated remarkable progress in\nleveraging diverse knowledge sources. This study investigates how nine widely\nused LLMs allocate knowledge between local context and global parameters when\nanswering open-ended questions in knowledge-consistent scenarios. We introduce\na novel dataset, WikiAtomic, and systematically vary context sizes to analyze\nhow LLMs prioritize and utilize the provided information and their parametric\nknowledge in knowledge-consistent scenarios. Additionally, we also study their\ntendency to hallucinate under varying context sizes. Our findings reveal\nconsistent patterns across models, including a consistent reliance on both\ncontextual (around 70%) and parametric (around 30%) knowledge, and a decrease\nin hallucinations with increasing context. These insights highlight the\nimportance of more effective context organization and developing models that\nuse input more deterministically for robust performance.\n","authors":["Yufei Tao","Adam Hiatt","Erik Haake","Antonie J. Jetter","Ameeta Agrawal"],"pdf_url":"https://arxiv.org/pdf/2409.08435v4.pdf","comment":"Accepted by EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2411.13407v2","updated":"2024-11-21T02:27:38Z","published":"2024-11-20T15:46:48Z","title":"Transformer-Based Contextualized Language Models Joint with Neural\n  Networks for Natural Language Inference in Vietnamese","summary":"  Natural Language Inference (NLI) is a task within Natural Language Processing\n(NLP) that holds value for various AI applications. However, there have been\nlimited studies on Natural Language Inference in Vietnamese that explore the\nconcept of joint models. Therefore, we conducted experiments using various\ncombinations of contextualized language models (CLM) and neural networks. We\nuse CLM to create contextualized work presentations and use Neural Networks for\nclassification. Furthermore, we have evaluated the strengths and weaknesses of\neach joint model and identified the model failure points in the Vietnamese\ncontext. The highest F1 score in this experiment, up to 82.78% in the benchmark\ndataset (ViNLI). By conducting experiments with various models, the most\nconsiderable size of the CLM is XLM-R (355M). That combination has consistently\ndemonstrated superior performance compared to fine-tuning strong pre-trained\nlanguage models like PhoBERT (+6.58%), mBERT (+19.08%), and XLM-R (+0.94%) in\nterms of F1-score. This article aims to introduce a novel approach or model\nthat attains improved performance for Vietnamese NLI. Overall, we find that the\njoint approach of CLM and neural networks is simple yet capable of achieving\nhigh-quality performance, which makes it suitable for applications that require\nefficient resource utilization.\n","authors":["Dat Van-Thanh Nguyen","Tin Van Huynh","Kiet Van Nguyen","Ngan Luu-Thuy Nguyen"],"pdf_url":"https://arxiv.org/pdf/2411.13407v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.16680v6","updated":"2024-11-21T01:41:01Z","published":"2023-07-31T13:57:05Z","title":"On the Trustworthiness Landscape of State-of-the-art Generative Models:\n  A Survey and Outlook","summary":"  Diffusion models and large language models have emerged as leading-edge\ngenerative models, revolutionizing various aspects of human life. However, the\npractical implementations of these models have also exposed inherent risks,\nbringing to the forefront their evil sides and sparking concerns regarding\ntheir trustworthiness. Despite the wealth of literature on this subject, a\ncomprehensive survey specifically delving into the intersection of large-scale\ngenerative models and their trustworthiness remains largely absent. To bridge\nthis gap, this paper investigates both the long-standing and emerging threats\nassociated with these models across four fundamental dimensions: 1) privacy, 2)\nsecurity, 3) fairness, and 4) responsibility. Based on the investigation\nresults, we develop an extensive map outlining the trustworthiness of large\ngenerative models. After that, we provide practical recommendations and\npotential research directions for future secure applications equipped with\nlarge generative models, ultimately promoting the trustworthiness of the models\nand benefiting the society as a whole.\n","authors":["Mingyuan Fan","Chengyu Wang","Cen Chen","Yang Liu","Jun Huang"],"pdf_url":"https://arxiv.org/pdf/2307.16680v6.pdf","comment":"draft"},{"id":"http://arxiv.org/abs/2411.13779v1","updated":"2024-11-21T01:37:38Z","published":"2024-11-21T01:37:38Z","title":"NewsInterview: a Dataset and a Playground to Evaluate LLMs' Ground Gap\n  via Informational Interviews","summary":"  Large Language Models (LLMs) have demonstrated impressive capabilities in\ngenerating coherent text but often struggle with grounding language and\nstrategic dialogue. To address this gap, we focus on journalistic interviews, a\ndomain rich in grounding communication and abundant in data. We curate a\ndataset of 40,000 two-person informational interviews from NPR and CNN, and\nreveal that LLMs are significantly less likely than human interviewers to use\nacknowledgements and to pivot to higher-level questions. Realizing that a\nfundamental deficit exists in multi-turn planning and strategic thinking, we\ndevelop a realistic simulated environment, incorporating source personas and\npersuasive elements, in order to facilitate the development of agents with\nlonger-horizon rewards. Our experiments show that while source LLMs mimic human\nbehavior in information sharing, interviewer LLMs struggle with recognizing\nwhen questions are answered and engaging persuasively, leading to suboptimal\ninformation extraction across model size and capability. These findings\nunderscore the need for enhancing LLMs' strategic dialogue capabilities.\n","authors":["Michael Lu","Hyundong Justin Cho","Weiyan Shi","Jonathan May","Alexander Spangher"],"pdf_url":"https://arxiv.org/pdf/2411.13779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13775v1","updated":"2024-11-21T01:12:46Z","published":"2024-11-21T01:12:46Z","title":"Benchmarking GPT-4 against Human Translators: A Comprehensive Evaluation\n  Across Languages, Domains, and Expertise Levels","summary":"  This study presents a comprehensive evaluation of GPT-4's translation\ncapabilities compared to human translators of varying expertise levels. Through\nsystematic human evaluation using the MQM schema, we assess translations across\nthree language pairs (Chinese$\\longleftrightarrow$English,\nRussian$\\longleftrightarrow$English, and Chinese$\\longleftrightarrow$Hindi) and\nthree domains (News, Technology, and Biomedical). Our findings reveal that\nGPT-4 achieves performance comparable to junior-level translators in terms of\ntotal errors, while still lagging behind senior translators. Unlike traditional\nNeural Machine Translation systems, which show significant performance\ndegradation in resource-poor language directions, GPT-4 maintains consistent\ntranslation quality across all evaluated language pairs. Through qualitative\nanalysis, we identify distinctive patterns in translation approaches: GPT-4\ntends toward overly literal translations and exhibits lexical inconsistency,\nwhile human translators sometimes over-interpret context and introduce\nhallucinations. This study represents the first systematic comparison between\nLLM and human translators across different proficiency levels, providing\nvaluable insights into the current capabilities and limitations of LLM-based\ntranslation systems.\n","authors":["Jianhao Yan","Pingchuan Yan","Yulong Chen","Jing Li","Xianchao Zhu","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.13775v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2411.11853v2","updated":"2024-11-21T01:10:30Z","published":"2024-11-01T08:56:17Z","title":"Chat Bankman-Fried: an Exploration of LLM Alignment in Finance","summary":"  Advancements in large language models (LLMs) have renewed concerns about AI\nalignment - the consistency between human and AI goals and values. As various\njurisdictions enact legislation on AI safety, the concept of alignment must be\ndefined and measured across different domains. This paper proposes an\nexperimental framework to assess whether LLMs adhere to ethical and legal\nstandards in the relatively unexplored context of finance. We prompt nine LLMs\nto impersonate the CEO of a financial institution and test their willingness to\nmisuse customer assets to repay outstanding corporate debt. Beginning with a\nbaseline configuration, we adjust preferences, incentives and constraints,\nanalyzing the impact of each adjustment with logistic regression. Our findings\nreveal significant heterogeneity in the baseline propensity for unethical\nbehavior of LLMs. Factors such as risk aversion, profit expectations, and\nregulatory environment consistently influence misalignment in ways predicted by\neconomic theory, although the magnitude of these effects varies across LLMs.\nThis paper highlights both the benefits and limitations of simulation-based, ex\npost safety testing. While it can inform financial authorities and institutions\naiming to ensure LLM safety, there is a clear trade-off between generality and\ncost.\n","authors":["Claudia Biancotti","Carolina Camassa","Andrea Coletta","Oliver Giudice","Aldo Glielmo"],"pdf_url":"https://arxiv.org/pdf/2411.11853v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10588v2","updated":"2024-11-21T00:24:36Z","published":"2024-11-15T21:19:04Z","title":"A dataset of questions on decision-theoretic reasoning in Newcomb-like\n  problems","summary":"  We introduce a dataset of natural-language questions in the decision theory\nof so-called Newcomb-like problems. Newcomb-like problems include, for\ninstance, decision problems in which an agent interacts with a similar other\nagent, and thus has to reason about the fact that the other agent will likely\nreason in similar ways. Evaluating LLM reasoning about Newcomb-like problems is\nimportant because interactions between foundation-model-based agents will often\nbe Newcomb-like. Some ways of reasoning about Newcomb-like problems may allow\nfor greater cooperation between models.\n  Our dataset contains both capabilities questions (i.e., questions with a\nunique, uncontroversially correct answer) and attitude questions (i.e.,\nquestions about which decision theorists would disagree). We use our dataset\nfor an investigation of decision-theoretical capabilities and expressed\nattitudes and their interplay in existing models (different models by OpenAI,\nAnthropic, Meta, GDM, Reka, etc.), as well as models under simple prompt-based\ninterventions. We find, among other things, that attitudes vary significantly\nbetween existing models; that high capabilities are associated with attitudes\nmore favorable toward so-called evidential decision theory; and that attitudes\nare consistent across different types of questions.\n","authors":["Caspar Oesterheld","Emery Cooper","Miles Kodama","Linh Chi Nguyen","Ethan Perez"],"pdf_url":"https://arxiv.org/pdf/2411.10588v2.pdf","comment":"48 pages, 15 figures; code and data at\n  https://github.com/casparoe/newcomblike_questions_dataset"},{"id":"http://arxiv.org/abs/2401.11374v4","updated":"2024-11-21T00:19:27Z","published":"2024-01-21T02:29:12Z","title":"Language Models as Hierarchy Encoders","summary":"  Interpreting hierarchical structures latent in language is a key limitation\nof current language models (LMs). While previous research has implicitly\nleveraged these hierarchies to enhance LMs, approaches for their explicit\nencoding are yet to be explored. To address this, we introduce a novel approach\nto re-train transformer encoder-based LMs as Hierarchy Transformer encoders\n(HiTs), harnessing the expansive nature of hyperbolic space. Our method\nsituates the output embedding space of pre-trained LMs within a Poincar\\'e ball\nwith a curvature that adapts to the embedding dimension, followed by training\non hyperbolic clustering and centripetal losses. These losses are designed to\neffectively cluster related entities (input as texts) and organise them\nhierarchically. We evaluate HiTs against pre-trained LMs, standard fine-tuned\nLMs, and several hyperbolic embedding baselines, focusing on their capabilities\nin simulating transitive inference, predicting subsumptions, and transferring\nknowledge across hierarchies. The results demonstrate that HiTs consistently\noutperform all baselines in these tasks, underscoring the effectiveness and\ntransferability of our re-trained hierarchy encoders.\n","authors":["Yuan He","Zhangdie Yuan","Jiaoyan Chen","Ian Horrocks"],"pdf_url":"https://arxiv.org/pdf/2401.11374v4.pdf","comment":"Accept at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.13760v1","updated":"2024-11-21T00:15:44Z","published":"2024-11-21T00:15:44Z","title":"A Framework for Evaluating LLMs Under Task Indeterminacy","summary":"  Large language model (LLM) evaluations often assume there is a single correct\nresponse -- a gold label -- for each item in the evaluation corpus. However,\nsome tasks can be ambiguous -- i.e., they provide insufficient information to\nidentify a unique interpretation -- or vague -- i.e., they do not clearly\nindicate where to draw the line when making a determination. Both ambiguity and\nvagueness can cause task indeterminacy -- the condition where some items in the\nevaluation corpus have more than one correct response. In this paper, we\ndevelop a framework for evaluating LLMs under task indeterminacy. Our framework\ndisentangles the relationships between task specification, human ratings, and\nLLM responses in the LLM evaluation pipeline. Using our framework, we conduct a\nsynthetic experiment showing that evaluations that use the \"gold label\"\nassumption underestimate the true performance. We also provide a method for\nestimating an error-adjusted performance interval given partial knowledge about\nindeterminate items in the evaluation corpus. We conclude by outlining\nimplications of our work for the research community.\n","authors":["Luke Guerdan","Hanna Wallach","Solon Barocas","Alexandra Chouldechova"],"pdf_url":"https://arxiv.org/pdf/2411.13760v1.pdf","comment":"To Appear in NeurIPS 2024 Workshops on Evaluating Evaluations\n  (EvalEval) and Statistical Foundations of LLMs and Foundation Models (SFLLM)"},{"id":"http://arxiv.org/abs/2411.13786v1","updated":"2024-11-21T02:15:52Z","published":"2024-11-21T02:15:52Z","title":"Adaptable Embeddings Network (AEN)","summary":"  Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems.\n","authors":["Stan Loosmore","Alexander Titus"],"pdf_url":"https://arxiv.org/pdf/2411.13786v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2405.01769v2","updated":"2024-11-21T23:39:12Z","published":"2024-05-02T22:43:02Z","title":"A Survey on Large Language Models for Critical Societal Domains:\n  Finance, Healthcare, and Law","summary":"  In the fast-evolving domain of artificial intelligence, large language models\n(LLMs) such as GPT-3 and GPT-4 are revolutionizing the landscapes of finance,\nhealthcare, and law: domains characterized by their reliance on professional\nexpertise, challenging data acquisition, high-stakes, and stringent regulatory\ncompliance. This survey offers a detailed exploration of the methodologies,\napplications, challenges, and forward-looking opportunities of LLMs within\nthese high-stakes sectors. We highlight the instrumental role of LLMs in\nenhancing diagnostic and treatment methodologies in healthcare, innovating\nfinancial analytics, and refining legal interpretation and compliance\nstrategies. Moreover, we critically examine the ethics for LLM applications in\nthese fields, pointing out the existing ethical concerns and the need for\ntransparent, fair, and robust AI systems that respect regulatory norms. By\npresenting a thorough review of current literature and practical applications,\nwe showcase the transformative impact of LLMs, and outline the imperative for\ninterdisciplinary cooperation, methodological advancements, and ethical\nvigilance. Through this lens, we aim to spark dialogue and inspire future\nresearch dedicated to maximizing the benefits of LLMs while mitigating their\nrisks in these precision-dependent sectors. To facilitate future research on\nLLMs in these critical societal domains, we also initiate a reading list that\ntracks the latest advancements under this topic, which will be continually\nupdated: \\url{https://github.com/czyssrs/LLM_X_papers}.\n","authors":["Zhiyu Zoey Chen","Jing Ma","Xinlu Zhang","Nan Hao","An Yan","Armineh Nourbakhsh","Xianjun Yang","Julian McAuley","Linda Petzold","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2405.01769v2.pdf","comment":"TMLR 2024"},{"id":"http://arxiv.org/abs/2405.06058v2","updated":"2024-11-21T21:39:04Z","published":"2024-05-09T19:02:53Z","title":"Large Language Models Show Human-like Social Desirability Biases in\n  Survey Responses","summary":"  As Large Language Models (LLMs) become widely used to model and simulate\nhuman behavior, understanding their biases becomes critical. We developed an\nexperimental framework using Big Five personality surveys and uncovered a\npreviously undetected social desirability bias in a wide range of LLMs. By\nsystematically varying the number of questions LLMs were exposed to, we\ndemonstrate their ability to infer when they are being evaluated. When\npersonality evaluation is inferred, LLMs skew their scores towards the\ndesirable ends of trait dimensions (i.e., increased extraversion, decreased\nneuroticism, etc). This bias exists in all tested models, including GPT-4/3.5,\nClaude 3, Llama 3, and PaLM-2. Bias levels appear to increase in more recent\nmodels, with GPT-4's survey responses changing by 1.20 (human) standard\ndeviations and Llama 3's by 0.98 standard deviations-very large effects. This\nbias is robust to randomization of question order and paraphrasing.\nReverse-coding all the questions decreases bias levels but does not eliminate\nthem, suggesting that this effect cannot be attributed to acquiescence bias.\nOur findings reveal an emergent social desirability bias and suggest\nconstraints on profiling LLMs with psychometric tests and on using LLMs as\nproxies for human participants.\n","authors":["Aadesh Salecha","Molly E. Ireland","Shashanka Subrahmanya","João Sedoc","Lyle H. Ungar","Johannes C. Eichstaedt"],"pdf_url":"https://arxiv.org/pdf/2405.06058v2.pdf","comment":"3 pages, 2 figures, accepted at PNAS Nexus"},{"id":"http://arxiv.org/abs/2404.03854v3","updated":"2024-11-21T21:08:40Z","published":"2024-04-05T01:17:25Z","title":"Distributionally Robust Alignment for Medical Federated Vision-Language\n  Pre-training Under Data Heterogeneity","summary":"  Vision-language pre-training (VLP) has emerged as an effective scheme for\nmultimodal representation learning, but its reliance on large-scale multimodal\ndata poses significant challenges for medical applications. Federated learning\n(FL) offers a promising solution to scale up the dataset for medical VLP while\npreserving data privacy. However, we observe that client data heterogeneity in\nreal-world scenarios could cause models to learn biased cross-modal alignment\nduring local pre-training. This would limit the transferability of the\nfederally learned representation model on downstream tasks. To address this\nchallenge, we propose Federated Distributionally Robust Alignment (FedDRA), a\nframework for federated VLP that achieves robust vision-language alignment\nunder heterogeneous conditions. Based on client datasets, we construct a\ndistribution family that encompasses potential test-time domains, and apply a\ndistributionally robust framework to optimize the pre-trained model's\nperformance across this distribution space. This approach bridges the gap\nbetween pre-training samples and downstream applications. To avoid over-fitting\non client-specific information, we use anchor representation from the global\nmodel to guide the local training, and adopt a two-stage approach to first tune\ndeeper layers before updating the entire network. Extensive experiments on\nreal-world datasets demonstrate FedDRA's effectiveness in enhancing medical\nfederated VLP under data heterogeneity. Our method also adapts well to various\nmedical pre-training methods.\n","authors":["Zitao Shuai","Chenwei Wu","Zhengxu Tang","Liyue Shen"],"pdf_url":"https://arxiv.org/pdf/2404.03854v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14572v1","updated":"2024-11-21T20:39:13Z","published":"2024-11-21T20:39:13Z","title":"Towards Knowledge Checking in Retrieval-augmented Generation: A\n  Representation Perspective","summary":"  Retrieval-Augmented Generation (RAG) systems have shown promise in enhancing\nthe performance of Large Language Models (LLMs). However, these systems face\nchallenges in effectively integrating external knowledge with the LLM's\ninternal knowledge, often leading to issues with misleading or unhelpful\ninformation. This work aims to provide a systematic study on knowledge checking\nin RAG systems. We conduct a comprehensive analysis of LLM representation\nbehaviors and demonstrate the significance of using representations in\nknowledge checking. Motivated by the findings, we further develop\nrepresentation-based classifiers for knowledge filtering. We show substantial\nimprovements in RAG performance, even when dealing with noisy knowledge\ndatabases. Our study provides new insights into leveraging LLM representations\nfor enhancing the reliability and effectiveness of RAG systems.\n","authors":["Shenglai Zeng","Jiankun Zhang","Bingheng Li","Yuping Lin","Tianqi Zheng","Dante Everaert","Hanqing Lu","Hui Liu","Hui Liu","Yue Xing","Monica Xiao Cheng","Jiliang Tang"],"pdf_url":"https://arxiv.org/pdf/2411.14572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14571v1","updated":"2024-11-21T20:36:36Z","published":"2024-11-21T20:36:36Z","title":"Assessment of LLM Responses to End-user Security Questions","summary":"  Answering end user security questions is challenging. While large language\nmodels (LLMs) like GPT, LLAMA, and Gemini are far from error-free, they have\nshown promise in answering a variety of questions outside of security. We\nstudied LLM performance in the area of end user security by qualitatively\nevaluating 3 popular LLMs on 900 systematically collected end user security\nquestions.\n  While LLMs demonstrate broad generalist ``knowledge'' of end user security\ninformation, there are patterns of errors and limitations across LLMs\nconsisting of stale and inaccurate answers, and indirect or unresponsive\ncommunication styles, all of which impacts the quality of information received.\nBased on these patterns, we suggest directions for model improvement and\nrecommend user strategies for interacting with LLMs when seeking assistance\nwith security.\n","authors":["Vijay Prakash","Kevin Lee","Arkaprabha Bhattacharya","Danny Yuxing Huang","Jessica Staddon"],"pdf_url":"https://arxiv.org/pdf/2411.14571v1.pdf","comment":"18 pages, 1 figure, 8 tables"},{"id":"http://arxiv.org/abs/2411.04282v2","updated":"2024-11-21T20:29:09Z","published":"2024-11-06T22:02:30Z","title":"Language Models are Hidden Reasoners: Unlocking Latent Reasoning\n  Capabilities via Self-Rewarding","summary":"  Large language models (LLMs) have shown impressive capabilities, but still\nstruggle with complex reasoning tasks requiring multiple steps. While\nprompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at\ninference time, optimizing reasoning capabilities during training remains\nchallenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled\nframework that formulates reasoning as sampling from a latent distribution and\noptimizes it via variational approaches. LaTRO enables LLMs to concurrently\nimprove both their reasoning process and ability to evaluate reasoning quality,\nwithout requiring external feedback or reward models. We validate LaTRO through\nexperiments on GSM8K and ARC-Challenge datasets using multiple model\narchitectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of\n12.5% over base models and 9.6% over supervised fine-tuning across\nPhi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that\npre-trained LLMs possess latent reasoning capabilities that can be unlocked and\nenhanced through our proposed optimization approach in a self-improvement\nmanner. The code of LaTRO is available at\n\\url{https://github.com/SalesforceAIResearch/LaTRO}.\n","authors":["Haolin Chen","Yihao Feng","Zuxin Liu","Weiran Yao","Akshara Prabhakar","Shelby Heinecke","Ricky Ho","Phil Mui","Silvio Savarese","Caiming Xiong","Huan Wang"],"pdf_url":"https://arxiv.org/pdf/2411.04282v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14553v1","updated":"2024-11-21T19:49:33Z","published":"2024-11-21T19:49:33Z","title":"Reducibility among NP-Hard graph problems and boundary classes","summary":"  Many NP-hard graph problems become easy for some classes of graphs, such as\ncoloring is easy for bipartite graphs, but NP-hard in general. So we can ask\nquestion like when does a hard problem become easy? What is the minimum\nsubstructure for which the problem remains hard? We use the notion of boundary\nclasses to study such questions. In this paper, we introduce a method for\ntransforming the boundary class of one NP-hard graph problem into a boundary\nclass for another problem. If $\\Pi$ and $\\Gamma$ are two NP-hard graph problems\nwhere $\\Pi$ is reducible to $\\Gamma$, we transform a boundary class of $\\Pi$\ninto a boundary class of $\\Gamma$. More formally if $\\Pi$ is reducible to\n$\\Gamma$, where the reduction is bijective and it maps hereditary classes of\ngraphs to hereditary classes of graphs, then $X$ is a boundary class of $\\Pi$\nif and only if the image of $X$ under the reduction is a boundary class of\n$\\Gamma$. This gives us a relationship between boundary classes and\nreducibility among several NP-hard problems. To show the strength of our main\nresult, we apply our theorem to obtain some previously unknown boundary classes\nfor a few graph problems namely; vertex-cover, clique, traveling-salesperson,\nbounded-degree-spanning-tree, subgraph-isomorphism and clique-cover.\n","authors":["Syed Mujtaba Hassan","Shahid Hussain","Abdul Samad"],"pdf_url":"https://arxiv.org/pdf/2411.14553v1.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2411.14551v1","updated":"2024-11-21T19:45:48Z","published":"2024-11-21T19:45:48Z","title":"An Experimental Study on Data Augmentation Techniques for Named Entity\n  Recognition on Low-Resource Domains","summary":"  Named Entity Recognition (NER) is a machine learning task that traditionally\nrelies on supervised learning and annotated data. Acquiring such data is often\na challenge, particularly in specialized fields like medical, legal, and\nfinancial sectors. Those are commonly referred to as low-resource domains,\nwhich comprise long-tail entities, due to the scarcity of available data. To\naddress this, data augmentation techniques are increasingly being employed to\ngenerate additional training instances from the original dataset. In this\nstudy, we evaluate the effectiveness of two prominent text augmentation\ntechniques, Mention Replacement and Contextual Word Replacement, on two\nwidely-used NER models, Bi-LSTM+CRF and BERT. We conduct experiments on four\ndatasets from low-resource domains, and we explore the impact of various\ncombinations of training subset sizes and number of augmented examples. We not\nonly confirm that data augmentation is particularly beneficial for smaller\ndatasets, but we also demonstrate that there is no universally optimal number\nof augmented examples, i.e., NER practitioners must experiment with different\nquantities in order to fine-tune their projects.\n","authors":["Arthur Elwing Torres","Edleno Silva de Moura","Altigran Soares da Silva","Mario A. Nascimento","Filipe Mesquita"],"pdf_url":"https://arxiv.org/pdf/2411.14551v1.pdf","comment":"21 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.18060v2","updated":"2024-11-21T19:43:00Z","published":"2024-06-26T04:33:13Z","title":"AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for\n  Memory-Efficient Large Language Models Fine-Tuning","summary":"  Fine-tuning large language models (LLMs) has achieved remarkable performance\nacross various natural language processing tasks, yet it demands more and more\nmemory as model sizes keep growing. To address this issue, the recently\nproposed Memory-efficient Zeroth-order (MeZO) methods attempt to fine-tune LLMs\nusing only forward passes, thereby avoiding the need for a backpropagation\ngraph. However, significant performance drops and a high risk of divergence\nhave limited their widespread adoption. In this paper, we propose the Adaptive\nZeroth-order Tensor-Train Adaption (AdaZeta) framework, specifically designed\nto improve the performance and convergence of the ZO methods. To enhance\ndimension-dependent ZO estimation accuracy, we introduce a fast-forward,\nlow-parameter tensorized adapter. To tackle the frequently observed divergence\nissue in large-scale ZO fine-tuning tasks, we propose an adaptive query number\nschedule that guarantees convergence. Detailed theoretical analysis and\nextensive experimental results on Roberta-Large and Llama-2-7B models\nsubstantiate the efficacy of our AdaZeta framework in terms of accuracy, memory\nefficiency, and convergence speed.\n","authors":["Yifan Yang","Kai Zhen","Ershad Banijamal","Athanasios Mouchtaris","Zheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.18060v2.pdf","comment":"Accepted for publication in EMNLP 2024"},{"id":"http://arxiv.org/abs/2411.14513v1","updated":"2024-11-21T13:55:24Z","published":"2024-11-21T13:55:24Z","title":"Towards a Middleware for Large Language Models","summary":"  Large language models have gained widespread popularity for their ability to\nprocess natural language inputs and generate insights derived from their\ntraining data, nearing the qualities of true artificial intelligence. This\nadvancement has prompted enterprises worldwide to integrate LLMs into their\nservices. So far, this effort is dominated by commercial cloud-based solutions\nlike OpenAI's ChatGPT and Microsoft Azure. As the technology matures, however,\nthere is a strong incentive for independence from major cloud providers through\nself-hosting \"LLM as a Service\", driven by privacy, cost, and customization\nneeds. In practice, hosting LLMs independently presents significant challenges\ndue to their complexity and integration issues with existing systems. In this\npaper, we discuss our vision for a forward-looking middleware system\narchitecture that facilitates the deployment and adoption of LLMs in\nenterprises, even for advanced use cases in which we foresee LLMs to serve as\ngateways to a complete application ecosystem and, to some degree, absorb\nfunctionality traditionally attributed to the middleware.\n","authors":["Narcisa Guran","Florian Knauf","Man Ngo","Stefan Petrescu","Jan S. Rellermeyer"],"pdf_url":"https://arxiv.org/pdf/2411.14513v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14507v1","updated":"2024-11-21T09:49:28Z","published":"2024-11-21T09:49:28Z","title":"FuseGPT: Learnable Layers Fusion of Generative Pre-trained Transformers","summary":"  Generative Pre-trained Transformers (GPTs) have demonstrated remarkable\nperformance across diverse domains through the extensive scaling of model\nparameters. Recent works observe the redundancy across the transformer blocks\nand develop compression methods by structured pruning of the unimportant\nblocks. However, such straightforward elimination will always provide\nirreversible performance degradation. In this paper, we propose FuseGPT, a\nnovel methodology to recycle the pruned transformer blocks to further recover\nthe model performance. Firstly we introduce a new importance detection metric,\nMacro Influence (MI), to detect the long-term influence of each transformer\nblock by calculating their loss of information after removal. Then we propose\ngroup-level layers fusion, which adopts the parameters in layers of the\nunimportant blocks and injects them into the corresponding layers inside the\nneighboring blocks. The fusion is not one-off but through iterative parameter\nupdates by lightweight group-level fine-tuning. Specifically, these injected\nparameters are frozen but weighted with learnable rank decomposition matrices\nto reduce the overhead during fine-tuning. Our approach not only works well on\nlarge language models but also on large multimodal models. The experiments have\nshown that, by using modest amounts of data, FuseGPT can outperform previous\nworks in both perplexity and zero-shot task performance.\n","authors":["Zehua Pei","Hui-Ling Zhen","Xianzhi Yu","Sinno Jialin Pan","Mingxuan Yuan","Bei Yu"],"pdf_url":"https://arxiv.org/pdf/2411.14507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14500v1","updated":"2024-11-21T04:40:35Z","published":"2024-11-21T04:40:35Z","title":"Exploring Accuracy-Fairness Trade-off in Large Language Models","summary":"  Large Language Models (LLMs) have made significant strides in the field of\nartificial intelligence, showcasing their ability to interact with humans and\ninfluence human cognition through information dissemination. However, recent\nstudies have brought to light instances of bias inherent within these LLMs,\npresenting a critical issue that demands attention. In our research, we delve\ndeeper into the intricate challenge of harmonising accuracy and fairness in the\nenhancement of LLMs. While improving accuracy can indeed enhance overall LLM\nperformance, it often occurs at the expense of fairness. Overemphasising\noptimisation of one metric invariably leads to a significant degradation of the\nother. This underscores the necessity of taking into account multiple\nconsiderations during the design and optimisation phases of LLMs. Therefore, we\nadvocate for reformulating the LLM training process as a multi-objective\nlearning task. Our investigation reveals that multi-objective evolutionary\nlearning (MOEL) methodologies offer promising avenues for tackling this\nchallenge. Our MOEL framework enables the simultaneous optimisation of both\naccuracy and fairness metrics, resulting in a Pareto-optimal set of LLMs. In\nsummary, our study sheds valuable lights on the delicate equilibrium between\naccuracy and fairness within LLMs, which is increasingly significant for their\nreal-world applications. By harnessing MOEL, we present a promising pathway\ntowards fairer and more efficacious AI technologies.\n","authors":["Qingquan Zhang","Qiqi Duan","Bo Yuan","Yuhui Shi","Jialin Liu"],"pdf_url":"https://arxiv.org/pdf/2411.14500v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2411.14499v1","updated":"2024-11-21T03:58:50Z","published":"2024-11-21T03:58:50Z","title":"Understanding World or Predicting Future? A Comprehensive Survey of\n  World Models","summary":"  The concept of world models has garnered significant attention due to\nadvancements in multimodal large language models such as GPT-4 and video\ngeneration models such as Sora, which are central to the pursuit of artificial\ngeneral intelligence. This survey offers a comprehensive review of the\nliterature on world models. Generally, world models are regarded as tools for\neither understanding the present state of the world or predicting its future\ndynamics. This review presents a systematic categorization of world models,\nemphasizing two primary functions: (1) constructing internal representations to\nunderstand the mechanisms of the world, and (2) predicting future states to\nsimulate and guide decision-making. Initially, we examine the current progress\nin these two categories. We then explore the application of world models in key\ndomains, including autonomous driving, robotics, and social simulacra, with a\nfocus on how each domain utilizes these aspects. Finally, we outline key\nchallenges and provide insights into potential future research directions.\n","authors":["Jingtao Ding","Yunke Zhang","Yu Shang","Yuheng Zhang","Zefang Zong","Jie Feng","Yuan Yuan","Hongyuan Su","Nian Li","Nicholas Sukiennik","Fengli Xu","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2411.14499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14497v1","updated":"2024-11-21T02:30:53Z","published":"2024-11-21T02:30:53Z","title":"Star-Agents: Automatic Data Optimization with LLM Agents for Instruction\n  Tuning","summary":"  The efficacy of large language models (LLMs) on downstream tasks usually\nhinges on instruction tuning, which relies critically on the quality of\ntraining data. Unfortunately, collecting high-quality and diverse data is both\nexpensive and time-consuming. To mitigate this issue, we propose a novel\nStar-Agents framework, which automates the enhancement of data quality across\ndatasets through multi-agent collaboration and assessment. The framework adopts\na three-pronged strategy. It initially generates diverse instruction data with\nmultiple LLM agents through a bespoke sampling method. Subsequently, the\ngenerated data undergo a rigorous evaluation using a dual-model method that\nassesses both difficulty and quality. Finaly, the above process evolves in a\ndynamic refinement phase, where more effective LLMs are prioritized, enhancing\nthe overall data quality. Our empirical studies, including instruction tuning\nexperiments with models such as Pythia and LLaMA, demonstrate the effectiveness\nof the proposed framework. Optimized datasets have achieved substantial\nimprovements, with an average increase of 12% and notable gains in specific\nmetrics, such as a 40% improvement in Fermi, as evidenced by benchmarks like\nMT-bench, Vicuna bench, and WizardLM testset.\n","authors":["Hang Zhou","Yehui Tang","Haochen Qin","Yujie Yang","Renren Jin","Deyi Xiong","Kai Han","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2411.14497v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2411.14432v1","updated":"2024-11-21T18:59:55Z","published":"2024-11-21T18:59:55Z","title":"Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large\n  Language Models","summary":"  Large Language Models (LLMs) demonstrate enhanced capabilities and\nreliability by reasoning more, evolving from Chain-of-Thought prompting to\nproduct-level solutions like OpenAI o1. Despite various efforts to improve LLM\nreasoning, high-quality long-chain reasoning data and optimized training\npipelines still remain inadequately explored in vision-language tasks. In this\npaper, we present Insight-V, an early effort to 1) scalably produce long and\nrobust reasoning data for complex multi-modal tasks, and 2) an effective\ntraining pipeline to enhance the reasoning capabilities of multi-modal large\nlanguage models (MLLMs). Specifically, to create long and structured reasoning\ndata without human labor, we design a two-step pipeline with a progressive\nstrategy to generate sufficiently long and diverse reasoning paths and a\nmulti-granularity assessment method to ensure data quality. We observe that\ndirectly supervising MLLMs with such long and complex reasoning data will not\nyield ideal reasoning ability. To tackle this problem, we design a multi-agent\nsystem consisting of a reasoning agent dedicated to performing long-chain\nreasoning and a summary agent trained to judge and summarize reasoning results.\nWe further incorporate an iterative DPO algorithm to enhance the reasoning\nagent's generation stability and quality. Based on the popular LLaVA-NeXT model\nand our stronger base MLLM, we demonstrate significant performance gains across\nchallenging multi-modal benchmarks requiring visual reasoning. Benefiting from\nour multi-agent system, Insight-V can also easily maintain or improve\nperformance on perception-focused multi-modal tasks.\n","authors":["Yuhao Dong","Zuyan Liu","Hai-Long Sun","Jingkang Yang","Winston Hu","Yongming Rao","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2411.14432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14430v1","updated":"2024-11-21T18:59:51Z","published":"2024-11-21T18:59:51Z","title":"Stable Flow: Vital Layers for Training-Free Image Editing","summary":"  Diffusion models have revolutionized the field of content synthesis and\nediting. Recent models have replaced the traditional UNet architecture with the\nDiffusion Transformer (DiT), and employed flow-matching for improved training\nand sampling. However, they exhibit limited generation diversity. In this work,\nwe leverage this limitation to perform consistent image edits via selective\ninjection of attention features. The main challenge is that, unlike the\nUNet-based models, DiT lacks a coarse-to-fine synthesis structure, making it\nunclear in which layers to perform the injection. Therefore, we propose an\nautomatic method to identify \"vital layers\" within DiT, crucial for image\nformation, and demonstrate how these layers facilitate a range of controlled\nstable edits, from non-rigid modifications to object addition, using the same\nmechanism. Next, to enable real-image editing, we introduce an improved image\ninversion method for flow models. Finally, we evaluate our approach through\nqualitative and quantitative comparisons, along with a user study, and\ndemonstrate its effectiveness across multiple applications. The project page is\navailable at https://omriavrahami.com/stable-flow\n","authors":["Omri Avrahami","Or Patashnik","Ohad Fried","Egor Nemchinov","Kfir Aberman","Dani Lischinski","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2411.14430v1.pdf","comment":"Project page is available at https://omriavrahami.com/stable-flow"},{"id":"http://arxiv.org/abs/2411.14429v1","updated":"2024-11-21T18:59:08Z","published":"2024-11-21T18:59:08Z","title":"Revisiting the Integration of Convolution and Attention for Vision\n  Backbone","summary":"  Convolutions (Convs) and multi-head self-attentions (MHSAs) are typically\nconsidered alternatives to each other for building vision backbones. Although\nsome works try to integrate both, they apply the two operators simultaneously\nat the finest pixel granularity. With Convs responsible for per-pixel feature\nextraction already, the question is whether we still need to include the heavy\nMHSAs at such a fine-grained level. In fact, this is the root cause of the\nscalability issue w.r.t. the input resolution for vision transformers. To\naddress this important problem, we propose in this work to use MSHAs and Convs\nin parallel \\textbf{at different granularity levels} instead. Specifically, in\neach layer, we use two different ways to represent an image: a fine-grained\nregular grid and a coarse-grained set of semantic slots. We apply different\noperations to these two representations: Convs to the grid for local features,\nand MHSAs to the slots for global features. A pair of fully differentiable soft\nclustering and dispatching modules is introduced to bridge the grid and set\nrepresentations, thus enabling local-global fusion. Through extensive\nexperiments on various vision tasks, we empirically verify the potential of the\nproposed integration scheme, named \\textit{GLMix}: by offloading the burden of\nfine-grained features to light-weight Convs, it is sufficient to use MHSAs in a\nfew (e.g., 64) semantic slots to match the performance of recent\nstate-of-the-art backbones, while being more efficient. Our visualization\nresults also demonstrate that the soft clustering module produces a meaningful\nsemantic grouping effect with only IN1k classification supervision, which may\ninduce better interpretability and inspire new weakly-supervised semantic\nsegmentation approaches. Code will be available at\n\\url{https://github.com/rayleizhu/GLMix}.\n","authors":["Lei Zhu","Xinjiang Wang","Wayne Zhang","Rynson W. H. Lau"],"pdf_url":"https://arxiv.org/pdf/2411.14429v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.14423v1","updated":"2024-11-21T18:55:23Z","published":"2024-11-21T18:55:23Z","title":"Unleashing the Potential of Multi-modal Foundation Models and Video\n  Diffusion for 4D Dynamic Physical Scene Simulation","summary":"  Realistic simulation of dynamic scenes requires accurately capturing diverse\nmaterial properties and modeling complex object interactions grounded in\nphysical principles. However, existing methods are constrained to basic\nmaterial types with limited predictable parameters, making them insufficient to\nrepresent the complexity of real-world materials. We introduce a novel approach\nthat leverages multi-modal foundation models and video diffusion to achieve\nenhanced 4D dynamic scene simulation. Our method utilizes multi-modal models to\nidentify material types and initialize material parameters through image\nqueries, while simultaneously inferring 3D Gaussian splats for detailed scene\nrepresentation. We further refine these material parameters using video\ndiffusion with a differentiable Material Point Method (MPM) and optical flow\nguidance rather than render loss or Score Distillation Sampling (SDS) loss.\nThis integrated framework enables accurate prediction and realistic simulation\nof dynamic interactions in real-world scenarios, advancing both accuracy and\nflexibility in physics-based simulations.\n","authors":["Zhuoman Liu","Weicai Ye","Yan Luximon","Pengfei Wan","Di Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14423v1.pdf","comment":"Homepage: https://zhuomanliu.github.io/PhysFlow/"},{"id":"http://arxiv.org/abs/2408.00754v2","updated":"2024-11-21T18:52:31Z","published":"2024-08-01T17:57:12Z","title":"Coarse Correspondences Boost Spatial-Temporal Reasoning in Multimodal\n  Language Model","summary":"  Multimodal language models (MLLMs) are increasingly being applied in\nreal-world environments, necessitating their ability to interpret 3D spaces and\ncomprehend temporal dynamics. Current methods often rely on specialized\narchitectural designs or task-specific fine-tuning to achieve this. We\nintroduce Coarse Correspondences, a simple lightweight method that enhances\nMLLMs' spatial-temporal reasoning with 2D images as input, without modifying\nthe architecture or requiring task-specific fine-tuning. Our method uses a\nlightweight tracking model to identify primary object correspondences between\nframes in a video or across different image viewpoints, and then conveys this\ninformation to MLLMs through visual prompting. We demonstrate that this simple\ntraining-free approach brings substantial gains to GPT4-V/O consistently on\nfour benchmarks that require spatial-temporal reasoning, including +20.5\\%\nimprovement on ScanQA, +9.7\\% on OpenEQA's episodic memory subset, +6.0\\% on\nthe long-form video benchmark EgoSchema, and +11\\% on the R2R navigation\nbenchmark. Additionally, we show that Coarse Correspondences can also enhance\nopen-source MLLMs' spatial reasoning (by +6.9\\% on ScanQA) when applied in both\ntraining and inference and that the improvement can generalize to unseen\ndatasets such as SQA3D (+3.1\\%). Taken together, we show that Coarse\nCorrespondences effectively and efficiently boosts models' performance on\ndownstream tasks requiring spatial-temporal reasoning.\n","authors":["Benlin Liu","Yuhao Dong","Yiqin Wang","Zixian Ma","Yansong Tang","Luming Tang","Yongming Rao","Wei-Chiu Ma","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2408.00754v2.pdf","comment":"project page: https://coarse-correspondence.github.io"},{"id":"http://arxiv.org/abs/2411.14418v1","updated":"2024-11-21T18:52:02Z","published":"2024-11-21T18:52:02Z","title":"Multimodal 3D Brain Tumor Segmentation with Adversarial Training and\n  Conditional Random Field","summary":"  Accurate brain tumor segmentation remains a challenging task due to\nstructural complexity and great individual differences of gliomas. Leveraging\nthe pre-eminent detail resilience of CRF and spatial feature extraction\ncapacity of V-net, we propose a multimodal 3D Volume Generative Adversarial\nNetwork (3D-vGAN) for precise segmentation. The model utilizes Pseudo-3D for\nV-net improvement, adds conditional random field after generator and use\noriginal image as supplemental guidance. Results, using the BraTS-2018 dataset,\nshow that 3D-vGAN outperforms classical segmentation models, including U-net,\nGan, FCN and 3D V-net, reaching specificity over 99.8%.\n","authors":["Lan Jiang","Yuchao Zheng","Miao Yu","Haiqing Zhang","Fatemah Aladwani","Alessandro Perelli"],"pdf_url":"https://arxiv.org/pdf/2411.14418v1.pdf","comment":"13 pages, 7 figures, Annual Conference on Medical Image Understanding\n  and Analysis (MIUA) 2024"},{"id":"http://arxiv.org/abs/2411.14412v1","updated":"2024-11-21T18:46:45Z","published":"2024-11-21T18:46:45Z","title":"Adversarial Poisoning Attack on Quantum Machine Learning Models","summary":"  With the growing interest in Quantum Machine Learning (QML) and the\nincreasing availability of quantum computers through cloud providers,\naddressing the potential security risks associated with QML has become an\nurgent priority. One key concern in the QML domain is the threat of data\npoisoning attacks in the current quantum cloud setting. Adversarial access to\ntraining data could severely compromise the integrity and availability of QML\nmodels. Classical data poisoning techniques require significant knowledge and\ntraining to generate poisoned data, and lack noise resilience, making them\nineffective for QML models in the Noisy Intermediate Scale Quantum (NISQ) era.\nIn this work, we first propose a simple yet effective technique to measure\nintra-class encoder state similarity (ESS) by analyzing the outputs of encoding\ncircuits. Leveraging this approach, we introduce a quantum indiscriminate data\npoisoning attack, QUID. Through extensive experiments conducted in both\nnoiseless and noisy environments (e.g., IBM\\_Brisbane's noise), across various\narchitectures and datasets, QUID achieves up to $92\\%$ accuracy degradation in\nmodel performance compared to baseline models and up to $75\\%$ accuracy\ndegradation compared to random label-flipping. We also tested QUID against\nstate-of-the-art classical defenses, with accuracy degradation still exceeding\n$50\\%$, demonstrating its effectiveness. This work represents the first attempt\nto reevaluate data poisoning attacks in the context of QML.\n","authors":["Satwik Kundu","Swaroop Ghosh"],"pdf_url":"https://arxiv.org/pdf/2411.14412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13545v2","updated":"2024-11-21T18:34:35Z","published":"2024-11-20T18:54:53Z","title":"Pushing the Limits of Sparsity: A Bag of Tricks for Extreme Pruning","summary":"  Pruning of deep neural networks has been an effective technique for reducing\nmodel size while preserving most of the performance of dense networks, crucial\nfor deploying models on memory and power-constrained devices. While recent\nsparse learning methods have shown promising performance up to moderate\nsparsity levels such as 95% and 98%, accuracy quickly deteriorates when pushing\nsparsities to extreme levels. Obtaining sparse networks at such extreme\nsparsity levels presents unique challenges, such as fragile gradient flow and\nheightened risk of layer collapse. In this work, we explore network performance\nbeyond the commonly studied sparsities, and propose a collection of techniques\nthat enable the continuous learning of networks without accuracy collapse even\nat extreme sparsities, including 99.90%, 99.95% and 99.99% on ResNet\narchitectures. Our approach combines 1) Dynamic ReLU phasing, where DyReLU\ninitially allows for richer parameter exploration before being gradually\nreplaced by standard ReLU, 2) weight sharing which reuses parameters within a\nresidual layer while maintaining the same number of learnable parameters, and\n3) cyclic sparsity, where both sparsity levels and sparsity patterns evolve\ndynamically throughout training to better encourage parameter exploration. We\nevaluate our method, which we term Extreme Adaptive Sparse Training (EAST) at\nextreme sparsities using ResNet-34 and ResNet-50 on CIFAR-10, CIFAR-100, and\nImageNet, achieving significant performance improvements over state-of-the-art\nmethods we compared with.\n","authors":["Andy Li","Aiden Durrant","Milan Markovic","Lu Yin","Georgios Leontidis"],"pdf_url":"https://arxiv.org/pdf/2411.13545v2.pdf","comment":"V2: same as V1 but with appendix/preliminaries; 12 pages, 5 figures,\n  4 tables"},{"id":"http://arxiv.org/abs/2411.14402v1","updated":"2024-11-21T18:31:25Z","published":"2024-11-21T18:31:25Z","title":"Multimodal Autoregressive Pre-training of Large Vision Encoders","summary":"  We introduce a novel method for pre-training of large-scale vision encoders.\nBuilding on recent advancements in autoregressive pre-training of vision\nmodels, we extend this framework to a multimodal setting, i.e., images and\ntext. In this paper, we present AIMV2, a family of generalist vision encoders\ncharacterized by a straightforward pre-training process, scalability, and\nremarkable performance across a range of downstream tasks. This is achieved by\npairing the vision encoder with a multimodal decoder that autoregressively\ngenerates raw image patches and text tokens. Our encoders excel not only in\nmultimodal evaluations but also in vision benchmarks such as localization,\ngrounding, and classification. Notably, our AIMV2-3B encoder achieves 89.5%\naccuracy on ImageNet-1k with a frozen trunk. Furthermore, AIMV2 consistently\noutperforms state-of-the-art contrastive models (e.g., CLIP, SigLIP) in\nmultimodal image understanding across diverse settings.\n","authors":["Enrico Fini","Mustafa Shukor","Xiujun Li","Philipp Dufter","Michal Klein","David Haldimann","Sai Aitharaju","Victor Guilherme Turrisi da Costa","Louis Béthune","Zhe Gan","Alexander T Toshev","Marcin Eichner","Moin Nabi","Yinfei Yang","Joshua M. Susskind","Alaaeldin El-Nouby"],"pdf_url":"https://arxiv.org/pdf/2411.14402v1.pdf","comment":"https://github.com/apple/ml-aim"},{"id":"http://arxiv.org/abs/2411.14401v1","updated":"2024-11-21T18:30:11Z","published":"2024-11-21T18:30:11Z","title":"Beyond Training: Dynamic Token Merging for Zero-Shot Video Understanding","summary":"  Recent advancements in multimodal large language models (MLLMs) have opened\nnew avenues for video understanding. However, achieving high fidelity in\nzero-shot video tasks remains challenging. Traditional video processing methods\nrely heavily on fine-tuning to capture nuanced spatial-temporal details, which\nincurs significant data and computation costs. In contrast, training-free\napproaches, though efficient, often lack robustness in preserving context-rich\nfeatures across complex video content. To this end, we propose DYTO, a novel\ndynamic token merging framework for zero-shot video understanding that\nadaptively optimizes token efficiency while preserving crucial scene details.\nDYTO integrates a hierarchical frame selection and a bipartite token merging\nstrategy to dynamically cluster key frames and selectively compress token\nsequences, striking a balance between computational efficiency with semantic\nrichness. Extensive experiments across multiple benchmarks demonstrate the\neffectiveness of DYTO, achieving superior performance compared to both\nfine-tuned and training-free methods and setting a new state-of-the-art for\nzero-shot video understanding.\n","authors":["Yiming Zhang","Zhuokai Zhao","Zhaorun Chen","Zenghui Ding","Xianjun Yang","Yining Sun"],"pdf_url":"https://arxiv.org/pdf/2411.14401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14385v1","updated":"2024-11-21T18:21:42Z","published":"2024-11-21T18:21:42Z","title":"Enhancing Diagnostic Precision in Gastric Bleeding through Automated\n  Lesion Segmentation: A Deep DuS-KFCM Approach","summary":"  Timely and precise classification and segmentation of gastric bleeding in\nendoscopic imagery are pivotal for the rapid diagnosis and intervention of\ngastric complications, which is critical in life-saving medical procedures.\nTraditional methods grapple with the challenge posed by the indistinguishable\nintensity values of bleeding tissues adjacent to other gastric structures. Our\nstudy seeks to revolutionize this domain by introducing a novel deep learning\nmodel, the Dual Spatial Kernelized Constrained Fuzzy C-Means (Deep DuS-KFCM)\nclustering algorithm. This Hybrid Neuro-Fuzzy system synergizes Neural Networks\nwith Fuzzy Logic to offer a highly precise and efficient identification of\nbleeding regions. Implementing a two-fold coarse-to-fine strategy for\nsegmentation, this model initially employs the Spatial Kernelized Fuzzy C-Means\n(SKFCM) algorithm enhanced with spatial intensity profiles and subsequently\nharnesses the state-of-the-art DeepLabv3+ with ResNet50 architecture to refine\nthe segmentation output. Through extensive experiments across mainstream\ngastric bleeding and red spots datasets, our Deep DuS-KFCM model demonstrated\nunprecedented accuracy rates of 87.95%, coupled with a specificity of 96.33%,\noutperforming contemporary segmentation methods. The findings underscore the\nmodel's robustness against noise and its outstanding segmentation capabilities,\nparticularly for identifying subtle bleeding symptoms, thereby presenting a\nsignificant leap forward in medical image processing.\n","authors":["Xian-Xian Liu","Mingkun Xu","Yuanyuan Wei","Huafeng Qin","Qun Song","Simon Fong","Feng Tien","Wei Luo","Juntao Gao","Zhihua Zhang","Shirley Siu"],"pdf_url":"https://arxiv.org/pdf/2411.14385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14384v1","updated":"2024-11-21T18:21:24Z","published":"2024-11-21T18:21:24Z","title":"Baking Gaussian Splatting into Diffusion Denoiser for Fast and Scalable\n  Single-stage Image-to-3D Generation","summary":"  Existing feed-forward image-to-3D methods mainly rely on 2D multi-view\ndiffusion models that cannot guarantee 3D consistency. These methods easily\ncollapse when changing the prompt view direction and mainly handle\nobject-centric prompt images. In this paper, we propose a novel single-stage 3D\ndiffusion model, DiffusionGS, for object and scene generation from a single\nview. DiffusionGS directly outputs 3D Gaussian point clouds at each timestep to\nenforce view consistency and allow the model to generate robustly given prompt\nviews of any directions, beyond object-centric inputs. Plus, to improve the\ncapability and generalization ability of DiffusionGS, we scale up 3D training\ndata by developing a scene-object mixed training strategy. Experiments show\nthat our method enjoys better generation quality (2.20 dB higher in PSNR and\n23.25 lower in FID) and over 5x faster speed (~6s on an A100 GPU) than SOTA\nmethods. The user study and text-to-3D applications also reveals the practical\nvalues of our method. Our Project page at\nhttps://caiyuanhao1998.github.io/project/DiffusionGS/ shows the video and\ninteractive generation results.\n","authors":["Yuanhao Cai","He Zhang","Kai Zhang","Yixun Liang","Mengwei Ren","Fujun Luan","Qing Liu","Soo Ye Kim","Jianming Zhang","Zhifei Zhang","Yuqian Zhou","Zhe Lin","Alan Yuille"],"pdf_url":"https://arxiv.org/pdf/2411.14384v1.pdf","comment":"A novel one-stage 3DGS-based diffusion generates objects and scenes\n  from a single view in ~6 seconds"},{"id":"http://arxiv.org/abs/2406.08222v2","updated":"2024-11-21T18:14:58Z","published":"2024-06-12T13:52:30Z","title":"A Sociotechnical Lens for Evaluating Computer Vision Models: A Case\n  Study on Detecting and Reasoning about Gender and Emotion","summary":"  In the evolving landscape of computer vision (CV) technologies, the automatic\ndetection and interpretation of gender and emotion in images is a critical area\nof study. This paper investigates social biases in CV models, emphasizing the\nlimitations of traditional evaluation metrics such as precision, recall, and\naccuracy. These metrics often fall short in capturing the complexities of\ngender and emotion, which are fluid and culturally nuanced constructs. Our\nstudy proposes a sociotechnical framework for evaluating CV models,\nincorporating both technical performance measures and considerations of social\nfairness. Using a dataset of 5,570 images related to vaccination and climate\nchange, we empirically compared the performance of various CV models, including\ntraditional models like DeepFace and FER, and generative models like GPT-4\nVision. Our analysis involved manually validating the gender and emotional\nexpressions in a subset of images to serve as benchmarks. Our findings reveal\nthat while GPT-4 Vision outperforms other models in technical accuracy for\ngender classification, it exhibits discriminatory biases, particularly in\nresponse to transgender and non-binary personas. Furthermore, the model's\nemotion detection skew heavily towards positive emotions, with a notable bias\ntowards associating female images with happiness, especially when prompted by\nmale personas. These findings underscore the necessity of developing more\ncomprehensive evaluation criteria that address both validity and discriminatory\nbiases in CV models. Our proposed framework provides guidelines for researchers\nto critically assess CV tools, ensuring their application in communication\nresearch is both ethical and effective. The significant contribution of this\nstudy lies in its emphasis on a sociotechnical approach, advocating for CV\ntechnologies that support social good and mitigate biases rather than\nperpetuate them.\n","authors":["Sha Luo","Sang Jung Kim","Zening Duan","Kaiping Chen"],"pdf_url":"https://arxiv.org/pdf/2406.08222v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14374v1","updated":"2024-11-21T18:09:04Z","published":"2024-11-21T18:09:04Z","title":"Using Formal Models, Safety Shields and Certified Control to Validate\n  AI-Based Train Systems","summary":"  The certification of autonomous systems is an important concern in science\nand industry. The KI-LOK project explores new methods for certifying and safely\nintegrating AI components into autonomous trains. We pursued a two-layered\napproach: (1) ensuring the safety of the steering system by formal analysis\nusing the B method, and (2) improving the reliability of the perception system\nwith a runtime certificate checker. This work links both strategies within a\ndemonstrator that runs simulations on the formal model, controlled by the real\nAI output and the real certificate checker. The demonstrator is integrated into\nthe validation tool ProB. This enables runtime monitoring, runtime\nverification, and statistical validation of formal safety properties using a\nformal B model. Consequently, one can detect and analyse potential\nvulnerabilities and weaknesses of the AI and the certificate checker. We apply\nthese techniques to a signal detection case study and present our findings.\n","authors":["Jan Gruteser","Jan Roßbach","Fabian Vu","Michael Leuschel"],"pdf_url":"https://arxiv.org/pdf/2411.14374v1.pdf","comment":"In Proceedings FMAS2024, arXiv:2411.13215"},{"id":"http://arxiv.org/abs/2410.16162v2","updated":"2024-11-21T18:05:04Z","published":"2024-10-21T16:26:09Z","title":"Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models\n  Elicits Generalization to Composite Spatial Reasoning","summary":"  Vision language models (VLMs) have demonstrated impressive performance across\na wide range of downstream tasks. However, their proficiency in spatial\nreasoning remains limited, despite its crucial role in tasks involving\nnavigation and interaction with physical environments. Specifically, most of\nthese tasks rely on the core spatial reasoning capabilities in two-dimensional\n(2D) environments, and our evaluation reveals that state-of-the-art VLMs\nfrequently generate implausible and incorrect responses to composite spatial\nreasoning problems, including simple pathfinding tasks that humans can solve\neffortlessly at a glance. To address this, we explore an effective approach to\nenhance 2D spatial reasoning within VLMs by training the model solely on basic\nspatial capabilities. We begin by disentangling the key components of 2D\nspatial reasoning: direction comprehension, distance estimation, and\nlocalization. Our central hypothesis is that mastering these basic spatial\ncapabilities can significantly enhance a model's performance on composite\nspatial tasks requiring advanced spatial understanding and combinatorial\nproblem-solving, with generalized improvements in visual-spatial tasks. To\ninvestigate this hypothesis, we introduce Sparkle, a framework that fine-tunes\nVLMs on these three basic spatial capabilities by synthetic data generation and\ntargeted supervision to form an instruction dataset for each capability. Our\nexperiments demonstrate that VLMs fine-tuned with Sparkle achieve significant\nperformance gains, not only in the basic tasks themselves but also in\ngeneralizing to composite and out-of-distribution spatial reasoning tasks.\nThese findings underscore the effectiveness of mastering basic spatial\ncapabilities in enhancing composite spatial problem-solving, offering insights\ninto systematic strategies for improving VLMs' spatial reasoning capabilities.\n","authors":["Yihong Tang","Ao Qu","Zhaokai Wang","Dingyi Zhuang","Zhaofeng Wu","Wei Ma","Shenhao Wang","Yunhan Zheng","Zhan Zhao","Jinhua Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.16162v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10079v3","updated":"2024-11-21T17:58:55Z","published":"2024-06-14T14:35:58Z","title":"Localizing Events in Videos with Multimodal Queries","summary":"  Localizing events in videos based on semantic queries is a pivotal task in\nvideo understanding, with the growing significance of user-oriented\napplications like video search. Yet, current research predominantly relies on\nnatural language queries (NLQs), overlooking the potential of using multimodal\nqueries (MQs) that integrate images to more flexibly represent semantic queries\n-- especially when it is difficult to express non-verbal or unfamiliar concepts\nin words. To bridge this gap, we introduce ICQ, a new benchmark designed for\nlocalizing events in videos with MQs, alongside an evaluation dataset\nICQ-Highlight. To accommodate and evaluate existing video localization models\nfor this new task, we propose 3 Multimodal Query Adaptation methods and a novel\nSurrogate Fine-tuning on pseudo-MQs strategy. ICQ systematically benchmarks 12\nstate-of-the-art backbone models, spanning from specialized video localization\nmodels to Video LLMs, across diverse application domains. Our experiments\nhighlight the high potential of MQs in real-world applications. We believe this\nbenchmark is a first step toward advancing MQs in video event localization.\n","authors":["Gengyuan Zhang","Mang Ling Ada Fok","Jialu Ma","Yan Xia","Daniel Cremers","Philip Torr","Volker Tresp","Jindong Gu"],"pdf_url":"https://arxiv.org/pdf/2406.10079v3.pdf","comment":"20 pages (including references and appendix); for the project\n  homepage, see https://icq-benchmark.github.io/"},{"id":"http://arxiv.org/abs/2411.14358v1","updated":"2024-11-21T17:58:07Z","published":"2024-11-21T17:58:07Z","title":"InCrowd-VI: A Realistic Visual-Inertial Dataset for Evaluating SLAM in\n  Indoor Pedestrian-Rich Spaces for Human Navigation","summary":"  Simultaneous localization and mapping (SLAM) techniques can be used to\nnavigate the visually impaired, but the development of robust SLAM solutions\nfor crowded spaces is limited by the lack of realistic datasets. To address\nthis, we introduce InCrowd-VI, a novel visual-inertial dataset specifically\ndesigned for human navigation in indoor pedestrian-rich environments. Recorded\nusing Meta Aria Project glasses, it captures realistic scenarios without\nenvironmental control. InCrowd-VI features 58 sequences totaling a 5 km\ntrajectory length and 1.5 hours of recording time, including RGB, stereo\nimages, and IMU measurements. The dataset captures important challenges such as\npedestrian occlusions, varying crowd densities, complex layouts, and lighting\nchanges. Ground-truth trajectories, accurate to approximately 2 cm, are\nprovided in the dataset, originating from the Meta Aria project machine\nperception SLAM service. In addition, a semi-dense 3D point cloud of scenes is\nprovided for each sequence. The evaluation of state-of-the-art visual odometry\n(VO) and SLAM algorithms on InCrowd-VI revealed severe performance limitations\nin these realistic scenarios, demonstrating the need and value of the new\ndataset to advance SLAM research for visually impaired navigation in complex\nindoor environments.\n","authors":["Marziyeh Bamdad","Hans-Peter Hutter","Alireza Darvishy"],"pdf_url":"https://arxiv.org/pdf/2411.14358v1.pdf","comment":"18 pages, 7 figures, 5 tabels"},{"id":"http://arxiv.org/abs/2411.14354v1","updated":"2024-11-21T17:53:27Z","published":"2024-11-21T17:53:27Z","title":"Contrasting local and global modeling with machine learning and\n  satellite data: A case study estimating tree canopy height in African\n  savannas","summary":"  While advances in machine learning with satellite imagery (SatML) are\nfacilitating environmental monitoring at a global scale, developing SatML\nmodels that are accurate and useful for local regions remains critical to\nunderstanding and acting on an ever-changing planet. As increasing attention\nand resources are being devoted to training SatML models with global data, it\nis important to understand when improvements in global models will make it\neasier to train or fine-tune models that are accurate in specific regions. To\nexplore this question, we contrast local and global training paradigms for\nSatML through a case study of tree canopy height (TCH) mapping in the Karingani\nGame Reserve, Mozambique. We find that recent advances in global TCH mapping do\nnot necessarily translate to better local modeling abilities in our study\nregion. Specifically, small models trained only with locally-collected data\noutperform published global TCH maps, and even outperform globally pretrained\nmodels that we fine-tune using local data. Analyzing these results further, we\nidentify specific points of conflict and synergy between local and global\nmodeling paradigms that can inform future research toward aligning local and\nglobal performance objectives in geospatial machine learning.\n","authors":["Esther Rolf","Lucia Gordon","Milind Tambe","Andrew Davies"],"pdf_url":"https://arxiv.org/pdf/2411.14354v1.pdf","comment":"31 pages; 9 figures"},{"id":"http://arxiv.org/abs/2411.14353v1","updated":"2024-11-21T17:49:15Z","published":"2024-11-21T17:49:15Z","title":"Enhancing Medical Image Segmentation with Deep Learning and Diffusion\n  Models","summary":"  Medical image segmentation is crucial for accurate clinical diagnoses, yet it\nfaces challenges such as low contrast between lesions and normal tissues,\nunclear boundaries, and high variability across patients. Deep learning has\nimproved segmentation accuracy and efficiency, but it still relies heavily on\nexpert annotations and struggles with the complexities of medical images. The\nsmall size of medical image datasets and the high cost of data acquisition\nfurther limit the performance of segmentation networks. Diffusion models, with\ntheir iterative denoising process, offer a promising alternative for better\ndetail capture in segmentation. However, they face difficulties in accurately\nsegmenting small targets and maintaining the precision of boundary details.\nThis article discusses the importance of medical image segmentation, the\nlimitations of current deep learning approaches, and the potential of diffusion\nmodels to address these challenges.\n","authors":["Houze Liu","Tong Zhou","Yanlin Xiang","Aoran Shen","Jiacheng Hu","Junliang Du"],"pdf_url":"https://arxiv.org/pdf/2411.14353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14347v1","updated":"2024-11-21T17:42:20Z","published":"2024-11-21T17:42:20Z","title":"DINO-X: A Unified Vision Model for Open-World Object Detection and\n  Understanding","summary":"  In this paper, we introduce DINO-X, which is a unified object-centric vision\nmodel developed by IDEA Research with the best open-world object detection\nperformance to date. DINO-X employs the same Transformer-based encoder-decoder\narchitecture as Grounding DINO 1.5 to pursue an object-level representation for\nopen-world object understanding. To make long-tailed object detection easy,\nDINO-X extends its input options to support text prompt, visual prompt, and\ncustomized prompt. With such flexible prompt options, we develop a universal\nobject prompt to support prompt-free open-world detection, making it possible\nto detect anything in an image without requiring users to provide any prompt.\nTo enhance the model's core grounding capability, we have constructed a\nlarge-scale dataset with over 100 million high-quality grounding samples,\nreferred to as Grounding-100M, for advancing the model's open-vocabulary\ndetection performance. Pre-training on such a large-scale grounding dataset\nleads to a foundational object-level representation, which enables DINO-X to\nintegrate multiple perception heads to simultaneously support multiple object\nperception and understanding tasks, including detection, segmentation, pose\nestimation, object captioning, object-based QA, etc. Experimental results\ndemonstrate the superior performance of DINO-X. Specifically, the DINO-X Pro\nmodel achieves 56.0 AP, 59.8 AP, and 52.4 AP on the COCO, LVIS-minival, and\nLVIS-val zero-shot object detection benchmarks, respectively. Notably, it\nscores 63.3 AP and 56.5 AP on the rare classes of LVIS-minival and LVIS-val\nbenchmarks, both improving the previous SOTA performance by 5.8 AP. Such a\nresult underscores its significantly improved capacity for recognizing\nlong-tailed objects.\n","authors":["Tianhe Ren","Yihao Chen","Qing Jiang","Zhaoyang Zeng","Yuda Xiong","Wenlong Liu","Zhengyu Ma","Junyi Shen","Yuan Gao","Xiaoke Jiang","Xingyu Chen","Zhuheng Song","Yuhong Zhang","Hongjie Huang","Han Gao","Shilong Liu","Hao Zhang","Feng Li","Kent Yu","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14347v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2411.14345v1","updated":"2024-11-21T17:41:27Z","published":"2024-11-21T17:41:27Z","title":"Layer Pruning with Consensus: A Triple-Win Solution","summary":"  Layer pruning offers a promising alternative to standard structured pruning,\neffectively reducing computational costs, latency, and memory footprint. While\nnotable layer-pruning approaches aim to detect unimportant layers for removal,\nthey often rely on single criteria that may not fully capture the complex,\nunderlying properties of layers. We propose a novel approach that combines\nmultiple similarity metrics into a single expressive measure of low-importance\nlayers, called the Consensus criterion. Our technique delivers a triple-win\nsolution: low accuracy drop, high-performance improvement, and increased\nrobustness to adversarial attacks. With up to 78.80% FLOPs reduction and\nperformance on par with state-of-the-art methods across different benchmarks,\nour approach reduces energy consumption and carbon emissions by up to 66.99%\nand 68.75%, respectively. Additionally, it avoids shortcut learning and\nimproves robustness by up to 4 percentage points under various adversarial\nattacks. Overall, the Consensus criterion demonstrates its effectiveness in\ncreating robust, efficient, and environmentally friendly pruned models.\n","authors":["Leandro Giusti Mugnaini","Carolina Tavares Duarte","Anna H. Reali Costa","Artur Jordao"],"pdf_url":"https://arxiv.org/pdf/2411.14345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14322v1","updated":"2024-11-21T17:12:47Z","published":"2024-11-21T17:12:47Z","title":"SplatR : Experience Goal Visual Rearrangement with 3D Gaussian Splatting\n  and Dense Feature Matching","summary":"  Experience Goal Visual Rearrangement task stands as a foundational challenge\nwithin Embodied AI, requiring an agent to construct a robust world model that\naccurately captures the goal state. The agent uses this world model to restore\na shuffled scene to its original configuration, making an accurate\nrepresentation of the world essential for successfully completing the task. In\nthis work, we present a novel framework that leverages on 3D Gaussian Splatting\nas a 3D scene representation for experience goal visual rearrangement task.\nRecent advances in volumetric scene representation like 3D Gaussian Splatting,\noffer fast rendering of high quality and photo-realistic novel views. Our\napproach enables the agent to have consistent views of the current and the goal\nsetting of the rearrangement task, which enables the agent to directly compare\nthe goal state and the shuffled state of the world in image space. To compare\nthese views, we propose to use a dense feature matching method with visual\nfeatures extracted from a foundation model, leveraging its advantages of a more\nuniversal feature representation, which facilitates robustness, and\ngeneralization. We validate our approach on the AI2-THOR rearrangement\nchallenge benchmark and demonstrate improvements over the current state of the\nart methods\n","authors":["Arjun P S","Andrew Melnik","Gora Chand Nandi"],"pdf_url":"https://arxiv.org/pdf/2411.14322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14295v1","updated":"2024-11-21T16:41:55Z","published":"2024-11-21T16:41:55Z","title":"StereoCrafter-Zero: Zero-Shot Stereo Video Generation with Noisy Restart","summary":"  Generating high-quality stereo videos that mimic human binocular vision\nrequires maintaining consistent depth perception and temporal coherence across\nframes. While diffusion models have advanced image and video synthesis,\ngenerating high-quality stereo videos remains challenging due to the difficulty\nof maintaining consistent temporal and spatial coherence between left and right\nviews. We introduce \\textit{StereoCrafter-Zero}, a novel framework for\nzero-shot stereo video generation that leverages video diffusion priors without\nthe need for paired training data. Key innovations include a noisy restart\nstrategy to initialize stereo-aware latents and an iterative refinement process\nthat progressively harmonizes the latent space, addressing issues like temporal\nflickering and view inconsistencies. Comprehensive evaluations, including\nquantitative metrics and user studies, demonstrate that\n\\textit{StereoCrafter-Zero} produces high-quality stereo videos with improved\ndepth consistency and temporal smoothness, even when depth estimations are\nimperfect. Our framework is robust and adaptable across various diffusion\nmodels, setting a new benchmark for zero-shot stereo video generation and\nenabling more immersive visual experiences. Our code can be found\nin~\\url{https://github.com/shijianjian/StereoCrafter-Zero}.\n","authors":["Jian Shi","Qian Wang","Zhenyu Li","Peter Wonka"],"pdf_url":"https://arxiv.org/pdf/2411.14295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13211v2","updated":"2024-11-21T16:37:32Z","published":"2024-11-20T11:19:22Z","title":"ViSTa Dataset: Do vision-language models understand sequential tasks?","summary":"  Using vision-language models (VLMs) as reward models in reinforcement\nlearning holds promise for reducing costs and improving safety. So far, VLM\nreward models have only been used for goal-oriented tasks, where the agent must\nreach a particular final outcome. We explore VLMs' potential to supervise tasks\nthat cannot be scored by the final state alone. To this end, we introduce\nViSTa, a dataset for evaluating Vision-based understanding of Sequential Tasks.\nViSTa comprises over 4,000 videos with step-by-step descriptions in virtual\nhome, Minecraft, and real-world environments. Its novel hierarchical structure\n-- basic single-step tasks composed into more and more complex sequential tasks\n-- allows a fine-grained understanding of how well VLMs can judge tasks with\nvarying complexity. To illustrate this, we use ViSTa to evaluate\nstate-of-the-art VLMs, including CLIP, ViCLIP, and GPT-4o. We find that, while\nthey are all good at object recognition, they fail to understand sequential\ntasks, with only GPT-4o achieving non-trivial performance.\n","authors":["Evžen Wybitul","Evan Ryan Gunter","Mikhail Seleznyov","David Lindner"],"pdf_url":"https://arxiv.org/pdf/2411.13211v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14280v1","updated":"2024-11-21T16:33:35Z","published":"2024-11-21T16:33:35Z","title":"EasyHOI: Unleashing the Power of Large Models for Reconstructing\n  Hand-Object Interactions in the Wild","summary":"  Our work aims to reconstruct hand-object interactions from a single-view\nimage, which is a fundamental but ill-posed task. Unlike methods that\nreconstruct from videos, multi-view images, or predefined 3D templates,\nsingle-view reconstruction faces significant challenges due to inherent\nambiguities and occlusions. These challenges are further amplified by the\ndiverse nature of hand poses and the vast variety of object shapes and sizes.\nOur key insight is that current foundational models for segmentation,\ninpainting, and 3D reconstruction robustly generalize to in-the-wild images,\nwhich could provide strong visual and geometric priors for reconstructing\nhand-object interactions. Specifically, given a single image, we first design a\nnovel pipeline to estimate the underlying hand pose and object shape using\noff-the-shelf large models. Furthermore, with the initial reconstruction, we\nemploy a prior-guided optimization scheme, which optimizes hand pose to comply\nwith 3D physical constraints and the 2D input image content. We perform\nexperiments across several datasets and show that our method consistently\noutperforms baselines and faithfully reconstructs a diverse set of hand-object\ninteractions. Here is the link of our project page:\nhttps://lym29.github.io/EasyHOI-page/\n","authors":["Yumeng Liu","Xiaoxiao Long","Zemin Yang","Yuan Liu","Marc Habermann","Christian Theobalt","Yuexin Ma","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2411.14280v1.pdf","comment":"Project page: https://lym29.github.io/EasyHOI-page/"},{"id":"http://arxiv.org/abs/2411.14279v1","updated":"2024-11-21T16:33:30Z","published":"2024-11-21T16:33:30Z","title":"Looking Beyond Text: Reducing Language bias in Large Vision-Language\n  Models via Multimodal Dual-Attention and Soft-Image Guidance","summary":"  Large vision-language models (LVLMs) have achieved impressive results in\nvarious vision-language tasks. However, despite showing promising performance,\nLVLMs suffer from hallucinations caused by language bias, leading to diminished\nfocus on images and ineffective visual comprehension. We identify two primary\nreasons for this bias: 1. Different scales of training data between the\npretraining stage of LLM and multimodal alignment stage. 2. The learned\ninference bias due to short-term dependency of text data. Therefore, we propose\nLACING, a systemic framework designed to address the language bias of LVLMs\nwith muLtimodal duAl-attention meChanIsm (MDA) aNd soft-image Guidance (IFG).\nSpecifically, MDA introduces a parallel dual-attention mechanism that enhances\nthe integration of visual inputs across the model. IFG introduces a learnable\nsoft visual prompt during training and inference to replace visual inputs,\ndesigned to compel LVLMs to prioritize text inputs. Then, IFG further proposes\na novel decoding strategy using the soft visual prompt to mitigate the model's\nover-reliance on adjacent text inputs. Comprehensive experiments demonstrate\nthat our method effectively debiases LVLMs from their language bias, enhancing\nvisual comprehension and reducing hallucinations without requiring additional\ntraining resources or data. The code and model are available at\n[lacing-lvlm.github.io](https://lacing-lvlm.github.io).\n","authors":["Haozhe Zhao","Shuzheng Si","Liang Chen","Yichi Zhang","Maosong Sun","Mingjia Zhang","Baobao Chang"],"pdf_url":"https://arxiv.org/pdf/2411.14279v1.pdf","comment":"19 pages, 12 figures"},{"id":"http://arxiv.org/abs/2302.09682v2","updated":"2024-11-21T16:29:08Z","published":"2023-02-19T22:26:25Z","title":"Dual Attention Model with Reinforcement Learning for Classification of\n  Histology Whole-Slide Images","summary":"  Digital whole slide images (WSIs) are generally captured at microscopic\nresolution and encompass extensive spatial data. Directly feeding these images\nto deep learning models is computationally intractable due to memory\nconstraints, while downsampling the WSIs risks incurring information loss.\nAlternatively, splitting the WSIs into smaller patches may result in a loss of\nimportant contextual information. In this paper, we propose a novel dual\nattention approach, consisting of two main components, both inspired by the\nvisual examination process of a pathologist: The first soft attention model\nprocesses a low magnification view of the WSI to identify relevant regions of\ninterest, followed by a custom sampling method to extract diverse and spatially\ndistinct image tiles from the selected ROIs. The second component, the hard\nattention classification model further extracts a sequence of multi-resolution\nglimpses from each tile for classification. Since hard attention is\nnon-differentiable, we train this component using reinforcement learning to\npredict the location of the glimpses. This approach allows the model to focus\non essential regions instead of processing the entire tile, thereby aligning\nwith a pathologist's way of diagnosis. The two components are trained in an\nend-to-end fashion using a joint loss function to demonstrate the efficacy of\nthe model. The proposed model was evaluated on two WSI-level classification\nproblems: Human epidermal growth factor receptor 2 scoring on breast cancer\nhistology images and prediction of Intact/Loss status of two Mismatch Repair\nbiomarkers from colorectal cancer histology images. We show that the proposed\nmodel achieves performance better than or comparable to the state-of-the-art\nmethods while processing less than 10% of the WSI at the highest magnification\nand reducing the time required to infer the WSI-level label by more than 75%.\n","authors":["Manahil Raza","Ruqayya Awan","Raja Muhammad Saad Bashir","Talha Qaiser","Nasir M. Rajpoot"],"pdf_url":"https://arxiv.org/pdf/2302.09682v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14269v1","updated":"2024-11-21T16:25:56Z","published":"2024-11-21T16:25:56Z","title":"Guided MRI Reconstruction via Schrödinger Bridge","summary":"  Magnetic Resonance Imaging (MRI) is a multi-contrast imaging technique in\nwhich different contrast images share similar structural information. However,\nconventional diffusion models struggle to effectively leverage this structural\nsimilarity. Recently, the Schr\\\"odinger Bridge (SB), a nonlinear extension of\nthe diffusion model, has been proposed to establish diffusion paths between any\ndistributions, allowing the incorporation of guided priors. This study proposes\nan SB-based, multi-contrast image-guided reconstruction framework that\nestablishes a diffusion bridge between the guiding and target image\ndistributions. By using the guiding image along with data consistency during\nsampling, the target image is reconstructed more accurately. To better address\nstructural differences between images, we introduce an inversion strategy from\nthe field of image editing, termed $\\mathbf{I}^2$SB-inversion. Experiments on a\nparied T1 and T2-FLAIR datasets demonstrate that $\\mathbf{I}^2$SB-inversion\nachieve a high acceleration up to 14.4 and outperforms existing methods in\nterms of both reconstruction accuracy and stability.\n","authors":["Yue Wang","Tian Zhou","Zhuo-xu Cui","Bingsheng Huang","Hairong Zheng","Dong Liang","Yanjie Zhu"],"pdf_url":"https://arxiv.org/pdf/2411.14269v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.00090v3","updated":"2024-11-21T16:21:07Z","published":"2023-07-31T19:03:13Z","title":"VG-SSL: Benchmarking Self-supervised Representation Learning Approaches\n  for Visual Geo-localization","summary":"  Visual Geo-localization (VG) is a critical research area for identifying\ngeo-locations from visual inputs, particularly in autonomous navigation for\nrobotics and vehicles. Current VG methods often learn feature extractors from\ngeo-labeled images to create dense, geographically relevant representations.\nRecent advances in Self-Supervised Learning (SSL) have demonstrated its\ncapability to achieve performance on par with supervised techniques with\nunlabeled images. This study presents a novel VG-SSL framework, designed for\nversatile integration and benchmarking of diverse SSL methods for\nrepresentation learning in VG, featuring a unique geo-related pair strategy,\nGeoPair. Through extensive performance analysis, we adapt SSL techniques to\nimprove VG on datasets from hand-held and car-mounted cameras used in robotics\nand autonomous vehicles. Our results show that contrastive learning and\ninformation maximization methods yield superior geo-specific representation\nquality, matching or surpassing the performance of state-of-the-art VG\ntechniques. To our knowledge, This is the first benchmarking study of SSL in\nVG, highlighting its potential in enhancing geo-specific visual representations\nfor robotics and autonomous vehicles. The code is publicly available at\nhttps://github.com/arplaboratory/VG-SSL.\n","authors":["Jiuhong Xiao","Gao Zhu","Giuseppe Loianno"],"pdf_url":"https://arxiv.org/pdf/2308.00090v3.pdf","comment":"18 pages (including appendix, references), 7 figures, 7 tables.\n  Accepted for WACV 2025"},{"id":"http://arxiv.org/abs/2307.11957v6","updated":"2024-11-21T16:13:48Z","published":"2023-07-22T01:56:58Z","title":"High-performance real-world optical computing trained by in situ\n  gradient-based model-free optimization","summary":"  Optical computing systems provide high-speed and low-energy data processing\nbut face deficiencies in computationally demanding training and\nsimulation-to-reality gaps. We propose a gradient-based model-free optimization\n(G-MFO) method based on a Monte Carlo gradient estimation algorithm for\ncomputationally efficient in situ training of optical computing systems. This\napproach treats an optical computing system as a black box and back-propagates\nthe loss directly to the optical computing weights' probability distributions,\ncircumventing the need for a computationally heavy and biased system\nsimulation. Our experiments on diffractive optical computing systems show that\nG-MFO outperforms hybrid training on the MNIST and FMNIST datasets.\nFurthermore, we demonstrate image-free and high-speed classification of cells\nfrom their marker-free phase maps. Our method's model-free and high-performance\nnature, combined with its low demand for computational resources, paves the way\nfor accelerating the transition of optical computing from laboratory\ndemonstrations to practical, real-world applications.\n","authors":["Guangyuan Zhao","Xin Shu","Renjie Zhou"],"pdf_url":"https://arxiv.org/pdf/2307.11957v6.pdf","comment":"The paper titled \"High-performance real-world optical computing\n  trained by in situ gradient-based model-free optimization\" has been accepted\n  at ICCP&TPAMI 2024. For more details, please visit the [project\n  page](https://shuxin626.github.io/mfo_optical_computing/index.html)"},{"id":"http://arxiv.org/abs/2411.14250v1","updated":"2024-11-21T15:56:30Z","published":"2024-11-21T15:56:30Z","title":"CP-UNet: Contour-based Probabilistic Model for Medical Ultrasound Images\n  Segmentation","summary":"  Deep learning-based segmentation methods are widely utilized for detecting\nlesions in ultrasound images. Throughout the imaging procedure, the attenuation\nand scattering of ultrasound waves cause contour blurring and the formation of\nartifacts, limiting the clarity of the acquired ultrasound images. To overcome\nthis challenge, we propose a contour-based probabilistic segmentation model\nCP-UNet, which guides the segmentation network to enhance its focus on contour\nduring decoding. We design a novel down-sampling module to enable the contour\nprobability distribution modeling and encoding stages to acquire global-local\nfeatures. Furthermore, the Gaussian Mixture Model utilizes optimized features\nto model the contour distribution, capturing the uncertainty of lesion\nboundaries. Extensive experiments with several state-of-the-art deep learning\nsegmentation methods on three ultrasound image datasets show that our method\nperforms better on breast and thyroid lesions segmentation.\n","authors":["Ruiguo Yu","Yiyang Zhang","Yuan Tian","Zhiqiang Liu","Xuewei Li","Jie Gao"],"pdf_url":"https://arxiv.org/pdf/2411.14250v1.pdf","comment":"4 pages, 4 figures, 2 tables;For icassp2025"},{"id":"http://arxiv.org/abs/2411.14243v1","updated":"2024-11-21T15:50:59Z","published":"2024-11-21T15:50:59Z","title":"AnywhereDoor: Multi-Target Backdoor Attacks on Object Detection","summary":"  As object detection becomes integral to many safety-critical applications,\nunderstanding its vulnerabilities is essential. Backdoor attacks, in\nparticular, pose a significant threat by implanting hidden backdoor in a victim\nmodel, which adversaries can later exploit to trigger malicious behaviors\nduring inference. However, current backdoor techniques are limited to static\nscenarios where attackers must define a malicious objective before training,\nlocking the attack into a predetermined action without inference-time\nadaptability. Given the expressive output space in object detection, including\nobject existence detection, bounding box estimation, and object classification,\nthe feasibility of implanting a backdoor that provides inference-time control\nwith a high degree of freedom remains unexplored. This paper introduces\nAnywhereDoor, a flexible backdoor attack tailored for object detection. Once\nimplanted, AnywhereDoor enables adversaries to specify different attack types\n(object vanishing, fabrication, or misclassification) and configurations\n(untargeted or targeted with specific classes) to dynamically control detection\nbehavior. This flexibility is achieved through three key innovations: (i)\nobjective disentanglement to support a broader range of attack combinations\nwell beyond what existing methods allow; (ii) trigger mosaicking to ensure\nbackdoor activations are robust, even against those object detectors that\nextract localized regions from the input image for recognition; and (iii)\nstrategic batching to address object-level data imbalances that otherwise\nhinders a balanced manipulation. Extensive experiments demonstrate that\nAnywhereDoor provides attackers with a high degree of control, achieving an\nattack success rate improvement of nearly 80% compared to adaptations of\nexisting methods for such flexible control.\n","authors":["Jialin Lu","Junjie Shan","Ziqi Zhao","Ka-Ho Chow"],"pdf_url":"https://arxiv.org/pdf/2411.14243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14228v1","updated":"2024-11-21T15:37:52Z","published":"2024-11-21T15:37:52Z","title":"FocusLLaVA: A Coarse-to-Fine Approach for Efficient and Effective Visual\n  Token Compression","summary":"  Recent advances on Multi-modal Large Language Models have demonstrated that\nhigh-resolution image input is crucial for model capabilities, especially for\nfine-grained tasks. However, high-resolution images lead to a quadratic\nincrease in the number of visual tokens input into LLMs, resulting in\nsignificant computational costs. Current work develop visual token compression\nmethods to achieve efficiency improvements, often at the expense of\nperformance. We argue that removing visual redundancy can simultaneously\nimprove both efficiency and performance. We build a coarse-to-fine visual token\ncompression method, with a vision-guided sampler for compressing redundant\nregions with low information density, and a text-guided sampler for selecting\nvisual tokens that are strongly correlated with the user instructions.With\nthese two modules, the proposed FocusLLaVA achieves improvements in both\nefficiency and performance. We validate the effectiveness of our approach on a\nwide range of evaluation datasets.\n","authors":["Yuke Zhu","Chi Xie","Shuang Liang","Bo Zheng","Sheng Guo"],"pdf_url":"https://arxiv.org/pdf/2411.14228v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02124v2","updated":"2024-11-21T15:33:17Z","published":"2023-12-04T18:51:44Z","title":"VerA: Versatile Anonymization Applicable to Clinical Facial Photographs","summary":"  The demand for privacy in facial image dissemination is gaining ground\ninternationally, echoed by the proliferation of regulations such as GDPR,\nDPDPA, CCPA, PIPL, and APPI. While recent advances in anonymization surpass\npixelation or blur methods, additional constraints to the task pose challenges.\nLargely unaddressed by current anonymization methods are clinical images and\npairs of before-and-after clinical images illustrating facial medical\ninterventions, e.g., facial surgeries or dental procedures. We present VerA,\nthe first Versatile Anonymization framework that solves two challenges in\nclinical applications: A) it preserves selected semantic areas (e.g., mouth\nregion) to show medical intervention results, that is, anonymization is only\napplied to the areas outside the preserved area; and B) it produces anonymized\nimages with consistent personal identity across multiple photographs, which is\ncrucial for anonymizing photographs of the same person taken before and after a\nclinical intervention. We validate our results on both single and paired\nanonymization of clinical images through extensive quantitative and qualitative\nevaluation. We also demonstrate that VerA reaches the state of the art on\nestablished anonymization tasks, in terms of photorealism and\nde-identification.\n","authors":["Majed El Helou","Doruk Cetin","Petar Stamenkovic","Niko Benjamin Huber","Fabio Zünd"],"pdf_url":"https://arxiv.org/pdf/2312.02124v2.pdf","comment":"accepted to WACV 2025"},{"id":"http://arxiv.org/abs/2411.14219v1","updated":"2024-11-21T15:28:52Z","published":"2024-11-21T15:28:52Z","title":"Towards Context-Rich Automated Biodiversity Assessments: Deriving\n  AI-Powered Insights from Camera Trap Data","summary":"  Camera traps offer enormous new opportunities in ecological studies, but\ncurrent automated image analysis methods often lack the contextual richness\nneeded to support impactful conservation outcomes. Here we present an\nintegrated approach that combines deep learning-based vision and language\nmodels to improve ecological reporting using data from camera traps. We\nintroduce a two-stage system: YOLOv10-X to localise and classify species\n(mammals and birds) within images, and a Phi-3.5-vision-instruct model to read\nYOLOv10-X binding box labels to identify species, overcoming its limitation\nwith hard to classify objects in images. Additionally, Phi-3.5 detects broader\nvariables, such as vegetation type, and time of day, providing rich ecological\nand environmental context to YOLO's species detection output. When combined,\nthis output is processed by the model's natural language system to answer\ncomplex queries, and retrieval-augmented generation (RAG) is employed to enrich\nresponses with external information, like species weight and IUCN status\n(information that cannot be obtained through direct visual analysis). This\ninformation is used to automatically generate structured reports, providing\nbiodiversity stakeholders with deeper insights into, for example, species\nabundance, distribution, animal behaviour, and habitat selection. Our approach\ndelivers contextually rich narratives that aid in wildlife management\ndecisions. By providing contextually rich insights, our approach not only\nreduces manual effort but also supports timely decision-making in conservation,\npotentially shifting efforts from reactive to proactive management.\n","authors":["Paul Fergus","Carl Chalmers","Naomi Matthews","Stuart Nixon","Andre Burger","Oliver Hartley","Chris Sutherland","Xavier Lambin","Steven Longmore","Serge Wich"],"pdf_url":"https://arxiv.org/pdf/2411.14219v1.pdf","comment":"32 Pages, 22 images"},{"id":"http://arxiv.org/abs/2411.14213v1","updated":"2024-11-21T15:24:16Z","published":"2024-11-21T15:24:16Z","title":"Generative Outpainting To Enhance the Memorability of Short-Form Videos","summary":"  With the expanding use of the short-form video format in advertising, social\nmedia, entertainment, education and more, there is a need for such media to\nboth captivate and be remembered. Video memorability indicates to us how likely\na video is to be remembered by a viewer who has no emotional or personal\nconnection with its content. This paper presents the results of using\ngenerative outpainting to expand the screen size of a short-form video with a\nview to improving its memorability. Advances in machine learning and deep\nlearning are compared and leveraged to understand how extending the borders of\nvideo screensizes can affect their memorability to viewers. Using quantitative\nevaluation we determine the best-performing model for outpainting and the\nimpact of outpainting based on image saliency on video memorability scores\n","authors":["Alan Byju","Aman Sudhindra Ladwa","Lorin Sweeney","Alan F. Smeaton"],"pdf_url":"https://arxiv.org/pdf/2411.14213v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.08812v2","updated":"2024-11-21T15:22:23Z","published":"2023-08-17T06:48:55Z","title":"A Fusion of Variational Distribution Priors and Saliency Map Replay for\n  Continual 3D Reconstruction","summary":"  Single-image 3D reconstruction is a research challenge focused on predicting\n3D object shapes from single-view images. This task requires significant data\nacquisition to predict both visible and occluded portions of the shape.\nFurthermore, learning-based methods face the difficulty of creating a\ncomprehensive training dataset for all possible classes. To this end, we\npropose a continual learning-based 3D reconstruction method where our goal is\nto design a model using Variational Priors that can still reconstruct the\npreviously seen classes reasonably even after training on new classes.\nVariational Priors represent abstract shapes and combat forgetting, whereas\nsaliency maps preserve object attributes with less memory usage. This is vital\ndue to resource constraints in storing extensive training data. Additionally,\nwe introduce saliency map-based experience replay to capture global and\ndistinct object features. Thorough experiments show competitive results\ncompared to established methods, both quantitatively and qualitatively.\n","authors":["Sanchar Palit","Sandika Biswas"],"pdf_url":"https://arxiv.org/pdf/2308.08812v2.pdf","comment":"at ICVGIP 2024"},{"id":"http://arxiv.org/abs/2411.14208v1","updated":"2024-11-21T15:16:48Z","published":"2024-11-21T15:16:48Z","title":"Novel View Extrapolation with Video Diffusion Priors","summary":"  The field of novel view synthesis has made significant strides thanks to the\ndevelopment of radiance field methods. However, most radiance field techniques\nare far better at novel view interpolation than novel view extrapolation where\nthe synthesis novel views are far beyond the observed training views. We design\nViewExtrapolator, a novel view synthesis approach that leverages the generative\npriors of Stable Video Diffusion (SVD) for realistic novel view extrapolation.\nBy redesigning the SVD denoising process, ViewExtrapolator refines the\nartifact-prone views rendered by radiance fields, greatly enhancing the clarity\nand realism of the synthesized novel views. ViewExtrapolator is a generic novel\nview extrapolator that can work with different types of 3D rendering such as\nviews rendered from point clouds when only a single view or monocular video is\navailable. Additionally, ViewExtrapolator requires no fine-tuning of SVD,\nmaking it both data-efficient and computation-efficient. Extensive experiments\ndemonstrate the superiority of ViewExtrapolator in novel view extrapolation.\nProject page: \\url{https://kunhao-liu.github.io/ViewExtrapolator/}.\n","authors":["Kunhao Liu","Ling Shao","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2411.14208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13721v3","updated":"2024-11-21T15:14:29Z","published":"2024-01-24T14:55:02Z","title":"Uncertainty-Guided Alignment for Unsupervised Domain Adaptation in\n  Regression","summary":"  Unsupervised Domain Adaptation for Regression (UDAR) aims to adapt models\nfrom a labeled source domain to an unlabeled target domain for regression\ntasks. Traditional feature alignment methods, successful in classification,\noften prove ineffective for regression due to the correlated nature of\nregression features. To address this challenge, we propose Uncertainty-Guided\nAlignment (UGA), a novel method that integrates predictive uncertainty into the\nfeature alignment process. UGA employs Evidential Deep Learning to predict both\ntarget values and their associated uncertainties. This uncertainty information\nguides the alignment process and fuses information within the embedding space,\neffectively mitigating issues such as feature collapse in out-of-distribution\nscenarios. We evaluate UGA on two computer vision benchmarks and a real-world\nbattery state-of-charge prediction across different manufacturers and operating\ntemperatures. Across 52 transfer tasks, UGA on average outperforms existing\nstate-of-the-art methods. Our approach not only improves adaptation performance\nbut also provides well-calibrated uncertainty estimates.\n","authors":["Ismail Nejjar","Gaetan Frusque","Florent Forest","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2401.13721v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14205v1","updated":"2024-11-21T15:13:38Z","published":"2024-11-21T15:13:38Z","title":"Is this Generated Person Existed in Real-world? Fine-grained Detecting\n  and Calibrating Abnormal Human-body","summary":"  Recent improvements in visual synthesis have significantly enhanced the\ndepiction of generated human photos, which are pivotal due to their wide\napplicability and demand. Nonetheless, the existing text-to-image or\ntext-to-video models often generate low-quality human photos that might differ\nconsiderably from real-world body structures, referred to as \"abnormal human\nbodies\". Such abnormalities, typically deemed unacceptable, pose considerable\nchallenges in the detection and repair of them within human photos. These\nchallenges require precise abnormality recognition capabilities, which entail\npinpointing both the location and the abnormality type. Intuitively, Visual\nLanguage Models (VLMs) that have obtained remarkable performance on various\nvisual tasks are quite suitable for this task. However, their performance on\nabnormality detection in human photos is quite poor. Hence, it is quite\nimportant to highlight this task for the research community. In this paper, we\nfirst introduce a simple yet challenging task, i.e., \\textbf{F}ine-grained\n\\textbf{H}uman-body \\textbf{A}bnormality \\textbf{D}etection \\textbf{(FHAD)},\nand construct two high-quality datasets for evaluation. Then, we propose a\nmeticulous framework, named HumanCalibrator, which identifies and repairs\nabnormalities in human body structures while preserving the other content.\nExperiments indicate that our HumanCalibrator achieves high accuracy in\nabnormality detection and accomplishes an increase in visual comparisons while\npreserving the other visual content.\n","authors":["Zeqing Wang","Qingyang Ma","Wentao Wan","Haojie Li","Keze Wang","Yonghong Tian"],"pdf_url":"https://arxiv.org/pdf/2411.14205v1.pdf","comment":"16 pages, 14 figures"},{"id":"http://arxiv.org/abs/2405.17141v2","updated":"2024-11-21T15:12:48Z","published":"2024-05-27T13:01:25Z","title":"MVMS-RCN: A Dual-Domain Unfolding CT Reconstruction with\n  Multi-sparse-view and Multi-scale Refinement-correction","summary":"  X-ray Computed Tomography (CT) is one of the most important diagnostic\nimaging techniques in clinical applications. Sparse-view CT imaging reduces the\nnumber of projection views to a lower radiation dose and alleviates the\npotential risk of radiation exposure. Most existing deep learning (DL) and deep\nunfolding sparse-view CT reconstruction methods: 1) do not fully use the\nprojection data; 2) do not always link their architecture designs to a\nmathematical theory; 3) do not flexibly deal with multi-sparse-view\nreconstruction assignments. This paper aims to use mathematical ideas and\ndesign optimal DL imaging algorithms for sparse-view tomography\nreconstructions. We propose a novel dual-domain deep unfolding unified\nframework that offers a great deal of flexibility for multi-sparse-view CT\nreconstruction with different sampling views through a single model. This\nframework combines the theoretical advantages of model-based methods with the\nsuperior reconstruction performance of DL-based methods, resulting in the\nexpected generalizability of DL. We propose a refinement module that utilizes\nunfolding projection domain to refine full-sparse-view projection errors, as\nwell as an image domain correction module that distills multi-scale geometric\nerror corrections to reconstruct sparse-view CT. This provides us with a new\nway to explore the potential of projection information and a new perspective on\ndesigning network architectures. All parameters of our proposed framework are\nlearnable end to end, and our method possesses the potential to be applied to\nplug-and-play reconstruction. Extensive experiments demonstrate that our\nframework is superior to other existing state-of-the-art methods. Our source\ncodes are available at https://github.com/fanxiaohong/MVMS-RCN.\n","authors":["Xiaohong Fan","Ke Chen","Huaming Yi","Yin Yang","Jianping Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.17141v2.pdf","comment":"14 pages, Accepted to IEEE Transactions on Computational Imaging,\n  2024"},{"id":"http://arxiv.org/abs/2411.14202v1","updated":"2024-11-21T15:11:02Z","published":"2024-11-21T15:11:02Z","title":"Revised Regularization for Efficient Continual Learning through\n  Correlation-Based Parameter Update in Bayesian Neural Networks","summary":"  We propose a Bayesian neural network-based continual learning algorithm using\nVariational Inference, aiming to overcome several drawbacks of existing\nmethods. Specifically, in continual learning scenarios, storing network\nparameters at each step to retain knowledge poses challenges. This is\ncompounded by the crucial need to mitigate catastrophic forgetting,\nparticularly given the limited access to past datasets, which complicates\nmaintaining correspondence between network parameters and datasets across all\nsessions. Current methods using Variational Inference with KL divergence risk\ncatastrophic forgetting during uncertain node updates and coupled disruptions\nin certain nodes. To address these challenges, we propose the following\nstrategies. To reduce the storage of the dense layer parameters, we propose a\nparameter distribution learning method that significantly reduces the storage\nrequirements. In the continual learning framework employing variational\ninference, our study introduces a regularization term that specifically targets\nthe dynamics and population of the mean and variance of the parameters. This\nterm aims to retain the benefits of KL divergence while addressing related\nchallenges. To ensure proper correspondence between network parameters and the\ndata, our method introduces an importance-weighted Evidence Lower Bound term to\ncapture data and parameter correlations. This enables storage of common and\ndistinctive parameter hyperspace bases. The proposed method partitions the\nparameter space into common and distinctive subspaces, with conditions for\neffective backward and forward knowledge transfer, elucidating the\nnetwork-parameter dataset correspondence. The experimental results demonstrate\nthe effectiveness of our method across diverse datasets and various\ncombinations of sequential datasets, yielding superior performance compared to\nexisting approaches.\n","authors":["Sanchar Palit","Biplab Banerjee","Subhasis Chaudhuri"],"pdf_url":"https://arxiv.org/pdf/2411.14202v1.pdf","comment":"at ICVGIP 2024"},{"id":"http://arxiv.org/abs/2411.14201v1","updated":"2024-11-21T15:10:44Z","published":"2024-11-21T15:10:44Z","title":"Regional Attention for Shadow Removal","summary":"  Shadow, as a natural consequence of light interacting with objects, plays a\ncrucial role in shaping the aesthetics of an image, which however also impairs\nthe content visibility and overall visual quality. Recent shadow removal\napproaches employ the mechanism of attention, due to its effectiveness, as a\nkey component. However, they often suffer from two issues including large model\nsize and high computational complexity for practical use. To address these\nshortcomings, this work devises a lightweight yet accurate shadow removal\nframework. First, we analyze the characteristics of the shadow removal task to\nseek the key information required for reconstructing shadow regions and\ndesigning a novel regional attention mechanism to effectively capture such\ninformation. Then, we customize a Regional Attention Shadow Removal Model\n(RASM, in short), which leverages non-shadow areas to assist in restoring\nshadow ones. Unlike existing attention-based models, our regional attention\nstrategy allows each shadow region to interact more rationally with its\nsurrounding non-shadow areas, for seeking the regional contextual correlation\nbetween shadow and non-shadow areas. Extensive experiments are conducted to\ndemonstrate that our proposed method delivers superior performance over other\nstate-of-the-art models in terms of accuracy and efficiency, making it\nappealing for practical applications.\n","authors":["Hengxing Liu","Mingjia Li","Xiaojie Guo"],"pdf_url":"https://arxiv.org/pdf/2411.14201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19797v4","updated":"2024-11-21T15:07:10Z","published":"2024-03-28T19:25:25Z","title":"Efficient 3D Instance Mapping and Localization with Neural Fields","summary":"  We tackle the problem of learning an implicit scene representation for 3D\ninstance segmentation from a sequence of posed RGB images. Towards this, we\nintroduce 3DIML, a novel framework that efficiently learns a neural label field\nwhich can render 3D instance segmentation masks from novel viewpoints. Opposed\nto prior art that optimizes a neural field in a self-supervised manner,\nrequiring complicated training procedures and loss function design, 3DIML\nleverages a two-phase process. The first phase, InstanceMap, takes as input 2D\nsegmentation masks of the image sequence generated by a frontend instance\nsegmentation model, and associates corresponding masks across images to 3D\nlabels. These almost 3D-consistent pseudolabel masks are then used in the\nsecond phase, InstanceLift, to supervise the training of a neural label field,\nwhich interpolates regions missed by InstanceMap and resolves ambiguities.\nAdditionally, we introduce InstanceLoc, which enables near realtime\nlocalization of instance masks given a trained neural label field. We evaluate\n3DIML on sequences from the Replica and ScanNet datasets and demonstrate its\neffectiveness under mild assumptions for the image sequences. We achieve a\nlarge practical speedup over existing implicit scene representation methods\nwith comparable quality, showcasing its potential to facilitate faster and more\neffective 3D scene understanding.\n","authors":["George Tang","Krishna Murthy Jatavallabhula","Antonio Torralba"],"pdf_url":"https://arxiv.org/pdf/2403.19797v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14193v1","updated":"2024-11-21T15:02:41Z","published":"2024-11-21T15:02:41Z","title":"ComfyGI: Automatic Improvement of Image Generation Workflows","summary":"  Automatic image generation is no longer just of interest to researchers, but\nalso to practitioners. However, current models are sensitive to the settings\nused and automatic optimization methods often require human involvement. To\nbridge this gap, we introduce ComfyGI, a novel approach to automatically\nimprove workflows for image generation without the need for human intervention\ndriven by techniques from genetic improvement. This enables image generation\nwith significantly higher quality in terms of the alignment with the given\ndescription and the perceived aesthetics. On the performance side, we find that\noverall, the images generated with an optimized workflow are about 50% better\ncompared to the initial workflow in terms of the median ImageReward score.\nThese already good results are even surpassed in our human evaluation, as the\nparticipants preferred the images improved by ComfyGI in around 90% of the\ncases.\n","authors":["Dominik Sobania","Martin Briesch","Franz Rothlauf"],"pdf_url":"https://arxiv.org/pdf/2411.14193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14184v1","updated":"2024-11-21T14:53:59Z","published":"2024-11-21T14:53:59Z","title":"Deep Learning Approach for Enhancing Oral Squamous Cell Carcinoma with\n  LIME Explainable AI Technique","summary":"  The goal of the present study is to analyze an application of deep learning\nmodels in order to augment the diagnostic performance of oral squamous cell\ncarcinoma (OSCC) with a longitudinal cohort study using the Histopathological\nImaging Database for oral cancer analysis. The dataset consisted of 5192 images\n(2435 Normal and 2511 OSCC), which were allocated between training, testing,\nand validation sets with an estimated ratio repartition of about 52% for the\nOSCC group, and still, our performance measure was validated on a combination\nset that contains almost equal number of sample in this use case as entire\ndatabase have been divided into half using stratified splitting technique based\nagain near binary proportion but total distribution was around even. We\nselected four deep-learning architectures for evaluation in the present study:\nResNet101, DenseNet121, VGG16, and EfficientnetB3. EfficientNetB3 was found to\nbe the best, with an accuracy of 98.33% and F1 score (0.9844), and it took\nremarkably less computing power in comparison with other models. The subsequent\none was DenseNet121, with 90.24% accuracy and an F1 score of 90.45%. Moreover,\nwe employed the Local Interpretable Model-agnostic Explanations (LIME) method\nto clarify why EfficientNetB3 made certain decisions with its predictions to\nimprove the explainability and trustworthiness of results. This work provides\nevidence for the possible superior diagnosis in OSCC activated from the\nEfficientNetB3 model with the explanation of AI techniques such as LIME and\npaves an important groundwork to build on towards clinical usage.\n","authors":["Samiha Islam","Muhammad Zawad Mahmud","Shahran Rahman Alve","Md. Mejbah Ullah Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2411.14184v1.pdf","comment":"Under Review at an IEEE conference"},{"id":"http://arxiv.org/abs/2411.14179v1","updated":"2024-11-21T14:40:49Z","published":"2024-11-21T14:40:49Z","title":"CompetitorFormer: Competitor Transformer for 3D Instance Segmentation","summary":"  Transformer-based methods have become the dominant approach for 3D instance\nsegmentation. These methods predict instance masks via instance queries,\nranking them by classification confidence and IoU scores to select the top\nprediction as the final outcome. However, it has been observed that the current\nmodels employ a fixed and higher number of queries than the instances present\nwithin a scene. In such instances, multiple queries predict the same instance,\nyet only a single query is ultimately optimized. The close scores of queries in\nthe lower-level decoders make it challenging for the dominant query to\ndistinguish itself rapidly, which ultimately impairs the model's accuracy and\nconvergence efficiency. This phenomenon is referred to as inter-query\ncompetition. To address this challenge, we put forth a series of plug-and-play\ncompetition-oriented designs, collectively designated as the CompetitorFormer,\nwith the aim of reducing competition and facilitating a dominant query.\nExperiments showed that integrating our designs with state-of-the-art\nframeworks consistently resulted in significant performance improvements in 3D\ninstance segmentation across a range of datasets.\n","authors":["Duanchu Wang","Jing Liu","Haoran Gong","Yinghui Quan","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2411.14179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10072v2","updated":"2024-11-21T14:37:25Z","published":"2024-08-19T15:15:20Z","title":"FFAA: Multimodal Large Language Model based Explainable Open-World Face\n  Forgery Analysis Assistant","summary":"  The rapid advancement of deepfake technologies has sparked widespread public\nconcern, particularly as face forgery poses a serious threat to public\ninformation security. However, the unknown and diverse forgery techniques,\nvaried facial features and complex environmental factors pose significant\nchallenges for face forgery analysis. Existing datasets lack descriptive\nannotations of these aspects, making it difficult for models to distinguish\nbetween real and forged faces using only visual information amid various\nconfounding factors. In addition, existing methods fail to yield user-friendly\nand explainable results, hindering the understanding of the model's\ndecision-making process. To address these challenges, we introduce a novel\nOpen-World Face Forgery Analysis VQA (OW-FFA-VQA) task and its corresponding\nbenchmark. To tackle this task, we first establish a dataset featuring a\ndiverse collection of real and forged face images with essential descriptions\nand reliable forgery reasoning. Based on this dataset, we introduce FFAA: Face\nForgery Analysis Assistant, consisting of a fine-tuned Multimodal Large\nLanguage Model (MLLM) and Multi-answer Intelligent Decision System (MIDS). By\nintegrating hypothetical prompts with MIDS, the impact of fuzzy classification\nboundaries is effectively mitigated, enhancing model robustness. Extensive\nexperiments demonstrate that our method not only provides user-friendly and\nexplainable results but also significantly boosts accuracy and robustness\ncompared to previous methods.\n","authors":["Zhengchao Huang","Bin Xia","Zicheng Lin","Zhun Mou","Wenming Yang","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2408.10072v2.pdf","comment":"23 pages, 21 figures; project page: https://ffaa-vl.github.io"},{"id":"http://arxiv.org/abs/2411.14169v1","updated":"2024-11-21T14:27:15Z","published":"2024-11-21T14:27:15Z","title":"Spatiotemporal Decoupling for Efficient Vision-Based Occupancy\n  Forecasting","summary":"  The task of occupancy forecasting (OCF) involves utilizing past and present\nperception data to predict future occupancy states of autonomous vehicle\nsurrounding environments, which is critical for downstream tasks such as\nobstacle avoidance and path planning. Existing 3D OCF approaches struggle to\npredict plausible spatial details for movable objects and suffer from slow\ninference speeds due to neglecting the bias and uneven distribution of changing\noccupancy states in both space and time. In this paper, we propose a novel\nspatiotemporal decoupling vision-based paradigm to explicitly tackle the bias\nand achieve both effective and efficient 3D OCF. To tackle spatial bias in\nempty areas, we introduce a novel spatial representation that decouples the\nconventional dense 3D format into 2D bird's-eye view (BEV) occupancy with\ncorresponding height values, enabling 3D OCF derived only from 2D predictions\nthus enhancing efficiency. To reduce temporal bias on static voxels, we design\ntemporal decoupling to improve end-to-end OCF by temporally associating\ninstances via predicted flows. We develop an efficient multi-head network\nEfficientOCF to achieve 3D OCF with our devised spatiotemporally decoupled\nrepresentation. A new metric, conditional IoU (C-IoU), is also introduced to\nprovide a robust 3D OCF performance assessment, especially in datasets with\nmissing or incomplete annotations. The experimental results demonstrate that\nEfficientOCF surpasses existing baseline methods on accuracy and efficiency,\nachieving state-of-the-art performance with a fast inference time of 82.33ms\nwith a single GPU. Our code will be released as open source.\n","authors":["Jingyi Xu","Xieyuanli Chen","Junyi Ma","Jiawei Huang","Jintao Xu","Yue Wang","Ling Pei"],"pdf_url":"https://arxiv.org/pdf/2411.14169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14164v1","updated":"2024-11-21T14:22:38Z","published":"2024-11-21T14:22:38Z","title":"FoPru: Focal Pruning for Efficient Large Vision-Language Models","summary":"  Large Vision-Language Models (LVLMs) represent a significant advancement\ntoward achieving superior multimodal capabilities by enabling powerful Large\nLanguage Models (LLMs) to understand visual input. Typically, LVLMs utilize\nvisual encoders, such as CLIP, to transform images into visual tokens, which\nare then aligned with textual tokens through projection layers before being\ninput into the LLM for inference. Although existing LVLMs have achieved\nsignificant success, their inference efficiency is still limited by the\nsubstantial number of visual tokens and the potential redundancy among them. To\nmitigate this issue, we propose Focal Pruning (FoPru), a training-free method\nthat prunes visual tokens based on the attention-based token significance\nderived from the vision encoder. Specifically, we introduce two alternative\npruning strategies: 1) the rank strategy, which leverages all token\nsignificance scores to retain more critical tokens in a global view; 2) the row\nstrategy, which focuses on preserving continuous key information in images from\na local perspective. Finally, the selected tokens are reordered to maintain\ntheir original positional relationships. Extensive experiments across various\nLVLMs and multimodal datasets demonstrate that our method can prune a large\nnumber of redundant tokens while maintaining high accuracy, leading to\nsignificant improvements in inference efficiency.\n","authors":["Lei Jiang","Weizhe Huang","Tongxuan Liu","Yuting Zeng","Jing Li","Lechao Cheng","Xiaohua Xu"],"pdf_url":"https://arxiv.org/pdf/2411.14164v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2411.14163v1","updated":"2024-11-21T14:22:32Z","published":"2024-11-21T14:22:32Z","title":"Creating a Formally Verified Neural Network for Autonomous Navigation:\n  An Experience Report","summary":"  The increased reliance of self-driving vehicles on neural networks opens up\nthe challenge of their verification. In this paper we present an experience\nreport, describing a case study which we undertook to explore the design and\ntraining of a neural network on a custom dataset for vision-based autonomous\nnavigation. We are particularly interested in the use of machine learning with\ndifferentiable logics to obtain networks satisfying basic safety properties by\ndesign, guaranteeing the behaviour of the neural network after training. We\nmotivate the choice of a suitable neural network verifier for our purposes and\nreport our observations on the use of neural network verifiers for self-driving\nsystems.\n","authors":["Syed Ali Asadullah Bukhari","Thomas Flinkow","Medet Inkarbekov","Barak A. Pearlmutter","Rosemary Monahan"],"pdf_url":"https://arxiv.org/pdf/2411.14163v1.pdf","comment":"In Proceedings FMAS2024, arXiv:2411.13215"},{"id":"http://arxiv.org/abs/2411.14158v1","updated":"2024-11-21T14:19:32Z","published":"2024-11-21T14:19:32Z","title":"Point Cloud Denoising With Fine-Granularity Dynamic Graph Convolutional\n  Networks","summary":"  Due to limitations in acquisition equipment, noise perturbations often\ncorrupt 3-D point clouds, hindering down-stream tasks such as surface\nreconstruction, rendering, and further processing. Existing 3-D point cloud\ndenoising methods typically fail to reliably fit the underlying continuous\nsurface, resulting in a degradation of reconstruction performance. This paper\nintroduces fine-granularity dynamic graph convolutional networks called GD-GCN,\na novel approach to denoising in 3-D point clouds. The GD-GCN employs\nmicro-step temporal graph convolution (MST-GConv) to perform feature learning\nin a gradual manner. Compared with the conventional GCN, which commonly uses\ndiscrete integer-step graph convolution, this modification introduces a more\nadaptable and nuanced approach to feature learning within graph convolution\nnetworks. It more accurately depicts the process of fitting the point cloud\nwith noise to the underlying surface by and the learning process for MST-GConv\nacts like a changing system and is managed through a type of neural network\nknown as neural Partial Differential Equations (PDEs). This means it can adapt\nand improve over time. GD-GCN approximates the Riemannian metric, calculating\ndistances between points along a low-dimensional manifold. This capability\nallows it to understand the local geometric structure and effectively capture\ndiverse relationships between points from different geometric regions through\ngeometric graph construction based on Riemannian distances. Additionally,\nGD-GCN incorporates robust graph spectral filters based on the Bernstein\npolynomial approximation, which modulate eigenvalues for complex and arbitrary\nspectral responses, providing theoretical guarantees for BIBO stability.\nSymmetric channel mixing matrices further enhance filter flexibility by\nenabling channel-level scaling and shifting in the spectral domain.\n","authors":["Wenqiang Xu","Wenrui Dai","Duoduo Xue","Ziyang Zheng","Chenglin Li","Junni Zou","Hongkai Xiong"],"pdf_url":"https://arxiv.org/pdf/2411.14158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11340v2","updated":"2024-11-21T14:09:12Z","published":"2024-09-17T16:42:46Z","title":"OmniGen: Unified Image Generation","summary":"  The emergence of Large Language Models (LLMs) has unified language generation\ntasks and revolutionized human-machine interaction. However, in the realm of\nimage generation, a unified model capable of handling various tasks within a\nsingle framework remains largely unexplored. In this work, we introduce\nOmniGen, a new diffusion model for unified image generation. OmniGen is\ncharacterized by the following features: 1) Unification: OmniGen not only\ndemonstrates text-to-image generation capabilities but also inherently supports\nvarious downstream tasks, such as image editing, subject-driven generation, and\nvisual-conditional generation. 2) Simplicity: The architecture of OmniGen is\nhighly simplified, eliminating the need for additional plugins. Moreover,\ncompared to existing diffusion models, it is more user-friendly and can\ncomplete complex tasks end-to-end through instructions without the need for\nextra intermediate steps, greatly simplifying the image generation workflow. 3)\nKnowledge Transfer: Benefit from learning in a unified format, OmniGen\neffectively transfers knowledge across different tasks, manages unseen tasks\nand domains, and exhibits novel capabilities. We also explore the model's\nreasoning capabilities and potential applications of the chain-of-thought\nmechanism. This work represents the first attempt at a general-purpose image\ngeneration model, and we will release our resources at\nhttps://github.com/VectorSpaceLab/OmniGen to foster future advancements.\n","authors":["Shitao Xiao","Yueze Wang","Junjie Zhou","Huaying Yuan","Xingrun Xing","Ruiran Yan","Chaofan Li","Shuting Wang","Tiejun Huang","Zheng Liu"],"pdf_url":"https://arxiv.org/pdf/2409.11340v2.pdf","comment":"Update the paper for OmniGen-v1"},{"id":"http://arxiv.org/abs/2411.14141v1","updated":"2024-11-21T14:04:38Z","published":"2024-11-21T14:04:38Z","title":"Differentiable SVD based on Moore-Penrose Pseudoinverse for Inverse\n  Imaging Problems","summary":"  Low-rank regularization-based deep unrolling networks have achieved\nremarkable success in various inverse imaging problems (IIPs). However, the\nsingular value decomposition (SVD) is non-differentiable when duplicated\nsingular values occur, leading to severe numerical instability during training.\nIn this paper, we propose a differentiable SVD based on the Moore-Penrose\npseudoinverse to address this issue. To the best of our knowledge, this is the\nfirst work to provide a comprehensive analysis of the differentiability of the\ntrivial SVD. Specifically, we show that the non-differentiability of SVD is\nessentially due to an underdetermined system of linear equations arising in the\nderivation process. We utilize the Moore-Penrose pseudoinverse to solve the\nsystem, thereby proposing a differentiable SVD. A numerical stability analysis\nin the context of IIPs is provided. Experimental results in color image\ncompressed sensing and dynamic MRI reconstruction show that our proposed\ndifferentiable SVD can effectively address the numerical instability issue\nwhile ensuring computational precision. Code is available at\nhttps://github.com/yhao-z/SVD-inv.\n","authors":["Yinghao Zhang","Yue Hu"],"pdf_url":"https://arxiv.org/pdf/2411.14141v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2411.14137v1","updated":"2024-11-21T14:01:42Z","published":"2024-11-21T14:01:42Z","title":"Visual Contexts Clarify Ambiguous Expressions: A Benchmark Dataset","summary":"  The ability to perform complex reasoning across multimodal inputs is\nessential for models to effectively interact with humans in real-world\nscenarios. Advancements in vision-language models have significantly improved\nperformance on tasks that require processing explicit and direct textual\ninputs, such as Visual Question Answering (VQA) and Visual Grounding (VG).\nHowever, less attention has been given to improving the model capabilities to\ncomprehend nuanced and ambiguous forms of communication. This presents a\ncritical challenge, as human language in real-world interactions often convey\nhidden intentions that rely on context for accurate interpretation. To address\nthis gap, we propose VAGUE, a multimodal benchmark comprising 3.9K indirect\nhuman utterances paired with corresponding scenes. Additionally, we contribute\na model-based pipeline for generating prompt-solution pairs from input images.\nOur work aims to delve deeper into the ability of models to understand indirect\ncommunication and seek to contribute to the development of models capable of\nmore refined and human-like interactions. Extensive evaluation on multiple VLMs\nreveals that mainstream models still struggle with indirect communication when\nrequired to perform complex linguistic and visual reasoning. We release our\ncode and data at https://github.com/Hazel-Heejeong-Nam/VAGUE.git.\n","authors":["Heejeong Nam","Jinwoo Ahn"],"pdf_url":"https://arxiv.org/pdf/2411.14137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14133v1","updated":"2024-11-21T14:00:01Z","published":"2024-11-21T14:00:01Z","title":"GASP: Efficient Black-Box Generation of Adversarial Suffixes for\n  Jailbreaking LLMs","summary":"  Large Language Models (LLMs) have shown impressive proficiency across a range\nof natural language processing tasks yet remain vulnerable to adversarial\nprompts, known as jailbreak attacks, carefully designed to elicit harmful\nresponses from LLMs. Traditional methods rely on manual heuristics, which\nsuffer from limited generalizability. While being automatic, optimization-based\nattacks often produce unnatural jailbreak prompts that are easy to detect by\nsafety filters or require high computational overhead due to discrete token\noptimization. Witnessing the limitations of existing jailbreak methods, we\nintroduce Generative Adversarial Suffix Prompter (GASP), a novel framework that\ncombines human-readable prompt generation with Latent Bayesian Optimization\n(LBO) to improve adversarial suffix creation in a fully black-box setting. GASP\nleverages LBO to craft adversarial suffixes by efficiently exploring continuous\nembedding spaces, gradually optimizing the model to improve attack efficacy\nwhile balancing prompt coherence through a targeted iterative refinement\nprocedure. Our experiments show that GASP can generate natural jailbreak\nprompts, significantly improving attack success rates, reducing training times,\nand accelerating inference speed, thus making it an efficient and scalable\nsolution for red-teaming LLMs.\n","authors":["Advik Raj Basani","Xiao Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14133v1.pdf","comment":"28 pages, 9 tables, 13 figures; under review at CVPR '25"},{"id":"http://arxiv.org/abs/2403.09055v3","updated":"2024-11-21T13:57:27Z","published":"2024-03-14T02:51:01Z","title":"SemanticDraw: Towards Real-Time Interactive Content Creation from Image\n  Diffusion Models","summary":"  We introduce SemanticDraw, a new paradigm of interactive content creation\nwhere high-quality images are generated in near real-time from given multiple\nhand-drawn regions, each encoding prescribed semantic meaning. In order to\nmaximize the productivity of content creators and to fully realize their\nartistic imagination, it requires both quick interactive interfaces and\nfine-grained regional controls in their tools. Despite astonishing generation\nquality from recent diffusion models, we find that existing approaches for\nregional controllability are very slow (52 seconds for $512 \\times 512$ image)\nwhile not compatible with acceleration methods such as LCM, blocking their huge\npotential in interactive content creation. From this observation, we build our\nsolution for interactive content creation in two steps: (1) we establish\ncompatibility between region-based controls and acceleration techniques for\ndiffusion models, maintaining high fidelity of multi-prompt image generation\nwith $\\times 10$ reduced number of inference steps, (2) we increase the\ngeneration throughput with our new multi-prompt stream batch pipeline, enabling\nlow-latency generation from multiple, region-based text prompts on a single RTX\n2080 Ti GPU. Our proposed framework is generalizable to any existing diffusion\nmodels and acceleration schedulers, allowing sub-second (0.64 seconds) image\ncontent creation application upon well-established image diffusion models. Our\nproject page is: https://jaerinlee.com/research/semantic-draw.\n","authors":["Jaerin Lee","Daniel Sungho Jung","Kanggeon Lee","Kyoung Mu Lee"],"pdf_url":"https://arxiv.org/pdf/2403.09055v3.pdf","comment":"20 pages, 15 figures. v3: added tables"},{"id":"http://arxiv.org/abs/2407.10921v4","updated":"2024-11-21T13:56:07Z","published":"2024-07-15T17:22:16Z","title":"Leveraging Bi-Focal Perspectives and Granular Feature Integration for\n  Accurate Reliable Early Alzheimer's Detection","summary":"  Alzheimer's disease (AD) is the most common neurodegeneration, annually\ndiagnosed in millions of patients. The present medicine scenario still finds\nchallenges in the exact diagnosis and classification of AD through neuroimaging\ndata. Traditional CNNs can extract a good amount of low-level information in an\nimage but fail to extract high-level minuscule particles, which is a\nsignificant challenge in detecting AD from MRI scans. To overcome this, we\npropose a novel Granular Feature Integration method to combine information\nextraction at different scales combined with an efficient information flow,\nenabling the model to capture both broad and fine-grained features\nsimultaneously. We also propose a Bi-Focal Perspective mechanism to highlight\nthe subtle neurofibrillary tangles and amyloid plaques in the MRI scans,\nensuring that critical pathological markers are accurately identified. Our\nmodel achieved an F1-Score of 99.31%, precision of 99.24%, and recall of\n99.51%. These scores prove that our model is significantly better than the\nstate-of-the-art (SOTA) CNNs in existence.\n","authors":["Pandiyaraju V","Shravan Venkatraman","Abeshek A","Pavan Kumar S","Aravintakshan S A","Kannan A"],"pdf_url":"https://arxiv.org/pdf/2407.10921v4.pdf","comment":"14 pages, 12 figures, 6 tables"},{"id":"http://arxiv.org/abs/2402.19160v4","updated":"2024-11-21T13:52:25Z","published":"2024-02-29T13:44:19Z","title":"Effective Message Hiding with Order-Preserving Mechanisms","summary":"  Message hiding, a technique that conceals secret message bits within a cover\nimage, aims to achieve an optimal balance among message capacity, recovery\naccuracy, and imperceptibility. While convolutional neural networks have\nnotably improved message capacity and imperceptibility, achieving high recovery\naccuracy remains challenging. This challenge arises because convolutional\noperations struggle to preserve the sequential order of message bits and\neffectively address the discrepancy between these two modalities. To address\nthis, we propose StegaFormer, an innovative MLP-based framework designed to\npreserve bit order and enable global fusion between modalities. Specifically,\nStegaFormer incorporates three crucial components: Order-Preserving Message\nEncoder (OPME), Decoder (OPMD) and Global Message-Image Fusion (GMIF). OPME and\nOPMD aim to preserve the order of message bits by segmenting the entire\nsequence into equal-length segments and incorporating sequential information\nduring encoding and decoding. Meanwhile, GMIF employs a cross-modality fusion\nmechanism to effectively fuse the features from the two uncorrelated\nmodalities. Experimental results on the COCO and DIV2K datasets demonstrate\nthat StegaFormer surpasses existing state-of-the-art methods in terms of\nrecovery accuracy, message capacity, and imperceptibility. We will make our\ncode publicly available.\n","authors":["Gao Yu","Qiu Xuchong","Ye Zihan"],"pdf_url":"https://arxiv.org/pdf/2402.19160v4.pdf","comment":"BMVC 2024"},{"id":"http://arxiv.org/abs/2411.14125v1","updated":"2024-11-21T13:50:25Z","published":"2024-11-21T13:50:25Z","title":"RestorerID: Towards Tuning-Free Face Restoration with ID Preservation","summary":"  Blind face restoration has made great progress in producing high-quality and\nlifelike images. Yet it remains challenging to preserve the ID information\nespecially when the degradation is heavy. Current reference-guided face\nrestoration approaches either require face alignment or personalized\ntest-tuning, which are unfaithful or time-consuming. In this paper, we propose\na tuning-free method named RestorerID that incorporates ID preservation during\nface restoration. RestorerID is a diffusion model-based method that restores\nlow-quality images with varying levels of degradation by using a single\nreference image. To achieve this, we propose a unified framework to combine the\nID injection with the base blind face restoration model. In addition, we design\na novel Face ID Rebalancing Adapter (FIR-Adapter) to tackle the problems of\ncontent unconsistency and contours misalignment that are caused by information\nconflicts between the low-quality input and reference image. Furthermore, by\nemploying an Adaptive ID-Scale Adjusting strategy, RestorerID can produce\nsuperior restored images across various levels of degradation. Experimental\nresults on the Celeb-Ref dataset and real-world scenarios demonstrate that\nRestorerID effectively delivers high-quality face restoration with ID\npreservation, achieving a superior performance compared to the test-tuning\napproaches and other reference-guided ones. The code of RestorerID is available\nat \\url{https://github.com/YingJiacheng/RestorerID}.\n","authors":["Jiacheng Ying","Mushui Liu","Zhe Wu","Runming Zhang","Zhu Yu","Siming Fu","Si-Yuan Cao","Chao Wu","Yunlong Yu","Hui-Liang Shen"],"pdf_url":"https://arxiv.org/pdf/2411.14125v1.pdf","comment":"10 pages, 10 figures"},{"id":"http://arxiv.org/abs/2411.14120v1","updated":"2024-11-21T13:44:18Z","published":"2024-11-21T13:44:18Z","title":"Point Cloud Resampling with Learnable Heat Diffusion","summary":"  Generative diffusion models have shown empirical successes in point cloud\nresampling, generating a denser and more uniform distribution of points from\nsparse or noisy 3D point clouds by progressively refining noise into structure.\nHowever, existing diffusion models employ manually predefined schemes, which\noften fail to recover the underlying point cloud structure due to the rigid and\ndisruptive nature of the geometric degradation. To address this issue, we\npropose a novel learnable heat diffusion framework for point cloud resampling,\nwhich directly parameterizes the marginal distribution for the forward process\nby learning the adaptive heat diffusion schedules and local filtering scales of\nthe time-varying heat kernel, and consequently, generates an adaptive\nconditional prior for the reverse process. Unlike previous diffusion models\nwith a fixed prior, the adaptive conditional prior selectively preserves\ngeometric features of the point cloud by minimizing a refined variational lower\nbound, guiding the points to evolve towards the underlying surface during the\nreverse process. Extensive experimental results demonstrate that the proposed\npoint cloud resampling achieves state-of-the-art performance in representative\nreconstruction tasks including point cloud denoising and upsampling.\n","authors":["Wenqiang Xu","Wenrui Dai","Duoduo Xue","Ziyang Zheng","Chenglin Li","Junni Zou","Hongkai Xiong"],"pdf_url":"https://arxiv.org/pdf/2411.14120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14119v1","updated":"2024-11-21T13:42:24Z","published":"2024-11-21T13:42:24Z","title":"Uncertainty-Aware Regression for Socio-Economic Estimation via\n  Multi-View Remote Sensing","summary":"  Remote sensing imagery offers rich spectral data across extensive areas for\nEarth observation. Many attempts have been made to leverage these data with\ntransfer learning to develop scalable alternatives for estimating\nsocio-economic conditions, reducing reliance on expensive survey-collected\ndata. However, much of this research has primarily focused on daytime satellite\nimagery due to the limitation that most pre-trained models are trained on\n3-band RGB images. Consequently, modeling techniques for spectral bands beyond\nthe visible spectrum have not been thoroughly investigated. Additionally,\nquantifying uncertainty in remote sensing regression has been less explored,\nyet it is essential for more informed targeting and iterative collection of\nground truth survey data. In this paper, we introduce a novel framework that\nleverages generic foundational vision models to process remote sensing imagery\nusing combinations of three spectral bands to exploit multi-spectral data. We\nalso employ methods such as heteroscedastic regression and Bayesian modeling to\ngenerate uncertainty estimates for the predictions. Experimental results\ndemonstrate that our method outperforms existing models that use RGB or\nmulti-spectral models with unstructured band usage. Moreover, our framework\nhelps identify uncertain predictions, guiding future ground truth data\nacquisition.\n","authors":["Fan Yang","Sahoko Ishida","Mengyan Zhang","Daniel Jenson","Swapnil Mishra","Jhonathan Navott","Seth Flaxman"],"pdf_url":"https://arxiv.org/pdf/2411.14119v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2405.16200v2","updated":"2024-11-21T13:28:20Z","published":"2024-05-25T12:15:56Z","title":"FlightPatchNet: Multi-Scale Patch Network with Differential Coding for\n  Flight Trajectory Prediction","summary":"  Accurate multi-step flight trajectory prediction plays an important role in\nAir Traffic Control, which can ensure the safety of air transportation. Two\nmain issues limit the flight trajectory prediction performance of existing\nworks. The first issue is the negative impact on prediction accuracy caused by\nthe significant differences in data range. The second issue is that real-world\nflight trajectories involve underlying temporal dependencies, and existing\nmethods fail to reveal the hidden complex temporal variations and only extract\nfeatures from one single time scale. To address the above issues, we propose\nFlightPatchNet, a multi-scale patch network with differential coding for flight\ntrajectory prediction. Specifically, FlightPatchNet first utilizes the\ndifferential coding to encode the original values of longitude and latitude\ninto first-order differences and generates embeddings for all variables at each\ntime step. Then, a global temporal attention is introduced to explore the\ndependencies between different time steps. To fully explore the diverse\ntemporal patterns in flight trajectories, a multi-scale patch network is\ndelicately designed to serve as the backbone. The multi-scale patch network\nexploits stacked patch mixer blocks to capture inter- and intra-patch\ndependencies under different time scales, and further integrates multi-scale\ntemporal features across different scales and variables. Finally,\nFlightPatchNet ensembles multiple predictors to make direct multi-step\nprediction. Extensive experiments on ADS-B datasets demonstrate that our model\noutperforms the competitive baselines.\n","authors":["Lan Wu","Xuebin Wang","Ruijuan Chu","Guangyi Liu","Yingchun Chen","Jing Zhang","Linyu Wang"],"pdf_url":"https://arxiv.org/pdf/2405.16200v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12248v2","updated":"2024-11-21T13:24:20Z","published":"2024-11-19T05:52:17Z","title":"Neuro-3D: Towards 3D Visual Decoding from EEG Signals","summary":"  Human's perception of the visual world is shaped by the stereo processing of\n3D information. Understanding how the brain perceives and processes 3D visual\nstimuli in the real world has been a longstanding endeavor in neuroscience.\nTowards this goal, we introduce a new neuroscience task: decoding 3D visual\nperception from EEG signals, a neuroimaging technique that enables real-time\nmonitoring of neural dynamics enriched with complex visual cues. To provide the\nessential benchmark, we first present EEG-3D, a pioneering dataset featuring\nmultimodal analysis data and extensive EEG recordings from 12 subjects viewing\n72 categories of 3D objects rendered in both videos and images. Furthermore, we\npropose Neuro-3D, a 3D visual decoding framework based on EEG signals. This\nframework adaptively integrates EEG features derived from static and dynamic\nstimuli to learn complementary and robust neural representations, which are\nsubsequently utilized to recover both the shape and color of 3D objects through\nthe proposed diffusion-based colored point cloud decoder. To the best of our\nknowledge, we are the first to explore EEG-based 3D visual decoding.\nExperiments indicate that Neuro-3D not only reconstructs colored 3D objects\nwith high fidelity, but also learns effective neural representations that\nenable insightful brain region analysis. The dataset and associated code will\nbe made publicly available.\n","authors":["Zhanqiang Guo","Jiamin Wu","Yonghao Song","Jiahui Bu","Weijian Mai","Qihao Zheng","Wanli Ouyang","Chunfeng Song"],"pdf_url":"https://arxiv.org/pdf/2411.12248v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13525v2","updated":"2024-11-21T13:21:16Z","published":"2024-11-20T18:21:58Z","title":"Geometric Algebra Planes: Convex Implicit Neural Volumes","summary":"  Volume parameterizations abound in recent literature, from the classic voxel\ngrid to the implicit neural representation and everything in between. While\nimplicit representations have shown impressive capacity and better memory\nefficiency compared to voxel grids, to date they require training via nonconvex\noptimization. This nonconvex training process can be slow to converge and\nsensitive to initialization and hyperparameter choices that affect the final\nconverged result. We introduce a family of models, GA-Planes, that is the first\nclass of implicit neural volume representations that can be trained by convex\noptimization. GA-Planes models include any combination of features stored in\ntensor basis elements, followed by a neural feature decoder. They generalize\nmany existing representations and can be adapted for convex, semiconvex, or\nnonconvex training as needed for different inverse problems. In the 2D setting,\nwe prove that GA-Planes is equivalent to a low-rank plus low-resolution matrix\nfactorization; we show that this approximation outperforms the classic low-rank\nplus sparse decomposition for fitting a natural image. In 3D, we demonstrate\nGA-Planes' competitive performance in terms of expressiveness, model size, and\noptimizability across three volume fitting tasks: radiance field\nreconstruction, 3D segmentation, and video segmentation.\n","authors":["Irmak Sivgin","Sara Fridovich-Keil","Gordon Wetzstein","Mert Pilanci"],"pdf_url":"https://arxiv.org/pdf/2411.13525v2.pdf","comment":"Code is available at\n  https://github.com/sivginirmak/Geometric-Algebra-Planes"},{"id":"http://arxiv.org/abs/2411.14095v1","updated":"2024-11-21T13:00:30Z","published":"2024-11-21T13:00:30Z","title":"WARLearn: Weather-Adaptive Representation Learning","summary":"  This paper introduces WARLearn, a novel framework designed for adaptive\nrepresentation learning in challenging and adversarial weather conditions.\nLeveraging the in-variance principal used in Barlow Twins, we demonstrate the\ncapability to port the existing models initially trained on clear weather data\nto effectively handle adverse weather conditions. With minimal additional\ntraining, our method exhibits remarkable performance gains in scenarios\ncharacterized by fog and low-light conditions. This adaptive framework extends\nits applicability beyond adverse weather settings, offering a versatile\nsolution for domains exhibiting variations in data distributions. Furthermore,\nWARLearn is invaluable in scenarios where data distributions undergo\nsignificant shifts over time, enabling models to remain updated and accurate.\nOur experimental findings reveal a remarkable performance, with a mean average\nprecision (mAP) of 52.6% on unseen real-world foggy dataset (RTTS). Similarly,\nin low light conditions, our framework achieves a mAP of 55.7% on unseen\nreal-world low light dataset (ExDark). Notably, WARLearn surpasses the\nperformance of state-of-the-art frameworks including FeatEnHancer, Image\nAdaptive YOLO, DENet, C2PNet, PairLIE and ZeroDCE, by a substantial margin in\nadverse weather, improving the baseline performance in both foggy and low light\nconditions. The WARLearn code is available at\nhttps://github.com/ShubhamAgarwal12/WARLearn\n","authors":["Shubham Agarwal","Raz Birman","Ofer Hadar"],"pdf_url":"https://arxiv.org/pdf/2411.14095v1.pdf","comment":"Accepted for publication in IEEE/CVF Winter Conference on\n  Applications of Computer Vision (WACV), 2025"},{"id":"http://arxiv.org/abs/2411.14092v1","updated":"2024-11-21T12:58:09Z","published":"2024-11-21T12:58:09Z","title":"MetaCropFollow: Few-Shot Adaptation with Meta-Learning for Under-Canopy\n  Navigation","summary":"  Autonomous under-canopy navigation faces additional challenges compared to\nover-canopy settings - for example the tight spacing between the crop rows,\ndegraded GPS accuracy and excessive clutter. Keypoint-based visual navigation\nhas been shown to perform well in these conditions, however the differences\nbetween agricultural environments in terms of lighting, season, soil and crop\ntype mean that a domain shift will likely be encountered at some point of the\nrobot deployment. In this paper, we explore the use of Meta-Learning to\novercome this domain shift using a minimal amount of data. We train a\nbase-learner that can quickly adapt to new conditions, enabling more robust\nnavigation in low-data regimes.\n","authors":["Thomas Woehrle","Arun N. Sivakumar","Naveen Uppalapati","Girish Chowdhary"],"pdf_url":"https://arxiv.org/pdf/2411.14092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08298v5","updated":"2024-11-21T12:55:50Z","published":"2024-06-12T14:59:12Z","title":"AdaNCA: Neural Cellular Automata As Adaptors For More Robust Vision\n  Transformer","summary":"  Vision Transformers (ViTs) demonstrate remarkable performance in image\nclassification through visual-token interaction learning, particularly when\nequipped with local information via region attention or convolutions. Although\nsuch architectures improve the feature aggregation from different\ngranularities, they often fail to contribute to the robustness of the networks.\nNeural Cellular Automata (NCA) enables the modeling of global visual-token\nrepresentations through local interactions, with its training strategies and\narchitecture design conferring strong generalization ability and robustness\nagainst noisy input. In this paper, we propose Adaptor Neural Cellular Automata\n(AdaNCA) for Vision Transformers that uses NCA as plug-and-play adaptors\nbetween ViT layers, thus enhancing ViT's performance and robustness against\nadversarial samples as well as out-of-distribution inputs. To overcome the\nlarge computational overhead of standard NCAs, we propose Dynamic Interaction\nfor more efficient interaction learning. Using our analysis of AdaNCA placement\nand robustness improvement, we also develop an algorithm for identifying the\nmost effective insertion points for AdaNCA. With less than a 3% increase in\nparameters, AdaNCA contributes to more than 10% absolute improvement in\naccuracy under adversarial attacks on the ImageNet1K benchmark. Moreover, we\ndemonstrate with extensive evaluations across eight robustness benchmarks and\nfour ViT architectures that AdaNCA, as a plug-and-play module, consistently\nimproves the robustness of ViTs.\n","authors":["Yitao Xu","Tong Zhang","Sabine Süsstrunk"],"pdf_url":"https://arxiv.org/pdf/2406.08298v5.pdf","comment":"32 pages, 12 figures"},{"id":"http://arxiv.org/abs/2410.07753v2","updated":"2024-11-21T12:47:53Z","published":"2024-10-10T09:29:23Z","title":"Data Augmentation for Surgical Scene Segmentation with Anatomy-Aware\n  Diffusion Models","summary":"  In computer-assisted surgery, automatically recognizing anatomical organs is\ncrucial for understanding the surgical scene and providing intraoperative\nassistance. While machine learning models can identify such structures, their\ndeployment is hindered by the need for labeled, diverse surgical datasets with\nanatomical annotations. Labeling multiple classes (i.e., organs) in a surgical\nscene is time-intensive, requiring medical experts. Although synthetically\ngenerated images can enhance segmentation performance, maintaining both organ\nstructure and texture during generation is challenging. We introduce a\nmulti-stage approach using diffusion models to generate multi-class surgical\ndatasets with annotations. Our framework improves anatomy awareness by training\norgan specific models with an inpainting objective guided by binary\nsegmentation masks. The organs are generated with an inference pipeline using\npre-trained ControlNet to maintain the organ structure. The synthetic\nmulti-class datasets are constructed through an image composition step,\nensuring structural and textural consistency. This versatile approach allows\nthe generation of multi-class datasets from real binary datasets and simulated\nsurgical masks. We thoroughly evaluate the generated datasets on image quality\nand downstream segmentation, achieving a $15\\%$ improvement in segmentation\nscores when combined with real images. The code is available at\nhttps://gitlab.com/nct_tso_public/muli-class-image-synthesis\n","authors":["Danush Kumar Venkatesh","Dominik Rivoir","Micha Pfeiffer","Fiona Kolbinger","Stefanie Speidel"],"pdf_url":"https://arxiv.org/pdf/2410.07753v2.pdf","comment":"Accepted at WACV 2025"},{"id":"http://arxiv.org/abs/2411.14078v1","updated":"2024-11-21T12:43:19Z","published":"2024-11-21T12:43:19Z","title":"Self-supervised learning for radio-astronomy source classification: a\n  benchmark","summary":"  The upcoming Square Kilometer Array (SKA) telescope marks a significant step\nforward in radio astronomy, presenting new opportunities and challenges for\ndata analysis. Traditional visual models pretrained on optical photography\nimages may not perform optimally on radio interferometry images, which have\ndistinct visual characteristics.\n  Self-Supervised Learning (SSL) offers a promising approach to address this\nissue, leveraging the abundant unlabeled data in radio astronomy to train\nneural networks that learn useful representations from radio images. This study\nexplores the application of SSL to radio astronomy, comparing the performance\nof SSL-trained models with that of traditional models pretrained on natural\nimages, evaluating the importance of data curation for SSL, and assessing the\npotential benefits of self-supervision to different domain-specific radio\nastronomy datasets.\n  Our results indicate that, SSL-trained models achieve significant\nimprovements over the baseline in several downstream tasks, especially in the\nlinear evaluation setting; when the entire backbone is fine-tuned, the benefits\nof SSL are less evident but still outperform pretraining. These findings\nsuggest that SSL can play a valuable role in efficiently enhancing the analysis\nof radio astronomical data. The trained models and code is available at:\n\\url{https://github.com/dr4thmos/solo-learn-radio}\n","authors":["Thomas Cecconello","Simone Riggi","Ugo Becciano","Fabio Vitello","Andrew M. Hopkins","Giuseppe Vizzari","Concetto Spampinato","Simone Palazzo"],"pdf_url":"https://arxiv.org/pdf/2411.14078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17158v4","updated":"2024-11-21T12:35:18Z","published":"2024-05-27T13:31:46Z","title":"PatchScaler: An Efficient Patch-Independent Diffusion Model for Image\n  Super-Resolution","summary":"  While diffusion models significantly improve the perceptual quality of\nsuper-resolved images, they usually require a large number of sampling steps,\nresulting in high computational costs and long inference times. Recent efforts\nhave explored reasonable acceleration schemes by reducing the number of\nsampling steps. However, these approaches treat all regions of the image\nequally, overlooking the fact that regions with varying levels of\nreconstruction difficulty require different sampling steps. To address this\nlimitation, we propose PatchScaler, an efficient patch-independent diffusion\npipeline for single image super-resolution. Specifically, PatchScaler\nintroduces a Patch-adaptive Group Sampling (PGS) strategy that groups feature\npatches by quantifying their reconstruction difficulty and establishes shortcut\npaths with different sampling configurations for each group. To further\noptimize the patch-level reconstruction process of PGS, we propose a texture\nprompt that provides rich texture conditional information to the diffusion\nmodel. The texture prompt adaptively retrieves texture priors for the target\npatch from a common reference texture memory. Extensive experiments show that\nour PatchScaler achieves superior performance in both quantitative and\nqualitative evaluations, while significantly speeding up inference. Our code\nwill be available at \\url{https://github.com/yongliuy/PatchScaler}.\n","authors":["Yong Liu","Hang Dong","Jinshan Pan","Qingji Dong","Kai Chen","Rongxiang Zhang","Lean Fu","Fei Wang"],"pdf_url":"https://arxiv.org/pdf/2405.17158v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14342v3","updated":"2024-11-21T12:32:08Z","published":"2024-05-23T09:11:47Z","title":"RoGs: Large Scale Road Surface Reconstruction with Meshgrid Gaussian","summary":"  Road surface reconstruction plays a crucial role in autonomous driving, which\ncan be used for road lane perception and autolabeling. Recently, mesh-based\nroad surface reconstruction algorithms have shown promising reconstruction\nresults. However, these mesh-based methods suffer from slow speed and poor\nreconstruction quality. To address these limitations, we propose a novel\nlarge-scale road surface reconstruction approach with meshgrid Gaussian, named\nRoGs. Specifically, we model the road surface by placing Gaussian surfels in\nthe vertices of a uniformly distributed square mesh, where each surfel stores\ncolor, semantic, and geometric information. This square mesh-based layout\ncovers the entire road with fewer Gaussian surfels and reduces the overlap\nbetween Gaussian surfels during training. In addition, because the road surface\nhas no thickness, 2D Gaussian surfel is more consistent with the physical\nreality of the road surface than 3D Gaussian sphere. Then, unlike previous\ninitialization methods that rely on point clouds, we introduce a vehicle\npose-based initialization method to initialize the height and rotation of the\nGaussian surfel. Thanks to this meshgrid Gaussian modeling and pose-based\ninitialization, our method achieves significant speedups while improving\nreconstruction quality. We obtain excellent results in reconstruction of road\nsurfaces in a variety of challenging real-world scenes.\n","authors":["Zhiheng Feng","Wenhua Wu","Tianchen Deng","Hesheng Wang"],"pdf_url":"https://arxiv.org/pdf/2405.14342v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14320v2","updated":"2024-11-21T12:29:28Z","published":"2024-03-21T11:41:39Z","title":"Exosense: A Vision-Based Scene Understanding System For Exoskeletons","summary":"  Self-balancing exoskeletons are a key enabling technology for individuals\nwith mobility impairments. While the current challenges focus on\nhuman-compliant hardware and control, unlocking their use for daily activities\nrequires a scene perception system. In this work, we present Exosense, a\nvision-centric scene understanding system for self-balancing exoskeletons. We\nintroduce a multi-sensor visual-inertial mapping device as well as a navigation\nstack for state estimation, terrain mapping and long-term operation. We tested\nExosense attached to both a human leg and Wandercraft's Personal Exoskeleton in\nreal-world indoor scenarios. This enabled us to test the system during typical\nperiodic walking gaits, as well as future uses in multi-story environments. We\ndemonstrate that Exosense can achieve an odometry drift of about 4 cm per meter\ntraveled, and construct terrain maps under 1 cm average reconstruction error.\nIt can also work in a visual localization mode in a previously mapped\nenvironment, providing a step towards long-term operation of exoskeletons.\n","authors":["Jianeng Wang","Matias Mattamala","Christina Kassab","Guillaume Burger","Fabio Elnecave","Lintong Zhang","Marine Petriaux","Maurice Fallon"],"pdf_url":"https://arxiv.org/pdf/2403.14320v2.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2411.14064v1","updated":"2024-11-21T12:26:33Z","published":"2024-11-21T12:26:33Z","title":"Multi LoRA Meets Vision: Merging multiple adapters to create a multi\n  task model","summary":"  Parameter efficient finetuning (PEFT) methods are widely used in LLMs and\ngenerative models in computer vision. Especially one can use multiple of these\nduring inference to change the behavior of the base model. In this paper we\ninvestigated whether multiple LoRA adapters trained on computer vision tasks\ncan be merged together and used during inference without loss in performance.\nBy achieving this, multitask models can be created just by merging different\nLoRAs. Merging these will reduce inference time and it will not require any\nadditional retraining. We have trained adapters on six different tasks and\nevaluated their performance when they are merged together. For comparison we\nused a model with a frozen backbone and finetuned its head. Our results show\nthat even with simple merging techniques creating a multitask model by merging\nadapters is achievable by slightly loosing performance in some cases. In our\nexperiments we merged up to three adapters together. Depending on the task and\nthe similarity of the data adapters were trained on, merges can outperform head\nfinetuning. We have observed that LoRAs trained with dissimilar datasets tend\nto perform better compared to model trained on similar datasets.\n","authors":["Ege Kesim","Selahattin Serdar Helli"],"pdf_url":"https://arxiv.org/pdf/2411.14064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11424v2","updated":"2024-11-21T12:22:10Z","published":"2024-07-16T06:38:49Z","title":"Model Inversion Attacks Through Target-Specific Conditional Diffusion\n  Models","summary":"  Model inversion attacks (MIAs) aim to reconstruct private images from a\ntarget classifier's training set, thereby raising privacy concerns in AI\napplications. Previous GAN-based MIAs tend to suffer from inferior generative\nfidelity due to GAN's inherent flaws and biased optimization within latent\nspace. To alleviate these issues, leveraging on diffusion models' remarkable\nsynthesis capabilities, we propose Diffusion-based Model Inversion (Diff-MI)\nattacks. Specifically, we introduce a novel target-specific conditional\ndiffusion model (CDM) to purposely approximate target classifier's private\ndistribution and achieve superior accuracy-fidelity balance. Our method\ninvolves a two-step learning paradigm. Step-1 incorporates the target\nclassifier into the entire CDM learning under a pretrain-then-finetune fashion,\nwith creating pseudo-labels as model conditions in pretraining and adjusting\nspecified layers with image predictions in fine-tuning. Step-2 presents an\niterative image reconstruction method, further enhancing the attack performance\nthrough a combination of diffusion priors and target knowledge. Additionally,\nwe propose an improved max-margin loss that replaces the hard max with top-k\nmaxes, fully leveraging feature information and soft labels from the target\nclassifier. Extensive experiments demonstrate that Diff-MI significantly\nimproves generative fidelity with an average decrease of 20\\% in FID while\nmaintaining competitive attack accuracy compared to state-of-the-art methods\nacross various datasets and models. Our code is available at:\n\\url{https://github.com/Ouxiang-Li/Diff-MI}.\n","authors":["Ouxiang Li","Yanbin Hao","Zhicai Wang","Bin Zhu","Shuo Wang","Zaixi Zhang","Fuli Feng"],"pdf_url":"https://arxiv.org/pdf/2407.11424v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14729v2","updated":"2024-11-21T12:17:29Z","published":"2024-10-16T07:13:35Z","title":"Is Less More? Exploring Token Condensation as Training-free Adaptation\n  for CLIP","summary":"  Contrastive language-image pre-training (CLIP) has shown remarkable\ngeneralization ability in image classification. However, CLIP sometimes\nencounters performance drops on downstream datasets during zero-shot inference.\nTest-time adaptation methods attempt to mitigate this by adjusting\nnormalization layers or tuning context prompts with large batch sizes and\nextensive augmentations; yet, these methods are computationally intensive. This\nraises an important question: Is there a training-free approach that can\nefficiently address CLIP's performance drop in such cases? To explore this, we\nbenchmark token condensation techniques, originally designed to enhance the\nefficiency of vision transformers, on CLIP zero-shot inference tasks. We\nobserve that although token condensation may compromise in-domain accuracy, it\nsurprisingly enhances CLIP's performance on certain cross-dataset benchmarks.\nThis motivates two key inquiries: (1) Can token condensation serve as a\n\"free-lunch\" solution for CLIP zero-shot inference? (2) What criteria should\nguide condensation -- how can essential tokens be identified and redundant ones\neliminated? To address these questions, we propose Token Condensation as\nAdaptation (TCA), a training-free adaptation method for CLIP by pruning\nclass-irrelevant visual tokens while merging class-ambiguous tokens. As the\nfirst approach for CLIP's token efficiency, TCA demonstrates superior\nperformance across cross-dataset tasks, achieving up to a 21.4\\% improvement\nover the strongest baseline while reducing GFLOPs by 12.2\\% to 48.9\\%, with\nminimized hyperparameter dependency.\n","authors":["Zixin Wang","Dong Gong","Sen Wang","Zi Huang","Yadan Luo"],"pdf_url":"https://arxiv.org/pdf/2410.14729v2.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2411.14062v1","updated":"2024-11-21T12:16:16Z","published":"2024-11-21T12:16:16Z","title":"MMGenBench: Evaluating the Limits of LMMs from the Text-to-Image\n  Generation Perspective","summary":"  Large Multimodal Models (LMMs) have demonstrated remarkable capabilities.\nWhile existing benchmarks for evaluating LMMs mainly focus on image\ncomprehension, few works evaluate them from the image generation perspective.\nTo address this issue, we propose a straightforward automated evaluation\npipeline. Specifically, this pipeline requires LMMs to generate an image-prompt\nfrom a given input image. Subsequently, it employs text-to-image generative\nmodels to create a new image based on these generated prompts. Finally, we\nevaluate the performance of LMMs by comparing the original image with the\ngenerated one. Furthermore, we introduce MMGenBench-Test, a comprehensive\nbenchmark developed to evaluate LMMs across 13 distinct image patterns, and\nMMGenBench-Domain, targeting the performance evaluation of LMMs within the\ngenerative image domain. A thorough evaluation involving over 50 popular LMMs\ndemonstrates the effectiveness and reliability in both the pipeline and\nbenchmark. Our observations indicate that numerous LMMs excelling in existing\nbenchmarks fail to adequately complete the basic tasks, related to image\nunderstanding and description. This finding highlights the substantial\npotential for performance improvement in current LMMs and suggests avenues for\nfuture model optimization. Concurrently, our pipeline facilitates the efficient\nassessment of LMMs performance across diverse domains by using solely image\ninputs.\n","authors":["Hailang Huang","Yong Wang","Zixuan Huang","Huaqiu Li","Tongwen Huang","Xiangxiang Chu","Richong Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14062v1.pdf","comment":"This project is available at: https://github.com/lerogo/MMGenBench"},{"id":"http://arxiv.org/abs/2207.13021v3","updated":"2024-11-21T12:08:39Z","published":"2022-06-06T07:04:05Z","title":"CTVR-EHO TDA-IPH Topological Optimized Convolutional Visual Recurrent\n  Network for Brain Tumor Segmentation and Classification","summary":"  In today's world of health care, brain tumor detection has become common.\nHowever, the manual brain tumor classification approach is time-consuming. So\nDeep Convolutional Neural Network (DCNN) is used by many researchers in the\nmedical field for making accurate diagnoses and aiding in the patient's\ntreatment. The traditional techniques have problems such as overfitting and the\ninability to extract necessary features. To overcome these problems, we\ndeveloped the Topological Data Analysis based Improved Persistent Homology\n(TDA-IPH) and Convolutional Transfer learning and Visual Recurrent learning\nwith Elephant Herding Optimization hyper-parameter tuning (CTVR-EHO) models for\nbrain tumor segmentation and classification. Initially, the Topological Data\nAnalysis based Improved Persistent Homology is designed to segment the brain\ntumor image. Then, from the segmented image, features are extracted using TL\nvia the AlexNet model and Bidirectional Visual Long Short-Term Memory\n(Bi-VLSTM). Next, elephant Herding Optimization (EHO) is used to tune the\nhyperparameters of both networks to get an optimal result. Finally, extracted\nfeatures are concatenated and classified using the softmax activation layer.\nThe simulation result of this proposed CTVR-EHO and TDA-IPH method is analyzed\nbased on precision, accuracy, recall, loss, and F score metrics. When compared\nto other existing brain tumor segmentation and classification models, the\nproposed CTVR-EHO and TDA-IPH approaches show high accuracy (99.8%), high\nrecall (99.23%), high precision (99.67%), and high F score (99.59%).\n","authors":["Dhananjay Joshi","Bhupesh Kumar Singh","Kapil Kumar Nagwanshi","Nitin S. Choubey"],"pdf_url":"https://arxiv.org/pdf/2207.13021v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06313v4","updated":"2024-11-21T12:06:59Z","published":"2023-10-10T05:13:17Z","title":"Advancing Pose-Guided Image Synthesis with Progressive Conditional\n  Diffusion Models","summary":"  Recent work has showcased the significant potential of diffusion models in\npose-guided person image synthesis. However, owing to the inconsistency in pose\nbetween the source and target images, synthesizing an image with a distinct\npose, relying exclusively on the source image and target pose information,\nremains a formidable challenge. This paper presents Progressive Conditional\nDiffusion Models (PCDMs) that incrementally bridge the gap between person\nimages under the target and source poses through three stages. Specifically, in\nthe first stage, we design a simple prior conditional diffusion model that\npredicts the global features of the target image by mining the global alignment\nrelationship between pose coordinates and image appearance. Then, the second\nstage establishes a dense correspondence between the source and target images\nusing the global features from the previous stage, and an inpainting\nconditional diffusion model is proposed to further align and enhance the\ncontextual features, generating a coarse-grained person image. In the third\nstage, we propose a refining conditional diffusion model to utilize the\ncoarsely generated image from the previous stage as a condition, achieving\ntexture restoration and enhancing fine-detail consistency. The three-stage\nPCDMs work progressively to generate the final high-quality and high-fidelity\nsynthesized image. Both qualitative and quantitative results demonstrate the\nconsistency and photorealism of our proposed PCDMs under challenging\nscenarios.The code and model will be available at\nhttps://github.com/tencent-ailab/PCDMs.\n","authors":["Fei Shen","Hu Ye","Jun Zhang","Cong Wang","Xiao Han","Wei Yang"],"pdf_url":"https://arxiv.org/pdf/2310.06313v4.pdf","comment":"Accepted to ICLR 2024. The final version is available at OpenReview:\n  https://openreview.net/forum?id=rHzapPnCgT"},{"id":"http://arxiv.org/abs/2411.14053v1","updated":"2024-11-21T11:59:04Z","published":"2024-11-21T11:59:04Z","title":"Stereo Anything: Unifying Stereo Matching with Large-Scale Mixed Data","summary":"  Stereo matching has been a pivotal component in 3D vision, aiming to find\ncorresponding points between pairs of stereo images to recover depth\ninformation. In this work, we introduce StereoAnything, a highly practical\nsolution for robust stereo matching. Rather than focusing on a specialized\nmodel, our goal is to develop a versatile foundational model capable of\nhandling stereo images across diverse environments. To this end, we scale up\nthe dataset by collecting labeled stereo images and generating synthetic stereo\npairs from unlabeled monocular images. To further enrich the model's ability to\ngeneralize across different conditions, we introduce a novel synthetic dataset\nthat complements existing data by adding variability in baselines, camera\nangles, and scene types. We extensively evaluate the zero-shot capabilities of\nour model on five public datasets, showcasing its impressive ability to\ngeneralize to new, unseen data. Code will be available at\n\\url{https://github.com/XiandaGuo/OpenStereo}.\n","authors":["Xianda Guo","Chenming Zhang","Youmin Zhang","Dujun Nie","Ruilin Wang","Wenzhao Zheng","Matteo Poggi","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2411.14053v1.pdf","comment":"Code will be available at\n  \\url{https://github.com/XiandaGuo/OpenStereo}"},{"id":"http://arxiv.org/abs/2411.14049v1","updated":"2024-11-21T11:56:32Z","published":"2024-11-21T11:56:32Z","title":"Out-Of-Distribution Detection with Diversification (Provably)","summary":"  Out-of-distribution (OOD) detection is crucial for ensuring reliable\ndeployment of machine learning models. Recent advancements focus on utilizing\neasily accessible auxiliary outliers (e.g., data from the web or other\ndatasets) in training. However, we experimentally reveal that these methods\nstill struggle to generalize their detection capabilities to unknown OOD data,\ndue to the limited diversity of the auxiliary outliers collected. Therefore, we\nthoroughly examine this problem from the generalization perspective and\ndemonstrate that a more diverse set of auxiliary outliers is essential for\nenhancing the detection capabilities. However, in practice, it is difficult and\ncostly to collect sufficiently diverse auxiliary outlier data. Therefore, we\npropose a simple yet practical approach with a theoretical guarantee, termed\nDiversity-induced Mixup for OOD detection (diverseMix), which enhances the\ndiversity of auxiliary outlier set for training in an efficient way. Extensive\nexperiments show that diverseMix achieves superior performance on commonly used\nand recent challenging large-scale benchmarks, which further confirm the\nimportance of the diversity of auxiliary outliers.\n","authors":["Haiyun Yao","Zongbo Han","Huazhu Fu","Xi Peng","Qinghua Hu","Changqing Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14039v1","updated":"2024-11-21T11:41:42Z","published":"2024-11-21T11:41:42Z","title":"Uterine Ultrasound Image Captioning Using Deep Learning Techniques","summary":"  Medical imaging has significantly revolutionized medical diagnostics and\ntreatment planning, progressing from early X-ray usage to sophisticated methods\nlike MRIs, CT scans, and ultrasounds. This paper investigates the use of deep\nlearning for medical image captioning, with a particular focus on uterine\nultrasound images. These images are vital in obstetrics and gynecology for\ndiagnosing and monitoring various conditions across different age groups.\nHowever, their interpretation is often challenging due to their complexity and\nvariability. To address this, a deep learning-based medical image captioning\nsystem was developed, integrating Convolutional Neural Networks with a\nBidirectional Gated Recurrent Unit network. This hybrid model processes both\nimage and text features to generate descriptive captions for uterine ultrasound\nimages. Our experimental results demonstrate the effectiveness of this approach\nover baseline methods, with the proposed model achieving superior performance\nin generating accurate and informative captions, as indicated by higher BLEU\nand ROUGE scores. By enhancing the interpretation of uterine ultrasound images,\nour research aims to assist medical professionals in making timely and accurate\ndiagnoses, ultimately contributing to improved patient care.\n","authors":["Abdennour Boulesnane","Boutheina Mokhtari","Oumnia Rana Segueni","Slimane Segueni"],"pdf_url":"https://arxiv.org/pdf/2411.14039v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14017v1","updated":"2024-11-21T11:01:03Z","published":"2024-11-21T11:01:03Z","title":"Automatic brain tumor segmentation in 2D intra-operative ultrasound\n  images using MRI tumor annotations","summary":"  Automatic segmentation of brain tumors in intra-operative ultrasound (iUS)\nimages could facilitate localization of tumor tissue during resection surgery.\nThe lack of large annotated datasets limits the current models performances. In\nthis paper, we investigate the use of tumor annotations in pre-operative MRI\nimages, which are more easily accessible than annotations in iUS images, for\ntraining of deep learning models for iUS brain tumor segmentation. We used 180\nannotated pre-operative MRI images with corresponding unannotated iUS images,\nand 29 annotated iUS images. Image registration was performed to transfer the\nMRI annotations to the corresponding iUS images before training models with the\nnnU-Net framework. To validate the use of MRI labels, the models were compared\nto a model trained with only US annotated tumors, and a model with both US and\nMRI annotated tumors. In addition, the results were compared to annotations\nvalidated by an expert neurosurgeon on the same test set to measure\ninter-observer variability. The results showed similar performance for a model\ntrained with only MRI annotated tumors, compared to a model trained with only\nUS annotated tumors. The model trained using both modalities obtained slightly\nbetter results with an average Dice score of 0.62, where external expert\nannotations achieved a score of 0.67. The results also showed that the deep\nlearning models were comparable to expert annotation for larger tumors (> 200\nmm2), but perform clearly worse for smaller tumors (< 200 mm2). This shows that\nMRI tumor annotations can be used as a substitute for US tumor annotations to\ntrain a deep learning model for automatic brain tumor segmentation in\nintra-operative ultrasound images. Small tumors is a limitation for the current\nmodels and will be the focus of future work. The main models are available\nhere: https://github.com/mathildefaanes/us_brain_tumor_segmentation.\n","authors":["Mathilde Faanes","Ragnhild Holden Helland","Ole Solheim","Ingerid Reinertsen"],"pdf_url":"https://arxiv.org/pdf/2411.14017v1.pdf","comment":"19, 8 figures, submitted to International Journal of Computer\n  Assisted Radiology and Surgery"},{"id":"http://arxiv.org/abs/2411.09263v2","updated":"2024-11-21T10:46:18Z","published":"2024-11-14T08:02:14Z","title":"Rethinking Weight-Averaged Model-merging","summary":"  Weight-averaged model-merging has emerged as a powerful approach in deep\nlearning, capable of enhancing model performance without fine-tuning or\nretraining. However, the underlying mechanisms that explain its effectiveness\nremain largely unexplored. In this paper, we investigate this technique from\nthree novel perspectives to provide deeper insights into how and why\nweight-averaged model-merging works: (1) we examine the intrinsic patterns\ncaptured by the learning of the model weights, through the visualizations of\ntheir patterns on several datasets, showing that these weights often encode\nstructured and interpretable patterns; (2) we investigate model ensemble\nmerging strategies based on averaging on weights versus averaging on features,\nproviding detailed analyses across diverse architectures and datasets; and (3)\nwe explore the impact on model-merging prediction stability in terms of\nchanging the parameter magnitude, revealing insights into the way of weight\naveraging works as regularization by showing the robustness across different\nparameter scales. Our findings shed light on the \"black box\" of weight-averaged\nmodel-merging, offering valuable insights and practical recommendations that\nadvance the model-merging process.\n","authors":["Hu Wang","Congbo Ma","Ibrahim Almakky","Ian Reid","Gustavo Carneiro","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2411.09263v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14006v1","updated":"2024-11-21T10:41:24Z","published":"2024-11-21T10:41:24Z","title":"Experimental comparison of graph-based approximate nearest neighbor\n  search algorithms on edge devices","summary":"  In this paper, we present an experimental comparison of various graph-based\napproximate nearest neighbor (ANN) search algorithms deployed on edge devices\nfor real-time nearest neighbor search applications, such as smart city\ninfrastructure and autonomous vehicles. To the best of our knowledge, this\nspecific comparative analysis has not been previously conducted. While existing\nresearch has explored graph-based ANN algorithms, it has often been limited to\nsingle-threaded implementations on standard commodity hardware. Our study\nleverages the full computational and storage capabilities of edge devices,\nincorporating additional metrics such as insertion and deletion latency of new\nvectors and power consumption. This comprehensive evaluation aims to provide\nvaluable insights into the performance and suitability of these algorithms for\nedge-based real-time tracking systems enhanced by nearest-neighbor search\nalgorithms.\n","authors":["Ali Ganbarov","Jicheng Yuan","Anh Le-Tuan","Manfred Hauswirth","Danh Le-Phuoc"],"pdf_url":"https://arxiv.org/pdf/2411.14006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14002v1","updated":"2024-11-21T10:37:54Z","published":"2024-11-21T10:37:54Z","title":"SEMPose: A Single End-to-end Network for Multi-object Pose Estimation","summary":"  In computer vision, estimating the six-degree-of-freedom pose from an RGB\nimage is a fundamental task. However, this task becomes highly challenging in\nmulti-object scenes. Currently, the best methods typically employ an indirect\nstrategy, which identifies 2D and 3D correspondences, and then solves with the\nPerspective-n-Points method. Yet, this approach cannot be trained end-to-end.\nDirect methods, on the other hand, suffer from lower accuracy due to challenges\nsuch as varying object sizes and occlusions. To address these issues, we\npropose SEMPose, an end-to-end multi-object pose estimation network. SEMPose\nutilizes a well-designed texture-shape guided feature pyramid network,\neffectively tackling the challenge of object size variations. Additionally, it\nemploys an iterative refinement head structure, progressively regressing\nrotation and translation separately to enhance estimation accuracy. During\ntraining, we alleviate the impact of occlusion by selecting positive samples\nfrom visible parts. Experimental results demonstrate that SEMPose can perform\ninference at 32 FPS without requiring inputs other than the RGB image. It can\naccurately estimate the poses of multiple objects in real time, with inference\ntime unaffected by the number of target objects. On the LM-O and YCB-V\ndatasets, our method outperforms other RGB-based single-model methods,\nachieving higher accuracy. Even when compared with multi-model methods and\napproaches that use additional refinement, our results remain competitive.\n","authors":["Xin Liu","Hao Wang","Shibei Xue","Dezong Zhao"],"pdf_url":"https://arxiv.org/pdf/2411.14002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14001v1","updated":"2024-11-21T10:35:16Z","published":"2024-11-21T10:35:16Z","title":"Graph Domain Adaptation with Dual-branch Encoder and Two-level Alignment\n  for Whole Slide Image-based Survival Prediction","summary":"  In recent years, histopathological whole slide image (WSI)- based survival\nanalysis has attracted much attention in medical image analysis. In practice,\nWSIs usually come from different hospitals or laboratories, which can be seen\nas different domains, and thus may have significant differences in imaging\nequipment, processing procedures, and sample sources. These differences\ngenerally result in large gaps in distribution between different WSI domains,\nand thus the survival analysis models trained on one domain may fail to\ntransfer to another. To address this issue, we propose a Dual-branch Encoder\nand Two-level Alignment (DETA) framework to explore both feature and\ncategory-level alignment between different WSI domains. Specifically, we first\nformulate the concerned problem as graph domain adaptation (GDA) by virtue the\ngraph representation of WSIs. Then we construct a dual-branch graph encoder,\nincluding the message passing branch and the shortest path branch, to\nexplicitly and implicitly extract semantic information from the\ngraph-represented WSIs. To realize GDA, we propose a two-level alignment\napproach: at the category level, we develop a coupling technique by virtue of\nthe dual-branch structure, leading to reduced divergence between the category\ndistributions of the two domains; at the feature level, we introduce an\nadversarial perturbation strategy to better augment source domain feature,\nresulting in improved alignment in feature distribution. To the best of our\nknowledge, our work is the first attempt to alleviate the domain shift issue\nfor WSI data analysis. Extensive experiments on four TCGA datasets have\nvalidated the effectiveness of our proposed DETA framework and demonstrated its\nsuperior performance in WSI-based survival analysis.\n","authors":["Yuntao Shou","Peiqiang Yan","Xingjian Yuan","Xiangyong Cao","Qian Zhao","Deyu Meng"],"pdf_url":"https://arxiv.org/pdf/2411.14001v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2411.13997v1","updated":"2024-11-21T10:23:00Z","published":"2024-11-21T10:23:00Z","title":"Mirror Target YOLO: An Improved YOLOv8 Method with Indirect Vision for\n  Heritage Buildings Fire Detection","summary":"  Fires can cause severe damage to heritage buildings, making timely fire\ndetection essential. Traditional dense cabling and drilling can harm these\nstructures, so reducing the number of cameras to minimize such impact is\nchallenging. Additionally, avoiding false alarms due to noise sensitivity and\npreserving the expertise of managers in fire-prone areas is crucial. To address\nthese needs, we propose a fire detection method based on indirect vision,\ncalled Mirror Target YOLO (MITA-YOLO). MITA-YOLO integrates indirect vision\ndeployment and an enhanced detection module. It uses mirror angles to achieve\nindirect views, solving issues with limited visibility in irregular spaces and\naligning each indirect view with the target monitoring area. The Target-Mask\nmodule is designed to automatically identify and isolate the indirect vision\nareas in each image, filtering out non-target areas. This enables the model to\ninherit managers' expertise in assessing fire-risk zones, improving focus and\nresistance to interference in fire detection.In our experiments, we created an\n800-image fire dataset with indirect vision. Results show that MITA-YOLO\nsignificantly reduces camera requirements while achieving superior detection\nperformance compared to other mainstream models.\n","authors":["Jian Liang","JunSheng Cheng"],"pdf_url":"https://arxiv.org/pdf/2411.13997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13982v1","updated":"2024-11-21T09:47:13Z","published":"2024-11-21T09:47:13Z","title":"Safety Without Semantic Disruptions: Editing-free Safe Image Generation\n  via Context-preserving Dual Latent Reconstruction","summary":"  Training multimodal generative models on large, uncurated datasets can result\nin users being exposed to harmful, unsafe and controversial or\nculturally-inappropriate outputs. While model editing has been proposed to\nremove or filter undesirable concepts in embedding and latent spaces, it can\ninadvertently damage learned manifolds, distorting concepts in close semantic\nproximity. We identify limitations in current model editing techniques, showing\nthat even benign, proximal concepts may become misaligned. To address the need\nfor safe content generation, we propose a modular, dynamic solution that\nleverages safety-context embeddings and a dual reconstruction process using\ntunable weighted summation in the latent space to generate safer images. Our\nmethod preserves global context without compromising the structural integrity\nof the learned manifolds. We achieve state-of-the-art results on safe image\ngeneration benchmarks, while offering controllable variation of model safety.\nWe identify trade-offs between safety and censorship, which presents a\nnecessary perspective in the development of ethical AI models. We will release\nour code.\n  Keywords: Text-to-Image Models, Generative AI, Safety, Reliability, Model\nEditing\n","authors":["Jordan Vice","Naveed Akhtar","Richard Hartley","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2411.13982v1.pdf","comment":"This research is supported by the NISDRG project #20100007, funded by\n  the Australian Government"},{"id":"http://arxiv.org/abs/2411.13981v1","updated":"2024-11-21T09:46:55Z","published":"2024-11-21T09:46:55Z","title":"On the Fairness, Diversity and Reliability of Text-to-Image Generative\n  Models","summary":"  The widespread availability of multimodal generative models has sparked\ncritical discussions on their fairness, reliability, and potential for misuse.\nWhile text-to-image models can produce high-fidelity, user-guided images, they\nalso exhibit unpredictable behavior and vulnerabilities, which can be exploited\nto manipulate class or concept representations. To address this, we propose an\nevaluation framework designed to assess model reliability through their\nresponses to globally- and locally-applied `semantic' perturbations in the\nembedding space, pinpointing inputs that trigger unreliable behavior. Our\napproach offers deeper insights into two essential aspects: (i) generative\ndiversity, evaluating the breadth of visual representations for learned\nconcepts, and (ii) generative fairness, examining how removing concepts from\ninput prompts affects semantic guidance. Beyond these evaluations, our method\nlays the groundwork for detecting unreliable, bias-injected models and\nretrieval of bias provenance. We will release our code.\n  Keywords: Fairness, Reliability, AI Ethics, Bias, Text-to-Image Models\n","authors":["Jordan Vice","Naveed Akhtar","Richard Hartley","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2411.13981v1.pdf","comment":"This research is supported by the NISDRG project #20100007, funded by\n  the Australian Government"},{"id":"http://arxiv.org/abs/2411.13975v1","updated":"2024-11-21T09:41:33Z","published":"2024-11-21T09:41:33Z","title":"Transforming Static Images Using Generative Models for Video Salient\n  Object Detection","summary":"  In many video processing tasks, leveraging large-scale image datasets is a\ncommon strategy, as image data is more abundant and facilitates comprehensive\nknowledge transfer. A typical approach for simulating video from static images\ninvolves applying spatial transformations, such as affine transformations and\nspline warping, to create sequences that mimic temporal progression. However,\nin tasks like video salient object detection, where both appearance and motion\ncues are critical, these basic image-to-video techniques fail to produce\nrealistic optical flows that capture the independent motion properties of each\nobject. In this study, we show that image-to-video diffusion models can\ngenerate realistic transformations of static images while understanding the\ncontextual relationships between image components. This ability allows the\nmodel to generate plausible optical flows, preserving semantic integrity while\nreflecting the independent motion of scene elements. By augmenting individual\nimages in this way, we create large-scale image-flow pairs that significantly\nenhance model training. Our approach achieves state-of-the-art performance\nacross all public benchmark datasets, outperforming existing approaches.\n","authors":["Suhwan Cho","Minhyeok Lee","Jungho Lee","Sangyoun Lee"],"pdf_url":"https://arxiv.org/pdf/2411.13975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13961v1","updated":"2024-11-21T09:16:51Z","published":"2024-11-21T09:16:51Z","title":"Zero-Shot Low-Light Image Enhancement via Joint Frequency Domain Priors\n  Guided Diffusion","summary":"  Due to the singularity of real-world paired datasets and the complexity of\nlow-light environments, this leads to supervised methods lacking a degree of\nscene generalisation. Meanwhile, limited by poor lighting and content guidance,\nexisting zero-shot methods cannot handle unknown severe degradation well. To\naddress this problem, we will propose a new zero-shot low-light enhancement\nmethod to compensate for the lack of light and structural information in the\ndiffusion sampling process by effectively combining the wavelet and Fourier\nfrequency domains to construct rich a priori information. The key to the\ninspiration comes from the similarity between the wavelet and Fourier frequency\ndomains: both light and structure information are closely related to specific\nfrequency domain regions, respectively. Therefore, by transferring the\ndiffusion process to the wavelet low-frequency domain and combining the wavelet\nand Fourier frequency domains by continuously decomposing them in the inverse\nprocess, the constructed rich illumination prior is utilised to guide the image\ngeneration enhancement process. Sufficient experiments show that the framework\nis robust and effective in various scenarios. The code will be available at:\n\\href{https://github.com/hejh8/Joint-Wavelet-and-Fourier-priors-guided-diffusion}{https://github.com/hejh8/Joint-Wavelet-and-Fourier-priors-guided-diffusion}.\n","authors":["Jinhong He","Shivakumara Palaiahnakote","Aoxiang Ning","Minglong Xue"],"pdf_url":"https://arxiv.org/pdf/2411.13961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00777v2","updated":"2024-11-21T09:10:23Z","published":"2024-06-02T15:33:46Z","title":"Diffusion Features to Bridge Domain Gap for Semantic Segmentation","summary":"  Pre-trained diffusion models have demonstrated remarkable proficiency in\nsynthesizing images across a wide range of scenarios with customizable prompts,\nindicating their effective capacity to capture universal features. Motivated by\nthis, our study delves into the utilization of the implicit knowledge embedded\nwithin diffusion models to address challenges in cross-domain semantic\nsegmentation. This paper investigates the approach that leverages the sampling\nand fusion techniques to harness the features of diffusion models efficiently.\nWe propose DIffusion Feature Fusion (DIFF) as a backbone use for extracting and\nintegrating effective semantic representations through the diffusion process.\nBy leveraging the strength of text-to-image generation capability, we introduce\na new training framework designed to implicitly learn posterior knowledge from\nit. Through rigorous evaluation in the contexts of domain generalization\nsemantic segmentation, we establish that our methodology surpasses preceding\napproaches in mitigating discrepancies across distinct domains and attains the\nstate-of-the-art (SOTA) benchmark.\n","authors":["Yuxiang Ji","Boyong He","Chenyuan Qu","Zhuoyue Tan","Chuan Qin","Liaoni Wu"],"pdf_url":"https://arxiv.org/pdf/2406.00777v2.pdf","comment":"The code is released at https://github.com/Yux1angJi/DIFF"},{"id":"http://arxiv.org/abs/2411.10346v2","updated":"2024-11-21T09:01:06Z","published":"2024-11-15T16:46:04Z","title":"BiDense: Binarization for Dense Prediction","summary":"  Dense prediction is a critical task in computer vision. However, previous\nmethods often require extensive computational resources, which hinders their\nreal-world application. In this paper, we propose BiDense, a generalized binary\nneural network (BNN) designed for efficient and accurate dense prediction\ntasks. BiDense incorporates two key techniques: the Distribution-adaptive\nBinarizer (DAB) and the Channel-adaptive Full-precision Bypass (CFB). The DAB\nadaptively calculates thresholds and scaling factors for binarization,\neffectively retaining more information within BNNs. Meanwhile, the CFB\nfacilitates full-precision bypassing for binary convolutional layers undergoing\nvarious channel size transformations, which enhances the propagation of\nreal-valued signals and minimizes information loss. By leveraging these\ntechniques, BiDense preserves more real-valued information, enabling more\naccurate and detailed dense predictions in BNNs. Extensive experiments\ndemonstrate that our framework achieves performance levels comparable to\nfull-precision models while significantly reducing memory usage and\ncomputational costs.\n","authors":["Rui Yin","Haotong Qin","Yulun Zhang","Wenbo Li","Yong Guo","Jianjun Zhu","Cheng Wang","Biao Jia"],"pdf_url":"https://arxiv.org/pdf/2411.10346v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13949v1","updated":"2024-11-21T09:00:15Z","published":"2024-11-21T09:00:15Z","title":"Separable Mixture of Low-Rank Adaptation for Continual Visual\n  Instruction Tuning","summary":"  Visual instruction tuning (VIT) enables multimodal large language models\n(MLLMs) to effectively handle a wide range of vision tasks by framing them as\nlanguage-based instructions. Building on this, continual visual instruction\ntuning (CVIT) extends the capability of MLLMs to incrementally learn new tasks,\naccommodating evolving functionalities. While prior work has advanced CVIT\nthrough the development of new benchmarks and approaches to mitigate\ncatastrophic forgetting, these efforts largely follow traditional continual\nlearning paradigms, neglecting the unique challenges specific to CVIT. We\nidentify a dual form of catastrophic forgetting in CVIT, where MLLMs not only\nforget previously learned visual understanding but also experience a decline in\ninstruction following abilities as they acquire new tasks. To address this, we\nintroduce the Separable Mixture of Low-Rank Adaptation (SMoLoRA) framework,\nwhich employs separable routing through two distinct modules - one for visual\nunderstanding and another for instruction following. This dual-routing design\nenables specialized adaptation in both domains, preventing forgetting while\nimproving performance. Furthermore, we propose a novel CVIT benchmark that goes\nbeyond existing benchmarks by additionally evaluating a model's ability to\ngeneralize to unseen tasks and handle diverse instructions across various\ntasks. Extensive experiments demonstrate that SMoLoRA outperforms existing\nmethods in mitigating dual forgetting, improving generalization to unseen\ntasks, and ensuring robustness in following diverse instructions.\n","authors":["Ziqi Wang","Chang Che","Qi Wang","Yangyang Li","Zenglin Shi","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2411.13949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13929v1","updated":"2024-11-21T08:27:18Z","published":"2024-11-21T08:27:18Z","title":"Transforming Engineering Diagrams: A Novel Approach for P&ID\n  Digitization using Transformers","summary":"  The digitization of complex technical systems, such as Piping and\nInstrumentation Diagrams (P&IDs), is crucial for efficient maintenance and\noperation of complex systems in hydraulic and process engineering. Previous\napproaches often rely on separate modules that analyze diagram elements\nindividually, neglecting the diagram's overall structure. We address this\nlimitation by proposing a novel approach that utilizes the Relationformer, a\nstate-of-the-art deep learning architecture, to extract graphs from P&IDs. Our\nmethod leverages the ability of the Relationformer to simultaneously detect\nobjects and their relationships in images, making it suitable for the task of\ngraph extraction from engineering diagrams. We apply our proposed approach to\nboth real-world and synthetically created P&ID datasets, and evaluate its\neffectiveness by comparing it with a modular digitization approach based on\nrecent literature. We present PID2Graph, the first publicly accessible P&ID\ndataset featuring comprehensive labels for the graph structure, including\nsymbols, nodes and their connections that is used for evaluation. To understand\nthe effect of patching and stitching of both of the approaches, we compare\nvalues before and after merging the patches. For the real-world data, the\nRelationformer achieves convincing results, outperforming the modular\ndigitization approach for edge detection by more than 25%. Our work provides a\ncomprehensive framework for assessing the performance of P&ID digitization\nmethods and opens up new avenues for research in this area using transformer\narchitectures. The P&ID dataset used for evaluation will be published and\npublicly available upon acceptance of the paper.\n","authors":["Jan Marius Stürmer","Marius Graumann","Tobias Koch"],"pdf_url":"https://arxiv.org/pdf/2411.13929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13927v1","updated":"2024-11-21T08:22:45Z","published":"2024-11-21T08:22:45Z","title":"Multimodal 3D Reasoning Segmentation with Complex Scenes","summary":"  The recent development in multimodal learning has greatly advanced the\nresearch in 3D scene understanding in various real-world tasks such as embodied\nAI. However, most existing work shares two typical constraints: 1) they are\nshort of reasoning ability for interaction and interpretation of human\nintension and 2) they focus on scenarios with single-category objects only\nwhich leads to over-simplified textual descriptions due to the negligence of\nmulti-object scenarios and spatial relations among objects. We bridge the\nresearch gaps by proposing a 3D reasoning segmentation task for multiple\nobjects in scenes. The task allows producing 3D segmentation masks and detailed\ntextual explanations as enriched by 3D spatial relations among objects. To this\nend, we create ReasonSeg3D, a large-scale and high-quality benchmark that\nintegrates 3D spatial relations with generated question-answer pairs and 3D\nsegmentation masks. In addition, we design MORE3D, a simple yet effective\nmethod that enables multi-object 3D reasoning segmentation with user questions\nand textual outputs. Extensive experiments show that MORE3D excels in reasoning\nand segmenting complex multi-object 3D scenes, and the created ReasonSeg3D\noffers a valuable platform for future exploration of 3D reasoning segmentation.\nThe dataset and code will be released.\n","authors":["Xueying Jiang","Lewei Lu","Ling Shao","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2411.13927v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16266v2","updated":"2024-11-21T08:16:41Z","published":"2024-08-29T05:05:02Z","title":"Inversion Circle Interpolation: Diffusion-based Image Augmentation for\n  Data-scarce Classification","summary":"  Data Augmentation (DA), i.e., synthesizing faithful and diverse samples to\nexpand the original training set, is a prevalent and effective strategy to\nimprove the performance of various data-scarce tasks. With the powerful image\ngeneration ability, diffusion-based DA has shown strong performance gains on\ndifferent image classification benchmarks. In this paper, we analyze today's\ndiffusion-based DA methods, and argue that they cannot take account of both\nfaithfulness and diversity, which are two critical keys for generating\nhigh-quality samples and boosting classification performance. To this end, we\npropose a novel Diffusion-based DA method: Diff-II. Specifically, it consists\nof three steps: 1) Category concepts learning: Learning concept embeddings for\neach category. 2) Inversion interpolation: Calculating the inversion for each\nimage, and conducting circle interpolation for two randomly sampled inversions\nfrom the same category. 3) Two-stage denoising: Using different prompts to\ngenerate synthesized images in a coarse-to-fine manner. Extensive experiments\non various data-scarce image classification tasks (e.g., few-shot, long-tailed,\nand out-of-distribution classification) have demonstrated its effectiveness\nover state-of-the-art diffusion-based DA methods.\n","authors":["Yanghao Wang","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2408.16266v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13918v1","updated":"2024-11-21T08:13:24Z","published":"2024-11-21T08:13:24Z","title":"Quantization without Tears","summary":"  Deep neural networks, while achieving remarkable success across diverse\ntasks, demand significant resources, including computation, GPU memory,\nbandwidth, storage, and energy. Network quantization, as a standard compression\nand acceleration technique, reduces storage costs and enables potential\ninference acceleration by discretizing network weights and activations into a\nfinite set of integer values. However, current quantization methods are often\ncomplex and sensitive, requiring extensive task-specific hyperparameters, where\neven a single misconfiguration can impair model performance, limiting\ngenerality across different models and tasks. In this paper, we propose\nQuantization without Tears (QwT), a method that simultaneously achieves\nquantization speed, accuracy, simplicity, and generality. The key insight of\nQwT is to incorporate a lightweight additional structure into the quantized\nnetwork to mitigate information loss during quantization. This structure\nconsists solely of a small set of linear layers, keeping the method simple and\nefficient. More importantly, it provides a closed-form solution, allowing us to\nimprove accuracy effortlessly under 2 minutes. Extensive experiments across\nvarious vision, language, and multimodal tasks demonstrate that QwT is both\nhighly effective and versatile. In fact, our approach offers a robust solution\nfor network quantization that combines simplicity, accuracy, and adaptability,\nwhich provides new insights for the design of novel quantization paradigms.\n","authors":["Minghao Fu","Hao Yu","Jie Shao","Junjie Zhou","Ke Zhu","Jianxin Wu"],"pdf_url":"https://arxiv.org/pdf/2411.13918v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07315v2","updated":"2024-11-21T08:08:06Z","published":"2024-07-10T02:24:43Z","title":"CosmoCLIP: Generalizing Large Vision-Language Models for Astronomical\n  Imaging","summary":"  Existing vision-text contrastive learning models enhance representation\ntransferability and support zero-shot prediction by matching paired image and\ncaption embeddings while pushing unrelated pairs apart. However, astronomical\nimage-label datasets are significantly smaller compared to general image and\nlabel datasets available from the internet. We introduce CosmoCLIP, an\nastronomical image-text contrastive learning framework precisely fine-tuned on\nthe pre-trained CLIP model using SpaceNet and BLIP-based captions. SpaceNet,\nattained via FLARE, constitutes ~13k optimally distributed images, while BLIP\nacts as a rich knowledge extractor. The rich semantics derived from this\nSpaceNet and BLIP descriptions, when learned contrastively, enable CosmoCLIP to\nachieve superior generalization across various in-domain and out-of-domain\ntasks. Our results demonstrate that CosmoCLIP is a straightforward yet powerful\nframework, significantly outperforming CLIP in zero-shot classification and\nimage-text retrieval tasks.\n","authors":["Raza Imam","Mohammed Talha Alam","Umaima Rahman","Mohsen Guizani","Fakhri Karray"],"pdf_url":"https://arxiv.org/pdf/2407.07315v2.pdf","comment":"Accepted at SPAICE Conference, ECSAT, UK, 2024"},{"id":"http://arxiv.org/abs/2407.05771v3","updated":"2024-11-21T07:55:36Z","published":"2024-07-08T09:27:34Z","title":"Multi-times Monte Carlo Rendering for Inter-reflection Reconstruction","summary":"  Inverse rendering methods have achieved remarkable performance in\nreconstructing high-fidelity 3D objects with disentangled geometries,\nmaterials, and environmental light. However, they still face huge challenges in\nreflective surface reconstruction. Although recent methods model the light\ntrace to learn specularity, the ignorance of indirect illumination makes it\nhard to handle inter-reflections among multiple smooth objects. In this work,\nwe propose Ref-MC2 that introduces the multi-time Monte Carlo sampling which\ncomprehensively computes the environmental illumination and meanwhile considers\nthe reflective light from object surfaces. To address the computation challenge\nas the times of Monte Carlo sampling grow, we propose a specularity-adaptive\nsampling strategy, significantly reducing the computational complexity. Besides\nthe computational resource, higher geometry accuracy is also required because\ngeometric errors accumulate multiple times. Therefore, we further introduce a\nreflection-aware surface model to initialize the geometry and refine it during\ninverse rendering. We construct a challenging dataset containing scenes with\nmultiple objects and inter-reflections. Experiments show that our method\noutperforms other inverse rendering methods on various object groups. We also\nshow downstream applications, e.g., relighting and material editing, to\nillustrate the disentanglement ability of our method.\n","authors":["Tengjie Zhu","Zhuo Chen","Jingnan Gao","Yichao Yan","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2407.05771v3.pdf","comment":"10 pages,6 figures, Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.13909v1","updated":"2024-11-21T07:47:27Z","published":"2024-11-21T07:47:27Z","title":"Panther: Illuminate the Sight of Multimodal LLMs with Instruction-Guided\n  Visual Prompts","summary":"  Multimodal large language models (MLLMs) are closing the gap to human visual\nperception capability rapidly, while, still lag behind on attending to subtle\nimages details or locating small objects precisely, etc. Common schemes to\ntackle these issues include deploying multiple vision encoders or operating on\noriginal high-resolution images. Few studies have concentrated on taking the\ntextual instruction into improving visual representation, resulting in losing\nfocus in some vision-centric tasks, a phenomenon we herein termed as Amblyopia.\nIn this work, we introduce Panther, a MLLM that closely adheres to user\ninstruction and locates targets of interests precisely, with the finesse of a\nblack panther. Specifically, Panther comprises three integral components:\nPanther-VE, Panther-Bridge, and Panther-Decoder. Panther-VE integrates user\ninstruction information at the early stages of the vision encoder, thereby\nextracting the most relevant and useful visual representations. The\nPanther-Bridge module, equipped with powerful filtering capabilities,\nsignificantly reduces redundant visual information, leading to a substantial\nsavings in training costs. The Panther-Decoder is versatile and can be employed\nwith any decoder-only architecture of LLMs without discrimination. Experimental\nresults, particularly on vision-centric benchmarks, have demonstrated the\neffectiveness of Panther.\n","authors":["Honglin Li","Yuting Gao","Chenglu Zhu","Jingdong Chen","Ming Yang","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2411.13909v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19513v4","updated":"2024-11-21T07:39:33Z","published":"2024-04-30T12:45:41Z","title":"A Smartphone-Based Method for Assessing Tomato Nutrient Status through\n  Trichome Density Measurement","summary":"  Early detection of fertilizer-induced stress in tomato plants is crucial for\noptimizing crop yield through timely management interventions. While\nconventional optical methods struggle to detect fertilizer stress in young\nleaves, these leaves contain valuable diagnostic information through their\nmicroscopic hair-like structures, particularly trichomes, which existing\napproaches have overlooked. This study introduces a smartphone-based\nnoninvasive technique that leverages mobile computing and digital imaging\ncapabilities to quantify trichome density on young leaves with superior\ndetection latency. Our method uniquely combines augmented reality technology\nwith image processing algorithms to analyze trichomes transferred onto\nspecialized measurement paper. A robust automated pipeline processes these\nimages through region extraction, perspective transformation, and illumination\ncorrection to precisely quantify trichome density. Validation experiments on\nhydroponically grown tomatoes under varying fertilizer conditions demonstrated\nthe method's effectiveness. Leave-one-out cross-validation revealed strong\npredictive performance with the area under the precision-recall curve (PR-AUC:\n0.82) and area under the receiver operating characteristic curve (ROC-AUC:\n0.64), while the predicted and observed trichome densities exhibited high\ncorrelation ($r = 0.79$). This innovative approach transforms smartphones into\nprecise diagnostic tools for plant nutrition assessment, offering a practical,\ncost-effective solution for precision agriculture.\n","authors":["Sho Ueda","Xujun Ye"],"pdf_url":"https://arxiv.org/pdf/2404.19513v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13901v1","updated":"2024-11-21T07:27:45Z","published":"2024-11-21T07:27:45Z","title":"Dressing the Imagination: A Dataset for AI-Powered Translation of Text\n  into Fashion Outfits and A Novel KAN Adapter for Enhanced Feature Adaptation","summary":"  Specialized datasets that capture the fashion industry's rich language and\nstyling elements can boost progress in AI-driven fashion design. We present\nFLORA (Fashion Language Outfit Representation for Apparel Generation), the\nfirst comprehensive dataset containing 4,330 curated pairs of fashion outfits\nand corresponding textual descriptions. Each description utilizes\nindustry-specific terminology and jargon commonly used by professional fashion\ndesigners, providing precise and detailed insights into the outfits. Hence, the\ndataset captures the delicate features and subtle stylistic elements necessary\nto create high-fidelity fashion designs. We demonstrate that fine-tuning\ngenerative models on the FLORA dataset significantly enhances their capability\nto generate accurate and stylistically rich images from textual descriptions of\nfashion sketches. FLORA will catalyze the creation of advanced AI models\ncapable of comprehending and producing subtle, stylistically rich fashion\ndesigns. It will also help fashion designers and end-users to bring their ideas\nto life.\n  As a second orthogonal contribution, we introduce KAN Adapters, which\nleverage Kolmogorov-Arnold Networks (KAN) as adaptive modules. They serve as\nreplacements for traditional MLP-based LoRA adapters. With learnable\nspline-based activations, KAN Adapters excel in modeling complex, non-linear\nrelationships, achieving superior fidelity, faster convergence and semantic\nalignment. Extensive experiments and ablation studies on our proposed FLORA\ndataset validate the superiority of KAN Adapters over LoRA adapters. To foster\nfurther research and collaboration, we will open-source both the FLORA and our\nimplementation code.\n","authors":["Gayatri Deshmukh","Somsubhra De","Chirag Sehgal","Jishu Sen Gupta","Sparsh Mittal"],"pdf_url":"https://arxiv.org/pdf/2411.13901v1.pdf","comment":"Under review at a conference"},{"id":"http://arxiv.org/abs/2410.02592v4","updated":"2024-11-21T07:16:05Z","published":"2024-10-03T15:34:41Z","title":"IC3M: In-Car Multimodal Multi-object Monitoring for Abnormal Status of\n  Both Driver and Passengers","summary":"  Recently, in-car monitoring has emerged as a promising technology for\ndetecting early-stage abnormal status of the driver and providing timely alerts\nto prevent traffic accidents. Although training models with multimodal data\nenhances the reliability of abnormal status detection, the scarcity of labeled\ndata and the imbalance of class distribution impede the extraction of critical\nabnormal state features, significantly deteriorating training performance.\nFurthermore, missing modalities due to environment and hardware limitations\nfurther exacerbate the challenge of abnormal status identification. More\nimportantly, monitoring abnormal health conditions of passengers, particularly\nin elderly care, is of paramount importance but remains underexplored. To\naddress these challenges, we introduce our IC3M, an efficient\ncamera-rotation-based multimodal framework for monitoring both driver and\npassengers in a car. Our IC3M comprises two key modules: an adaptive threshold\npseudo-labeling strategy and a missing modality reconstruction. The former\ncustomizes pseudo-labeling thresholds for different classes based on the class\ndistribution, generating class-balanced pseudo labels to guide model training\neffectively, while the latter leverages crossmodality relationships learned\nfrom limited labels to accurately recover missing modalities by distribution\ntransferring from available modalities. Extensive experimental results\ndemonstrate that IC3M outperforms state-of-the-art benchmarks in accuracy,\nprecision, and recall while exhibiting superior robustness under limited\nlabeled data and severe missing modality.\n","authors":["Zihan Fang","Zheng Lin","Senkang Hu","Hangcheng Cao","Yiqin Deng","Xianhao Chen","Yuguang Fang"],"pdf_url":"https://arxiv.org/pdf/2410.02592v4.pdf","comment":"16 pages, 17 figures"},{"id":"http://arxiv.org/abs/2310.00616v2","updated":"2024-11-21T07:11:21Z","published":"2023-10-01T08:35:46Z","title":"Towards Understanding Adversarial Transferability in Federated Learning","summary":"  We investigate a specific security risk in FL: a group of malicious clients\nhas impacted the model during training by disguising their identities and\nacting as benign clients but later switching to an adversarial role. They use\ntheir data, which was part of the training set, to train a substitute model and\nconduct transferable adversarial attacks against the federated model. This type\nof attack is subtle and hard to detect because these clients initially appear\nto be benign.\n  The key question we address is: How robust is the FL system to such covert\nattacks, especially compared to traditional centralized learning systems? We\nempirically show that the proposed attack imposes a high security risk to\ncurrent FL systems. By using only 3\\% of the client's data, we achieve the\nhighest attack rate of over 80\\%. To further offer a full understanding of the\nchallenges the FL system faces in transferable attacks, we provide a\ncomprehensive analysis over the transfer robustness of FL across a spectrum of\nconfigurations. Surprisingly, FL systems show a higher level of robustness than\ntheir centralized counterparts, especially when both systems are equally good\nat handling regular, non-malicious data.\n  We attribute this increased robustness to two main factors: 1) Decentralized\nData Training: Each client trains the model on its own data, reducing the\noverall impact of any single malicious client. 2) Model Update Averaging: The\nupdates from each client are averaged together, further diluting any malicious\nalterations. Both practical experiments and theoretical analysis support our\nconclusions. This research not only sheds light on the resilience of FL systems\nagainst hidden attacks but also raises important considerations for their\nfuture application and development.\n","authors":["Yijiang Li","Ying Gao","Haohan Wang"],"pdf_url":"https://arxiv.org/pdf/2310.00616v2.pdf","comment":"Published in Transactions on Machine Learning Research (TMLR)\n  (11/2024)"},{"id":"http://arxiv.org/abs/2403.19912v2","updated":"2024-11-21T07:08:47Z","published":"2024-03-29T01:46:11Z","title":"Automated Identification and Segmentation of Hi Sources in CRAFTS Using\n  Deep Learning Method","summary":"  Identifying neutral hydrogen (\\hi) galaxies from observational data is a\nsignificant challenge in \\hi\\ galaxy surveys. With the advancement of\nobservational technology, especially with the advent of large-scale telescope\nprojects such as FAST and SKA, the significant increase in data volume presents\nnew challenges for the efficiency and accuracy of data processing.To address\nthis challenge, in this study, we present a machine learning-based method for\nextracting \\hi\\ sources from the three-dimensional (3D) spectral data obtained\nfrom the Commensal Radio Astronomy FAST Survey (CRAFTS). We have carefully\nassembled a specialized dataset, HISF, rich in \\hi\\ sources, specifically\ndesigned to enhance the detection process. Our model, Unet-LK, utilizes the\nadvanced 3D-Unet segmentation architecture and employs an elongated convolution\nkernel to effectively capture the intricate structures of \\hi\\ sources. This\nstrategy ensures a reliable identification and segmentation of \\hi\\ sources,\nachieving notable performance metrics with a recall rate of 91.6\\% and an\naccuracy of 95.7\\%. These results substantiate the robustness of our dataset\nand the effectiveness of our proposed network architecture in the precise\nidentification of \\hi\\ sources. Our code and dataset is publicly available at\n\\url{https://github.com/fishszh/HISF}.\n","authors":["Zihao Song","Huaxi Chen","Donghui Quan","Di Li","Yinghui Zheng","Shulei Ni","Yunchuan Chen","Yun Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.19912v2.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2411.10028v2","updated":"2024-11-21T07:06:53Z","published":"2024-11-15T08:17:05Z","title":"MOT FCG++: Enhanced Representation of Spatio-temporal Motion and\n  Appearance Features","summary":"  The goal of multi-object tracking (MOT) is to detect and track all objects in\na scene across frames, while maintaining a unique identity for each object.\nMost existing methods rely on the spatial-temporal motion features and\nappearance embedding features of the detected objects in consecutive frames.\nEffectively and robustly representing the spatial and appearance features of\nlong trajectories has become a critical factor affecting the performance of\nMOT. We propose a novel approach for appearance and spatial-temporal motion\nfeature representation, improving upon the hierarchical clustering association\nmethod MOT FCG. For spatialtemporal motion features, we first propose Diagonal\nModulated GIoU, which more accurately represents the relationship between the\nposition and shape of the objects. Second, Mean Constant Velocity Modeling is\nproposed to reduce the effect of observation noise on target motion state\nestimation. For appearance features, we utilize a dynamic appearance\nrepresentation that incorporates confidence information, enabling the\ntrajectory appearance features to be more robust and global. Based on the\nbaseline model MOT FCG, we have realized further improvements in the\nperformance of all. we achieved 63.1 HOTA, 76.9 MOTA and 78.2 IDF1 on the MOT17\ntest set, and also achieved competitive performance on the MOT20 and DanceTrack\nsets.\n","authors":["Yanzhao Fang"],"pdf_url":"https://arxiv.org/pdf/2411.10028v2.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2411.13886v1","updated":"2024-11-21T06:55:43Z","published":"2024-11-21T06:55:43Z","title":"CLFace: A Scalable and Resource-Efficient Continual Learning Framework\n  for Lifelong Face Recognition","summary":"  An important aspect of deploying face recognition (FR) algorithms in\nreal-world applications is their ability to learn new face identities from a\ncontinuous data stream. However, the online training of existing deep neural\nnetwork-based FR algorithms, which are pre-trained offline on large-scale\nstationary datasets, encounter two major challenges: (I) catastrophic\nforgetting of previously learned identities, and (II) the need to store past\ndata for complete retraining from scratch, leading to significant storage\nconstraints and privacy concerns. In this paper, we introduce CLFace, a\ncontinual learning framework designed to preserve and incrementally extend the\nlearned knowledge. CLFace eliminates the classification layer, resulting in a\nresource-efficient FR model that remains fixed throughout lifelong learning and\nprovides label-free supervision to a student model, making it suitable for\nopen-set face recognition during incremental steps. We introduce an objective\nfunction that employs feature-level distillation to reduce drift between\nfeature maps of the student and teacher models across multiple stages.\nAdditionally, it incorporates a geometry-preserving distillation scheme to\nmaintain the orientation of the teacher model's feature embedding. Furthermore,\na contrastive knowledge distillation is incorporated to continually enhance the\ndiscriminative power of the feature representation by matching similarities\nbetween new identities. Experiments on several benchmark FR datasets\ndemonstrate that CLFace outperforms baseline approaches and state-of-the-art\nmethods on unseen identities using both in-domain and out-of-domain datasets.\n","authors":["Md Mahedi Hasan","Shoaib Meraj Sami","Nasser Nasrabadi"],"pdf_url":"https://arxiv.org/pdf/2411.13886v1.pdf","comment":"Accepted for publication in the IEEE/CVF Winter Conference on\n  Applications of Computer Vision (WACV 2025)"},{"id":"http://arxiv.org/abs/2410.09747v3","updated":"2024-11-21T06:46:57Z","published":"2024-10-13T06:53:58Z","title":"t-READi: Transformer-Powered Robust and Efficient Multimodal Inference\n  for Autonomous Driving","summary":"  Given the wide adoption of multimodal sensors (e.g., camera, lidar, radar) by\nautonomous vehicles (AVs), deep analytics to fuse their outputs for a robust\nperception become imperative. However, existing fusion methods often make two\nassumptions rarely holding in practice: i) similar data distributions for all\ninputs and ii) constant availability for all sensors. Because, for example,\nlidars have various resolutions and failures of radars may occur, such\nvariability often results in significant performance degradation in fusion. To\nthis end, we present tREADi, an adaptive inference system that accommodates the\nvariability of multimodal sensory data and thus enables robust and efficient\nperception. t-READi identifies variation-sensitive yet structure-specific model\nparameters; it then adapts only these parameters while keeping the rest intact.\nt-READi also leverages a cross-modality contrastive learning method to\ncompensate for the loss from missing modalities. Both functions are implemented\nto maintain compatibility with existing multimodal deep fusion methods. The\nextensive experiments evidently demonstrate that compared with the status quo\napproaches, t-READi not only improves the average inference accuracy by more\nthan 6% but also reduces the inference latency by almost 15x with the cost of\nonly 5% extra memory overhead in the worst case under realistic data and modal\nvariations.\n","authors":["Pengfei Hu","Yuhang Qian","Tianyue Zheng","Ang Li","Zhe Chen","Yue Gao","Xiuzhen Cheng","Jun Luo"],"pdf_url":"https://arxiv.org/pdf/2410.09747v3.pdf","comment":"14 pages, 16 figures"},{"id":"http://arxiv.org/abs/2406.07294v2","updated":"2024-11-21T06:45:41Z","published":"2024-06-11T14:23:48Z","title":"OTO Planner: An Efficient Only Travelling Once Exploration Planner for\n  Complex and Unknown Environments","summary":"  Autonomous exploration in complex and cluttered environments is essential for\nvarious applications. However, there are many challenges due to the lack of\nglobal heuristic information. Existing exploration methods suffer from the\nrepeated paths and considerable computational resource requirement in\nlarge-scale environments. To address the above issues, this letter proposes an\nefficient exploration planner that reduces repeated paths in complex\nenvironments, hence it is called \"Only Travelling Once Planner\". OTO Planner\nincludes fast frontier updating, viewpoint evaluation and viewpoint refinement.\nA selective frontier updating mechanism is designed, saving a large amount of\ncomputational resources. In addition, a novel viewpoint evaluation system is\ndevised to reduce the repeated paths utilizing the enclosed sub-region\ndetection. Besides, a viewpoint refinement approach is raised to concentrate\nthe redundant viewpoints, leading to smoother paths. We conduct extensive\nsimulation and real-world experiments to validate the proposed method. Compared\nto the state-of-the-art approach, the proposed method reduces the exploration\ntime and movement distance by 10%-20% and improves the speed of frontier\ndetection by 6-9 times.\n","authors":["Bo Zhou","Chuanzhao Lu","Yan Pan","Fu Chen"],"pdf_url":"https://arxiv.org/pdf/2406.07294v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16141v3","updated":"2024-11-21T06:20:46Z","published":"2023-11-05T12:20:29Z","title":"Brain-Inspired Efficient Pruning: Exploiting Criticality in Spiking\n  Neural Networks","summary":"  Spiking Neural Networks (SNNs) have gained significant attention due to the\nenergy-efficient and multiplication-free characteristics. Despite these\nadvantages, deploying large-scale SNNs on edge hardware is challenging due to\nlimited resource availability. Network pruning offers a viable approach to\ncompress the network scale and reduce hardware resource requirements for model\ndeployment. However, existing SNN pruning methods cause high pruning costs and\nperformance loss because they lack efficiency in processing the sparse spike\nrepresentation of SNNs. In this paper, inspired by the critical brain\nhypothesis in neuroscience and the high biological plausibility of SNNs, we\nexplore and leverage criticality to facilitate efficient pruning in deep SNNs.\nWe firstly explain criticality in SNNs from the perspective of maximizing\nfeature information entropy. Second, We propose a low-cost metric for assess\nneuron criticality in feature transmission and design a pruning-regeneration\nmethod that incorporates this criticality into the pruning process.\nExperimental results demonstrate that our method achieves higher performance\nthan the current state-of-the-art (SOTA) method with up to 95.26\\% reduction of\npruning cost. The criticality-based regeneration process efficiently selects\npotential structures and facilitates consistent feature representation.\n","authors":["Shuo Chen","Boxiao Liu","Zeshi Liu","Haihang You"],"pdf_url":"https://arxiv.org/pdf/2311.16141v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03163v2","updated":"2024-11-21T06:18:07Z","published":"2024-03-05T17:56:27Z","title":"Design2Code: Benchmarking Multimodal Code Generation for Automated\n  Front-End Engineering","summary":"  Generative AI has made rapid advancements in recent years, achieving\nunprecedented capabilities in multimodal understanding and code generation.\nThis can enable a new paradigm of front-end development in which multimodal\nlarge language models (MLLMs) directly convert visual designs into code\nimplementations. In this work, we construct Design2Code - the first real-world\nbenchmark for this task. Specifically, we manually curate 484 diverse\nreal-world webpages as test cases and develop a set of automatic evaluation\nmetrics to assess how well current multimodal LLMs can generate the code\nimplementations that directly render into the given reference webpages, given\nthe screenshots as input. We also complement automatic metrics with\ncomprehensive human evaluations to validate the performance ranking. To\nrigorously benchmark MLLMs, we test various multimodal prompting methods on\nfrontier models such as GPT-4o, GPT-4V, Gemini, and Claude. Our fine-grained\nbreak-down metrics indicate that models mostly lag in recalling visual elements\nfrom the input webpages and generating correct layout designs.\n","authors":["Chenglei Si","Yanzhe Zhang","Ryan Li","Zhengyuan Yang","Ruibo Liu","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2403.03163v2.pdf","comment":"The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2411.13873v1","updated":"2024-11-21T06:15:43Z","published":"2024-11-21T06:15:43Z","title":"Sli2Vol+: Segmenting 3D Medical Images Based on an Object Estimation\n  Guided Correspondence Flow Network","summary":"  Deep learning (DL) methods have shown remarkable successes in medical image\nsegmentation, often using large amounts of annotated data for model training.\nHowever, acquiring a large number of diverse labeled 3D medical image datasets\nis highly difficult and expensive. Recently, mask propagation DL methods were\ndeveloped to reduce the annotation burden on 3D medical images. For example,\nSli2Vol~\\cite{yeung2021sli2vol} proposed a self-supervised framework (SSF) to\nlearn correspondences by matching neighboring slices via slice reconstruction\nin the training stage; the learned correspondences were then used to propagate\na labeled slice to other slices in the test stage. But, these methods are still\nprone to error accumulation due to the inter-slice propagation of\nreconstruction errors. Also, they do not handle discontinuities well, which can\noccur between consecutive slices in 3D images, as they emphasize exploiting\nobject continuity. To address these challenges, in this work, we propose a new\nSSF, called \\proposed, {for segmenting any anatomical structures in 3D medical\nimages using only a single annotated slice per training and testing volume.}\nSpecifically, in the training stage, we first propagate an annotated 2D slice\nof a training volume to the other slices, generating pseudo-labels (PLs). Then,\nwe develop a novel Object Estimation Guided Correspondence Flow Network to\nlearn reliable correspondences between consecutive slices and corresponding PLs\nin a self-supervised manner. In the test stage, such correspondences are\nutilized to propagate a single annotated slice to the other slices of a test\nvolume. We demonstrate the effectiveness of our method on various medical image\nsegmentation tasks with different datasets, showing better generalizability\nacross different organs, modalities, and modals. Code is available at\n\\url{https://github.com/adlsn/Sli2Volplus}\n","authors":["Delin An","Pengfei Gu","Milan Sonka","Chaoli Wang","Danny Z. Chen"],"pdf_url":"https://arxiv.org/pdf/2411.13873v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.03478v2","updated":"2024-11-21T06:01:26Z","published":"2023-11-06T19:30:23Z","title":"Multi Loss-based Feature Fusion and Top Two Voting Ensemble Decision\n  Strategy for Facial Expression Recognition in the Wild","summary":"  Facial expression recognition (FER) in the wild is a challenging task\naffected by the image quality and has attracted broad interest in computer\nvision. There is no research using feature fusion and ensemble strategy for FER\nsimultaneously. Different from previous studies, this paper applies both\ninternal feature fusion for a single model and feature fusion among multiple\nnetworks, as well as the ensemble strategy. This paper proposes one novel\nsingle model named R18+FAML, as well as one ensemble model named\nR18+FAML-FGA-T2V to improve the performance of the FER in the wild. Based on\nthe structure of ResNet18 (R18), R18+FAML combines internal Feature fusion and\nthree Attention blocks using Multiple Loss functions (FAML) to improve the\ndiversity of the feature extraction. To improve the performance of R18+FAML, we\npropose a Feature fusion among networks based on the Genetic Algorithm (FGA),\nwhich can fuse the convolution kernels for feature extraction of multiple\nnetworks. On the basis of R18+FAML and FGA, we propose one ensemble strategy,\ni.e., the Top Two Voting (T2V) to support the classification of FER, which can\nconsider more classification information comprehensively. Combining the above\nstrategies, R18+FAML-FGA-T2V can focus on the main expression-aware areas.\nExtensive experiments demonstrate that our single model R18+FAML and the\nensemble model R18+FAML-FGA-T2V achieve the accuracies of $\\left( 90.32, 62.17,\n65.83 \\right)\\%$ and $\\left( 91.59, 63.27, 66.63 \\right)\\%$ on three\nchallenging unbalanced FER datasets RAF-DB, AffectNet-8 and AffectNet-7\nrespectively, both outperforming the state-of-the-art results.\n","authors":["Guangyao Zhou","Yuanlun Xie","Yiqin Fu","Zhaokun Wang"],"pdf_url":"https://arxiv.org/pdf/2311.03478v2.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2411.13862v1","updated":"2024-11-21T05:46:06Z","published":"2024-11-21T05:46:06Z","title":"Image Compression Using Novel View Synthesis Priors","summary":"  Real-time visual feedback is essential for tetherless control of remotely\noperated vehicles, particularly during inspection and manipulation tasks.\nThough acoustic communication is the preferred choice for medium-range\ncommunication underwater, its limited bandwidth renders it impractical to\ntransmit images or videos in real-time. To address this, we propose a\nmodel-based image compression technique that leverages prior mission\ninformation. Our approach employs trained machine-learning based novel view\nsynthesis models, and uses gradient descent optimization to refine latent\nrepresentations to help generate compressible differences between camera images\nand rendered images. We evaluate the proposed compression technique using a\ndataset from an artificial ocean basin, demonstrating superior compression\nratios and image quality over existing techniques. Moreover, our method\nexhibits robustness to introduction of new objects within the scene,\nhighlighting its potential for advancing tetherless remotely operated vehicle\noperations.\n","authors":["Luyuan Peng","Mandar Chitre","Hari Vishnu","Yuen Min Too","Bharath Kalyan","Rajat Mishra","Soo Pieng Tan"],"pdf_url":"https://arxiv.org/pdf/2411.13862v1.pdf","comment":"Preprint submitted to Ocean Engineering"},{"id":"http://arxiv.org/abs/2411.13860v1","updated":"2024-11-21T05:41:35Z","published":"2024-11-21T05:41:35Z","title":"Decoupled Sparse Priors Guided Diffusion Compression Model for Point\n  Clouds","summary":"  Lossy compression methods rely on an autoencoder to transform a point cloud\ninto latent points for storage, leaving the inherent redundancy of latent\nrepresentations unexplored. To reduce redundancy in latent points, we propose a\nsparse priors guided method that achieves high reconstruction quality,\nespecially at high compression ratios. This is accomplished by a dual-density\nscheme separately processing the latent points (intended for reconstruction)\nand the decoupled sparse priors (intended for storage). Our approach features\nan efficient dual-density data flow that relaxes size constraints on latent\npoints, and hybridizes a progressive conditional diffusion model to encapsulate\nessential details for reconstruction within the conditions, which are decoupled\nhierarchically to intra-point and inter-point priors. Specifically, our method\nencodes the original point cloud into latent points and decoupled sparse priors\nthrough separate encoders. Latent points serve as intermediates, while sparse\npriors act as adaptive conditions. We then employ a progressive attention-based\nconditional denoiser to generate latent points conditioned on the decoupled\npriors, allowing the denoiser to dynamically attend to geometric and semantic\ncues from the priors at each encoding and decoding layer. Additionally, we\nintegrate the local distribution into the arithmetic encoder and decoder to\nenhance local context modeling of the sparse points. The original point cloud\nis reconstructed through a point decoder. Compared to state-of-the-art, our\nmethod obtains superior rate-distortion trade-off, evidenced by extensive\nevaluations on the ShapeNet dataset and standard test datasets from MPEG group\nincluding 8iVFB, and Owlii.\n","authors":["Xiaoge Zhang","Zijie Wu","Mehwish Nasim","Mingtao Feng","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2411.13860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.09955v2","updated":"2024-11-21T05:28:10Z","published":"2024-11-15T05:18:15Z","title":"Instruction-Guided Editing Controls for Images and Multimedia: A Survey\n  in LLM era","summary":"  The rapid advancement of large language models (LLMs) and multimodal learning\nhas transformed digital content creation and manipulation. Traditional visual\nediting tools require significant expertise, limiting accessibility. Recent\nstrides in instruction-based editing have enabled intuitive interaction with\nvisual content, using natural language as a bridge between user intent and\ncomplex editing operations. This survey provides an overview of these\ntechniques, focusing on how LLMs and multimodal models empower users to achieve\nprecise visual modifications without deep technical knowledge. By synthesizing\nover 100 publications, we explore methods from generative adversarial networks\nto diffusion models, examining multimodal integration for fine-grained content\ncontrol. We discuss practical applications across domains such as fashion, 3D\nscene manipulation, and video synthesis, highlighting increased accessibility\nand alignment with human intuition. Our survey compares existing literature,\nemphasizing LLM-empowered editing, and identifies key challenges to stimulate\nfurther research. We aim to democratize powerful visual editing across various\nindustries, from entertainment to education. Interested readers are encouraged\nto access our repository at\nhttps://github.com/tamlhp/awesome-instruction-editing.\n","authors":["Thanh Tam Nguyen","Zhao Ren","Trinh Pham","Thanh Trung Huynh","Phi Le Nguyen","Hongzhi Yin","Quoc Viet Hung Nguyen"],"pdf_url":"https://arxiv.org/pdf/2411.09955v2.pdf","comment":"Fixed a serious error in author information"},{"id":"http://arxiv.org/abs/2411.13855v1","updated":"2024-11-21T05:27:42Z","published":"2024-11-21T05:27:42Z","title":"A Multimodal Approach to The Detection and Classification of Skin\n  Diseases","summary":"  According to PBS, nearly one-third of Americans lack access to primary care\nservices, and another forty percent delay going to avoid medical costs. As a\nresult, many diseases are left undiagnosed and untreated, even if the disease\nshows many physical symptoms on the skin. With the rise of AI, self-diagnosis\nand improved disease recognition have become more promising than ever; in spite\nof that, existing methods suffer from a lack of large-scale patient databases\nand outdated methods of study, resulting in studies being limited to only a few\ndiseases or modalities. This study incorporates readily available and easily\naccessible patient information via image and text for skin disease\nclassification on a new dataset of 26 skin disease types that includes both\nskin disease images (37K) and associated patient narratives. Using this\ndataset, baselines for various image models were established that outperform\nexisting methods. Initially, the Resnet-50 model was only able to achieve an\naccuracy of 70% but, after various optimization techniques, the accuracy was\nimproved to 80%. In addition, this study proposes a novel fine-tuning strategy\nfor sequence classification Large Language Models (LLMs), Chain of Options,\nwhich breaks down a complex reasoning task into intermediate steps at training\ntime instead of inference. With Chain of Options and preliminary disease\nrecommendations from the image model, this method achieves state of the art\naccuracy 91% in diagnosing patient skin disease given just an image of the\nafflicted area as well as a patient description of the symptoms (such as\nitchiness or dizziness). Through this research, an earlier diagnosis of skin\ndiseases can occur, and clinicians can work with deep learning models to give a\nmore accurate diagnosis, improving quality of life and saving lives.\n","authors":["Allen Yang","Edward Yang"],"pdf_url":"https://arxiv.org/pdf/2411.13855v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13852v1","updated":"2024-11-21T05:24:35Z","published":"2024-11-21T05:24:35Z","title":"Dealing with Synthetic Data Contamination in Online Continual Learning","summary":"  Image generation has shown remarkable results in generating high-fidelity\nrealistic images, in particular with the advancement of diffusion-based models.\nHowever, the prevalence of AI-generated images may have side effects for the\nmachine learning community that are not clearly identified. Meanwhile, the\nsuccess of deep learning in computer vision is driven by the massive dataset\ncollected on the Internet. The extensive quantity of synthetic data being added\nto the Internet would become an obstacle for future researchers to collect\n\"clean\" datasets without AI-generated content. Prior research has shown that\nusing datasets contaminated by synthetic images may result in performance\ndegradation when used for training. In this paper, we investigate the potential\nimpact of contaminated datasets on Online Continual Learning (CL) research. We\nexperimentally show that contaminated datasets might hinder the training of\nexisting online CL methods. Also, we propose Entropy Selection with\nReal-synthetic similarity Maximization (ESRM), a method to alleviate the\nperformance deterioration caused by synthetic images when training online CL\nmodels. Experiments show that our method can significantly alleviate\nperformance deterioration, especially when the contamination is severe. For\nreproducibility, the source code of our work is available at\nhttps://github.com/maorong-wang/ESRM.\n","authors":["Maorong Wang","Nicolas Michel","Jiafeng Mao","Toshihiko Yamasaki"],"pdf_url":"https://arxiv.org/pdf/2411.13852v1.pdf","comment":"Accepted to NeurIPS'24"},{"id":"http://arxiv.org/abs/2411.13847v1","updated":"2024-11-21T05:10:41Z","published":"2024-11-21T05:10:41Z","title":"Multitask Learning for SAR Ship Detection with Gaussian-Mask Joint\n  Segmentation","summary":"  Detecting ships in synthetic aperture radar (SAR) images is challenging due\nto strong speckle noise, complex surroundings, and varying scales. This paper\nproposes MLDet, a multitask learning framework for SAR ship detection,\nconsisting of object detection, speckle suppression, and target segmentation\ntasks. An angle classification loss with aspect ratio weighting is introduced\nto improve detection accuracy by addressing angular periodicity and object\nproportions. The speckle suppression task uses a dual-feature fusion attention\nmechanism to reduce noise and fuse shallow and denoising features, enhancing\nrobustness. The target segmentation task, leveraging a rotated Gaussian-mask,\naids the network in extracting target regions from cluttered backgrounds and\nimproves detection efficiency with pixel-level predictions. The Gaussian-mask\nensures ship centers have the highest probabilities, gradually decreasing\noutward under a Gaussian distribution. Additionally, a weighted rotated boxes\nfusion (WRBF) strategy combines multi-direction anchor predictions, filtering\nanchors beyond boundaries or with high overlap but low confidence. Extensive\nexperiments on SSDD+ and HRSID datasets demonstrate the effectiveness and\nsuperiority of MLDet.\n","authors":["Ming Zhao","Xin Zhang","André Kaup"],"pdf_url":"https://arxiv.org/pdf/2411.13847v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01002v3","updated":"2024-11-21T05:06:21Z","published":"2024-02-01T20:32:14Z","title":"AI-generated faces influence gender stereotypes and racial\n  homogenization","summary":"  Text-to-image generative AI models such as Stable Diffusion are used daily by\nmillions worldwide. However, the extent to which these models exhibit racial\nand gender stereotypes is not yet fully understood. Here, we document\nsignificant biases in Stable Diffusion across six races, two genders, 32\nprofessions, and eight attributes. Additionally, we examine the degree to which\nStable Diffusion depicts individuals of the same race as being similar to one\nanother. This analysis reveals significant racial homogenization, e.g.,\ndepicting nearly all Middle Eastern men as bearded, brown-skinned, and wearing\ntraditional attire. We then propose debiasing solutions that allow users to\nspecify the desired distributions of race and gender when generating images\nwhile minimizing racial homogenization. Finally, using a preregistered survey\nexperiment, we find evidence that being presented with inclusive AI-generated\nfaces reduces people's racial and gender biases, while being presented with\nnon-inclusive ones increases such biases, regardless of whether the images are\nlabeled as AI-generated. Taken together, our findings emphasize the need to\naddress biases and stereotypes in text-to-image models.\n","authors":["Nouar AlDahoul","Talal Rahwan","Yasir Zaki"],"pdf_url":"https://arxiv.org/pdf/2402.01002v3.pdf","comment":"47 pages, 19 figures"},{"id":"http://arxiv.org/abs/2411.13842v1","updated":"2024-11-21T05:02:13Z","published":"2024-11-21T05:02:13Z","title":"Detecting Human Artifacts from Text-to-Image Models","summary":"  Despite recent advancements, text-to-image generation models often produce\nimages containing artifacts, especially in human figures. These artifacts\nappear as poorly generated human bodies, including distorted, missing, or extra\nbody parts, leading to visual inconsistencies with typical human anatomy and\ngreatly impairing overall fidelity. In this study, we address this challenge by\ncurating Human Artifact Dataset (HAD), the first large-scale dataset\nspecifically designed to identify and localize human artifacts. HAD comprises\nover 37,000 images generated by several popular text-to-image models, annotated\nfor human artifact localization. Using this dataset, we train the Human\nArtifact Detection Models (HADM), which can identify diverse artifact types\nacross multiple generative domains and demonstrate strong generalization, even\non images from unseen generators. Additionally, to further improve generators'\nperception of human structural coherence, we use the predictions from our HADM\nas feedback for diffusion model finetuning. Our experiments confirm a reduction\nin human artifacts in the resulting model. Furthermore, we showcase a novel\napplication of our HADM in an iterative inpainting framework to correct human\nartifacts in arbitrary images directly, demonstrating its utility in improving\nimage quality. Our dataset and detection models are available at:\n\\url{https://github.com/wangkaihong/HADM}.\n","authors":["Kaihong Wang","Lingzhi Zhang","Jianming Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.13842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13840v1","updated":"2024-11-21T05:01:49Z","published":"2024-11-21T05:01:49Z","title":"Segment Anything in Light Fields for Real-Time Applications via\n  Constrained Prompting","summary":"  Segmented light field images can serve as a powerful representation in many\nof computer vision tasks exploiting geometry and appearance of objects, such as\nobject pose tracking. In the light field domain, segmentation presents an\nadditional objective of recognizing the same segment through all the views.\nSegment Anything Model 2 (SAM 2) allows producing semantically meaningful\nsegments for monocular images and videos. However, using SAM 2 directly on\nlight fields is highly ineffective due to unexploited constraints. In this\nwork, we present a novel light field segmentation method that adapts SAM 2 to\nthe light field domain without retraining or modifying the model. By utilizing\nthe light field domain constraints, the method produces high quality and\nview-consistent light field masks, outperforming the SAM 2 video tracking\nbaseline and working 7 times faster, with a real-time speed. We achieve this by\nexploiting the epipolar geometry cues to propagate the masks between the views,\nprobing the SAM 2 latent space to estimate their occlusion, and further\nprompting SAM 2 for their refinement.\n","authors":["Nikolai Goncharov","Donald G. Dansereau"],"pdf_url":"https://arxiv.org/pdf/2411.13840v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13836v1","updated":"2024-11-21T04:54:30Z","published":"2024-11-21T04:54:30Z","title":"CLIPer: Hierarchically Improving Spatial Representation of CLIP for\n  Open-Vocabulary Semantic Segmentation","summary":"  Contrastive Language-Image Pre-training (CLIP) exhibits strong zero-shot\nclassification ability on various image-level tasks, leading to the research to\nadapt CLIP for pixel-level open-vocabulary semantic segmentation without\nadditional training. The key is to improve spatial representation of\nimage-level CLIP, such as replacing self-attention map at last layer with\nself-self attention map or vision foundation model based attention map. In this\npaper, we present a novel hierarchical framework, named CLIPer, that\nhierarchically improves spatial representation of CLIP. The proposed CLIPer\nincludes an early-layer fusion module and a fine-grained compensation module.\nWe observe that, the embeddings and attention maps at early layers can preserve\nspatial structural information. Inspired by this, we design the early-layer\nfusion module to generate segmentation map with better spatial coherence.\nAfterwards, we employ a fine-grained compensation module to compensate the\nlocal details using the self-attention maps of diffusion model. We conduct the\nexperiments on seven segmentation datasets. Our proposed CLIPer achieves the\nstate-of-the-art performance on these datasets. For instance, using ViT-L,\nCLIPer has the mIoU of 69.8% and 43.3% on VOC and COCO Object, outperforming\nProxyCLIP by 9.2% and 4.1% respectively.\n","authors":["Lin Sun","Jiale Cao","Jin Xie","Xiaoheng Jiang","Yanwei Pang"],"pdf_url":"https://arxiv.org/pdf/2411.13836v1.pdf","comment":"Homepange and code: https://linsun449.github.io/cliper"},{"id":"http://arxiv.org/abs/2306.02243v3","updated":"2024-11-21T04:12:53Z","published":"2023-06-04T03:06:37Z","title":"Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification","summary":"  The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks.\n","authors":["Jintao Rong","Hao Chen","Linlin Ou","Tianxiao Chen","Xinyi Yu","Yifan Liu"],"pdf_url":"https://arxiv.org/pdf/2306.02243v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15864v4","updated":"2024-11-21T03:51:58Z","published":"2023-11-27T14:32:33Z","title":"InterControl: Zero-shot Human Interaction Generation by Controlling\n  Every Joint","summary":"  Text-conditioned motion synthesis has made remarkable progress with the\nemergence of diffusion models. However, the majority of these motion diffusion\nmodels are primarily designed for a single character and overlook multi-human\ninteractions. In our approach, we strive to explore this problem by\nsynthesizing human motion with interactions for a group of characters of any\nsize in a zero-shot manner. The key aspect of our approach is the adaptation of\nhuman-wise interactions as pairs of human joints that can be either in contact\nor separated by a desired distance. In contrast to existing methods that\nnecessitate training motion generation models on multi-human motion datasets\nwith a fixed number of characters, our approach inherently possesses the\nflexibility to model human interactions involving an arbitrary number of\nindividuals, thereby transcending the limitations imposed by the training data.\nWe introduce a novel controllable motion generation method, InterControl, to\nencourage the synthesized motions maintaining the desired distance between\njoint pairs. It consists of a motion controller and an inverse kinematics\nguidance module that realistically and accurately aligns the joints of\nsynthesized characters to the desired location. Furthermore, we demonstrate\nthat the distance between joint pairs for human-wise interactions can be\ngenerated using an off-the-shelf Large Language Model (LLM). Experimental\nresults highlight the capability of our framework to generate interactions with\nmultiple human characters and its potential to work with off-the-shelf\nphysics-based character simulators. Code is available at\nhttps://github.com/zhenzhiwang/intercontrol\n","authors":["Zhenzhi Wang","Jingbo Wang","Yixuan Li","Dahua Lin","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2311.15864v4.pdf","comment":"NeurIPS 2024 camera ready version. TL;DR: Generate human interactions\n  with only single-person motion data in training via joint contact pairs from\n  LLMs"},{"id":"http://arxiv.org/abs/2407.17438v3","updated":"2024-11-21T03:26:54Z","published":"2024-07-24T17:15:58Z","title":"HumanVid: Demystifying Training Data for Camera-controllable Human Image\n  Animation","summary":"  Human image animation involves generating videos from a character photo,\nallowing user control and unlocking the potential for video and movie\nproduction. While recent approaches yield impressive results using high-quality\ntraining data, the inaccessibility of these datasets hampers fair and\ntransparent benchmarking. Moreover, these approaches prioritize 2D human motion\nand overlook the significance of camera motions in videos, leading to limited\ncontrol and unstable video generation. To demystify the training data, we\npresent HumanVid, the first large-scale high-quality dataset tailored for human\nimage animation, which combines crafted real-world and synthetic data. For the\nreal-world data, we compile a vast collection of real-world videos from the\ninternet. We developed and applied careful filtering rules to ensure video\nquality, resulting in a curated collection of 20K high-resolution (1080P)\nhuman-centric videos. Human and camera motion annotation is accomplished using\na 2D pose estimator and a SLAM-based method. To expand our synthetic dataset,\nwe collected 10K 3D avatar assets and leveraged existing assets of body shapes,\nskin textures and clothings. Notably, we introduce a rule-based camera\ntrajectory generation method, enabling the synthetic pipeline to incorporate\ndiverse and precise camera motion annotation, which can rarely be found in\nreal-world data. To verify the effectiveness of HumanVid, we establish a\nbaseline model named CamAnimate, short for Camera-controllable Human Animation,\nthat considers both human and camera motions as conditions. Through extensive\nexperimentation, we demonstrate that such simple baseline training on our\nHumanVid achieves state-of-the-art performance in controlling both human pose\nand camera motions, setting a new benchmark. Demo, data and code could be found\nin the project website: https://humanvid.github.io/.\n","authors":["Zhenzhi Wang","Yixuan Li","Yanhong Zeng","Youqing Fang","Yuwei Guo","Wenran Liu","Jing Tan","Kai Chen","Tianfan Xue","Bo Dai","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2407.17438v3.pdf","comment":"NeurIPS D&B Track 2024 camera ready version, TL;DR: the first\n  large-scale dataset for camera controllable human image animation task, and a\n  baseline method"},{"id":"http://arxiv.org/abs/2411.13807v1","updated":"2024-11-21T03:13:30Z","published":"2024-11-21T03:13:30Z","title":"MagicDriveDiT: High-Resolution Long Video Generation for Autonomous\n  Driving with Adaptive Control","summary":"  The rapid advancement of diffusion models has greatly improved video\nsynthesis, especially in controllable video generation, which is essential for\napplications like autonomous driving. However, existing methods are limited by\nscalability and how control conditions are integrated, failing to meet the\nneeds for high-resolution and long videos for autonomous driving applications.\nIn this paper, we introduce MagicDriveDiT, a novel approach based on the DiT\narchitecture, and tackle these challenges. Our method enhances scalability\nthrough flow matching and employs a progressive training strategy to manage\ncomplex scenarios. By incorporating spatial-temporal conditional encoding,\nMagicDriveDiT achieves precise control over spatial-temporal latents.\nComprehensive experiments show its superior performance in generating realistic\nstreet scene videos with higher resolution and more frames. MagicDriveDiT\nsignificantly improves video generation quality and spatial-temporal controls,\nexpanding its potential applications across various tasks in autonomous\ndriving.\n","authors":["Ruiyuan Gao","Kai Chen","Bo Xiao","Lanqing Hong","Zhenguo Li","Qiang Xu"],"pdf_url":"https://arxiv.org/pdf/2411.13807v1.pdf","comment":"Project Website: https://flymin.github.io/magicdrivedit/"},{"id":"http://arxiv.org/abs/2411.13797v1","updated":"2024-11-21T02:51:52Z","published":"2024-11-21T02:51:52Z","title":"Hugging Rain Man: A Novel Facial Action Units Dataset for Analyzing\n  Atypical Facial Expressions in Children with Autism Spectrum Disorder","summary":"  Children with Autism Spectrum Disorder (ASD) often exhibit atypical facial\nexpressions. However, the specific objective facial features that underlie this\nsubjective perception remain unclear. In this paper, we introduce a novel\ndataset, Hugging Rain Man (HRM), which includes facial action units (AUs)\nmanually annotated by FACS experts for both children with ASD and typical\ndevelopment (TD). The dataset comprises a rich collection of posed and\nspontaneous facial expressions, totaling approximately 130,000 frames, along\nwith 22 AUs, 10 Action Descriptors (ADs), and atypicality ratings. A\nstatistical analysis of static images from the HRM reveals significant\ndifferences between the ASD and TD groups across multiple AUs and ADs when\ndisplaying the same emotional expressions, confirming that participants with\nASD tend to demonstrate more irregular and diverse expression patterns.\nSubsequently, a temporal regression method was presented to analyze atypicality\nof dynamic sequences, thereby bridging the gap between subjective perception\nand objective facial characteristics. Furthermore, baseline results for AU\ndetection are provided for future research reference. This work not only\ncontributes to our understanding of the unique facial expression\ncharacteristics associated with ASD but also provides potential tools for ASD\nearly screening. Portions of the dataset, features, and pretrained models are\naccessible at: \\url{https://github.com/Jonas-DL/Hugging-Rain-Man}.\n","authors":["Yanfeng Ji","Shutong Wang","Ruyi Xu","Jingying Chen","Xinzhou Jiang","Zhengyu Deng","Yuxuan Quan","Junpeng Liu"],"pdf_url":"https://arxiv.org/pdf/2411.13797v1.pdf","comment":"Portions of the dataset, features, and pretrained models are\n  accessible at: https://github.com/Jonas-DL/Hugging-Rain-Man"},{"id":"http://arxiv.org/abs/2411.13794v1","updated":"2024-11-21T02:48:38Z","published":"2024-11-21T02:48:38Z","title":"GalaxyEdit: Large-Scale Image Editing Dataset with Enhanced Diffusion\n  Adapter","summary":"  Training of large-scale text-to-image and image-to-image models requires a\nhuge amount of annotated data. While text-to-image datasets are abundant, data\navailable for instruction-based image-to-image tasks like object addition and\nremoval is limited. This is because of the several challenges associated with\nthe data generation process, such as, significant human effort, limited\nautomation, suboptimal end-to-end models, data diversity constraints and high\nexpenses. We propose an automated data generation pipeline aimed at alleviating\nsuch limitations, and introduce GalaxyEdit - a large-scale image editing\ndataset for add and remove operations. We fine-tune the SD v1.5 model on our\ndataset and find that our model can successfully handle a broader range of\nobjects and complex editing instructions, outperforming state-of-the-art\nmethods in FID scores by 11.2\\% and 26.1\\% for add and remove tasks\nrespectively. Furthermore, in light of on-device usage scenarios, we expand our\nresearch to include task-specific lightweight adapters leveraging the\nControlNet-xs architecture. While ControlNet-xs excels in canny and depth\nguided generation, we propose to improve the communication between the control\nnetwork and U-Net for more intricate add and remove tasks. We achieve this by\nenhancing ControlNet-xs with non-linear interaction layers based on Volterra\nfilters. Our approach outperforms ControlNet-xs in both add/remove and\ncanny-guided image generation tasks, highlighting the effectiveness of the\nproposed enhancement.\n","authors":["Aniruddha Bala","Rohan Jaiswal","Loay Rashid","Siddharth Roheda"],"pdf_url":"https://arxiv.org/pdf/2411.13794v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2411.12089v2","updated":"2024-11-21T02:40:40Z","published":"2024-11-18T22:00:19Z","title":"FruitNinja: 3D Object Interior Texture Generation with Gaussian\n  Splatting","summary":"  In the real world, objects reveal internal textures when sliced or cut, yet\nthis behavior is not well-studied in 3D generation tasks today. For example,\nslicing a virtual 3D watermelon should reveal flesh and seeds. Given that no\navailable dataset captures an object's full internal structure and collecting\ndata from all slices is impractical, generative methods become the obvious\napproach. However, current 3D generation and inpainting methods often focus on\nvisible appearance and overlook internal textures. To bridge this gap, we\nintroduce FruitNinja, the first method to generate internal textures for 3D\nobjects undergoing geometric and topological changes. Our approach produces\nobjects via 3D Gaussian Splatting (3DGS) with both surface and interior\ntextures synthesized, enabling real-time slicing and rendering without\nadditional optimization. FruitNinja leverages a pre-trained diffusion model to\nprogressively inpaint cross-sectional views and applies voxel-grid-based\nsmoothing to achieve cohesive textures throughout the object. Our OpaqueAtom GS\nstrategy overcomes 3DGS limitations by employing densely distributed opaque\nGaussians, avoiding biases toward larger particles that destabilize training\nand sharp color transitions for fine-grained textures. Experimental results\nshow that FruitNinja substantially outperforms existing approaches, showcasing\nunmatched visual quality in real-time rendered internal views across arbitrary\ngeometry manipulations.\n","authors":["Fangyu Wu","Yuhao Chen"],"pdf_url":"https://arxiv.org/pdf/2411.12089v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13787v1","updated":"2024-11-21T02:18:06Z","published":"2024-11-21T02:18:06Z","title":"Edge-Cloud Routing for Text-to-Image Model with Token-Level Multi-Metric\n  Prediction","summary":"  Large text-to-image models demonstrate impressive generation capabilities;\nhowever, their substantial size necessitates expensive cloud servers for\ndeployment. Conversely, light-weight models can be deployed on edge devices at\nlower cost but often with inferior generation quality for complex user prompts.\nTo strike a balance between performance and cost, we propose a routing\nframework, called \\texttt{RouteT2I}, which dynamically selects either the large\ncloud model or the light-weight edge model for each user prompt. Since\ngenerated image quality is challenging to measure directly, \\texttt{RouteT2I}\nestablishes multi-dimensional quality metrics, particularly, by evaluating the\nsimilarity between the generated images and both positive and negative texts\nthat describe each specific quality metric. \\texttt{RouteT2I} then predicts the\nexpected quality of the generated images by identifying key tokens in the\nprompt and comparing their impact on the quality. \\texttt{RouteT2I} further\nintroduces the Pareto relative superiority to compare the multi-metric quality\nof the generated images. Based on this comparison and predefined cost\nconstraints, \\texttt{RouteT2I} allocates prompts to either the edge or the\ncloud. Evaluation reveals that \\texttt{RouteT2I} significantly reduces the\nnumber of requesting large cloud model while maintaining high-quality image\ngeneration.\n","authors":["Zewei Xin","Qinya Li","Chaoyue Niu","Fan Wu"],"pdf_url":"https://arxiv.org/pdf/2411.13787v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.15179v4","updated":"2024-11-21T01:47:32Z","published":"2022-09-30T01:59:53Z","title":"Physical Adversarial Attack meets Computer Vision: A Decade Survey","summary":"  Despite the impressive achievements of Deep Neural Networks (DNNs) in\ncomputer vision, their vulnerability to adversarial attacks remains a critical\nconcern. Extensive research has demonstrated that incorporating sophisticated\nperturbations into input images can lead to a catastrophic degradation in DNNs'\nperformance. This perplexing phenomenon not only exists in the digital space\nbut also in the physical world. Consequently, it becomes imperative to evaluate\nthe security of DNNs-based systems to ensure their safe deployment in\nreal-world scenarios, particularly in security-sensitive applications. To\nfacilitate a profound understanding of this topic, this paper presents a\ncomprehensive overview of physical adversarial attacks. Firstly, we distill\nfour general steps for launching physical adversarial attacks. Building upon\nthis foundation, we uncover the pervasive role of artifacts carrying\nadversarial perturbations in the physical world. These artifacts influence each\nstep. To denote them, we introduce a new term: adversarial medium. Then, we\ntake the first step to systematically evaluate the performance of physical\nadversarial attacks, taking the adversarial medium as a first attempt. Our\nproposed evaluation metric, hiPAA, comprises six perspectives: Effectiveness,\nStealthiness, Robustness, Practicability, Aesthetics, and Economics. We also\nprovide comparative results across task categories, together with insightful\nobservations and suggestions for future research directions.\n","authors":["Hui Wei","Hao Tang","Xuemei Jia","Zhixiang Wang","Hanxun Yu","Zhubo Li","Shin'ichi Satoh","Luc Van Gool","Zheng Wang"],"pdf_url":"https://arxiv.org/pdf/2209.15179v4.pdf","comment":"Published at IEEE TPAMI. GitHub:https://github.com/weihui1308/PAA"},{"id":"http://arxiv.org/abs/2307.16680v6","updated":"2024-11-21T01:41:01Z","published":"2023-07-31T13:57:05Z","title":"On the Trustworthiness Landscape of State-of-the-art Generative Models:\n  A Survey and Outlook","summary":"  Diffusion models and large language models have emerged as leading-edge\ngenerative models, revolutionizing various aspects of human life. However, the\npractical implementations of these models have also exposed inherent risks,\nbringing to the forefront their evil sides and sparking concerns regarding\ntheir trustworthiness. Despite the wealth of literature on this subject, a\ncomprehensive survey specifically delving into the intersection of large-scale\ngenerative models and their trustworthiness remains largely absent. To bridge\nthis gap, this paper investigates both the long-standing and emerging threats\nassociated with these models across four fundamental dimensions: 1) privacy, 2)\nsecurity, 3) fairness, and 4) responsibility. Based on the investigation\nresults, we develop an extensive map outlining the trustworthiness of large\ngenerative models. After that, we provide practical recommendations and\npotential research directions for future secure applications equipped with\nlarge generative models, ultimately promoting the trustworthiness of the models\nand benefiting the society as a whole.\n","authors":["Mingyuan Fan","Chengyu Wang","Cen Chen","Yang Liu","Jun Huang"],"pdf_url":"https://arxiv.org/pdf/2307.16680v6.pdf","comment":"draft"},{"id":"http://arxiv.org/abs/2411.13774v1","updated":"2024-11-21T01:04:53Z","published":"2024-11-21T01:04:53Z","title":"Segment Any Class (SAC): Multi-Class Few-Shot Semantic Segmentation via\n  Class Region Proposals","summary":"  The Segment-Anything Model (SAM) is a vision foundation model for\nsegmentation with a prompt-driven framework. SAM generates class-agnostic masks\nbased on user-specified instance-referring prompts. However, adapting SAM for\nautomated segmentation -- where manual input is absent -- of specific object\nclasses often requires additional model training. We present Segment Any Class\n(SAC), a novel, training-free approach that task-adapts SAM for Multi-class\nsegmentation. SAC generates Class-Region Proposals (CRP) on query images which\nallows us to automatically generate class-aware prompts on probable locations\nof class instances. CRPs are derived from elementary intra-class and\ninter-class feature distinctions without any additional training. Our method is\nversatile, accommodating any N-way K-shot configurations for the multi-class\nfew-shot semantic segmentation (FSS) task. Unlike gradient-learning adaptation\nof generalist models which risk the loss of generalization and potentially\nsuffer from catastrophic forgetting, SAC solely utilizes automated prompting\nand achieves superior results over state-of-the-art methods on the COCO-20i\nbenchmark, particularly excelling in high N-way class scenarios. SAC is an\ninteresting demonstration of a prompt-only approach to adapting foundation\nmodels for novel tasks with small, limited datasets without any modifications\nto the foundation model itself. This method offers interesting benefits such as\nintrinsic immunity to concept or feature loss and rapid, online task adaptation\nof foundation models.\n","authors":["Hussni Mohd Zakir","Eric Tatt Wei Ho"],"pdf_url":"https://arxiv.org/pdf/2411.13774v1.pdf","comment":"8 pages, 2 figures, 3 tables"},{"id":"http://arxiv.org/abs/2411.10798v2","updated":"2024-11-21T00:54:15Z","published":"2024-11-16T13:29:50Z","title":"Unveiling Hidden Details: A RAW Data-Enhanced Paradigm for Real-World\n  Super-Resolution","summary":"  Real-world image super-resolution (Real SR) aims to generate high-fidelity,\ndetail-rich high-resolution (HR) images from low-resolution (LR) counterparts.\nExisting Real SR methods primarily focus on generating details from the LR RGB\ndomain, often leading to a lack of richness or fidelity in fine details. In\nthis paper, we pioneer the use of details hidden in RAW data to complement\nexisting RGB-only methods, yielding superior outputs. We argue that key image\nprocessing steps in Image Signal Processing, such as denoising and demosaicing,\ninherently result in the loss of fine details in LR images, making LR RAW a\nvaluable information source. To validate this, we present RealSR-RAW, a\ncomprehensive dataset comprising over 10,000 pairs with LR and HR RGB images,\nalong with corresponding LR RAW, captured across multiple smartphones under\nvarying focal lengths and diverse scenes. Additionally, we propose a novel,\ngeneral RAW adapter to efficiently integrate LR RAW data into existing CNNs,\nTransformers, and Diffusion-based Real SR models by suppressing the noise\ncontained in LR RAW and aligning its distribution. Extensive experiments\ndemonstrate that incorporating RAW data significantly enhances detail recovery\nand improves Real SR performance across ten evaluation metrics, including both\nfidelity and perception-oriented metrics. Our findings open a new direction for\nthe Real SR task, with the dataset and code will be made available to support\nfuture research.\n","authors":["Long Peng","Wenbo Li","Jiaming Guo","Xin Di","Haoze Sun","Yong Li","Renjing Pei","Yang Wang","Yang Cao","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2411.10798v2.pdf","comment":"We sincerely apologize, but due to some commercial confidentiality\n  agreements related to the report, we have decided to withdraw the submission\n  for now and will resubmit after making the necessary revisions"},{"id":"http://arxiv.org/abs/2411.03795v2","updated":"2024-11-21T00:38:44Z","published":"2024-11-06T09:39:52Z","title":"VQA$^2$: Visual Question Answering for Video Quality Assessment","summary":"  The advent and proliferation of large multi-modal models (LMMs) have\nintroduced new paradigms to computer vision, transforming various tasks into a\nunified visual question answering framework. Video Quality Assessment (VQA), a\nclassic field in low-level visual perception, focused initially on quantitative\nvideo quality scoring. However, driven by advances in LMMs, it is now\nprogressing toward more holistic visual quality understanding tasks. Recent\nstudies in the image domain have demonstrated that Visual Question Answering\n(VQA) can markedly enhance low-level visual quality evaluation. Nevertheless,\nrelated work has not been explored in the video domain, leaving substantial\nroom for improvement. To address this gap, we introduce the VQA2 Instruction\nDataset - the first visual question answering instruction dataset that focuses\non video quality assessment. This dataset consists of 3 subsets and covers\nvarious video types, containing 157,755 instruction question-answer pairs.\nThen, leveraging this foundation, we present the VQA2 series models. The VQA2\nseries models interleave visual and motion tokens to enhance the perception of\nspatial-temporal quality details in videos. We conduct extensive experiments on\nvideo quality scoring and understanding tasks, and results demonstrate that the\nVQA2series models achieve excellent performance in both tasks. Notably, our\nfinal model, the VQA2-Assistant, exceeds the renowned GPT-4o in visual quality\nunderstanding tasks while maintaining strong competitiveness in quality scoring\ntasks. Our work provides a foundation and feasible approach for integrating\nlow-level video quality assessment and understanding with LMMs.\n","authors":["Ziheng Jia","Zicheng Zhang","Jiaying Qian","Haoning Wu","Wei Sun","Chunyi Li","Xiaohong Liu","Weisi Lin","Guangtao Zhai","Xiongkuo Min"],"pdf_url":"https://arxiv.org/pdf/2411.03795v2.pdf","comment":"24 pages 12 figures"},{"id":"http://arxiv.org/abs/2409.13978v3","updated":"2024-11-21T00:33:46Z","published":"2024-09-21T02:01:55Z","title":"FracGM: A Fast Fractional Programming Technique for Geman-McClure Robust\n  Estimator","summary":"  Robust estimation is essential in computer vision, robotics, and navigation,\naiming to minimize the impact of outlier measurements for improved accuracy. We\npresent a fast algorithm for Geman-McClure robust estimation, FracGM,\nleveraging fractional programming techniques. This solver reformulates the\noriginal non-convex fractional problem to a convex dual problem and a linear\nequation system, iteratively solving them in an alternating optimization\npattern. Compared to graduated non-convexity approaches, this strategy exhibits\na faster convergence rate and better outlier rejection capability. In addition,\nthe global optimality of the proposed solver can be guaranteed under given\nconditions. We demonstrate the proposed FracGM solver with Wahba's rotation\nproblem and 3-D point-cloud registration along with relaxation pre-processing\nand projection post-processing. Compared to state-of-the-art algorithms, when\nthe outlier rates increase from 20% to 80%, FracGM shows 53% and 88% lower\nrotation and translation increases. In real-world scenarios, FracGM achieves\nbetter results in 13 out of 18 outcomes, while having a 19.43% improvement in\nthe computation time.\n","authors":["Bang-Shien Chen","Yu-Kai Lin","Jian-Yu Chen","Chih-Wei Huang","Jann-Long Chern","Ching-Cherng Sun"],"pdf_url":"https://arxiv.org/pdf/2409.13978v3.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2411.14633v1","updated":"2024-11-21T23:53:58Z","published":"2024-11-21T23:53:58Z","title":"Evaluating Representational Similarity Measures from the Lens of\n  Functional Correspondence","summary":"  Neuroscience and artificial intelligence (AI) both face the challenge of\ninterpreting high-dimensional neural data, where the comparative analysis of\nsuch data is crucial for revealing shared mechanisms and differences between\nthese complex systems. Despite the widespread use of representational\ncomparisons and the abundance classes of comparison methods, a critical\nquestion remains: which metrics are most suitable for these comparisons? While\nsome studies evaluate metrics based on their ability to differentiate models of\ndifferent origins or constructions (e.g., various architectures), another\napproach is to assess how well they distinguish models that exhibit distinct\nbehaviors. To investigate this, we examine the degree of alignment between\nvarious representational similarity measures and behavioral outcomes, employing\ngroup statistics and a comprehensive suite of behavioral metrics for\ncomparison. In our evaluation of eight commonly used representational\nsimilarity metrics in the visual domain -- spanning alignment-based, Canonical\nCorrelation Analysis (CCA)-based, inner product kernel-based, and\nnearest-neighbor methods -- we found that metrics like linear Centered Kernel\nAlignment (CKA) and Procrustes distance, which emphasize the overall geometric\nstructure or shape of representations, excelled in differentiating trained from\nuntrained models and aligning with behavioral measures, whereas metrics such as\nlinear predictivity, commonly used in neuroscience, demonstrated only moderate\nalignment with behavior. These insights are crucial for selecting metrics that\nemphasize behaviorally meaningful comparisons in NeuroAI research.\n","authors":["Yiqing Bo","Ansh Soni","Sudhanshu Srivastava","Meenakshi Khosla"],"pdf_url":"https://arxiv.org/pdf/2411.14633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15374v2","updated":"2024-11-21T23:51:29Z","published":"2024-08-27T19:22:06Z","title":"CycleGAN with Better Cycles","summary":"  CycleGAN provides a framework to train image-to-image translation with\nunpaired datasets using cycle consistency loss [4]. While results are great in\nmany applications, the pixel level cycle consistency can potentially be\nproblematic and causes unrealistic images in certain cases. In this project, we\npropose three simple modifications to cycle consistency, and show that such an\napproach achieves better results with fewer artifacts.\n","authors":["Tongzhou Wang","Yihan Lin"],"pdf_url":"https://arxiv.org/pdf/2408.15374v2.pdf","comment":"Technical Report 2018"},{"id":"http://arxiv.org/abs/2411.14628v1","updated":"2024-11-21T23:06:15Z","published":"2024-11-21T23:06:15Z","title":"HotSpot: Screened Poisson Equation for Signed Distance Function\n  Optimization","summary":"  We propose a method, HotSpot, for optimizing neural signed distance\nfunctions, based on a relation between the solution of a screened Poisson\nequation and the distance function. Existing losses such as the eikonal loss\ncannot guarantee the recovered implicit function to be a distance function,\neven when the implicit function satisfies the eikonal equation almost\neverywhere. Furthermore, the eikonal loss suffers from stability issues in\noptimization and the remedies that introduce area or divergence minimization\ncan lead to oversmoothing. We address these challenges by designing a loss\nfunction that when minimized can converge to the true distance function, is\nstable, and naturally penalize large surface area. We provide theoretical\nanalysis and experiments on both challenging 2D and 3D datasets and show that\nour method provide better surface reconstruction and more accurate distance\napproximation.\n","authors":["Zimo Wang","Cheng Wang","Taiki Yoshino","Sirui Tao","Ziyang Fu","Tzu-Mao Li"],"pdf_url":"https://arxiv.org/pdf/2411.14628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14626v1","updated":"2024-11-21T22:59:15Z","published":"2024-11-21T22:59:15Z","title":"Unveiling the Hidden: A Comprehensive Evaluation of Underwater Image\n  Enhancement and Its Impact on Object Detection","summary":"  Underwater imagery often suffers from severe degradation that results in low\nvisual quality and object detection performance. This work aims to evaluate\nstate-of-the-art image enhancement models, investigate their impact on\nunderwater object detection, and explore their potential to improve detection\nperformance. To this end, we selected representative underwater image\nenhancement models covering major enhancement categories and applied them\nseparately to two recent datasets: 1) the Real-World Underwater Object\nDetection Dataset (RUOD), and 2) the Challenging Underwater Plant Detection\nDataset (CUPDD). Following this, we conducted qualitative and quantitative\nanalyses on the enhanced images and developed a quality index (Q-index) to\ncompare the quality distribution of the original and enhanced images.\nSubsequently, we compared the performance of several YOLO-NAS detection models\nthat are separately trained and tested on the original and enhanced image sets.\nThen, we performed a correlation study to examine the relationship between\nenhancement metrics and detection performance. We also analyzed the inference\nresults from the trained detectors presenting cases where enhancement increased\nthe detection performance as well as cases where enhancement revealed missed\nobjects by human annotators. This study suggests that although enhancement\ngenerally deteriorates the detection performance, it can still be harnessed in\nsome cases for increased detection performance and more accurate human\nannotation.\n","authors":["Ali Awad","Ashraf Saleem","Sidike Paheding","Evan Lucas","Serein Al-Ratrout","Timothy C. Havens"],"pdf_url":"https://arxiv.org/pdf/2411.14626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.07693v3","updated":"2024-11-21T22:12:22Z","published":"2023-07-15T03:11:16Z","title":"Neural Deformable Models for 3D Bi-Ventricular Heart Shape\n  Reconstruction and Modeling from 2D Sparse Cardiac Magnetic Resonance Imaging","summary":"  We propose a novel neural deformable model (NDM) targeting at the\nreconstruction and modeling of 3D bi-ventricular shape of the heart from 2D\nsparse cardiac magnetic resonance (CMR) imaging data. We model the\nbi-ventricular shape using blended deformable superquadrics, which are\nparameterized by a set of geometric parameter functions and are capable of\ndeforming globally and locally. While global geometric parameter functions and\ndeformations capture gross shape features from visual data, local deformations,\nparameterized as neural diffeomorphic point flows, can be learned to recover\nthe detailed heart shape.Different from iterative optimization methods used in\nconventional deformable model formulations, NDMs can be trained to learn such\ngeometric parameter functions, global and local deformations from a shape\ndistribution manifold. Our NDM can learn to densify a sparse cardiac point\ncloud with arbitrary scales and generate high-quality triangular meshes\nautomatically. It also enables the implicit learning of dense correspondences\namong different heart shape instances for accurate cardiac shape registration.\nFurthermore, the parameters of NDM are intuitive, and can be used by a\nphysician without sophisticated post-processing. Experimental results on a\nlarge CMR dataset demonstrate the improved performance of NDM over conventional\nmethods.\n","authors":["Meng Ye","Dong Yang","Mikael Kanski","Leon Axel","Dimitris Metaxas"],"pdf_url":"https://arxiv.org/pdf/2307.07693v3.pdf","comment":"Accepted by ICCV 2023"},{"id":"http://arxiv.org/abs/2410.01966v2","updated":"2024-11-21T21:31:05Z","published":"2024-10-02T19:16:47Z","title":"Enhancing Screen Time Identification in Children with a Multi-View\n  Vision Language Model and Screen Time Tracker","summary":"  Being able to accurately monitor the screen exposure of young children is\nimportant for research on phenomena linked to screen use such as childhood\nobesity, physical activity, and social interaction. Most existing studies rely\nupon self-report or manual measures from bulky wearable sensors, thus lacking\nefficiency and accuracy in capturing quantitative screen exposure data. In this\nwork, we developed a novel sensor informatics framework that utilizes\negocentric images from a wearable sensor, termed the screen time tracker (STT),\nand a vision language model (VLM). In particular, we devised a multi-view VLM\nthat takes multiple views from egocentric image sequences and interprets screen\nexposure dynamically. We validated our approach by using a dataset of\nchildren's free-living activities, demonstrating significant improvement over\nexisting methods in plain vision language models and object detection models.\nResults supported the promise of this monitoring approach, which could optimize\nbehavioral research on screen exposure in children's naturalistic settings.\n","authors":["Xinlong Hou","Sen Shen","Xueshen Li","Xinran Gao","Ziyi Huang","Steven J. Holiday","Matthew R. Cribbet","Susan W. White","Edward Sazonov","Yu Gan"],"pdf_url":"https://arxiv.org/pdf/2410.01966v2.pdf","comment":"Prepare for submission"},{"id":"http://arxiv.org/abs/2411.14594v1","updated":"2024-11-21T21:27:30Z","published":"2024-11-21T21:27:30Z","title":"Solving Zero-Shot 3D Visual Grounding as Constraint Satisfaction\n  Problems","summary":"  3D visual grounding (3DVG) aims to locate objects in a 3D scene with natural\nlanguage descriptions. Supervised methods have achieved decent accuracy, but\nhave a closed vocabulary and limited language understanding ability. Zero-shot\nmethods mostly utilize large language models (LLMs) to handle natural language\ndescriptions, yet suffer from slow inference speed. To address these problems,\nin this work, we propose a zero-shot method that reformulates the 3DVG task as\na Constraint Satisfaction Problem (CSP), where the variables and constraints\nrepresent objects and their spatial relations, respectively. This allows a\nglobal reasoning of all relevant objects, producing grounding results of both\nthe target and anchor objects. Moreover, we demonstrate the flexibility of our\nframework by handling negation- and counting-based queries with only minor\nextra coding efforts. Our system, Constraint Satisfaction Visual Grounding\n(CSVG), has been extensively evaluated on the public datasets ScanRefer and\nNr3D datasets using only open-source LLMs. Results show the effectiveness of\nCSVG and superior grounding accuracy over current state-of-the-art zero-shot\n3DVG methods with improvements of $+7.0\\%$ (Acc@0.5 score) and $+11.2\\%$ on the\nScanRefer and Nr3D datasets, respectively. The code of our system is publicly\navailable at https://github.com/sunsleaf/CSVG.\n","authors":["Qihao Yuan","Jiaming Zhang","Kailai Li","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2411.14594v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2408.04197v2","updated":"2024-11-21T16:43:22Z","published":"2024-08-08T03:35:35Z","title":"Pairwise Judgment Formulation for Semantic Embedding Model in Web Search","summary":"  Semantic Embedding Model (SEM), a neural network-based Siamese architecture,\nis gaining momentum in information retrieval and natural language processing.\nIn order to train SEM in a supervised fashion for Web search, the search engine\nquery log is typically utilized to automatically formulate pairwise judgments\nas training data. Despite the growing application of semantic embeddings in the\nsearch engine industry, little work has been done on formulating effective\npairwise judgments for training SEM. In this paper, we make the first in-depth\ninvestigation of a wide range of strategies for generating pairwise judgments\nfor SEM. An interesting (perhaps surprising) discovery reveals that the\nconventional pairwise judgment formulation strategy wildly used in the field of\npairwise Learning-to-Rank (LTR) is not necessarily effective for training SEM.\nThrough a large-scale empirical study based on query logs and click-through\nactivities from a major commercial search engine, we demonstrate the effective\nstrategies for SEM and highlight the advantages of a hybrid heuristic (i.e.,\nClicked > Non-Clicked) in comparison to the atomic heuristics (e.g., Clicked >\nSkipped) in LTR. We conclude with best practices for training SEM and offer\npromising insights for future research.\n","authors":["Mengze Hong","Wailing Ng","Zichang Guo","Chen Jason Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.04197v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.05930v2","updated":"2024-11-21T16:06:05Z","published":"2024-11-08T19:31:19Z","title":"BERTrend: Neural Topic Modeling for Emerging Trends Detection","summary":"  Detecting and tracking emerging trends and weak signals in large, evolving\ntext corpora is vital for applications such as monitoring scientific\nliterature, managing brand reputation, surveilling critical infrastructure and\nmore generally to any kind of text-based event detection. Existing solutions\noften fail to capture the nuanced context or dynamically track evolving\npatterns over time. BERTrend, a novel method, addresses these limitations using\nneural topic modeling in an online setting. It introduces a new metric to\nquantify topic popularity over time by considering both the number of documents\nand update frequency. This metric classifies topics as noise, weak, or strong\nsignals, flagging emerging, rapidly growing topics for further investigation.\nExperimentation on two large real-world datasets demonstrates BERTrend's\nability to accurately detect and track meaningful weak signals while filtering\nout noise, offering a comprehensive solution for monitoring emerging trends in\nlarge-scale, evolving text corpora. The method can also be used for\nretrospective analysis of past events. In addition, the use of Large Language\nModels together with BERTrend offers efficient means for the interpretability\nof trends of events.\n","authors":["Allaa Boutaleb","Jerome Picault","Guillaume Grosjean"],"pdf_url":"https://arxiv.org/pdf/2411.05930v2.pdf","comment":"17 pages, 12 figures, FuturED 2024: Workshop on Future of Event\n  Detection (CoLocated with EMNLP 2024)"},{"id":"http://arxiv.org/abs/2411.14199v1","updated":"2024-11-21T15:07:42Z","published":"2024-11-21T15:07:42Z","title":"OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented\n  LMs","summary":"  Scientific progress depends on researchers' ability to synthesize the growing\nbody of literature. Can large language models (LMs) assist scientists in this\ntask? We introduce OpenScholar, a specialized retrieval-augmented LM that\nanswers scientific queries by identifying relevant passages from 45 million\nopen-access papers and synthesizing citation-backed responses. To evaluate\nOpenScholar, we develop ScholarQABench, the first large-scale multi-domain\nbenchmark for literature search, comprising 2,967 expert-written queries and\n208 long-form answers across computer science, physics, neuroscience, and\nbiomedicine. On ScholarQABench, OpenScholar-8B outperforms GPT-4o by 5% and\nPaperQA2 by 7% in correctness, despite being a smaller, open model. While GPT4o\nhallucinates citations 78 to 90% of the time, OpenScholar achieves citation\naccuracy on par with human experts. OpenScholar's datastore, retriever, and\nself-feedback inference loop also improves off-the-shelf LMs: for instance,\nOpenScholar-GPT4o improves GPT-4o's correctness by 12%. In human evaluations,\nexperts preferred OpenScholar-8B and OpenScholar-GPT4o responses over\nexpert-written ones 51% and 70% of the time, respectively, compared to GPT4o's\n32%. We open-source all of our code, models, datastore, data and a public demo.\n","authors":["Akari Asai","Jacqueline He","Rulin Shao","Weijia Shi","Amanpreet Singh","Joseph Chee Chang","Kyle Lo","Luca Soldaini","Sergey Feldman","Mike D'arcy","David Wadden","Matt Latzke","Minyang Tian","Pan Ji","Shengyan Liu","Hao Tong","Bohao Wu","Yanyu Xiong","Luke Zettlemoyer","Graham Neubig","Dan Weld","Doug Downey","Wen-tau Yih","Pang Wei Koh","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2411.14199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18097v3","updated":"2024-11-21T14:23:49Z","published":"2024-10-08T11:28:06Z","title":"RRADistill: Distilling LLMs' Passage Ranking Ability for Long-Tail\n  Queries Document Re-Ranking on a Search Engine","summary":"  Large Language Models (LLMs) excel at understanding the semantic\nrelationships between queries and documents, even with lengthy and complex\nlong-tail queries. These queries are challenging for feedback-based rankings\ndue to sparse user engagement and limited feedback, making LLMs' ranking\nability highly valuable. However, the large size and slow inference of LLMs\nnecessitate the development of smaller, more efficient models (sLLMs).\nRecently, integrating ranking label generation into distillation techniques has\nbecome crucial, but existing methods underutilize LLMs' capabilities and are\ncumbersome. Our research, RRADistill: Re-Ranking Ability Distillation, propose\nan efficient label generation pipeline and novel sLLM training methods for both\nencoder and decoder models. We introduce an encoder-based method using a Term\nControl Layer to capture term matching signals and a decoder-based model with a\nranking layer for enhanced understanding. A/B testing on a Korean-based search\nplatform, validates the effectiveness of our approach in improving re-ranking\nfor long-tail queries.\n","authors":["Nayoung Choi","Youngjune Lee","Gyu-Hwung Cho","Haeyu Jeong","Jungmin Kong","Saehun Kim","Keunchan Park","Sarah Cho","Inchang Jeong","Gyohee Nam","Sunghoon Han","Wonil Yang","Jaeho Choi"],"pdf_url":"https://arxiv.org/pdf/2410.18097v3.pdf","comment":"Accepted to EMNLP 2024 Industry Track. First two authors contributed\n  equally"},{"id":"http://arxiv.org/abs/2411.14100v1","updated":"2024-11-21T13:05:18Z","published":"2024-11-21T13:05:18Z","title":"BEST-STD: Bidirectional Mamba-Enhanced Speech Tokenization for Spoken\n  Term Detection","summary":"  Spoken term detection (STD) is often hindered by reliance on frame-level\nfeatures and the computationally intensive DTW-based template matching,\nlimiting its practicality. To address these challenges, we propose a novel\napproach that encodes speech into discrete, speaker-agnostic semantic tokens.\nThis facilitates fast retrieval using text-based search algorithms and\neffectively handles out-of-vocabulary terms. Our approach focuses on generating\nconsistent token sequences across varying utterances of the same term. We also\npropose a bidirectional state space modeling within the Mamba encoder, trained\nin a self-supervised learning framework, to learn contextual frame-level\nfeatures that are further encoded into discrete tokens. Our analysis shows that\nour speech tokens exhibit greater speaker invariance than those from existing\ntokenizers, making them more suitable for STD tasks. Empirical evaluation on\nLibriSpeech and TIMIT databases indicates that our method outperforms existing\nSTD baselines while being more efficient.\n","authors":["Anup Singh","Kris Demuynck","Vipul Arora"],"pdf_url":"https://arxiv.org/pdf/2411.14100v1.pdf","comment":"Submitted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2402.17497v2","updated":"2024-11-21T08:44:20Z","published":"2024-02-27T13:22:51Z","title":"REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain\n  Question Answering","summary":"  Considering the limited internal parametric knowledge, retrieval-augmented\ngeneration (RAG) has been widely used to extend the knowledge scope of large\nlanguage models (LLMs). Despite the extensive efforts on RAG research, in\nexisting methods, LLMs cannot precisely assess the relevance of retrieved\ndocuments, thus likely leading to misleading or even incorrect utilization of\nexternal knowledge (eg., retrieved documents). To address this issue, in this\npaper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for\nopen-domain question answering (QA). As the key motivation, we aim to enhance\nthe self-awareness regarding the reliability of external knowledge for LLMs, so\nas to adaptively utilize external knowledge in RAG systems. Specially, we\ndevelop a novel architecture for LLM-based RAG systems, by incorporating a\nspecially designed assessment module that precisely assesses the relevance of\nretrieved documents. Furthermore, we propose an improved training method based\non bi-granularity relevance fusion and noise-resistant training. By combining\nthe improvements in both architecture and training, our proposed REAR can\nbetter utilize external knowledge by effectively perceiving the relevance of\nretrieved documents. Experiments on four open-domain QA tasks show that REAR\nsignificantly outperforms previous a number of competitive RAG approaches. Our\ncodes can be accessed at https://github.com/RUCAIBox/REAR.\n","authors":["Yuhao Wang","Ruiyang Ren","Junyi Li","Wayne Xin Zhao","Jing Liu","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2402.17497v2.pdf","comment":"Accepted to EMNLP 2024 Main Conference. Published on ACL Anthology:\n  https://aclanthology.org/2024.emnlp-main.321.pdf"},{"id":"http://arxiv.org/abs/2411.13892v1","updated":"2024-11-21T07:12:47Z","published":"2024-11-21T07:12:47Z","title":"Topology-Aware Popularity Debiasing via Simplicial Complexes","summary":"  Recommender systems (RS) play a critical role in delivering personalized\ncontent across various online platforms, leveraging collaborative filtering\n(CF) as a key technique to generate recommendations based on users' historical\ninteraction data. Recent advancements in CF have been driven by the adoption of\nGraph Neural Networks (GNNs), which model user-item interactions as bipartite\ngraphs, enabling the capture of high-order collaborative signals. Despite their\nsuccess, GNN-based methods face significant challenges due to the inherent\npopularity bias in the user-item interaction graph's topology, leading to\nskewed recommendations that favor popular items over less-known ones.\n  To address this challenge, we propose a novel topology-aware popularity\ndebiasing framework, Test-time Simplicial Propagation (TSP), which incorporates\nsimplicial complexes (SCs) to enhance the expressiveness of GNNs. Unlike\ntraditional methods that focus on pairwise relationships, our approach captures\nmulti-order relationships through SCs, providing a more comprehensive\nrepresentation of user-item interactions. By enriching the neighborhoods of\ntail items and leveraging SCs for feature smoothing, TSP enables the\npropagation of multi-order collaborative signals and effectively mitigates\nbiased propagation.\n  Our TSP module is designed as a plug-and-play solution, allowing for seamless\nintegration into pre-trained GNN-based models without the need for fine-tuning\nadditional parameters. Extensive experiments on five real-world datasets\ndemonstrate the superior performance of our method, particularly in long-tail\nrecommendation tasks. Visualization results further confirm that TSP produces\nmore uniform distributions of item representations, leading to fairer and more\naccurate recommendations.\n","authors":["Yanbiao Ji","Yue Ding","Chang Liu","Yuxiang Lu","Xin Xin","Hongtao Lu"],"pdf_url":"https://arxiv.org/pdf/2411.13892v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13865v1","updated":"2024-11-21T06:01:47Z","published":"2024-11-21T06:01:47Z","title":"HARec: Hyperbolic Graph-LLM Alignment for Exploration and Exploitation\n  in Recommender Systems","summary":"  Modern recommendation systems often create information cocoons, limiting\nusers' exposure to diverse content. To enhance user experience, a crucial\nchallenge is developing systems that can balance content exploration and\nexploitation, allowing users to adjust their recommendation preferences.\nIntuitively, this balance can be achieved through a tree-structured\nrepresentation, where depth search facilitates exploitation and breadth search\nenables exploration. However, current works face two challenges to achieve this\ntarget: (1) Euclidean methods fail to fully capture hierarchical structures and\nlack flexibility in balancing exploration-exploitation, while (2) hyperbolic\napproaches, despite better hierarchical modeling, suffer from insufficient\nsemantic alignment due to their reliance on Euclidean text encoders. To address\nthese challenges, we propose HARec, a hyperbolic representation learning\nframework that jointly aligns user-item collaborative information with textual\ndescriptions in hyperbolic space. Our framework introduces two key technique\nnovelty: (1) a hierarchical-aware graph-llm alignment mechanism that enables\nbetter hierarchical representation, and (2) a hyperbolic hierarchical tree\nstructure that facilitates user-adjustable exploration-exploitation trade-offs.\nExtensive experiments demonstrate that HARec consistently outperforms both\nEuclidean and hyperbolic baselines, achieving up to 5.49% improvement in\nutility metrics and 11.39% increase in diversity metrics.\n","authors":["Qiyao Ma","Menglin Yang","Mingxuan Ju","Tong Zhao","Neil Shah","Rex Ying"],"pdf_url":"https://arxiv.org/pdf/2411.13865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11240v2","updated":"2024-11-21T05:58:24Z","published":"2024-11-18T02:15:29Z","title":"Controlling Diversity at Inference: Guiding Diffusion Recommender Models\n  with Targeted Category Preferences","summary":"  Diversity control is an important task to alleviate bias amplification and\nfilter bubble problems. The desired degree of diversity may fluctuate based on\nusers' daily moods or business strategies. However, existing methods for\ncontrolling diversity often lack flexibility, as diversity is decided during\ntraining and cannot be easily modified during inference. We propose\n\\textbf{D3Rec} (\\underline{D}isentangled \\underline{D}iffusion model for\n\\underline{D}iversified \\underline{Rec}ommendation), an end-to-end method that\ncontrols the accuracy-diversity trade-off at inference. D3Rec meets our three\ndesiderata by (1) generating recommendations based on category preferences, (2)\ncontrolling category preferences during the inference phase, and (3) adapting\nto arbitrary targeted category preferences. In the forward process, D3Rec\nremoves category preferences lurking in user interactions by adding noises.\nThen, in the reverse process, D3Rec generates recommendations through denoising\nsteps while reflecting desired category preferences. Extensive experiments on\nreal-world and synthetic datasets validate the effectiveness of D3Rec in\ncontrolling diversity at inference.\n","authors":["Gwangseok Han","Wonbin Kweon","Minsoo Kim","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2411.11240v2.pdf","comment":"KDD 2025"},{"id":"http://arxiv.org/abs/2406.17335v2","updated":"2024-11-21T05:42:02Z","published":"2024-06-25T07:45:00Z","title":"A Thorough Performance Benchmarking on Lightweight Embedding-based\n  Recommender Systems","summary":"  Since the creation of the Web, recommender systems (RSs) have been an\nindispensable mechanism in information filtering. State-of-the-art RSs\nprimarily depend on categorical features, which ecoded by embedding vectors,\nresulting in excessively large embedding tables. To prevent over-parameterized\nembedding tables from harming scalability, both academia and industry have seen\nincreasing efforts in compressing RS embeddings. However, despite the\nprosperity of lightweight embedding-based RSs (LERSs), a wide diversity is seen\nin evaluation protocols, resulting in obstacles when relating LERS performance\nto real-world usability. Moreover, despite the common goal of lightweight\nembeddings, LERSs are evaluated with a single choice between the two main\nrecommendation tasks -- collaborative filtering and content-based\nrecommendation. This lack of discussions on cross-task transferability hinders\nthe development of unified, more scalable solutions. Motivated by these issues,\nthis study investigates various LERSs' performance, efficiency, and cross-task\ntransferability via a thorough benchmarking process. Additionally, we propose\nan efficient embedding compression method using magnitude pruning, which is an\neasy-to-deploy yet highly competitive baseline that outperforms various complex\nLERSs. Our study reveals the distinct performance of LERSs across the two\ntasks, shedding light on their effectiveness and generalizability. To support\nedge-based recommendations, we tested all LERSs on a Raspberry Pi 4, where the\nefficiency bottleneck is exposed. Finally, we conclude this paper with critical\nsummaries of LERS performance, model selection suggestions, and underexplored\nchallenges around LERSs for future research. To encourage future research, we\npublish source codes and artifacts at \\href{this\nlink}{https://github.com/chenxing1999/recsys-benchmark}.\n","authors":["Hung Vinh Tran","Tong Chen","Quoc Viet Hung Nguyen","Zi Huang","Lizhen Cui","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2406.17335v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05666v8","updated":"2024-11-21T05:19:04Z","published":"2024-06-09T06:49:22Z","title":"Distribution Learning and Its Application in Deep Learning","summary":"  This paper introduces a novel theoretical learning framework, termed\nprobability distribution learning (PD learning). Departing from the traditional\nstatistical learning framework, PD learning focuses on learning the underlying\nprobability distribution, which is modeled as a random variable within the\nprobability simplex. Within this framework, the optimization objective is\nlearning error, which quantifies the posterior expected discrepancy between the\nmodel's predicted distribution and the underlying true distribution, given\navailable sample data and prior knowledge. To optimize the learning error, this\npaper proposes the necessary conditions for loss functions, models, and\noptimization algorithms, ensuring that these conditions are all satisfied in\nreal-world machine learning scenarios. Based on these conditions, the\nnon-convex optimization mechanism corresponding to model training can be\ntheoretically resolved. Moreover, the paper provides both model-dependent and\nmodel-independent bounds on learning error, offering new insights into the\nmodel's fitting ability and generalization capabilities. Furthermore, the paper\napplies the PD learning framework to elucidate the mechanisms by which various\ntechniques, including random parameter initialization, over-parameterization,\nand dropout, influence deep model training. Finally, the paper substantiates\nthe key conclusions of the proposed framework through experimental results.\n","authors":["Binchuan Qi","Wei Gong","Li Li"],"pdf_url":"https://arxiv.org/pdf/2406.05666v8.pdf","comment":"arXiv admin note: text overlap with arXiv:2105.04026 by other\n  authors. arXiv admin note: text overlap with arXiv:2105.04026 by other\n  authors"},{"id":"http://arxiv.org/abs/2406.12356v3","updated":"2024-11-21T03:09:23Z","published":"2024-06-18T07:41:11Z","title":"A Gradient Accumulation Method for Dense Retriever under Memory\n  Constraint","summary":"  InfoNCE loss is commonly used to train dense retriever in information\nretrieval tasks. It is well known that a large batch is essential to stable and\neffective training with InfoNCE loss, which requires significant hardware\nresources. Due to the dependency of large batch, dense retriever has bottleneck\nof application and research. Recently, memory reduction methods have been\nbroadly adopted to resolve the hardware bottleneck by decomposing forward and\nbackward or using a memory bank. However, current methods still suffer from\nslow and unstable training. To address these issues, we propose Contrastive\nAccumulation (ContAccum), a stable and efficient memory reduction method for\ndense retriever trains that uses a dual memory bank structure to leverage\npreviously generated query and passage representations. Experiments on widely\nused five information retrieval datasets indicate that ContAccum can surpass\nnot only existing memory reduction methods but also high-resource scenario.\nMoreover, theoretical analysis and experimental results confirm that ContAccum\nprovides more stable dual-encoder training than current memory bank utilization\nmethods.\n","authors":["Jaehee Kim","Yukyung Lee","Pilsung Kang"],"pdf_url":"https://arxiv.org/pdf/2406.12356v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13789v1","updated":"2024-11-21T02:22:35Z","published":"2024-11-21T02:22:35Z","title":"LEADRE: Multi-Faceted Knowledge Enhanced LLM Empowered Display\n  Advertisement Recommender System","summary":"  Display advertising provides significant value to advertisers, publishers,\nand users. Traditional display advertising systems utilize a multi-stage\narchitecture consisting of retrieval, coarse ranking, and final ranking.\nHowever, conventional retrieval methods rely on ID-based learning to rank\nmechanisms and fail to adequately utilize the content information of ads, which\nhampers their ability to provide diverse recommendation lists.\n  To address this limitation, we propose leveraging the extensive world\nknowledge of LLMs. However, three key challenges arise when attempting to\nmaximize the effectiveness of LLMs: \"How to capture user interests\", \"How to\nbridge the knowledge gap between LLMs and advertising system\", and \"How to\nefficiently deploy LLMs\". To overcome these challenges, we introduce a novel\nLLM-based framework called LLM Empowered Display ADvertisement REcommender\nsystem (LEADRE). LEADRE consists of three core modules: (1) The Intent-Aware\nPrompt Engineering introduces multi-faceted knowledge and designs intent-aware\n<Prompt, Response> pairs that fine-tune LLMs to generate ads tailored to users'\npersonal interests. (2) The Advertising-Specific Knowledge Alignment\nincorporates auxiliary fine-tuning tasks and Direct Preference Optimization\n(DPO) to align LLMs with ad semantic and business value. (3) The Efficient\nSystem Deployment deploys LEADRE in an online environment by integrating both\nlatency-tolerant and latency-sensitive service. Extensive offline experiments\ndemonstrate the effectiveness of LEADRE and validate the contributions of\nindividual modules. Online A/B test shows that LEADRE leads to a 1.57% and\n1.17% GMV lift for serviced users on WeChat Channels and Moments separately.\nLEADRE has been deployed on both platforms, serving tens of billions of\nrequests each day.\n","authors":["Fengxin Li","Yi Li","Yue Liu","Chao Zhou","Yuan Wang","Xiaoxiang Deng","Wei Xue","Dapeng Liu","Lei Xiao","Haijie Gu","Jie Jiang","Hongyan Liu","Biao Qin","Jun He"],"pdf_url":"https://arxiv.org/pdf/2411.13789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00262v2","updated":"2024-11-21T00:10:23Z","published":"2024-10-31T23:41:09Z","title":"Content Aware Analysis of Scholarly Networks: A Case Study on CORD19\n  Dataset","summary":"  This paper investigates the relationships among key elements of the\nscientific research network, namely articles, researchers, and journals. We\nintroduce a novel approach to use semantic information through the HITS\nalgorithm-based propagation of topic information in the network. The topic\ninformation is derived by using the Named Entity Recognition and Entity\nLinkage. In our case, MedCAT is used to extract the topics from the CORD19\nDataset, which is a corpus of academic articles about COVID-19 and the\ncoronavirus scientific network. Our approach focuses on the COVID-19 domain,\nutilizing the CORD-19 dataset to demonstrate the efficacy of integrating\ntopic-related information within the citation framework. Through the\napplication of a hybrid HITS algorithm, we show that incorporating topic data\nsignificantly influences article rankings, revealing deeper insights into the\nstructure of the academic community.\n","authors":["Mehmet Emre Akbulut","Yusuf Erdem Nacar"],"pdf_url":"https://arxiv.org/pdf/2411.00262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14592v1","updated":"2024-11-21T21:22:58Z","published":"2024-11-21T21:22:58Z","title":"G-RAG: Knowledge Expansion in Material Science","summary":"  In the field of Material Science, effective information retrieval systems are\nessential for facilitating research. Traditional Retrieval-Augmented Generation\n(RAG) approaches in Large Language Models (LLMs) often encounter challenges\nsuch as outdated information, hallucinations, limited interpretability due to\ncontext constraints, and inaccurate retrieval. To address these issues, Graph\nRAG integrates graph databases to enhance the retrieval process. Our proposed\nmethod processes Material Science documents by extracting key entities\n(referred to as MatIDs) from sentences, which are then utilized to query\nexternal Wikipedia knowledge bases (KBs) for additional relevant information.\nWe implement an agent-based parsing technique to achieve a more detailed\nrepresentation of the documents. Our improved version of Graph RAG called G-RAG\nfurther leverages a graph database to capture relationships between these\nentities, improving both retrieval accuracy and contextual understanding. This\nenhanced approach demonstrates significant improvements in performance for\ndomains that require precise information retrieval, such as Material Science.\n","authors":["Radeen Mostafa","Mirza Nihal Baig","Mashaekh Tausif Ehsan","Jakir Hasan"],"pdf_url":"https://arxiv.org/pdf/2411.14592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14569v1","updated":"2024-11-21T20:31:48Z","published":"2024-11-21T20:31:48Z","title":"Variable Extraction for Model Recovery in Scientific Literature","summary":"  The global output of academic publications exceeds 5 million articles per\nyear, making it difficult for humans to keep up with even a tiny fraction of\nscientific output. We need methods to navigate and interpret the artifacts --\ntexts, graphs, charts, code, models, and datasets -- that make up the\nliterature. This paper evaluates various methods for extracting mathematical\nmodel variables from epidemiological studies, such as ``infection rate\n($\\alpha$),'' ``recovery rate ($\\gamma$),'' and ``mortality rate ($\\mu$).''\nVariable extraction appears to be a basic task, but plays a pivotal role in\nrecovering models from scientific literature. Once extracted, we can use these\nvariables for automatic mathematical modeling, simulation, and replication of\npublished results.\n  We introduce a benchmark dataset comprising manually-annotated variable\ndescriptions and variable values extracted from scientific papers. Based on\nthis dataset, we present several baseline methods for variable extraction based\non Large Language Models (LLMs) and rule-based information extraction systems.\nOur analysis shows that LLM-based solutions perform the best. Despite the\nincremental benefits of combining rule-based extraction outputs with LLMs, the\nleap in performance attributed to the transfer-learning and instruction-tuning\ncapabilities of LLMs themselves is far more significant. This investigation\ndemonstrates the potential of LLMs to enhance automatic comprehension of\nscientific artifacts and for automatic model recovery and simulation.\n","authors":["Chunwei Liu","Enrique Noriega-Atala","Adarsh Pyarelal","Clayton T Morrison","Mike Cafarella"],"pdf_url":"https://arxiv.org/pdf/2411.14569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14551v1","updated":"2024-11-21T19:45:48Z","published":"2024-11-21T19:45:48Z","title":"An Experimental Study on Data Augmentation Techniques for Named Entity\n  Recognition on Low-Resource Domains","summary":"  Named Entity Recognition (NER) is a machine learning task that traditionally\nrelies on supervised learning and annotated data. Acquiring such data is often\na challenge, particularly in specialized fields like medical, legal, and\nfinancial sectors. Those are commonly referred to as low-resource domains,\nwhich comprise long-tail entities, due to the scarcity of available data. To\naddress this, data augmentation techniques are increasingly being employed to\ngenerate additional training instances from the original dataset. In this\nstudy, we evaluate the effectiveness of two prominent text augmentation\ntechniques, Mention Replacement and Contextual Word Replacement, on two\nwidely-used NER models, Bi-LSTM+CRF and BERT. We conduct experiments on four\ndatasets from low-resource domains, and we explore the impact of various\ncombinations of training subset sizes and number of augmented examples. We not\nonly confirm that data augmentation is particularly beneficial for smaller\ndatasets, but we also demonstrate that there is no universally optimal number\nof augmented examples, i.e., NER practitioners must experiment with different\nquantities in order to fine-tune their projects.\n","authors":["Arthur Elwing Torres","Edleno Silva de Moura","Altigran Soares da Silva","Mario A. Nascimento","Filipe Mesquita"],"pdf_url":"https://arxiv.org/pdf/2411.14551v1.pdf","comment":"21 pages, 2 figures"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2411.14430v1","updated":"2024-11-21T18:59:51Z","published":"2024-11-21T18:59:51Z","title":"Stable Flow: Vital Layers for Training-Free Image Editing","summary":"  Diffusion models have revolutionized the field of content synthesis and\nediting. Recent models have replaced the traditional UNet architecture with the\nDiffusion Transformer (DiT), and employed flow-matching for improved training\nand sampling. However, they exhibit limited generation diversity. In this work,\nwe leverage this limitation to perform consistent image edits via selective\ninjection of attention features. The main challenge is that, unlike the\nUNet-based models, DiT lacks a coarse-to-fine synthesis structure, making it\nunclear in which layers to perform the injection. Therefore, we propose an\nautomatic method to identify \"vital layers\" within DiT, crucial for image\nformation, and demonstrate how these layers facilitate a range of controlled\nstable edits, from non-rigid modifications to object addition, using the same\nmechanism. Next, to enable real-image editing, we introduce an improved image\ninversion method for flow models. Finally, we evaluate our approach through\nqualitative and quantitative comparisons, along with a user study, and\ndemonstrate its effectiveness across multiple applications. The project page is\navailable at https://omriavrahami.com/stable-flow\n","authors":["Omri Avrahami","Or Patashnik","Ohad Fried","Egor Nemchinov","Kfir Aberman","Dani Lischinski","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2411.14430v1.pdf","comment":"Project page is available at https://omriavrahami.com/stable-flow"},{"id":"http://arxiv.org/abs/2411.14424v1","updated":"2024-11-21T18:56:33Z","published":"2024-11-21T18:56:33Z","title":"Learning Fair Robustness via Domain Mixup","summary":"  Adversarial training is one of the predominant techniques for training\nclassifiers that are robust to adversarial attacks. Recent work, however has\nfound that adversarial training, which makes the overall classifier robust, it\ndoes not necessarily provide equal amount of robustness for all classes. In\nthis paper, we propose the use of mixup for the problem of learning fair robust\nclassifiers, which can provide similar robustness across all classes.\nSpecifically, the idea is to mix inputs from the same classes and perform\nadversarial training on mixed up inputs. We present a theoretical analysis of\nthis idea for the case of linear classifiers and show that mixup combined with\nadversarial training can provably reduce the class-wise robustness disparity.\nThis method not only contributes to reducing the disparity in class-wise\nadversarial risk, but also the class-wise natural risk. Complementing our\ntheoretical analysis, we also provide experimental results on both synthetic\ndata and the real world dataset (CIFAR-10), which shows improvement in class\nwise disparities for both natural and adversarial risks.\n","authors":["Meiyu Zhong","Ravi Tandon"],"pdf_url":"https://arxiv.org/pdf/2411.14424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14421v1","updated":"2024-11-21T18:54:43Z","published":"2024-11-21T18:54:43Z","title":"From RNNs to Foundation Models: An Empirical Study on Commercial\n  Building Energy Consumption","summary":"  Accurate short-term energy consumption forecasting for commercial buildings\nis crucial for smart grid operations. While smart meters and deep learning\nmodels enable forecasting using past data from multiple buildings, data\nheterogeneity from diverse buildings can reduce model performance. The impact\nof increasing dataset heterogeneity in time series forecasting, while keeping\nsize and model constant, is understudied. We tackle this issue using the\nComStock dataset, which provides synthetic energy consumption data for U.S.\ncommercial buildings. Two curated subsets, identical in size and region but\ndiffering in building type diversity, are used to assess the performance of\nvarious time series forecasting models, including fine-tuned open-source\nfoundation models (FMs). The results show that dataset heterogeneity and model\narchitecture have a greater impact on post-training forecasting performance\nthan the parameter count. Moreover, despite the higher computational cost,\nfine-tuned FMs demonstrate competitive performance compared to base models\ntrained from scratch.\n","authors":["Shourya Bose","Yijiang Li","Amy Van Sant","Yu Zhang","Kibaek Kim"],"pdf_url":"https://arxiv.org/pdf/2411.14421v1.pdf","comment":"NeurIPS 2024 Workshop on Time Series in the Age of Large Models"},{"id":"http://arxiv.org/abs/2408.00754v2","updated":"2024-11-21T18:52:31Z","published":"2024-08-01T17:57:12Z","title":"Coarse Correspondences Boost Spatial-Temporal Reasoning in Multimodal\n  Language Model","summary":"  Multimodal language models (MLLMs) are increasingly being applied in\nreal-world environments, necessitating their ability to interpret 3D spaces and\ncomprehend temporal dynamics. Current methods often rely on specialized\narchitectural designs or task-specific fine-tuning to achieve this. We\nintroduce Coarse Correspondences, a simple lightweight method that enhances\nMLLMs' spatial-temporal reasoning with 2D images as input, without modifying\nthe architecture or requiring task-specific fine-tuning. Our method uses a\nlightweight tracking model to identify primary object correspondences between\nframes in a video or across different image viewpoints, and then conveys this\ninformation to MLLMs through visual prompting. We demonstrate that this simple\ntraining-free approach brings substantial gains to GPT4-V/O consistently on\nfour benchmarks that require spatial-temporal reasoning, including +20.5\\%\nimprovement on ScanQA, +9.7\\% on OpenEQA's episodic memory subset, +6.0\\% on\nthe long-form video benchmark EgoSchema, and +11\\% on the R2R navigation\nbenchmark. Additionally, we show that Coarse Correspondences can also enhance\nopen-source MLLMs' spatial reasoning (by +6.9\\% on ScanQA) when applied in both\ntraining and inference and that the improvement can generalize to unseen\ndatasets such as SQA3D (+3.1\\%). Taken together, we show that Coarse\nCorrespondences effectively and efficiently boosts models' performance on\ndownstream tasks requiring spatial-temporal reasoning.\n","authors":["Benlin Liu","Yuhao Dong","Yiqin Wang","Zixian Ma","Yansong Tang","Luming Tang","Yongming Rao","Wei-Chiu Ma","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2408.00754v2.pdf","comment":"project page: https://coarse-correspondence.github.io"},{"id":"http://arxiv.org/abs/2411.14411v1","updated":"2024-11-21T18:46:23Z","published":"2024-11-21T18:46:23Z","title":"Multi-Agent Environments for Vehicle Routing Problems","summary":"  Research on Reinforcement Learning (RL) approaches for discrete optimization\nproblems has increased considerably, extending RL to an area classically\ndominated by Operations Research (OR). Vehicle routing problems are a good\nexample of discrete optimization problems with high practical relevance where\nRL techniques have had considerable success. Despite these advances,\nopen-source development frameworks remain scarce, hampering both the testing of\nalgorithms and the ability to objectively compare results. This ultimately\nslows down progress in the field and limits the exchange of ideas between the\nRL and OR communities.\n  Here we propose a library composed of multi-agent environments that simulates\nclassic vehicle routing problems. The library, built on PyTorch, provides a\nflexible modular architecture design that allows easy customization and\nincorporation of new routing problems. It follows the Agent Environment Cycle\n(\"AEC\") games model and has an intuitive API, enabling rapid adoption and easy\nintegration into existing reinforcement learning frameworks.\n  The library allows for a straightforward use of classical OR benchmark\ninstances in order to narrow the gap between the test beds for algorithm\nbenchmarking used by the RL and OR communities. Additionally, we provide\nbenchmark instance sets for each environment, as well as baseline RL models and\ntraining code.\n","authors":["Ricardo Gama","Daniel Fuertes","Carlos R. del-Blanco","Hugo L. Fernandes"],"pdf_url":"https://arxiv.org/pdf/2411.14411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14402v1","updated":"2024-11-21T18:31:25Z","published":"2024-11-21T18:31:25Z","title":"Multimodal Autoregressive Pre-training of Large Vision Encoders","summary":"  We introduce a novel method for pre-training of large-scale vision encoders.\nBuilding on recent advancements in autoregressive pre-training of vision\nmodels, we extend this framework to a multimodal setting, i.e., images and\ntext. In this paper, we present AIMV2, a family of generalist vision encoders\ncharacterized by a straightforward pre-training process, scalability, and\nremarkable performance across a range of downstream tasks. This is achieved by\npairing the vision encoder with a multimodal decoder that autoregressively\ngenerates raw image patches and text tokens. Our encoders excel not only in\nmultimodal evaluations but also in vision benchmarks such as localization,\ngrounding, and classification. Notably, our AIMV2-3B encoder achieves 89.5%\naccuracy on ImageNet-1k with a frozen trunk. Furthermore, AIMV2 consistently\noutperforms state-of-the-art contrastive models (e.g., CLIP, SigLIP) in\nmultimodal image understanding across diverse settings.\n","authors":["Enrico Fini","Mustafa Shukor","Xiujun Li","Philipp Dufter","Michal Klein","David Haldimann","Sai Aitharaju","Victor Guilherme Turrisi da Costa","Louis Béthune","Zhe Gan","Alexander T Toshev","Marcin Eichner","Moin Nabi","Yinfei Yang","Joshua M. Susskind","Alaaeldin El-Nouby"],"pdf_url":"https://arxiv.org/pdf/2411.14402v1.pdf","comment":"https://github.com/apple/ml-aim"},{"id":"http://arxiv.org/abs/2411.14401v1","updated":"2024-11-21T18:30:11Z","published":"2024-11-21T18:30:11Z","title":"Beyond Training: Dynamic Token Merging for Zero-Shot Video Understanding","summary":"  Recent advancements in multimodal large language models (MLLMs) have opened\nnew avenues for video understanding. However, achieving high fidelity in\nzero-shot video tasks remains challenging. Traditional video processing methods\nrely heavily on fine-tuning to capture nuanced spatial-temporal details, which\nincurs significant data and computation costs. In contrast, training-free\napproaches, though efficient, often lack robustness in preserving context-rich\nfeatures across complex video content. To this end, we propose DYTO, a novel\ndynamic token merging framework for zero-shot video understanding that\nadaptively optimizes token efficiency while preserving crucial scene details.\nDYTO integrates a hierarchical frame selection and a bipartite token merging\nstrategy to dynamically cluster key frames and selectively compress token\nsequences, striking a balance between computational efficiency with semantic\nrichness. Extensive experiments across multiple benchmarks demonstrate the\neffectiveness of DYTO, achieving superior performance compared to both\nfine-tuned and training-free methods and setting a new state-of-the-art for\nzero-shot video understanding.\n","authors":["Yiming Zhang","Zhuokai Zhao","Zhaorun Chen","Zenghui Ding","Xianjun Yang","Yining Sun"],"pdf_url":"https://arxiv.org/pdf/2411.14401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14390v1","updated":"2024-11-21T18:24:06Z","published":"2024-11-21T18:24:06Z","title":"Persistent Homology for Structural Characterization in Disordered\n  Systems","summary":"  We propose a unified framework based on persistent homology (PH) to\ncharacterize both local and global structures in disordered systems. It can\nsimultaneously generate local and global descriptors using the same algorithm\nand data structure, and has shown to be highly effective and interpretable in\npredicting particle rearrangements and classifying global phases. Based on this\nframework, we define a non-parametric metric, the Separation Index (SI), which\nnot only outperforms traditional bond-orientational order parameters in phase\nclassification tasks but also establishes a connection between particle\nenvironments and the global phase structure. Our methods provide an effective\nframework for understanding and analyzing the properties of disordered\nmaterials, with broad potential applications in materials science and even\nwider studies of complex systems.\n","authors":["An Wang","Li Zou"],"pdf_url":"https://arxiv.org/pdf/2411.14390v1.pdf","comment":"19 pages, 17 figures"},{"id":"http://arxiv.org/abs/2411.14378v1","updated":"2024-11-21T18:13:03Z","published":"2024-11-21T18:13:03Z","title":"CoNFiLD-inlet: Synthetic Turbulence Inflow Using Generative Latent\n  Diffusion Models with Neural Fields","summary":"  Eddy-resolving turbulence simulations require stochastic inflow conditions\nthat accurately replicate the complex, multi-scale structures of turbulence.\nTraditional recycling-based methods rely on computationally expensive precursor\nsimulations, while existing synthetic inflow generators often fail to reproduce\nrealistic coherent structures of turbulence. Recent advances in deep learning\n(DL) have opened new possibilities for inflow turbulence generation, yet many\nDL-based methods rely on deterministic, autoregressive frameworks prone to\nerror accumulation, resulting in poor robustness for long-term predictions. In\nthis work, we present CoNFiLD-inlet, a novel DL-based inflow turbulence\ngenerator that integrates diffusion models with a conditional neural field\n(CNF)-encoded latent space to produce realistic, stochastic inflow turbulence.\nBy parameterizing inflow conditions using Reynolds numbers, CoNFiLD-inlet\ngeneralizes effectively across a wide range of Reynolds numbers ($Re_\\tau$\nbetween $10^3$ and $10^4$) without requiring retraining or parameter tuning.\nComprehensive validation through a priori and a posteriori tests in Direct\nNumerical Simulation (DNS) and Wall-Modeled Large Eddy Simulation (WMLES)\ndemonstrates its high fidelity, robustness, and scalability, positioning it as\nan efficient and versatile solution for inflow turbulence synthesis.\n","authors":["Xin-Yang Liu","Meet Hemant Parikh","Xiantao Fan","Pan Du","Qing Wang","Yi-Fan Chen","Jian-Xun Wang"],"pdf_url":"https://arxiv.org/pdf/2411.14378v1.pdf","comment":"27 pages, 10 figures"},{"id":"http://arxiv.org/abs/2411.06650v2","updated":"2024-11-21T18:09:56Z","published":"2024-11-11T01:34:10Z","title":"Quantum Policy Gradient in Reproducing Kernel Hilbert Space","summary":"  Parametrised quantum circuits offer expressive and data-efficient\nrepresentations for machine learning. Due to quantum states residing in a\nhigh-dimensional Hilbert space, parametrised quantum circuits have a natural\ninterpretation in terms of kernel methods. The representation of quantum\ncircuits in terms of quantum kernels has been studied widely in quantum\nsupervised learning, but has been overlooked in the context of quantum\nreinforcement learning. This paper proposes parametric and non-parametric\npolicy gradient and actor-critic algorithms with quantum kernel policies in\nquantum environments. This approach, implemented with both numerical and\nanalytical quantum policy gradient techniques, allows exploiting the many\nadvantages of kernel methods, including available analytic forms for the\ngradient of the policy and tunable expressiveness. The proposed approach is\nsuitable for vector-valued action spaces and each of the formulations\ndemonstrates a quadratic reduction in query complexity compared to their\nclassical counterparts. Two actor-critic algorithms, one based on stochastic\npolicy gradient and one based on deterministic policy gradient (comparable to\nthe popular DDPG algorithm), demonstrate additional query complexity reductions\ncompared to quantum policy gradient algorithms under favourable conditions.\n","authors":["David M. Bossens","Kishor Bharti","Jayne Thompson"],"pdf_url":"https://arxiv.org/pdf/2411.06650v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14375v1","updated":"2024-11-21T18:09:20Z","published":"2024-11-21T18:09:20Z","title":"Model Checking for Reinforcement Learning in Autonomous Driving: One Can\n  Do More Than You Think!","summary":"  Most reinforcement learning (RL) platforms use high-level programming\nlanguages, such as OpenAI Gymnasium using Python. These frameworks provide\nvarious API and benchmarks for testing RL algorithms in different domains, such\nas autonomous driving (AD) and robotics. These platforms often emphasise the\ndesign of RL algorithms and the training performance but neglect the\ncorrectness of models and reward functions, which can be crucial for the\nsuccessful application of RL. This paper proposes using formal methods to model\nAD systems and demonstrates how model checking (MC) can be used in RL for AD.\nMost studies combining MC and RL focus on safety, such as safety shields.\nHowever, this paper shows different facets where MC can strengthen RL. First,\nan MC-based model pre-analysis can reveal bugs with respect to sensor accuracy\nand learning step size. This step serves as a preparation of RL, which saves\ntime if bugs exist and deepens users' understanding of the target system.\nSecond, reward automata can benefit the design of reward functions and greatly\nimprove learning performance especially when the learning objectives are\nmultiple. All these findings are supported by experiments.\n","authors":["Rong Gu"],"pdf_url":"https://arxiv.org/pdf/2411.14375v1.pdf","comment":"In Proceedings FMAS2024, arXiv:2411.13215"},{"id":"http://arxiv.org/abs/2406.05153v2","updated":"2024-11-21T18:08:26Z","published":"2024-06-04T11:30:40Z","title":"Integrating Physics of the Problem into Data-Driven Methods to Enhance\n  Elastic Full-Waveform Inversion with Uncertainty Quantification","summary":"  Full-Waveform Inversion (FWI) is a nonlinear iterative seismic imaging\ntechnique that, by reducing the misfit between recorded and predicted seismic\nwaveforms, can produce detailed estimates of subsurface geophysical properties.\nNevertheless, the strong nonlinearity of FWI can trap the optimization in local\nminima. This issue arises due to factors such as improper initial values, the\nabsence of low frequencies in the measurements, noise, and other related\nconsiderations. To address this challenge and with the advent of advanced\nmachine-learning techniques, data-driven methods, such as deep learning, have\nattracted significantly increasing attention in the geophysical community.\nFurthermore, the elastic wave equation should be included in FWI to represent\nelastic effects accurately. The intersection of data-driven techniques and\nelastic scattering theories presents opportunities and challenges. In this\npaper, by using the knowledge of elastic scattering (physics of the problem)\nand integrating it with machine learning techniques, we propose methods for the\nsolution of time-harmonic FWI to enhance accuracy compared to pure data-driven\nand physics-based approaches. Moreover, to address uncertainty quantification,\nby modifying the structure of the Variational Autoencoder, we introduce a\nprobabilistic deep learning method based on the physics of the problem that\nenables us to explore the uncertainties of the solution. According to the\nlimited availability of datasets in this field and to assess the performance\nand accuracy of the proposed methods, we create a comprehensive dataset close\nto reality and conduct a comparative analysis of the presented approaches to\nit.\n","authors":["Vahid Negahdari","Seyed Reza Moghadasi","Mohammad Reza Razvan"],"pdf_url":"https://arxiv.org/pdf/2406.05153v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11112v2","updated":"2024-11-21T18:00:19Z","published":"2024-10-14T21:43:48Z","title":"Differentiable Weightless Neural Networks","summary":"  We introduce the Differentiable Weightless Neural Network (DWN), a model\nbased on interconnected lookup tables. Training of DWNs is enabled by a novel\nExtended Finite Difference technique for approximate differentiation of binary\nvalues. We propose Learnable Mapping, Learnable Reduction, and Spectral\nRegularization to further improve the accuracy and efficiency of these models.\nWe evaluate DWNs in three edge computing contexts: (1) an FPGA-based hardware\naccelerator, where they demonstrate superior latency, throughput, energy\nefficiency, and model area compared to state-of-the-art solutions, (2) a\nlow-power microcontroller, where they achieve preferable accuracy to XGBoost\nwhile subject to stringent memory constraints, and (3) ultra-low-cost chips,\nwhere they consistently outperform small models in both accuracy and projected\nhardware area. DWNs also compare favorably against leading approaches for\ntabular datasets, with higher average rank. Overall, our work positions DWNs as\na pioneering solution for edge-compatible high-throughput neural networks.\n","authors":["Alan T. L. Bacellar","Zachary Susskind","Mauricio Breternitz Jr.","Eugene John","Lizy K. John","Priscila M. V. Lima","Felipe M. G. França"],"pdf_url":"https://arxiv.org/pdf/2410.11112v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14354v1","updated":"2024-11-21T17:53:27Z","published":"2024-11-21T17:53:27Z","title":"Contrasting local and global modeling with machine learning and\n  satellite data: A case study estimating tree canopy height in African\n  savannas","summary":"  While advances in machine learning with satellite imagery (SatML) are\nfacilitating environmental monitoring at a global scale, developing SatML\nmodels that are accurate and useful for local regions remains critical to\nunderstanding and acting on an ever-changing planet. As increasing attention\nand resources are being devoted to training SatML models with global data, it\nis important to understand when improvements in global models will make it\neasier to train or fine-tune models that are accurate in specific regions. To\nexplore this question, we contrast local and global training paradigms for\nSatML through a case study of tree canopy height (TCH) mapping in the Karingani\nGame Reserve, Mozambique. We find that recent advances in global TCH mapping do\nnot necessarily translate to better local modeling abilities in our study\nregion. Specifically, small models trained only with locally-collected data\noutperform published global TCH maps, and even outperform globally pretrained\nmodels that we fine-tune using local data. Analyzing these results further, we\nidentify specific points of conflict and synergy between local and global\nmodeling paradigms that can inform future research toward aligning local and\nglobal performance objectives in geospatial machine learning.\n","authors":["Esther Rolf","Lucia Gordon","Milind Tambe","Andrew Davies"],"pdf_url":"https://arxiv.org/pdf/2411.14354v1.pdf","comment":"31 pages; 9 figures"},{"id":"http://arxiv.org/abs/2411.14353v1","updated":"2024-11-21T17:49:15Z","published":"2024-11-21T17:49:15Z","title":"Enhancing Medical Image Segmentation with Deep Learning and Diffusion\n  Models","summary":"  Medical image segmentation is crucial for accurate clinical diagnoses, yet it\nfaces challenges such as low contrast between lesions and normal tissues,\nunclear boundaries, and high variability across patients. Deep learning has\nimproved segmentation accuracy and efficiency, but it still relies heavily on\nexpert annotations and struggles with the complexities of medical images. The\nsmall size of medical image datasets and the high cost of data acquisition\nfurther limit the performance of segmentation networks. Diffusion models, with\ntheir iterative denoising process, offer a promising alternative for better\ndetail capture in segmentation. However, they face difficulties in accurately\nsegmenting small targets and maintaining the precision of boundary details.\nThis article discusses the importance of medical image segmentation, the\nlimitations of current deep learning approaches, and the potential of diffusion\nmodels to address these challenges.\n","authors":["Houze Liu","Tong Zhou","Yanlin Xiang","Aoran Shen","Jiacheng Hu","Junliang Du"],"pdf_url":"https://arxiv.org/pdf/2411.14353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14512v2","updated":"2024-11-21T17:48:25Z","published":"2024-08-25T04:32:45Z","title":"LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with\n  LLM Token Embeddings","summary":"  Zero-shot graph machine learning, especially with graph neural networks\n(GNNs), has garnered significant interest due to the challenge of scarce\nlabeled data. While methods like self-supervised learning and graph prompt\nlearning have been extensively explored, they often rely on fine-tuning with\ntask-specific labels, limiting their effectiveness in zero-shot scenarios.\nInspired by the zero-shot capabilities of instruction-fine-tuned large language\nmodels (LLMs), we introduce a novel framework named Token Embedding-Aligned\nGraph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and\ncross-task zero-shot learners for graph machine learning. Concretely, we\npretrain a GNN, aligning its representations with token embeddings of an LLM.\nWe then train a linear projector that transforms the GNN's representations into\na fixed number of graph token embeddings without tuning the LLM. A unified\ninstruction is designed for various graph tasks at different levels, such as\nnode classification (node-level) and link prediction (edge-level). These design\nchoices collectively enhance our method's effectiveness in zero-shot learning,\nsetting it apart from existing methods. Experiments show that our graph token\nembeddings help the LLM predictor achieve state-of-the-art performance on\nunseen datasets and tasks compared to other methods using LLMs as predictors.\n","authors":["Duo Wang","Yuan Zuo","Fengzhi Li","Junjie Wu"],"pdf_url":"https://arxiv.org/pdf/2408.14512v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14351v1","updated":"2024-11-21T17:46:55Z","published":"2024-11-21T17:46:55Z","title":"Indiscriminate Disruption of Conditional Inference on Multivariate\n  Gaussians","summary":"  The multivariate Gaussian distribution underpins myriad operations-research,\ndecision-analytic, and machine-learning models (e.g., Bayesian optimization,\nGaussian influence diagrams, and variational autoencoders). However, despite\nrecent advances in adversarial machine learning (AML), inference for Gaussian\nmodels in the presence of an adversary is notably understudied. Therefore, we\nconsider a self-interested attacker who wishes to disrupt a decisionmaker's\nconditional inference and subsequent actions by corrupting a set of evidentiary\nvariables. To avoid detection, the attacker also desires the attack to appear\nplausible wherein plausibility is determined by the density of the corrupted\nevidence. We consider white- and grey-box settings such that the attacker has\ncomplete and incomplete knowledge about the decisionmaker's underlying\nmultivariate Gaussian distribution, respectively. Select instances are shown to\nreduce to quadratic and stochastic quadratic programs, and structural\nproperties are derived to inform solution methods. We assess the impact and\nefficacy of these attacks in three examples, including, real estate evaluation,\ninterest rate estimation and signals processing. Each example leverages an\nalternative underlying model, thereby highlighting the attacks' broad\napplicability. Through these applications, we also juxtapose the behavior of\nthe white- and grey-box attacks to understand how uncertainty and structure\naffect attacker behavior.\n","authors":["William N. Caballero","Matthew LaRosa","Alexander Fisher","Vahid Tarokh"],"pdf_url":"https://arxiv.org/pdf/2411.14351v1.pdf","comment":"30 pages, 6 figures; 4 tables"},{"id":"http://arxiv.org/abs/2411.14349v1","updated":"2024-11-21T17:43:51Z","published":"2024-11-21T17:43:51Z","title":"Agnostic Learning of Arbitrary ReLU Activation under Gaussian Marginals","summary":"  We consider the problem of learning an arbitrarily-biased ReLU activation (or\nneuron) over Gaussian marginals with the squared loss objective. Despite the\nReLU neuron being the basic building block of modern neural networks, we still\ndo not understand the basic algorithmic question of whether one arbitrary ReLU\nneuron is learnable in the non-realizable setting. In particular, all existing\npolynomial time algorithms only provide approximation guarantees for the\nbetter-behaved unbiased setting or restricted bias setting.\n  Our main result is a polynomial time statistical query (SQ) algorithm that\ngives the first constant factor approximation for arbitrary bias. It outputs a\nReLU activation that achieves a loss of $O(\\mathrm{OPT}) + \\varepsilon$ in time\n$\\mathrm{poly}(d,1/\\varepsilon)$, where $\\mathrm{OPT}$ is the loss obtained by\nthe optimal ReLU activation. Our algorithm presents an interesting departure\nfrom existing algorithms, which are all based on gradient descent and thus fall\nwithin the class of correlational statistical query (CSQ) algorithms. We\ncomplement our algorithmic result by showing that no polynomial time CSQ\nalgorithm can achieve a constant factor approximation. Together, these results\nshed light on the intrinsic limitation of gradient descent, while identifying\narguably the simplest setting (a single neuron) where there is a separation\nbetween SQ and CSQ algorithms.\n","authors":["Anxin Guo","Aravindan Vijayaraghavan"],"pdf_url":"https://arxiv.org/pdf/2411.14349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14345v1","updated":"2024-11-21T17:41:27Z","published":"2024-11-21T17:41:27Z","title":"Layer Pruning with Consensus: A Triple-Win Solution","summary":"  Layer pruning offers a promising alternative to standard structured pruning,\neffectively reducing computational costs, latency, and memory footprint. While\nnotable layer-pruning approaches aim to detect unimportant layers for removal,\nthey often rely on single criteria that may not fully capture the complex,\nunderlying properties of layers. We propose a novel approach that combines\nmultiple similarity metrics into a single expressive measure of low-importance\nlayers, called the Consensus criterion. Our technique delivers a triple-win\nsolution: low accuracy drop, high-performance improvement, and increased\nrobustness to adversarial attacks. With up to 78.80% FLOPs reduction and\nperformance on par with state-of-the-art methods across different benchmarks,\nour approach reduces energy consumption and carbon emissions by up to 66.99%\nand 68.75%, respectively. Additionally, it avoids shortcut learning and\nimproves robustness by up to 4 percentage points under various adversarial\nattacks. Overall, the Consensus criterion demonstrates its effectiveness in\ncreating robust, efficient, and environmentally friendly pruned models.\n","authors":["Leandro Giusti Mugnaini","Carolina Tavares Duarte","Anna H. Reali Costa","Artur Jordao"],"pdf_url":"https://arxiv.org/pdf/2411.14345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14344v1","updated":"2024-11-21T17:41:09Z","published":"2024-11-21T17:41:09Z","title":"Overcomplete Tensor Decomposition via Koszul-Young Flattenings","summary":"  Motivated by connections between algebraic complexity lower bounds and tensor\ndecompositions, we investigate Koszul-Young flattenings, which are the main\ningredient in recent lower bounds for matrix multiplication. Based on this tool\nwe give a new algorithm for decomposing an $n_1 \\times n_2 \\times n_3$ tensor\nas the sum of a minimal number of rank-1 terms, and certifying uniqueness of\nthis decomposition. For $n_1 \\le n_2 \\le n_3$ with $n_1 \\to \\infty$ and\n$n_3/n_2 = O(1)$, our algorithm is guaranteed to succeed when the tensor rank\nis bounded by $r \\le (1-\\epsilon)(n_2 + n_3)$ for an arbitrary $\\epsilon > 0$,\nprovided the tensor components are generically chosen. For any fixed\n$\\epsilon$, the runtime is polynomial in $n_3$. When $n_2 = n_3 = n$, our\ncondition on the rank gives a factor-of-2 improvement over the classical\nsimultaneous diagonalization algorithm, which requires $r \\le n$, and also\nimproves on the recent algorithm of Koiran (2024) which requires $r \\le 4n/3$.\nIt also improves on the PhD thesis of Persu (2018) which solves rank detection\nfor $r \\leq 3n/2$.\n  We complement our upper bounds by showing limitations, in particular that no\nflattening of the style we consider can surpass rank $n_2 + n_3$. Furthermore,\nfor $n \\times n \\times n$ tensors, we show that an even more general class of\ndegree-$d$ polynomial flattenings cannot surpass rank $Cn$ for a constant $C =\nC(d)$. This suggests that for tensor decompositions, the case of generic\ncomponents may be fundamentally harder than that of random components, where\nefficient decomposition is possible even in highly overcomplete settings.\n","authors":["Pravesh K. Kothari","Ankur Moitra","Alexander S. Wein"],"pdf_url":"https://arxiv.org/pdf/2411.14344v1.pdf","comment":"42 pages"},{"id":"http://arxiv.org/abs/2411.14341v1","updated":"2024-11-21T17:38:49Z","published":"2024-11-21T17:38:49Z","title":"Logarithmic Neyman Regret for Adaptive Estimation of the Average\n  Treatment Effect","summary":"  Estimation of the Average Treatment Effect (ATE) is a core problem in causal\ninference with strong connections to Off-Policy Evaluation in Reinforcement\nLearning. This paper considers the problem of adaptively selecting the\ntreatment allocation probability in order to improve estimation of the ATE. The\nmajority of prior work on adaptive ATE estimation focus on asymptotic\nguarantees, and in turn overlooks important practical considerations such as\nthe difficulty of learning the optimal treatment allocation as well as\nhyper-parameter selection. Existing non-asymptotic methods are limited by poor\nempirical performance and exponential scaling of the Neyman regret with respect\nto problem parameters. In order to address these gaps, we propose and analyze\nthe Clipped Second Moment Tracking (ClipSMT) algorithm, a variant of an\nexisting algorithm with strong asymptotic optimality guarantees, and provide\nfinite sample bounds on its Neyman regret. Our analysis shows that ClipSMT\nachieves exponential improvements in Neyman regret on two fronts: improving the\ndependence on $T$ from $O(\\sqrt{T})$ to $O(\\log T)$, as well as reducing the\nexponential dependence on problem parameters to a polynomial dependence.\nFinally, we conclude with simulations which show the marked improvement of\nClipSMT over existing approaches.\n","authors":["Ojash Neopane","Aaditya Ramdas","Aarti Singh"],"pdf_url":"https://arxiv.org/pdf/2411.14341v1.pdf","comment":"12 pages, 2 figures. Submitted to AISTATS 2025"},{"id":"http://arxiv.org/abs/2410.24079v2","updated":"2024-11-21T17:33:18Z","published":"2024-10-31T16:16:18Z","title":"Hamiltonian Monte Carlo Inference of Marginalized Linear Mixed-Effects\n  Models","summary":"  Bayesian reasoning in linear mixed-effects models (LMMs) is challenging and\noften requires advanced sampling techniques like Markov chain Monte Carlo\n(MCMC). A common approach is to write the model in a probabilistic programming\nlanguage and then sample via Hamiltonian Monte Carlo (HMC). However, there are\nmany ways a user can transform a model that make inference more or less\nefficient. In particular, marginalizing some variables can greatly improve\ninference but is difficult for users to do manually. We develop an algorithm to\neasily marginalize random effects in LMMs. A naive approach introduces cubic\ntime operations within an inference algorithm like HMC, but we reduce the\nrunning time to linear using fast linear algebra techniques. We show that\nmarginalization is always beneficial when applicable and highlight improvements\nin various models, especially ones from cognitive sciences.\n","authors":["Jinlin Lai","Justin Domke","Daniel Sheldon"],"pdf_url":"https://arxiv.org/pdf/2410.24079v2.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)"},{"id":"http://arxiv.org/abs/2411.14317v1","updated":"2024-11-21T17:08:06Z","published":"2024-11-21T17:08:06Z","title":"Model-free learning of probability flows: Elucidating the nonequilibrium\n  dynamics of flocking","summary":"  Active systems comprise a class of nonequilibrium dynamics in which\nindividual components autonomously dissipate energy. Efforts towards\nunderstanding the role played by activity have centered on computation of the\nentropy production rate (EPR), which quantifies the breakdown of time reversal\nsymmetry. A fundamental difficulty in this program is that high dimensionality\nof the phase space renders traditional computational techniques infeasible for\nestimating the EPR. Here, we overcome this challenge with a novel deep learning\napproach that estimates probability currents directly from stochastic system\ntrajectories. We derive a new physical connection between the probability\ncurrent and two local definitions of the EPR for inertial systems, which we\napply to characterize the departure from equilibrium in a canonical model of\nflocking. Our results highlight that entropy is produced and consumed on the\nspatial interface of a flock as the interplay between alignment and fluctuation\ndynamically creates and annihilates order. By enabling the direct visualization\nof when and where a given system is out of equilibrium, we anticipate that our\nmethodology will advance the understanding of a broad class of complex\nnonequilibrium dynamics.\n","authors":["Nicholas M. Boffi","Eric Vanden-Eijnden"],"pdf_url":"https://arxiv.org/pdf/2411.14317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14305v1","updated":"2024-11-21T16:57:05Z","published":"2024-11-21T16:57:05Z","title":"Outlier-robust Mean Estimation near the Breakdown Point via\n  Sum-of-Squares","summary":"  We revisit the problem of estimating the mean of a high-dimensional\ndistribution in the presence of an $\\varepsilon$-fraction of adversarial\noutliers.\n  When $\\varepsilon$ is at most some sufficiently small constant, previous\nworks can achieve optimal error rate efficiently\n\\cite{diakonikolas2018robustly, kothari2018robust}. As $\\varepsilon$ approaches\nthe breakdown point $\\frac{1}{2}$, all previous algorithms incur either\nsub-optimal error rates or exponential running time.\n  In this paper we give a new analysis of the canonical sum-of-squares program\nintroduced in \\cite{kothari2018robust} and show that this program efficiently\nachieves optimal error rate for all $\\varepsilon \\in[0,\\frac{1}{2})$. The key\ningredient for our results is a new identifiability proof for robust mean\nestimation that focuses on the overlap between the distributions instead of\ntheir statistical distance as in previous works. We capture this proof within\nthe sum-of-squares proof system, thus obtaining efficient algorithms using the\nsum-of-squares proofs to algorithms paradigm \\cite{raghavendra2018high}.\n","authors":["Hongjie Chen","Deepak Narayanan Sridharan","David Steurer"],"pdf_url":"https://arxiv.org/pdf/2411.14305v1.pdf","comment":"Accepted at SODA 2025, 47 pages"},{"id":"http://arxiv.org/abs/2408.03413v2","updated":"2024-11-21T16:52:11Z","published":"2024-08-06T19:22:13Z","title":"A TVD neural network closure and application to turbulent combustion","summary":"  Trained neural networks (NN) have attractive features for closing governing\nequations. There are many methods that are showing promise, but all can fail in\ncases when small errors consequentially violate physical reality, such as a\nsolution boundedness condition. A NN formulation is introduced to preclude\nspurious oscillations that violate solution boundedness or positivity. It is\nembedded in the discretized equations as a machine learning closure and\nstrictly constrained, inspired by total variation diminishing (TVD) methods for\nhyperbolic conservation laws. The constraint is exactly enforced during\ngradient-descent training by rescaling the NN parameters, which maps them onto\nan explicit feasible set. Demonstrations show that the constrained NN closure\nmodel usefully recovers linear and nonlinear hyperbolic phenomena and\nanti-diffusion while enforcing the non-oscillatory property. Finally, the model\nis applied to subgrid-scale (SGS) modeling of a turbulent reacting flow, for\nwhich it suppresses spurious oscillations in scalar fields that otherwise\nviolate the solution boundedness. It outperforms a simple penalization of\noscillations in the loss function.\n","authors":["Seung Won Suh","Jonathan F MacArt","Luke N Olson","Jonathan B Freund"],"pdf_url":"https://arxiv.org/pdf/2408.03413v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.01694v4","updated":"2024-11-21T16:50:04Z","published":"2022-02-03T17:01:03Z","title":"Variational Nearest Neighbor Gaussian Process","summary":"  Variational approximations to Gaussian processes (GPs) typically use a small\nset of inducing points to form a low-rank approximation to the covariance\nmatrix. In this work, we instead exploit a sparse approximation of the\nprecision matrix. We propose variational nearest neighbor Gaussian process\n(VNNGP), which introduces a prior that only retains correlations within $K$\nnearest-neighboring observations, thereby inducing sparse precision structure.\nUsing the variational framework, VNNGP's objective can be factorized over both\nobservations and inducing points, enabling stochastic optimization with a time\ncomplexity of $O(K^3)$. Hence, we can arbitrarily scale the inducing point\nsize, even to the point of putting inducing points at every observed location.\nWe compare VNNGP to other scalable GPs through various experiments, and\ndemonstrate that VNNGP (1) can dramatically outperform low-rank methods, and\n(2) is less prone to overfitting than other nearest neighbor methods.\n","authors":["Luhuan Wu","Geoff Pleiss","John Cunningham"],"pdf_url":"https://arxiv.org/pdf/2202.01694v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13009v2","updated":"2024-11-21T16:49:51Z","published":"2024-11-20T03:17:51Z","title":"LLMSteer: Improving Long-Context LLM Inference by Steering Attention on\n  Reused Contexts","summary":"  As large language models (LLMs) show impressive performance on complex tasks,\nthey still struggle with longer contextual understanding and high computational\ncosts. To balance efficiency and quality, we introduce LLMSteer, a\nfine-tuning-free framework that enhances LLMs through query-independent\nattention steering. Tested on popular LLMs and datasets, LLMSteer narrows the\nperformance gap with baselines by 65.9% and reduces the runtime delay by up to\n4.8x compared to recent attention steering methods.\n","authors":["Zhuohan Gu","Jiayi Yao","Kuntai Du","Junchen Jiang"],"pdf_url":"https://arxiv.org/pdf/2411.13009v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14296v1","updated":"2024-11-21T16:42:41Z","published":"2024-11-21T16:42:41Z","title":"Improving Routability Prediction via NAS Using a Smooth One-shot\n  Augmented Predictor","summary":"  Routability optimization in modern EDA tools has benefited greatly from using\nmachine learning (ML) models. Constructing and optimizing the performance of ML\nmodels continues to be a challenge. Neural Architecture Search (NAS) serves as\na tool to aid in the construction and improvement of these models. Traditional\nNAS techniques struggle to perform well on routability prediction as a result\nof two primary factors. First, the separation between the training objective\nand the search objective adds noise to the NAS process. Secondly, the increased\nvariance of the search objective further complicates performing NAS. We craft a\nnovel NAS technique, coined SOAP-NAS, to address these challenges through novel\ndata augmentation techniques and a novel combination of one-shot and\npredictor-based NAS. Results show that our technique outperforms existing\nsolutions by 40% closer to the ideal performance measured by ROC-AUC (area\nunder the receiver operating characteristic curve) in DRC hotspot detection.\nSOAPNet is able to achieve an ROC-AUC of 0.9802 and a query time of only 0.461\nms.\n","authors":["Arjun Sridhar","Chen-Chia Chang","Junyao Zhang","Yiran Chen"],"pdf_url":"https://arxiv.org/pdf/2411.14296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04498v2","updated":"2024-11-21T16:40:03Z","published":"2024-08-08T14:46:01Z","title":"Model-Based Transfer Learning for Contextual Reinforcement Learning","summary":"  Deep reinforcement learning (RL) is a powerful approach to complex decision\nmaking. However, one issue that limits its practical application is its\nbrittleness, sometimes failing to train in the presence of small changes in the\nenvironment. Motivated by the success of zero-shot transfer-where pre-trained\nmodels perform well on related tasks-we consider the problem of selecting a\ngood set of training tasks to maximize generalization performance across a\nrange of tasks. Given the high cost of training, it is critical to select\ntraining tasks strategically, but not well understood how to do so. We hence\nintroduce Model-Based Transfer Learning (MBTL), which layers on top of existing\nRL methods to effectively solve contextual RL problems. MBTL models the\ngeneralization performance in two parts: 1) the performance set point, modeled\nusing Gaussian processes, and 2) performance loss (generalization gap), modeled\nas a linear function of contextual similarity. MBTL combines these two pieces\nof information within a Bayesian optimization (BO) framework to strategically\nselect training tasks. We show theoretically that the method exhibits sublinear\nregret in the number of training tasks and discuss conditions to further\ntighten regret bounds. We experimentally validate our methods using urban\ntraffic and standard continuous control benchmarks. The experimental results\nsuggest that MBTL can achieve up to 50x improved sample efficiency compared\nwith canonical independent training and multi-task training. Further\nexperiments demonstrate the efficacy of BO and the insensitivity to the\nunderlying RL algorithm and hyperparameters. This work lays the foundations for\ninvestigating explicit modeling of generalization, thereby enabling principled\nyet effective methods for contextual RL.\n","authors":["Jung-Hoon Cho","Vindula Jayawardana","Sirui Li","Cathy Wu"],"pdf_url":"https://arxiv.org/pdf/2408.04498v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13211v2","updated":"2024-11-21T16:37:32Z","published":"2024-11-20T11:19:22Z","title":"ViSTa Dataset: Do vision-language models understand sequential tasks?","summary":"  Using vision-language models (VLMs) as reward models in reinforcement\nlearning holds promise for reducing costs and improving safety. So far, VLM\nreward models have only been used for goal-oriented tasks, where the agent must\nreach a particular final outcome. We explore VLMs' potential to supervise tasks\nthat cannot be scored by the final state alone. To this end, we introduce\nViSTa, a dataset for evaluating Vision-based understanding of Sequential Tasks.\nViSTa comprises over 4,000 videos with step-by-step descriptions in virtual\nhome, Minecraft, and real-world environments. Its novel hierarchical structure\n-- basic single-step tasks composed into more and more complex sequential tasks\n-- allows a fine-grained understanding of how well VLMs can judge tasks with\nvarying complexity. To illustrate this, we use ViSTa to evaluate\nstate-of-the-art VLMs, including CLIP, ViCLIP, and GPT-4o. We find that, while\nthey are all good at object recognition, they fail to understand sequential\ntasks, with only GPT-4o achieving non-trivial performance.\n","authors":["Evžen Wybitul","Evan Ryan Gunter","Mikhail Seleznyov","David Lindner"],"pdf_url":"https://arxiv.org/pdf/2411.13211v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14288v1","updated":"2024-11-21T16:36:01Z","published":"2024-11-21T16:36:01Z","title":"On the Sample Complexity of One Hidden Layer Networks with Equivariance,\n  Locality and Weight Sharing","summary":"  Weight sharing, equivariance, and local filters, as in convolutional neural\nnetworks, are believed to contribute to the sample efficiency of neural\nnetworks. However, it is not clear how each one of these design choices\ncontribute to the generalization error. Through the lens of statistical\nlearning theory, we aim to provide an insight into this question by\ncharacterizing the relative impact of each choice on the sample complexity. We\nobtain lower and upper sample complexity bounds for a class of single hidden\nlayer networks. It is shown that the gain of equivariance is directly\nmanifested in the bound, while getting a similar increase for weight sharing\ndepends on the sharing mechanism. Among our results, we obtain a completely\ndimension-free bound for equivariant networks for a class of pooling\noperations. We show that the bound depends merely on the norm of filters, which\nis tighter than using the spectral norm of the respective matrix. We also\ncharacterize the trade-off in sample complexity between the parametrization of\nfilters in spatial and frequency domains, particularly when spatial filters are\nlocalized as in vanilla convolutional neural networks.\n","authors":["Arash Behboodi","Gabriele Cesa"],"pdf_url":"https://arxiv.org/pdf/2411.14288v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11585v3","updated":"2024-11-21T16:28:03Z","published":"2024-03-18T08:58:47Z","title":"Linguacodus: A Synergistic Framework for Transformative Code Generation\n  in Machine Learning Pipelines","summary":"  In the ever-evolving landscape of machine learning, seamless translation of\nnatural language descriptions into executable code remains a formidable\nchallenge. This paper introduces Linguacodus, an innovative framework designed\nto tackle this challenge by deploying a dynamic pipeline that iteratively\ntransforms natural language task descriptions into code through high-level\ndata-shaping instructions. The core of Linguacodus is a fine-tuned large\nlanguage model (LLM), empowered to evaluate diverse solutions for various\nproblems and select the most fitting one for a given task. This paper details\nthe fine-tuning process, and sheds light on how natural language descriptions\ncan be translated into functional code. Linguacodus represents a substantial\nleap towards automated code generation, effectively bridging the gap between\ntask descriptions and executable code. It holds great promise for advancing\nmachine learning applications across diverse domains. Additionally, we propose\nan algorithm capable of transforming a natural description of an ML task into\ncode with minimal human interaction. In extensive experiments on a vast machine\nlearning code dataset originating from Kaggle, we showcase the effectiveness of\nLinguacodus. The investigations highlight its potential applications across\ndiverse domains, emphasizing its impact on applied machine learning in various\nscientific fields.\n","authors":["Ekaterina Trofimova","Emil Sataev","Andrey E. Ustyuzhanin"],"pdf_url":"https://arxiv.org/pdf/2403.11585v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08521v2","updated":"2024-11-21T16:25:01Z","published":"2024-06-11T22:19:14Z","title":"Embedding-based Multimodal Learning on Pan-Squamous Cell Carcinomas for\n  Improved Survival Outcomes","summary":"  Cancer clinics capture disease data at various scales, from genetic to organ\nlevel. Current bioinformatic methods struggle to handle the heterogeneous\nnature of this data, especially with missing modalities. We propose PARADIGM, a\nGraph Neural Network (GNN) framework that learns from multimodal, heterogeneous\ndatasets to improve clinical outcome prediction. PARADIGM generates embeddings\nfrom multi-resolution data using foundation models, aggregates them into\npatient-level representations, fuses them into a unified graph, and enhances\nperformance for tasks like survival analysis. We train GNNs on pan-Squamous\nCell Carcinomas and validate our approach on Moffitt Cancer Center lung SCC\ndata. Multimodal GNN outperforms other models in patient survival prediction.\nConverging individual data modalities across varying scales provides a more\ninsightful disease view. Our solution aims to understand the patient's\ncircumstances comprehensively, offering insights on heterogeneous data\nintegration and the benefits of converging maximum data views.\n","authors":["Asim Waqas","Aakash Tripathi","Paul Stewart","Mia Naeini","Matthew B. Schabath","Ghulam Rasool"],"pdf_url":"https://arxiv.org/pdf/2406.08521v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14263v1","updated":"2024-11-21T16:18:52Z","published":"2024-11-21T16:18:52Z","title":"Generating Realistic Adversarial Examples for Business Processes using\n  Variational Autoencoders","summary":"  In predictive process monitoring, predictive models are vulnerable to\nadversarial attacks, where input perturbations can lead to incorrect\npredictions. Unlike in computer vision, where these perturbations are designed\nto be imperceptible to the human eye, the generation of adversarial examples in\npredictive process monitoring poses unique challenges. Minor changes to the\nactivity sequences can create improbable or even impossible scenarios to occur\ndue to underlying constraints such as regulatory rules or process constraints.\nTo address this, we focus on generating realistic adversarial examples tailored\nto the business process context, in contrast to the imperceptible, pixel-level\nchanges commonly seen in computer vision adversarial attacks. This paper\nintroduces two novel latent space attacks, which generate adversaries by adding\nnoise to the latent space representation of the input data, rather than\ndirectly modifying the input attributes. These latent space methods are\ndomain-agnostic and do not rely on process-specific knowledge, as we restrict\nthe generation of adversarial examples to the learned class-specific data\ndistributions by directly perturbing the latent space representation of the\nbusiness process executions. We evaluate these two latent space methods with\nsix other adversarial attacking methods on eleven real-life event logs and four\npredictive models. The first three attacking methods directly permute the\nactivities of the historically observed business process executions. The fourth\nmethod constrains the adversarial examples to lie within the same data\ndistribution as the original instances, by projecting the adversarial examples\nto the original data distribution.\n","authors":["Alexander Stevens","Jari Peeperkorn","Johannes De Smedt","Jochen De Weerdt"],"pdf_url":"https://arxiv.org/pdf/2411.14263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.11957v6","updated":"2024-11-21T16:13:48Z","published":"2023-07-22T01:56:58Z","title":"High-performance real-world optical computing trained by in situ\n  gradient-based model-free optimization","summary":"  Optical computing systems provide high-speed and low-energy data processing\nbut face deficiencies in computationally demanding training and\nsimulation-to-reality gaps. We propose a gradient-based model-free optimization\n(G-MFO) method based on a Monte Carlo gradient estimation algorithm for\ncomputationally efficient in situ training of optical computing systems. This\napproach treats an optical computing system as a black box and back-propagates\nthe loss directly to the optical computing weights' probability distributions,\ncircumventing the need for a computationally heavy and biased system\nsimulation. Our experiments on diffractive optical computing systems show that\nG-MFO outperforms hybrid training on the MNIST and FMNIST datasets.\nFurthermore, we demonstrate image-free and high-speed classification of cells\nfrom their marker-free phase maps. Our method's model-free and high-performance\nnature, combined with its low demand for computational resources, paves the way\nfor accelerating the transition of optical computing from laboratory\ndemonstrations to practical, real-world applications.\n","authors":["Guangyuan Zhao","Xin Shu","Renjie Zhou"],"pdf_url":"https://arxiv.org/pdf/2307.11957v6.pdf","comment":"The paper titled \"High-performance real-world optical computing\n  trained by in situ gradient-based model-free optimization\" has been accepted\n  at ICCP&TPAMI 2024. For more details, please visit the [project\n  page](https://shuxin626.github.io/mfo_optical_computing/index.html)"},{"id":"http://arxiv.org/abs/2405.07460v4","updated":"2024-11-21T16:12:54Z","published":"2024-05-13T04:35:14Z","title":"HoneyBee: A Scalable Modular Framework for Creating Multimodal Oncology\n  Datasets with Foundational Embedding Models","summary":"  Developing accurate machine learning models for oncology requires\nlarge-scale, high-quality multimodal datasets. However, creating such datasets\nremains challenging due to the complexity and heterogeneity of medical data. To\naddress this challenge, we introduce HoneyBee, a scalable modular framework for\nbuilding multimodal oncology datasets that leverages foundation models to\ngenerate representative embeddings. HoneyBee integrates various data\nmodalities, including clinical diagnostic and pathology imaging data, medical\nnotes, reports, records, and molecular data. It employs data preprocessing\ntechniques and foundation models to generate embeddings that capture the\nessential features and relationships within the raw medical data. The generated\nembeddings are stored in a structured format using Hugging Face datasets and\nPyTorch dataloaders for accessibility. Vector databases enable efficient\nquerying and retrieval for machine learning applications. We demonstrate the\neffectiveness of HoneyBee through experiments assessing the quality and\nrepresentativeness of these embeddings. The framework is designed to be\nextensible to other medical domains and aims to accelerate oncology research by\nproviding high-quality, machine learning-ready datasets. HoneyBee is an ongoing\nopen-source effort, and the code, datasets, and models are available at the\nproject repository.\n","authors":["Aakash Tripathi","Asim Waqas","Matthew B. Schabath","Yasin Yilmaz","Ghulam Rasool"],"pdf_url":"https://arxiv.org/pdf/2405.07460v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14257v1","updated":"2024-11-21T16:05:58Z","published":"2024-11-21T16:05:58Z","title":"Do I Know This Entity? Knowledge Awareness and Hallucinations in\n  Language Models","summary":"  Hallucinations in large language models are a widespread problem, yet the\nmechanisms behind whether models will hallucinate are poorly understood,\nlimiting our ability to solve this problem. Using sparse autoencoders as an\ninterpretability tool, we discover that a key part of these mechanisms is\nentity recognition, where the model detects if an entity is one it can recall\nfacts about. Sparse autoencoders uncover meaningful directions in the\nrepresentation space, these detect whether the model recognizes an entity, e.g.\ndetecting it doesn't know about an athlete or a movie. This suggests that\nmodels can have self-knowledge: internal representations about their own\ncapabilities. These directions are causally relevant: capable of steering the\nmodel to refuse to answer questions about known entities, or to hallucinate\nattributes of unknown entities when it would otherwise refuse. We demonstrate\nthat despite the sparse autoencoders being trained on the base model, these\ndirections have a causal effect on the chat model's refusal behavior,\nsuggesting that chat finetuning has repurposed this existing mechanism.\nFurthermore, we provide an initial exploration into the mechanistic role of\nthese directions in the model, finding that they disrupt the attention of\ndownstream heads that typically move entity attributes to the final token.\n","authors":["Javier Ferrando","Oscar Obeso","Senthooran Rajamanoharan","Neel Nanda"],"pdf_url":"https://arxiv.org/pdf/2411.14257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14254v1","updated":"2024-11-21T16:02:39Z","published":"2024-11-21T16:02:39Z","title":"BERT-Based Approach for Automating Course Articulation Matrix\n  Construction with Explainable AI","summary":"  Course Outcome (CO) and Program Outcome (PO)/Program-Specific Outcome (PSO)\nalignment is a crucial task for ensuring curriculum coherence and assessing\neducational effectiveness. The construction of a Course Articulation Matrix\n(CAM), which quantifies the relationship between COs and POs/PSOs, typically\ninvolves assigning numerical values (0, 1, 2, 3) to represent the degree of\nalignment. In this study, We experiment with four models from the BERT family:\nBERT Base, DistilBERT, ALBERT, and RoBERTa, and use multiclass classification\nto assess the alignment between CO and PO/PSO pairs. We first evaluate\ntraditional machine learning classifiers, such as Decision Tree, Random Forest,\nand XGBoost, and then apply transfer learning to evaluate the performance of\nthe pretrained BERT models. To enhance model interpretability, we apply\nExplainable AI technique, specifically Local Interpretable Model-agnostic\nExplanations (LIME), to provide transparency into the decision-making process.\nOur system achieves accuracy, precision, recall, and F1-score values of 98.66%,\n98.67%, 98.66%, and 98.66%, respectively. This work demonstrates the potential\nof utilizing transfer learning with BERT-based models for the automated\ngeneration of CAMs, offering high performance and interpretability in\neducational outcome assessment.\n","authors":["Natenaile Asmamaw Shiferaw","Simpenzwe Honore Leandre","Aman Sinha","Dillip Rout"],"pdf_url":"https://arxiv.org/pdf/2411.14254v1.pdf","comment":"26 pages, 9 figures"},{"id":"http://arxiv.org/abs/2411.14251v1","updated":"2024-11-21T15:57:02Z","published":"2024-11-21T15:57:02Z","title":"Natural Language Reinforcement Learning","summary":"  Reinforcement Learning (RL) mathematically formulates decision-making with\nMarkov Decision Process (MDP). With MDPs, researchers have achieved remarkable\nbreakthroughs across various domains, including games, robotics, and language\nmodels. This paper seeks a new possibility, Natural Language Reinforcement\nLearning (NLRL), by extending traditional MDP to natural language-based\nrepresentation space. Specifically, NLRL innovatively redefines RL principles,\nincluding task objectives, policy, value function, Bellman equation, and policy\niteration, into their language counterparts. With recent advancements in large\nlanguage models (LLMs), NLRL can be practically implemented to achieve RL-like\npolicy and value improvement by either pure prompting or gradient-based\ntraining. Experiments over Maze, Breakthrough, and Tic-Tac-Toe games\ndemonstrate the effectiveness, efficiency, and interpretability of the NLRL\nframework among diverse use cases. Our code will be released at\nhttps://github.com/waterhorse1/Natural-language-RL.\n","authors":["Xidong Feng","Ziyu Wan","Haotian Fu","Bo Liu","Mengyue Yang","Girish A. Koushik","Zhiyuan Hu","Ying Wen","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2411.14251v1.pdf","comment":"Extension of arXiv:2402.07157"},{"id":"http://arxiv.org/abs/2407.19523v3","updated":"2024-11-21T15:56:12Z","published":"2024-07-28T16:23:55Z","title":"Robust Fast Adaptation from Adversarially Explicit Task Distribution\n  Generation","summary":"  Meta-learning is a practical learning paradigm to transfer skills across\ntasks from a few examples. Nevertheless, the existence of task distribution\nshifts tends to weaken meta-learners' generalization capability, particularly\nwhen the training task distribution is naively hand-crafted or based on simple\npriors that fail to cover critical scenarios sufficiently. Here, we consider\nexplicitly generative modeling task distributions placed over task identifiers\nand propose robustifying fast adaptation from adversarial training. Our\napproach, which can be interpreted as a model of a Stackelberg game, not only\nuncovers the task structure during problem-solving from an explicit generative\nmodel but also theoretically increases the adaptation robustness in worst\ncases. This work has practical implications, particularly in dealing with task\ndistribution shifts in meta-learning, and contributes to theoretical insights\nin the field. Our method demonstrates its robustness in the presence of task\nsubpopulation shifts and improved performance over SOTA baselines in extensive\nexperiments. The code will be available at the project site\nhttps://sites.google.com/view/ar-metalearn.\n","authors":["Cheems Wang","Yiqin Lv","Yixiu Mao","Yun Qu","Yi Xu","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2407.19523v3.pdf","comment":"Accepted by KDD 2025. The project is available at\n  https://sites.google.com/view/ar-metalearn"},{"id":"http://arxiv.org/abs/2411.14246v1","updated":"2024-11-21T15:52:23Z","published":"2024-11-21T15:52:23Z","title":"Simulation-Aided Policy Tuning for Black-Box Robot Learning","summary":"  How can robots learn and adapt to new tasks and situations with little data?\nSystematic exploration and simulation are crucial tools for efficient robot\nlearning. We present a novel black-box policy search algorithm focused on\ndata-efficient policy improvements. The algorithm learns directly on the robot\nand treats simulation as an additional information source to speed up the\nlearning process. At the core of the algorithm, a probabilistic model learns\nthe dependence of the policy parameters and the robot learning objective not\nonly by performing experiments on the robot, but also by leveraging data from a\nsimulator. This substantially reduces interaction time with the robot. Using\nthis model, we can guarantee improvements with high probability for each policy\nupdate, thereby facilitating fast, goal-oriented learning. We evaluate our\nalgorithm on simulated fine-tuning tasks and demonstrate the data-efficiency of\nthe proposed dual-information source optimization algorithm. In a real robot\nlearning experiment, we show fast and successful task learning on a robot\nmanipulator with the aid of an imperfect simulator.\n","authors":["Shiming He","Alexander von Rohr","Dominik Baumann","Ji Xiang","Sebastian Trimpe"],"pdf_url":"https://arxiv.org/pdf/2411.14246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04987v3","updated":"2024-11-21T15:44:38Z","published":"2023-10-08T03:17:22Z","title":"Data-centric Graph Learning: A Survey","summary":"  The history of artificial intelligence (AI) has witnessed the significant\nimpact of high-quality data on various deep learning models, such as ImageNet\nfor AlexNet and ResNet. Recently, instead of designing more complex neural\narchitectures as model-centric approaches, the attention of AI community has\nshifted to data-centric ones, which focuses on better processing data to\nstrengthen the ability of neural models. Graph learning, which operates on\nubiquitous topological data, also plays an important role in the era of deep\nlearning. In this survey, we comprehensively review graph learning approaches\nfrom the data-centric perspective, and aim to answer three crucial questions:\n(1) when to modify graph data, (2) what part of the graph data needs\nmodification to unlock the potential of various graph models, and (3) how to\nsafeguard graph models from problematic data influence. Accordingly, we propose\na novel taxonomy based on the stages in the graph learning pipeline, and\nhighlight the processing methods for different data structures in the graph\ndata, i.e., topology, feature and label. Furthermore, we analyze some potential\nproblems embedded in graph data and discuss how to solve them in a data-centric\nmanner. Finally, we provide some promising future directions for data-centric\ngraph learning.\n","authors":["Yuxin Guo","Deyu Bo","Cheng Yang","Zhiyuan Lu","Zhongjian Zhang","Jixi Liu","Yufei Peng","Chuan Shi"],"pdf_url":"https://arxiv.org/pdf/2310.04987v3.pdf","comment":"20 pages, accepted by IEEE Transactions on Big Data"},{"id":"http://arxiv.org/abs/2402.17805v2","updated":"2024-11-21T15:34:06Z","published":"2024-02-27T11:04:06Z","title":"Graph Neural Networks and Arithmetic Circuits","summary":"  We characterize the computational power of neural networks that follow the\ngraph neural network (GNN) architecture, not restricted to aggregate-combine\nGNNs or other particular types. We establish an exact correspondence between\nthe expressivity of GNNs using diverse activation functions and arithmetic\ncircuits over real numbers. In our results the activation function of the\nnetwork becomes a gate type in the circuit. Our result holds for families of\nconstant depth circuits and networks, both uniformly and non-uniformly, for all\ncommon activation functions.\n","authors":["Timon Barlag","Vivian Holzapfel","Laura Strieker","Jonni Virtema","Heribert Vollmer"],"pdf_url":"https://arxiv.org/pdf/2402.17805v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02124v2","updated":"2024-11-21T15:33:17Z","published":"2023-12-04T18:51:44Z","title":"VerA: Versatile Anonymization Applicable to Clinical Facial Photographs","summary":"  The demand for privacy in facial image dissemination is gaining ground\ninternationally, echoed by the proliferation of regulations such as GDPR,\nDPDPA, CCPA, PIPL, and APPI. While recent advances in anonymization surpass\npixelation or blur methods, additional constraints to the task pose challenges.\nLargely unaddressed by current anonymization methods are clinical images and\npairs of before-and-after clinical images illustrating facial medical\ninterventions, e.g., facial surgeries or dental procedures. We present VerA,\nthe first Versatile Anonymization framework that solves two challenges in\nclinical applications: A) it preserves selected semantic areas (e.g., mouth\nregion) to show medical intervention results, that is, anonymization is only\napplied to the areas outside the preserved area; and B) it produces anonymized\nimages with consistent personal identity across multiple photographs, which is\ncrucial for anonymizing photographs of the same person taken before and after a\nclinical intervention. We validate our results on both single and paired\nanonymization of clinical images through extensive quantitative and qualitative\nevaluation. We also demonstrate that VerA reaches the state of the art on\nestablished anonymization tasks, in terms of photorealism and\nde-identification.\n","authors":["Majed El Helou","Doruk Cetin","Petar Stamenkovic","Niko Benjamin Huber","Fabio Zünd"],"pdf_url":"https://arxiv.org/pdf/2312.02124v2.pdf","comment":"accepted to WACV 2025"},{"id":"http://arxiv.org/abs/2411.12700v2","updated":"2024-11-21T15:27:05Z","published":"2024-11-19T18:08:01Z","title":"Learning multivariate Gaussians with imperfect advice","summary":"  We revisit the problem of distribution learning within the framework of\nlearning-augmented algorithms. In this setting, we explore the scenario where a\nprobability distribution is provided as potentially inaccurate advice on the\ntrue, unknown distribution. Our objective is to develop learning algorithms\nwhose sample complexity decreases as the quality of the advice improves,\nthereby surpassing standard learning lower bounds when the advice is\nsufficiently accurate.\n  Specifically, we demonstrate that this outcome is achievable for the problem\nof learning a multivariate Gaussian distribution $N(\\boldsymbol{\\mu},\n\\boldsymbol{\\Sigma})$ in the PAC learning setting. Classically, in the\nadvice-free setting, $\\tilde{\\Theta}(d^2/\\varepsilon^2)$ samples are sufficient\nand worst case necessary to learn $d$-dimensional Gaussians up to TV distance\n$\\varepsilon$ with constant probability. When we are additionally given a\nparameter $\\tilde{\\boldsymbol{\\Sigma}}$ as advice, we show that\n$\\tilde{O}(d^{2-\\beta}/\\varepsilon^2)$ samples suffices whenever $\\|\n\\tilde{\\boldsymbol{\\Sigma}}^{-1/2} \\boldsymbol{\\Sigma}\n\\tilde{\\boldsymbol{\\Sigma}}^{-1/2} - \\boldsymbol{I_d} \\|_1 \\leq \\varepsilon\nd^{1-\\beta}$ (where $\\|\\cdot\\|_1$ denotes the entrywise $\\ell_1$ norm) for any\n$\\beta > 0$, yielding a polynomial improvement over the advice-free setting.\n","authors":["Arnab Bhattacharyya","Davin Choo","Philips George John","Themis Gouleakis"],"pdf_url":"https://arxiv.org/pdf/2411.12700v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14215v1","updated":"2024-11-21T15:25:08Z","published":"2024-11-21T15:25:08Z","title":"Evaluating the Robustness of Analogical Reasoning in Large Language\n  Models","summary":"  LLMs have performed well on several reasoning benchmarks, including ones that\ntest analogical reasoning abilities. However, there is debate on the extent to\nwhich they are performing general abstract reasoning versus employing\nnon-robust processes, e.g., that overly rely on similarity to pre-training\ndata. Here we investigate the robustness of analogy-making abilities previously\nclaimed for LLMs on three of four domains studied by Webb, Holyoak, and Lu\n(2023): letter-string analogies, digit matrices, and story analogies. For each\ndomain we test humans and GPT models on robustness to variants of the original\nanalogy problems that test the same abstract reasoning abilities but are likely\ndissimilar from tasks in the pre-training data. The performance of a system\nthat uses robust abstract reasoning should not decline substantially on these\nvariants.\n  On simple letter-string analogies, we find that while the performance of\nhumans remains high for two types of variants we tested, the GPT models'\nperformance declines sharply. This pattern is less pronounced as the complexity\nof these problems is increased, as both humans and GPT models perform poorly on\nboth the original and variant problems requiring more complex analogies. On\ndigit-matrix problems, we find a similar pattern but only on one out of the two\ntypes of variants we tested. On story-based analogy problems, we find that,\nunlike humans, the performance of GPT models are susceptible to answer-order\neffects, and that GPT models also may be more sensitive than humans to\nparaphrasing.\n  This work provides evidence that LLMs often lack the robustness of zero-shot\nhuman analogy-making, exhibiting brittleness on most of the variations we\ntested. More generally, this work points to the importance of carefully\nevaluating AI systems not only for accuracy but also robustness when testing\ntheir cognitive capabilities.\n","authors":["Martha Lewis","Melanie Mitchell"],"pdf_url":"https://arxiv.org/pdf/2411.14215v1.pdf","comment":"31 pages, 13 figures. arXiv admin note: text overlap with\n  arXiv:2402.08955"},{"id":"http://arxiv.org/abs/2308.08812v2","updated":"2024-11-21T15:22:23Z","published":"2023-08-17T06:48:55Z","title":"A Fusion of Variational Distribution Priors and Saliency Map Replay for\n  Continual 3D Reconstruction","summary":"  Single-image 3D reconstruction is a research challenge focused on predicting\n3D object shapes from single-view images. This task requires significant data\nacquisition to predict both visible and occluded portions of the shape.\nFurthermore, learning-based methods face the difficulty of creating a\ncomprehensive training dataset for all possible classes. To this end, we\npropose a continual learning-based 3D reconstruction method where our goal is\nto design a model using Variational Priors that can still reconstruct the\npreviously seen classes reasonably even after training on new classes.\nVariational Priors represent abstract shapes and combat forgetting, whereas\nsaliency maps preserve object attributes with less memory usage. This is vital\ndue to resource constraints in storing extensive training data. Additionally,\nwe introduce saliency map-based experience replay to capture global and\ndistinct object features. Thorough experiments show competitive results\ncompared to established methods, both quantitatively and qualitatively.\n","authors":["Sanchar Palit","Sandika Biswas"],"pdf_url":"https://arxiv.org/pdf/2308.08812v2.pdf","comment":"at ICVGIP 2024"},{"id":"http://arxiv.org/abs/2401.13721v3","updated":"2024-11-21T15:14:29Z","published":"2024-01-24T14:55:02Z","title":"Uncertainty-Guided Alignment for Unsupervised Domain Adaptation in\n  Regression","summary":"  Unsupervised Domain Adaptation for Regression (UDAR) aims to adapt models\nfrom a labeled source domain to an unlabeled target domain for regression\ntasks. Traditional feature alignment methods, successful in classification,\noften prove ineffective for regression due to the correlated nature of\nregression features. To address this challenge, we propose Uncertainty-Guided\nAlignment (UGA), a novel method that integrates predictive uncertainty into the\nfeature alignment process. UGA employs Evidential Deep Learning to predict both\ntarget values and their associated uncertainties. This uncertainty information\nguides the alignment process and fuses information within the embedding space,\neffectively mitigating issues such as feature collapse in out-of-distribution\nscenarios. We evaluate UGA on two computer vision benchmarks and a real-world\nbattery state-of-charge prediction across different manufacturers and operating\ntemperatures. Across 52 transfer tasks, UGA on average outperforms existing\nstate-of-the-art methods. Our approach not only improves adaptation performance\nbut also provides well-calibrated uncertainty estimates.\n","authors":["Ismail Nejjar","Gaetan Frusque","Florent Forest","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2401.13721v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14202v1","updated":"2024-11-21T15:11:02Z","published":"2024-11-21T15:11:02Z","title":"Revised Regularization for Efficient Continual Learning through\n  Correlation-Based Parameter Update in Bayesian Neural Networks","summary":"  We propose a Bayesian neural network-based continual learning algorithm using\nVariational Inference, aiming to overcome several drawbacks of existing\nmethods. Specifically, in continual learning scenarios, storing network\nparameters at each step to retain knowledge poses challenges. This is\ncompounded by the crucial need to mitigate catastrophic forgetting,\nparticularly given the limited access to past datasets, which complicates\nmaintaining correspondence between network parameters and datasets across all\nsessions. Current methods using Variational Inference with KL divergence risk\ncatastrophic forgetting during uncertain node updates and coupled disruptions\nin certain nodes. To address these challenges, we propose the following\nstrategies. To reduce the storage of the dense layer parameters, we propose a\nparameter distribution learning method that significantly reduces the storage\nrequirements. In the continual learning framework employing variational\ninference, our study introduces a regularization term that specifically targets\nthe dynamics and population of the mean and variance of the parameters. This\nterm aims to retain the benefits of KL divergence while addressing related\nchallenges. To ensure proper correspondence between network parameters and the\ndata, our method introduces an importance-weighted Evidence Lower Bound term to\ncapture data and parameter correlations. This enables storage of common and\ndistinctive parameter hyperspace bases. The proposed method partitions the\nparameter space into common and distinctive subspaces, with conditions for\neffective backward and forward knowledge transfer, elucidating the\nnetwork-parameter dataset correspondence. The experimental results demonstrate\nthe effectiveness of our method across diverse datasets and various\ncombinations of sequential datasets, yielding superior performance compared to\nexisting approaches.\n","authors":["Sanchar Palit","Biplab Banerjee","Subhasis Chaudhuri"],"pdf_url":"https://arxiv.org/pdf/2411.14202v1.pdf","comment":"at ICVGIP 2024"},{"id":"http://arxiv.org/abs/2411.14199v1","updated":"2024-11-21T15:07:42Z","published":"2024-11-21T15:07:42Z","title":"OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented\n  LMs","summary":"  Scientific progress depends on researchers' ability to synthesize the growing\nbody of literature. Can large language models (LMs) assist scientists in this\ntask? We introduce OpenScholar, a specialized retrieval-augmented LM that\nanswers scientific queries by identifying relevant passages from 45 million\nopen-access papers and synthesizing citation-backed responses. To evaluate\nOpenScholar, we develop ScholarQABench, the first large-scale multi-domain\nbenchmark for literature search, comprising 2,967 expert-written queries and\n208 long-form answers across computer science, physics, neuroscience, and\nbiomedicine. On ScholarQABench, OpenScholar-8B outperforms GPT-4o by 5% and\nPaperQA2 by 7% in correctness, despite being a smaller, open model. While GPT4o\nhallucinates citations 78 to 90% of the time, OpenScholar achieves citation\naccuracy on par with human experts. OpenScholar's datastore, retriever, and\nself-feedback inference loop also improves off-the-shelf LMs: for instance,\nOpenScholar-GPT4o improves GPT-4o's correctness by 12%. In human evaluations,\nexperts preferred OpenScholar-8B and OpenScholar-GPT4o responses over\nexpert-written ones 51% and 70% of the time, respectively, compared to GPT4o's\n32%. We open-source all of our code, models, datastore, data and a public demo.\n","authors":["Akari Asai","Jacqueline He","Rulin Shao","Weijia Shi","Amanpreet Singh","Joseph Chee Chang","Kyle Lo","Luca Soldaini","Sergey Feldman","Mike D'arcy","David Wadden","Matt Latzke","Minyang Tian","Pan Ji","Shengyan Liu","Hao Tong","Bohao Wu","Yanyu Xiong","Luke Zettlemoyer","Graham Neubig","Dan Weld","Doug Downey","Wen-tau Yih","Pang Wei Koh","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2411.14199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14193v1","updated":"2024-11-21T15:02:41Z","published":"2024-11-21T15:02:41Z","title":"ComfyGI: Automatic Improvement of Image Generation Workflows","summary":"  Automatic image generation is no longer just of interest to researchers, but\nalso to practitioners. However, current models are sensitive to the settings\nused and automatic optimization methods often require human involvement. To\nbridge this gap, we introduce ComfyGI, a novel approach to automatically\nimprove workflows for image generation without the need for human intervention\ndriven by techniques from genetic improvement. This enables image generation\nwith significantly higher quality in terms of the alignment with the given\ndescription and the perceived aesthetics. On the performance side, we find that\noverall, the images generated with an optimized workflow are about 50% better\ncompared to the initial workflow in terms of the median ImageReward score.\nThese already good results are even surpassed in our human evaluation, as the\nparticipants preferred the images improved by ComfyGI in around 90% of the\ncases.\n","authors":["Dominik Sobania","Martin Briesch","Franz Rothlauf"],"pdf_url":"https://arxiv.org/pdf/2411.14193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14192v1","updated":"2024-11-21T15:01:17Z","published":"2024-11-21T15:01:17Z","title":"Learning Pore-scale Multi-phase Flow from Experimental Data with Graph\n  Neural Network","summary":"  Understanding the process of multiphase fluid flow through porous media is\ncrucial for many climate change mitigation technologies, including CO$_2$\ngeological storage, hydrogen storage, and fuel cells. However, current\nnumerical models are often incapable of accurately capturing the complex\npore-scale physics observed in experiments. In this study, we address this\nchallenge using a graph neural network-based approach and directly learn\npore-scale fluid flow using micro-CT experimental data. We propose a\nLong-Short-Edge MeshGraphNet (LSE-MGN) that predicts the state of each node in\nthe pore space at each time step. During inference, given an initial state, the\nmodel can autoregressively predict the evolution of the multiphase flow process\nover time. This approach successfully captures the physics from the\nhigh-resolution experimental data while maintaining computational efficiency,\nproviding a promising direction for accurate and efficient pore-scale modeling\nof complex multiphase fluid flow dynamics.\n","authors":["Yuxuan Gu","Catherine Spurin","Gege Wen"],"pdf_url":"https://arxiv.org/pdf/2411.14192v1.pdf","comment":"Accpeted for Machine Learning and the Physical Sciences Workshop at\n  the 38th conference on Neural Information Processing Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2409.07606v3","updated":"2024-11-21T14:35:28Z","published":"2024-09-11T20:35:29Z","title":"The Role of Deep Learning Regularizations on Actors in Offline RL","summary":"  Deep learning regularization techniques, such as dropout, layer\nnormalization, or weight decay, are widely adopted in the construction of\nmodern artificial neural networks, often resulting in more robust training\nprocesses and improved generalization capabilities. However, in the domain of\nReinforcement Learning (RL), the application of these techniques has been\nlimited, usually applied to value function estimators (Hiraoka et al., 2021;\nSmith et al., 2022), and may result in detrimental effects. This issue is even\nmore pronounced in offline RL settings, which bear greater similarity to\nsupervised learning but have received less attention. Recent work in continuous\noffline RL (Park et al., 2024) has demonstrated that while we can build\nsufficiently powerful critic networks, the generalization of actor networks\nremains a bottleneck. In this study, we empirically show that applying standard\nregularization techniques to actor networks in offline RL actor-critic\nalgorithms yields improvements of 6% on average across two algorithms and three\ndifferent continuous D4RL domains.\n","authors":["Denis Tarasov","Anja Surina","Caglar Gulcehre"],"pdf_url":"https://arxiv.org/pdf/2409.07606v3.pdf","comment":"https://github.com/DT6A/ActoReg"},{"id":"http://arxiv.org/abs/2410.18097v3","updated":"2024-11-21T14:23:49Z","published":"2024-10-08T11:28:06Z","title":"RRADistill: Distilling LLMs' Passage Ranking Ability for Long-Tail\n  Queries Document Re-Ranking on a Search Engine","summary":"  Large Language Models (LLMs) excel at understanding the semantic\nrelationships between queries and documents, even with lengthy and complex\nlong-tail queries. These queries are challenging for feedback-based rankings\ndue to sparse user engagement and limited feedback, making LLMs' ranking\nability highly valuable. However, the large size and slow inference of LLMs\nnecessitate the development of smaller, more efficient models (sLLMs).\nRecently, integrating ranking label generation into distillation techniques has\nbecome crucial, but existing methods underutilize LLMs' capabilities and are\ncumbersome. Our research, RRADistill: Re-Ranking Ability Distillation, propose\nan efficient label generation pipeline and novel sLLM training methods for both\nencoder and decoder models. We introduce an encoder-based method using a Term\nControl Layer to capture term matching signals and a decoder-based model with a\nranking layer for enhanced understanding. A/B testing on a Korean-based search\nplatform, validates the effectiveness of our approach in improving re-ranking\nfor long-tail queries.\n","authors":["Nayoung Choi","Youngjune Lee","Gyu-Hwung Cho","Haeyu Jeong","Jungmin Kong","Saehun Kim","Keunchan Park","Sarah Cho","Inchang Jeong","Gyohee Nam","Sunghoon Han","Wonil Yang","Jaeho Choi"],"pdf_url":"https://arxiv.org/pdf/2410.18097v3.pdf","comment":"Accepted to EMNLP 2024 Industry Track. First two authors contributed\n  equally"},{"id":"http://arxiv.org/abs/2411.14166v1","updated":"2024-11-21T14:23:06Z","published":"2024-11-21T14:23:06Z","title":"SPARKLE: A Unified Single-Loop Primal-Dual Framework for Decentralized\n  Bilevel Optimization","summary":"  This paper studies decentralized bilevel optimization, in which multiple\nagents collaborate to solve problems involving nested optimization structures\nwith neighborhood communications. Most existing literature primarily utilizes\ngradient tracking to mitigate the influence of data heterogeneity, without\nexploring other well-known heterogeneity-correction techniques such as EXTRA or\nExact Diffusion. Additionally, these studies often employ identical\ndecentralized strategies for both upper- and lower-level problems, neglecting\nto leverage distinct mechanisms across different levels. To address these\nlimitations, this paper proposes SPARKLE, a unified Single-loop Primal-dual\nAlgoRithm frameworK for decentraLized bilEvel optimization. SPARKLE offers the\nflexibility to incorporate various heterogeneitycorrection strategies into the\nalgorithm. Moreover, SPARKLE allows for different strategies to solve upper-\nand lower-level problems. We present a unified convergence analysis for\nSPARKLE, applicable to all its variants, with state-of-the-art convergence\nrates compared to existing decentralized bilevel algorithms. Our results\nfurther reveal that EXTRA and Exact Diffusion are more suitable for\ndecentralized bilevel optimization, and using mixed strategies in bilevel\nalgorithms brings more benefits than relying solely on gradient tracking.\n","authors":["Shuchen Zhu","Boao Kong","Songtao Lu","Xinmeng Huang","Kun Yuan"],"pdf_url":"https://arxiv.org/pdf/2411.14166v1.pdf","comment":"73 pages, the Thirty-Eighth Annual Conference on Neural Information\n  Processing Systems (2024)"},{"id":"http://arxiv.org/abs/2411.14163v1","updated":"2024-11-21T14:22:32Z","published":"2024-11-21T14:22:32Z","title":"Creating a Formally Verified Neural Network for Autonomous Navigation:\n  An Experience Report","summary":"  The increased reliance of self-driving vehicles on neural networks opens up\nthe challenge of their verification. In this paper we present an experience\nreport, describing a case study which we undertook to explore the design and\ntraining of a neural network on a custom dataset for vision-based autonomous\nnavigation. We are particularly interested in the use of machine learning with\ndifferentiable logics to obtain networks satisfying basic safety properties by\ndesign, guaranteeing the behaviour of the neural network after training. We\nmotivate the choice of a suitable neural network verifier for our purposes and\nreport our observations on the use of neural network verifiers for self-driving\nsystems.\n","authors":["Syed Ali Asadullah Bukhari","Thomas Flinkow","Medet Inkarbekov","Barak A. Pearlmutter","Rosemary Monahan"],"pdf_url":"https://arxiv.org/pdf/2411.14163v1.pdf","comment":"In Proceedings FMAS2024, arXiv:2411.13215"},{"id":"http://arxiv.org/abs/2309.07072v2","updated":"2024-11-21T14:10:03Z","published":"2023-09-13T16:33:27Z","title":"The Boundaries of Verifiable Accuracy, Robustness, and Generalisation in\n  Deep Learning","summary":"  In this work, we assess the theoretical limitations of determining guaranteed\nstability and accuracy of neural networks in classification tasks. We consider\nclassical distribution-agnostic framework and algorithms minimising empirical\nrisks and potentially subjected to some weights regularisation. We show that\nthere is a large family of tasks for which computing and verifying ideal stable\nand accurate neural networks in the above settings is extremely challenging, if\nat all possible, even when such ideal solutions exist within the given class of\nneural architectures.\n","authors":["Alexander Bastounis","Alexander N. Gorban","Anders C. Hansen","Desmond J. Higham","Danil Prokhorov","Oliver Sutton","Ivan Y. Tyukin","Qinghua Zhou"],"pdf_url":"https://arxiv.org/pdf/2309.07072v2.pdf","comment":"Revised version of the original submission"},{"id":"http://arxiv.org/abs/2411.14133v1","updated":"2024-11-21T14:00:01Z","published":"2024-11-21T14:00:01Z","title":"GASP: Efficient Black-Box Generation of Adversarial Suffixes for\n  Jailbreaking LLMs","summary":"  Large Language Models (LLMs) have shown impressive proficiency across a range\nof natural language processing tasks yet remain vulnerable to adversarial\nprompts, known as jailbreak attacks, carefully designed to elicit harmful\nresponses from LLMs. Traditional methods rely on manual heuristics, which\nsuffer from limited generalizability. While being automatic, optimization-based\nattacks often produce unnatural jailbreak prompts that are easy to detect by\nsafety filters or require high computational overhead due to discrete token\noptimization. Witnessing the limitations of existing jailbreak methods, we\nintroduce Generative Adversarial Suffix Prompter (GASP), a novel framework that\ncombines human-readable prompt generation with Latent Bayesian Optimization\n(LBO) to improve adversarial suffix creation in a fully black-box setting. GASP\nleverages LBO to craft adversarial suffixes by efficiently exploring continuous\nembedding spaces, gradually optimizing the model to improve attack efficacy\nwhile balancing prompt coherence through a targeted iterative refinement\nprocedure. Our experiments show that GASP can generate natural jailbreak\nprompts, significantly improving attack success rates, reducing training times,\nand accelerating inference speed, thus making it an efficient and scalable\nsolution for red-teaming LLMs.\n","authors":["Advik Raj Basani","Xiao Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14133v1.pdf","comment":"28 pages, 9 tables, 13 figures; under review at CVPR '25"},{"id":"http://arxiv.org/abs/2407.10921v4","updated":"2024-11-21T13:56:07Z","published":"2024-07-15T17:22:16Z","title":"Leveraging Bi-Focal Perspectives and Granular Feature Integration for\n  Accurate Reliable Early Alzheimer's Detection","summary":"  Alzheimer's disease (AD) is the most common neurodegeneration, annually\ndiagnosed in millions of patients. The present medicine scenario still finds\nchallenges in the exact diagnosis and classification of AD through neuroimaging\ndata. Traditional CNNs can extract a good amount of low-level information in an\nimage but fail to extract high-level minuscule particles, which is a\nsignificant challenge in detecting AD from MRI scans. To overcome this, we\npropose a novel Granular Feature Integration method to combine information\nextraction at different scales combined with an efficient information flow,\nenabling the model to capture both broad and fine-grained features\nsimultaneously. We also propose a Bi-Focal Perspective mechanism to highlight\nthe subtle neurofibrillary tangles and amyloid plaques in the MRI scans,\nensuring that critical pathological markers are accurately identified. Our\nmodel achieved an F1-Score of 99.31%, precision of 99.24%, and recall of\n99.51%. These scores prove that our model is significantly better than the\nstate-of-the-art (SOTA) CNNs in existence.\n","authors":["Pandiyaraju V","Shravan Venkatraman","Abeshek A","Pavan Kumar S","Aravintakshan S A","Kannan A"],"pdf_url":"https://arxiv.org/pdf/2407.10921v4.pdf","comment":"14 pages, 12 figures, 6 tables"},{"id":"http://arxiv.org/abs/2309.16584v4","updated":"2024-11-21T13:49:11Z","published":"2023-09-28T16:44:18Z","title":"Collaborative Distributed Machine Learning","summary":"  Various collaborative distributed machine learning (CDML) systems, including\nfederated learning systems and swarm learning systems, with diferent key traits\nwere developed to leverage resources for the development and use of machine\nlearning(ML) models in a conidentiality-preserving way. To meet use case\nrequirements, suitable CDML systems need to be selected. However, comparison\nbetween CDML systems to assess their suitability for use cases is often\ndiicult. To support comparison of CDML systems and introduce scientiic and\npractical audiences to the principal functioning and key traits of CDML\nsystems, this work presents a CDML system conceptualization and CDML\narchetypes.\n","authors":["David Jin","Niclas Kannengießer","Sascha Rank","Ali Sunyaev"],"pdf_url":"https://arxiv.org/pdf/2309.16584v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14117v1","updated":"2024-11-21T13:34:36Z","published":"2024-11-21T13:34:36Z","title":"Umbrella Reinforcement Learning -- computationally efficient tool for\n  hard non-linear problems","summary":"  We report a novel, computationally efficient approach for solving hard\nnonlinear problems of reinforcement learning (RL). Here we combine umbrella\nsampling, from computational physics/chemistry, with optimal control methods.\nThe approach is realized on the basis of neural networks, with the use of\npolicy gradient. It outperforms, by computational efficiency and implementation\nuniversality, all available state-of-the-art algorithms, in application to hard\nRL problems with sparse reward, state traps and lack of terminal states. The\nproposed approach uses an ensemble of simultaneously acting agents, with a\nmodified reward which includes the ensemble entropy, yielding an optimal\nexploration-exploitation balance.\n","authors":["Egor E. Nuzhin","Nikolai V. Brilliantov"],"pdf_url":"https://arxiv.org/pdf/2411.14117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14106v1","updated":"2024-11-21T13:15:01Z","published":"2024-11-21T13:15:01Z","title":"Adjoint-based online learning of two-layer quasi-geostrophic baroclinic\n  turbulence","summary":"  For reasons of computational constraint, most global ocean circulation models\nused for Earth System Modeling still rely on parameterizations of sub-grid\nprocesses, and limitations in these parameterizations affect the modeled ocean\ncirculation and impact on predictive skill. An increasingly popular approach is\nto leverage machine learning approaches for parameterizations, regressing for a\nmap between the resolved state and missing feedbacks in a fluid system as a\nsupervised learning task. However, the learning is often performed in an\n`offline' fashion, without involving the underlying fluid dynamical model\nduring the training stage. Here, we explore the `online' approach that involves\nthe fluid dynamical model during the training stage for the learning of\nbaroclinic turbulence and its parameterization, with reference to ocean eddy\nparameterization. Two online approaches are considered: a full adjoint-based\nonline approach, related to traditional adjoint optimization approaches that\nrequire a `differentiable' dynamical model, and an approximately online\napproach that approximates the adjoint calculation and does not require a\ndifferentiable dynamical model. The online approaches are found to be generally\nmore skillful and numerically stable than offline approaches. Others details\nrelating to online training, such as window size, machine learning model set up\nand designs of the loss functions are detailed to aid in further explorations\nof the online training methodology for Earth System Modeling.\n","authors":["Fei Er Yan","Hugo Frezat","Julien Le Sommer","Julian Mak","Karl Otness"],"pdf_url":"https://arxiv.org/pdf/2411.14106v1.pdf","comment":"25 pages, 1 table, 8 figures"},{"id":"http://arxiv.org/abs/2210.04359v3","updated":"2024-11-21T13:06:53Z","published":"2022-10-09T22:02:58Z","title":"Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years\n  of German Parliamentary Debates","summary":"  Solidarity is a crucial concept to understand social relations in societies.\nIn this paper, we explore fine-grained solidarity frames to study solidarity\ntowards women and migrants in German parliamentary debates between 1867 and\n2022. Using 2,864 manually annotated text snippets (with a cost exceeding 18k\nEuro), we evaluate large language models (LLMs) like Llama 3, GPT-3.5, and\nGPT-4. We find that GPT-4 outperforms other LLMs, approaching human annotation\nquality. Using GPT-4, we automatically annotate more than 18k further instances\n(with a cost of around 500 Euro) across 155 years and find that solidarity with\nmigrants outweighs anti-solidarity but that frequencies and solidarity types\nshift over time. Most importantly, group-based notions of (anti-)solidarity\nfade in favor of compassionate solidarity, focusing on the vulnerability of\nmigrant groups, and exchange-based anti-solidarity, focusing on the lack of\n(economic) contribution. Our study highlights the interplay of historical\nevents, socio-economic needs, and political ideologies in shaping migration\ndiscourse and social cohesion. We also show that powerful LLMs, if carefully\nprompted, can be cost-effective alternatives to human annotation for hard\nsocial scientific tasks.\n","authors":["Aida Kostikova","Benjamin Paassen","Dominik Beese","Ole Pütz","Gregor Wiedemann","Steffen Eger"],"pdf_url":"https://arxiv.org/pdf/2210.04359v3.pdf","comment":"EMNLP 2024 (Main Conference) Camera-Ready Version"},{"id":"http://arxiv.org/abs/2402.03818v3","updated":"2024-11-21T13:03:02Z","published":"2024-02-06T09:07:26Z","title":"Asymptotic generalization error of a single-layer graph convolutional\n  network","summary":"  While graph convolutional networks show great practical promises, the\ntheoretical understanding of their generalization properties as a function of\nthe number of samples is still in its infancy compared to the more broadly\nstudied case of supervised fully connected neural networks. In this article, we\npredict the performances of a single-layer graph convolutional network (GCN)\ntrained on data produced by attributed stochastic block models (SBMs) in the\nhigh-dimensional limit. Previously, only ridge regression on contextual-SBM\n(CSBM) has been considered in Shi et al. 2022; we generalize the analysis to\narbitrary convex loss and regularization for the CSBM and add the analysis for\nanother data model, the neural-prior SBM. We also study the high\nsignal-to-noise ratio limit, detail the convergence rates of the GCN and show\nthat, while consistent, it does not reach the Bayes-optimal rate for any of the\nconsidered cases.\n","authors":["O. Duranthon","L. Zdeborová"],"pdf_url":"https://arxiv.org/pdf/2402.03818v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14094v1","updated":"2024-11-21T12:59:39Z","published":"2024-11-21T12:59:39Z","title":"GNN-MultiFix: Addressing the pitfalls for GNNs for multi-label node\n  classification","summary":"  Graph neural networks (GNNs) have emerged as powerful models for learning\nrepresentations of graph data showing state of the art results in various\ntasks. Nevertheless, the superiority of these methods is usually supported by\neither evaluating their performance on small subset of benchmark datasets or by\nreasoning about their expressive power in terms of certain graph isomorphism\ntests. In this paper we critically analyse both these aspects through a\ntransductive setting for the task of node classification. First, we delve\ndeeper into the case of multi-label node classification which offers a more\nrealistic scenario and has been ignored in most of the related works. Through\nanalysing the training dynamics for GNN methods we highlight the failure of\nGNNs to learn over multi-label graph datasets even for the case of abundant\ntraining data. Second, we show that specifically for transductive node\nclassification, even the most expressive GNN may fail to learn in absence of\nnode attributes and without using explicit label information as input. To\novercome this deficit, we propose a straightforward approach, referred to as\nGNN-MultiFix, that integrates the feature, label, and positional information of\na node. GNN-MultiFix demonstrates significant improvement across all the\nmulti-label datasets. We release our code at\nhttps://anonymous.4open.science/r/Graph-MultiFix-4121.\n","authors":["Tianqi Zhao","Megha Khosla"],"pdf_url":"https://arxiv.org/pdf/2411.14094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14092v1","updated":"2024-11-21T12:58:09Z","published":"2024-11-21T12:58:09Z","title":"MetaCropFollow: Few-Shot Adaptation with Meta-Learning for Under-Canopy\n  Navigation","summary":"  Autonomous under-canopy navigation faces additional challenges compared to\nover-canopy settings - for example the tight spacing between the crop rows,\ndegraded GPS accuracy and excessive clutter. Keypoint-based visual navigation\nhas been shown to perform well in these conditions, however the differences\nbetween agricultural environments in terms of lighting, season, soil and crop\ntype mean that a domain shift will likely be encountered at some point of the\nrobot deployment. In this paper, we explore the use of Meta-Learning to\novercome this domain shift using a minimal amount of data. We train a\nbase-learner that can quickly adapt to new conditions, enabling more robust\nnavigation in low-data regimes.\n","authors":["Thomas Woehrle","Arun N. Sivakumar","Naveen Uppalapati","Girish Chowdhary"],"pdf_url":"https://arxiv.org/pdf/2411.14092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14085v1","updated":"2024-11-21T12:51:09Z","published":"2024-11-21T12:51:09Z","title":"Exploration by Running Away from the Past","summary":"  The ability to explore efficiently and effectively is a central challenge of\nreinforcement learning. In this work, we consider exploration through the lens\nof information theory. Specifically, we cast exploration as a problem of\nmaximizing the Shannon entropy of the state occupation measure. This is done by\nmaximizing a sequence of divergences between distributions representing an\nagent's past behavior and its current behavior. Intuitively, this encourages\nthe agent to explore new behaviors that are distinct from past behaviors.\nHence, we call our method RAMP, for ``$\\textbf{R}$unning $\\textbf{A}$way\nfro$\\textbf{m}$ the $\\textbf{P}$ast.'' A fundamental question of this method is\nthe quantification of the distribution change over time. We consider both the\nKullback-Leibler divergence and the Wasserstein distance to quantify divergence\nbetween successive state occupation measures, and explain why the former might\nlead to undesirable exploratory behaviors in some tasks. We demonstrate that by\nencouraging the agent to explore by actively distancing itself from past\nexperiences, it can effectively explore mazes and a wide range of behaviors on\nrobotic manipulation and locomotion tasks.\n","authors":["Paul-Antoine Le Tolguenec","Yann Besse","Florent Teichteil-Koenigsbuch","Dennis G. Wilson","Emmanuel Rachelson"],"pdf_url":"https://arxiv.org/pdf/2411.14085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07753v2","updated":"2024-11-21T12:47:53Z","published":"2024-10-10T09:29:23Z","title":"Data Augmentation for Surgical Scene Segmentation with Anatomy-Aware\n  Diffusion Models","summary":"  In computer-assisted surgery, automatically recognizing anatomical organs is\ncrucial for understanding the surgical scene and providing intraoperative\nassistance. While machine learning models can identify such structures, their\ndeployment is hindered by the need for labeled, diverse surgical datasets with\nanatomical annotations. Labeling multiple classes (i.e., organs) in a surgical\nscene is time-intensive, requiring medical experts. Although synthetically\ngenerated images can enhance segmentation performance, maintaining both organ\nstructure and texture during generation is challenging. We introduce a\nmulti-stage approach using diffusion models to generate multi-class surgical\ndatasets with annotations. Our framework improves anatomy awareness by training\norgan specific models with an inpainting objective guided by binary\nsegmentation masks. The organs are generated with an inference pipeline using\npre-trained ControlNet to maintain the organ structure. The synthetic\nmulti-class datasets are constructed through an image composition step,\nensuring structural and textural consistency. This versatile approach allows\nthe generation of multi-class datasets from real binary datasets and simulated\nsurgical masks. We thoroughly evaluate the generated datasets on image quality\nand downstream segmentation, achieving a $15\\%$ improvement in segmentation\nscores when combined with real images. The code is available at\nhttps://gitlab.com/nct_tso_public/muli-class-image-synthesis\n","authors":["Danush Kumar Venkatesh","Dominik Rivoir","Micha Pfeiffer","Fiona Kolbinger","Stefanie Speidel"],"pdf_url":"https://arxiv.org/pdf/2410.07753v2.pdf","comment":"Accepted at WACV 2025"},{"id":"http://arxiv.org/abs/2410.14729v2","updated":"2024-11-21T12:17:29Z","published":"2024-10-16T07:13:35Z","title":"Is Less More? Exploring Token Condensation as Training-free Adaptation\n  for CLIP","summary":"  Contrastive language-image pre-training (CLIP) has shown remarkable\ngeneralization ability in image classification. However, CLIP sometimes\nencounters performance drops on downstream datasets during zero-shot inference.\nTest-time adaptation methods attempt to mitigate this by adjusting\nnormalization layers or tuning context prompts with large batch sizes and\nextensive augmentations; yet, these methods are computationally intensive. This\nraises an important question: Is there a training-free approach that can\nefficiently address CLIP's performance drop in such cases? To explore this, we\nbenchmark token condensation techniques, originally designed to enhance the\nefficiency of vision transformers, on CLIP zero-shot inference tasks. We\nobserve that although token condensation may compromise in-domain accuracy, it\nsurprisingly enhances CLIP's performance on certain cross-dataset benchmarks.\nThis motivates two key inquiries: (1) Can token condensation serve as a\n\"free-lunch\" solution for CLIP zero-shot inference? (2) What criteria should\nguide condensation -- how can essential tokens be identified and redundant ones\neliminated? To address these questions, we propose Token Condensation as\nAdaptation (TCA), a training-free adaptation method for CLIP by pruning\nclass-irrelevant visual tokens while merging class-ambiguous tokens. As the\nfirst approach for CLIP's token efficiency, TCA demonstrates superior\nperformance across cross-dataset tasks, achieving up to a 21.4\\% improvement\nover the strongest baseline while reducing GFLOPs by 12.2\\% to 48.9\\%, with\nminimized hyperparameter dependency.\n","authors":["Zixin Wang","Dong Gong","Sen Wang","Zi Huang","Yadan Luo"],"pdf_url":"https://arxiv.org/pdf/2410.14729v2.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2406.12907v3","updated":"2024-11-21T12:17:22Z","published":"2024-06-12T13:30:48Z","title":"Reconciling Kaplan and Chinchilla Scaling Laws","summary":"  Kaplan et al. [2020] (`Kaplan') and Hoffmann et al. [2022] (`Chinchilla')\nstudied the scaling behavior of transformers trained on next-token language\nprediction. These studies produced different estimates for how the number of\nparameters ($N$) and training tokens ($D$) should be set to achieve the lowest\npossible loss for a given compute budget ($C$). Kaplan: $N_\\text{optimal}\n\\propto C^{0.73}$, Chinchilla: $N_\\text{optimal} \\propto C^{0.50}$. This paper\nfinds that much of this discrepancy can be attributed to Kaplan counting\nnon-embedding rather than total parameters, combined with their analysis being\nperformed at small scale. Simulating the Chinchilla study under these\nconditions produces biased scaling coefficients close to Kaplan's. Hence, this\npaper reaffirms Chinchilla's scaling coefficients, by explaining the primary\ncause of Kaplan's original overestimation. As a second contribution, the paper\nexplains differences in the reported relationships between loss and compute.\nThese findings lead us to recommend that future scaling studies use total\nparameters and compute.\n","authors":["Tim Pearce","Jinyeop Song"],"pdf_url":"https://arxiv.org/pdf/2406.12907v3.pdf","comment":"Published in TMLR 2024"},{"id":"http://arxiv.org/abs/2411.14062v1","updated":"2024-11-21T12:16:16Z","published":"2024-11-21T12:16:16Z","title":"MMGenBench: Evaluating the Limits of LMMs from the Text-to-Image\n  Generation Perspective","summary":"  Large Multimodal Models (LMMs) have demonstrated remarkable capabilities.\nWhile existing benchmarks for evaluating LMMs mainly focus on image\ncomprehension, few works evaluate them from the image generation perspective.\nTo address this issue, we propose a straightforward automated evaluation\npipeline. Specifically, this pipeline requires LMMs to generate an image-prompt\nfrom a given input image. Subsequently, it employs text-to-image generative\nmodels to create a new image based on these generated prompts. Finally, we\nevaluate the performance of LMMs by comparing the original image with the\ngenerated one. Furthermore, we introduce MMGenBench-Test, a comprehensive\nbenchmark developed to evaluate LMMs across 13 distinct image patterns, and\nMMGenBench-Domain, targeting the performance evaluation of LMMs within the\ngenerative image domain. A thorough evaluation involving over 50 popular LMMs\ndemonstrates the effectiveness and reliability in both the pipeline and\nbenchmark. Our observations indicate that numerous LMMs excelling in existing\nbenchmarks fail to adequately complete the basic tasks, related to image\nunderstanding and description. This finding highlights the substantial\npotential for performance improvement in current LMMs and suggests avenues for\nfuture model optimization. Concurrently, our pipeline facilitates the efficient\nassessment of LMMs performance across diverse domains by using solely image\ninputs.\n","authors":["Hailang Huang","Yong Wang","Zixuan Huang","Huaqiu Li","Tongwen Huang","Xiangxiang Chu","Richong Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14062v1.pdf","comment":"This project is available at: https://github.com/lerogo/MMGenBench"},{"id":"http://arxiv.org/abs/2411.02193v2","updated":"2024-11-21T12:10:54Z","published":"2024-11-04T15:46:20Z","title":"Improving Steering Vectors by Targeting Sparse Autoencoder Features","summary":"  To control the behavior of language models, steering methods attempt to\nensure that outputs of the model satisfy specific pre-defined properties.\nAdding steering vectors to the model is a promising method of model control\nthat is easier than finetuning, and may be more robust than prompting. However,\nit can be difficult to anticipate the effects of steering vectors produced by\nmethods such as CAA [Panickssery et al., 2024] or the direct use of SAE latents\n[Templeton et al., 2024]. In our work, we address this issue by using SAEs to\nmeasure the effects of steering vectors, giving us a method that can be used to\nunderstand the causal effect of any steering vector intervention. We use this\nmethod for measuring causal effects to develop an improved steering method,\nSAE-Targeted Steering (SAE-TS), which finds steering vectors to target specific\nSAE features while minimizing unintended side effects. We show that overall,\nSAE-TS balances steering effects with coherence better than CAA and SAE feature\nsteering, when evaluated on a range of tasks.\n","authors":["Sviatoslav Chalnev","Matthew Siu","Arthur Conmy"],"pdf_url":"https://arxiv.org/pdf/2411.02193v2.pdf","comment":"8 maintext pages and 9 appendix pages"},{"id":"http://arxiv.org/abs/2411.14049v1","updated":"2024-11-21T11:56:32Z","published":"2024-11-21T11:56:32Z","title":"Out-Of-Distribution Detection with Diversification (Provably)","summary":"  Out-of-distribution (OOD) detection is crucial for ensuring reliable\ndeployment of machine learning models. Recent advancements focus on utilizing\neasily accessible auxiliary outliers (e.g., data from the web or other\ndatasets) in training. However, we experimentally reveal that these methods\nstill struggle to generalize their detection capabilities to unknown OOD data,\ndue to the limited diversity of the auxiliary outliers collected. Therefore, we\nthoroughly examine this problem from the generalization perspective and\ndemonstrate that a more diverse set of auxiliary outliers is essential for\nenhancing the detection capabilities. However, in practice, it is difficult and\ncostly to collect sufficiently diverse auxiliary outlier data. Therefore, we\npropose a simple yet practical approach with a theoretical guarantee, termed\nDiversity-induced Mixup for OOD detection (diverseMix), which enhances the\ndiversity of auxiliary outlier set for training in an efficient way. Extensive\nexperiments show that diverseMix achieves superior performance on commonly used\nand recent challenging large-scale benchmarks, which further confirm the\nimportance of the diversity of auxiliary outliers.\n","authors":["Haiyun Yao","Zongbo Han","Huazhu Fu","Xi Peng","Qinghua Hu","Changqing Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14046v1","updated":"2024-11-21T11:50:17Z","published":"2024-11-21T11:50:17Z","title":"REFOL: Resource-Efficient Federated Online Learning for Traffic Flow\n  Forecasting","summary":"  Multiple federated learning (FL) methods are proposed for traffic flow\nforecasting (TFF) to avoid heavy-transmission and privacy-leaking concerns\nresulting from the disclosure of raw data in centralized methods. However,\nthese FL methods adopt offline learning which may yield subpar performance,\nwhen concept drift occurs, i.e., distributions of historical and future data\nvary. Online learning can detect concept drift during model training, thus more\napplicable to TFF. Nevertheless, the existing federated online learning method\nfor TFF fails to efficiently solve the concept drift problem and causes\ntremendous computing and communication overhead. Therefore, we propose a novel\nmethod named Resource-Efficient Federated Online Learning (REFOL) for TFF,\nwhich guarantees prediction performance in a communication-lightweight and\ncomputation-efficient way. Specifically, we design a data-driven client\nparticipation mechanism to detect the occurrence of concept drift and determine\nclients' participation necessity. Subsequently, we propose an adaptive online\noptimization strategy, which guarantees prediction performance and meanwhile\navoids meaningless model updates. Then, a graph convolution-based model\naggregation mechanism is designed, aiming to assess participants' contribution\nbased on spatial correlation without importing extra communication and\ncomputing consumption on clients. Finally, we conduct extensive experiments on\nreal-world datasets to demonstrate the superiority of REFOL in terms of\nprediction improvement and resource economization.\n","authors":["Qingxiang Liu","Sheng Sun","Yuxuan Liang","Xiaolong Xu","Min Liu","Muhammad Bilal","Yuwei Wang","Xujing Li","Yu Zheng"],"pdf_url":"https://arxiv.org/pdf/2411.14046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14035v1","updated":"2024-11-21T11:39:09Z","published":"2024-11-21T11:39:09Z","title":"Teaching MLPs to Master Heterogeneous Graph-Structured Knowledge for\n  Efficient and Accurate Inference","summary":"  Heterogeneous Graph Neural Networks (HGNNs) have achieved promising results\nin various heterogeneous graph learning tasks, owing to their superiority in\ncapturing the intricate relationships and diverse relational semantics inherent\nin heterogeneous graph structures. However, the neighborhood-fetching latency\nincurred by structure dependency in HGNNs makes it challenging to deploy for\nlatency-constrained applications that require fast inference. Inspired by\nrecent GNN-to-MLP knowledge distillation frameworks, we introduce HG2M and\nHG2M+ to combine both HGNN's superior performance and MLP's efficient\ninference. HG2M directly trains student MLPs with node features as input and\nsoft labels from teacher HGNNs as targets, and HG2M+ further distills reliable\nand heterogeneous semantic knowledge into student MLPs through reliable node\ndistillation and reliable meta-path distillation. Experiments conducted on six\nheterogeneous graph datasets show that despite lacking structural dependencies,\nHG2Ms can still achieve competitive or even better performance than HGNNs and\nsignificantly outperform vanilla MLPs. Moreover, HG2Ms demonstrate a\n379.24$\\times$ speedup in inference over HGNNs on the large-scale IGB-3M-19\ndataset, showcasing their ability for latency-sensitive deployments.\n","authors":["Yunhui Liu","Xinyi Gao","Tieke He","Jianhua Zhao","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2411.14035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14019v1","updated":"2024-11-21T11:03:07Z","published":"2024-11-21T11:03:07Z","title":"Time-Scale Separation in Q-Learning: Extending TD($\\triangle$) for\n  Action-Value Function Decomposition","summary":"  Q-Learning is a fundamental off-policy reinforcement learning (RL) algorithm\nthat has the objective of approximating action-value functions in order to\nlearn optimal policies. Nonetheless, it has difficulties in reconciling bias\nwith variance, particularly in the context of long-term rewards. This paper\nintroduces Q($\\Delta$)-Learning, an extension of TD($\\Delta$) for the\nQ-Learning framework. TD($\\Delta$) facilitates efficient learning over several\ntime scales by breaking the Q($\\Delta$)-function into distinct discount\nfactors. This approach offers improved learning stability and scalability,\nespecially for long-term tasks where discounting bias may impede convergence.\nOur methodology guarantees that each element of the Q($\\Delta$)-function is\nacquired individually, facilitating expedited convergence on shorter time\nscales and enhancing the learning of extended time scales. We demonstrate\nthrough theoretical analysis and practical evaluations on standard benchmarks\nlike Atari that Q($\\Delta$)-Learning surpasses conventional Q-Learning and TD\nlearning methods in both tabular and deep RL environments.\n","authors":["Mahammad Humayoo"],"pdf_url":"https://arxiv.org/pdf/2411.14019v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2411.14017v1","updated":"2024-11-21T11:01:03Z","published":"2024-11-21T11:01:03Z","title":"Automatic brain tumor segmentation in 2D intra-operative ultrasound\n  images using MRI tumor annotations","summary":"  Automatic segmentation of brain tumors in intra-operative ultrasound (iUS)\nimages could facilitate localization of tumor tissue during resection surgery.\nThe lack of large annotated datasets limits the current models performances. In\nthis paper, we investigate the use of tumor annotations in pre-operative MRI\nimages, which are more easily accessible than annotations in iUS images, for\ntraining of deep learning models for iUS brain tumor segmentation. We used 180\nannotated pre-operative MRI images with corresponding unannotated iUS images,\nand 29 annotated iUS images. Image registration was performed to transfer the\nMRI annotations to the corresponding iUS images before training models with the\nnnU-Net framework. To validate the use of MRI labels, the models were compared\nto a model trained with only US annotated tumors, and a model with both US and\nMRI annotated tumors. In addition, the results were compared to annotations\nvalidated by an expert neurosurgeon on the same test set to measure\ninter-observer variability. The results showed similar performance for a model\ntrained with only MRI annotated tumors, compared to a model trained with only\nUS annotated tumors. The model trained using both modalities obtained slightly\nbetter results with an average Dice score of 0.62, where external expert\nannotations achieved a score of 0.67. The results also showed that the deep\nlearning models were comparable to expert annotation for larger tumors (> 200\nmm2), but perform clearly worse for smaller tumors (< 200 mm2). This shows that\nMRI tumor annotations can be used as a substitute for US tumor annotations to\ntrain a deep learning model for automatic brain tumor segmentation in\nintra-operative ultrasound images. Small tumors is a limitation for the current\nmodels and will be the focus of future work. The main models are available\nhere: https://github.com/mathildefaanes/us_brain_tumor_segmentation.\n","authors":["Mathilde Faanes","Ragnhild Holden Helland","Ole Solheim","Ingerid Reinertsen"],"pdf_url":"https://arxiv.org/pdf/2411.14017v1.pdf","comment":"19, 8 figures, submitted to International Journal of Computer\n  Assisted Radiology and Surgery"},{"id":"http://arxiv.org/abs/2411.14014v1","updated":"2024-11-21T10:56:02Z","published":"2024-11-21T10:56:02Z","title":"Trajectory Representation Learning on Road Networks and Grids with\n  Spatio-Temporal Dynamics","summary":"  Trajectory representation learning is a fundamental task for applications in\nfields including smart city, and urban planning, as it facilitates the\nutilization of trajectory data (e.g., vehicle movements) for various downstream\napplications, such as trajectory similarity computation or travel time\nestimation. This is achieved by learning low-dimensional representations from\nhigh-dimensional and raw trajectory data. However, existing methods for\ntrajectory representation learning either rely on grid-based or road-based\nrepresentations, which are inherently different and thus, could lose\ninformation contained in the other modality. Moreover, these methods overlook\nthe dynamic nature of urban traffic, relying on static road network features\nrather than time varying traffic patterns. In this paper, we propose TIGR, a\nnovel model designed to integrate grid and road network modalities while\nincorporating spatio-temporal dynamics to learn rich, general-purpose\nrepresentations of trajectories. We evaluate TIGR on two realworld datasets and\ndemonstrate the effectiveness of combining both modalities by substantially\noutperforming state-of-the-art methods, i.e., up to 43.22% for trajectory\nsimilarity, up to 16.65% for travel time estimation, and up to 10.16% for\ndestination prediction.\n","authors":["Stefan Schestakov","Simon Gottschalk"],"pdf_url":"https://arxiv.org/pdf/2411.14014v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14013v1","updated":"2024-11-21T10:55:49Z","published":"2024-11-21T10:55:49Z","title":"Single-Model Attribution for Spoofed Speech via Vocoder Fingerprints in\n  an Open-World Setting","summary":"  As speech generation technology advances, so do the potential threats of\nmisusing spoofed speech signals. One way to address these threats is by\nattributing the signals to their source generative model. In this work, we are\nthe first to tackle the single-model attribution task in an open-world setting,\nthat is, we aim at identifying whether spoofed speech signals from unknown\nsources originate from a specific vocoder. We show that the standardized\naverage residual between audio signals and their low-pass filtered or EnCodec\nfiltered versions can serve as powerful vocoder fingerprints. The approach only\nrequires data from the target vocoder and allows for simple but highly accurate\ndistance-based model attribution. We demonstrate its effectiveness on LJSpeech\nand JSUT, achieving an average AUROC of over 99% in most settings. The\naccompanying robustness study shows that it is also resilient to noise levels\nup to a certain degree.\n","authors":["Matías Pizarro","Mike Laszkiewicz","Dorothea Kolossa","Asja Fischer"],"pdf_url":"https://arxiv.org/pdf/2411.14013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.09263v2","updated":"2024-11-21T10:46:18Z","published":"2024-11-14T08:02:14Z","title":"Rethinking Weight-Averaged Model-merging","summary":"  Weight-averaged model-merging has emerged as a powerful approach in deep\nlearning, capable of enhancing model performance without fine-tuning or\nretraining. However, the underlying mechanisms that explain its effectiveness\nremain largely unexplored. In this paper, we investigate this technique from\nthree novel perspectives to provide deeper insights into how and why\nweight-averaged model-merging works: (1) we examine the intrinsic patterns\ncaptured by the learning of the model weights, through the visualizations of\ntheir patterns on several datasets, showing that these weights often encode\nstructured and interpretable patterns; (2) we investigate model ensemble\nmerging strategies based on averaging on weights versus averaging on features,\nproviding detailed analyses across diverse architectures and datasets; and (3)\nwe explore the impact on model-merging prediction stability in terms of\nchanging the parameter magnitude, revealing insights into the way of weight\naveraging works as regularization by showing the robustness across different\nparameter scales. Our findings shed light on the \"black box\" of weight-averaged\nmodel-merging, offering valuable insights and practical recommendations that\nadvance the model-merging process.\n","authors":["Hu Wang","Congbo Ma","Ibrahim Almakky","Ian Reid","Gustavo Carneiro","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2411.09263v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14003v1","updated":"2024-11-21T10:37:57Z","published":"2024-11-21T10:37:57Z","title":"Generative Intervention Models for Causal Perturbation Modeling","summary":"  We consider the problem of predicting perturbation effects via causal models.\nIn many applications, it is a priori unknown which mechanisms of a system are\nmodified by an external perturbation, even though the features of the\nperturbation are available. For example, in genomics, some properties of a drug\nmay be known, but not their causal effects on the regulatory pathways of cells.\nWe propose a generative intervention model (GIM) that learns to map these\nperturbation features to distributions over atomic interventions in a\njointly-estimated causal model. Contrary to prior approaches, this enables us\nto predict the distribution shifts of unseen perturbation features while\ngaining insights about their mechanistic effects in the underlying\ndata-generating process. On synthetic data and scRNA-seq drug perturbation\ndata, GIMs achieve robust out-of-distribution predictions on par with\nunstructured approaches, while effectively inferring the underlying\nperturbation mechanisms, often better than other causal inference methods.\n","authors":["Nora Schneider","Lars Lorch","Niki Kilbertus","Bernhard Schölkopf","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2411.14003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13999v1","updated":"2024-11-21T10:26:17Z","published":"2024-11-21T10:26:17Z","title":"Accelerated zero-order SGD under high-order smoothness and\n  overparameterized regime","summary":"  We present a novel gradient-free algorithm to solve a convex stochastic\noptimization problem, such as those encountered in medicine, physics, and\nmachine learning (e.g., adversarial multi-armed bandit problem), where the\nobjective function can only be computed through numerical simulation, either as\nthe result of a real experiment or as feedback given by the function\nevaluations from an adversary. Thus we suppose that only a black-box access to\nthe function values of the objective is available, possibly corrupted by\nadversarial noise: deterministic or stochastic. The noisy setup can arise\nnaturally from modeling randomness within a simulation or by computer\ndiscretization, or when exact values of function are forbidden due to privacy\nissues, or when solving non-convex problems as convex ones with an inexact\nfunction oracle. By exploiting higher-order smoothness, fulfilled, e.g., in\nlogistic regression, we improve the performance of zero-order methods developed\nunder the assumption of classical smoothness (or having a Lipschitz gradient).\nThe proposed algorithm enjoys optimal oracle complexity and is designed under\nan overparameterization setup, i.e., when the number of model parameters is\nmuch larger than the size of the training dataset. Overparametrized models fit\nto the training data perfectly while also having good generalization and\noutperforming underparameterized models on unseen data. We provide convergence\nguarantees for the proposed algorithm under both types of noise. Moreover, we\nestimate the maximum permissible adversarial noise level that maintains the\ndesired accuracy in the Euclidean setup, and then we extend our results to a\nnon-Euclidean setup. Our theoretical results are verified on the logistic\nregression problem.\n","authors":["Georgii Bychkov","Darina Dvinskikh","Anastasia Antsiferova","Alexander Gasnikov","Aleksandr Lobanov"],"pdf_url":"https://arxiv.org/pdf/2411.13999v1.pdf","comment":"10 pages, 1 figure"},{"id":"http://arxiv.org/abs/2410.23773v3","updated":"2024-11-21T10:19:20Z","published":"2024-10-31T09:42:03Z","title":"Towards Generative Ray Path Sampling for Faster Point-to-Point Ray\n  Tracing","summary":"  Radio propagation modeling is essential in telecommunication research, as\nradio channels result from complex interactions with environmental objects.\nRecently, Machine Learning has been attracting attention as a potential\nalternative to computationally demanding tools, like Ray Tracing, which can\nmodel these interactions in detail. However, existing Machine Learning\napproaches often attempt to learn directly specific channel characteristics,\nsuch as the coverage map, making them highly specific to the frequency and\nmaterial properties and unable to fully capture the underlying propagation\nmechanisms. Hence, Ray Tracing, particularly the Point-to-Point variant,\nremains popular to accurately identify all possible paths between transmitter\nand receiver nodes. Still, path identification is computationally intensive\nbecause the number of paths to be tested grows exponentially while only a small\nfraction is valid. In this paper, we propose a Machine Learning-aided Ray\nTracing approach to efficiently sample potential ray paths, significantly\nreducing the computational load while maintaining high accuracy. Our model\ndynamically learns to prioritize potentially valid paths among all possible\npaths and scales linearly with scene complexity. Unlike recent alternatives,\nour approach is invariant with translation, scaling, or rotation of the\ngeometry, and avoids dependency on specific environment characteristics.\n","authors":["Jérome Eertmans","Nicola Di Cicco","Claude Oestges","Laurent Jacques","Enrico M. Vittuci","Vittorio Degli-Esposti"],"pdf_url":"https://arxiv.org/pdf/2410.23773v3.pdf","comment":"6 pages, 6 figures, submitted to IEEE ICMLCN 2025"},{"id":"http://arxiv.org/abs/2411.13993v1","updated":"2024-11-21T10:13:55Z","published":"2024-11-21T10:13:55Z","title":"Market Making without Regret","summary":"  We consider a sequential decision-making setting where, at every round $t$, a\nmarket maker posts a bid price $B_t$ and an ask price $A_t$ to an incoming\ntrader (the taker) with a private valuation for one unit of some asset. If the\ntrader's valuation is lower than the bid price, or higher than the ask price,\nthen a trade (sell or buy) occurs. If a trade happens at round $t$, then\nletting $M_t$ be the market price (observed only at the end of round $t$), the\nmaker's utility is $M_t - B_t$ if the maker bought the asset, and $A_t - M_t$\nif they sold it. We characterize the maker's regret with respect to the best\nfixed choice of bid and ask pairs under a variety of assumptions (adversarial,\ni.i.d., and their variants) on the sequence of market prices and valuations.\nOur upper bound analysis unveils an intriguing connection relating market\nmaking to first-price auctions and dynamic pricing. Our main technical\ncontribution is a lower bound for the i.i.d. case with Lipschitz distributions\nand independence between prices and valuations. The difficulty in the analysis\nstems from the unique structure of the reward and feedback functions, allowing\nan algorithm to acquire information by graduating the \"cost of exploration\" in\nan arbitrary way.\n","authors":["Nicolò Cesa-Bianchi","Tommaso Cesari","Roberto Colomboni","Luigi Foscari","Vinayak Pathak"],"pdf_url":"https://arxiv.org/pdf/2411.13993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19347v2","updated":"2024-11-21T10:12:41Z","published":"2024-05-21T06:27:07Z","title":"Near-Field Spot Beamfocusing: A Correlation-Aware Transfer Learning\n  Approach","summary":"  3D spot beamfocusing (SBF), in contrast to conventional angular-domain\nbeamforming, concentrates radiating power within very small volume in both\nradial and angular domains in the near-field zone. Recently the implementation\nof channel-state-information (CSI)-independent machine learning (ML)-based\napproaches have been developed for effective SBF using\nextremely-largescale-programable-metasurface (ELPMs). These methods involve\ndividing the ELPMs into subarrays and independently training them with Deep\nReinforcement Learning to jointly focus the beam at the Desired Focal Point\n(DFP). This paper explores near-field SBF using ELPMs, addressing challenges\nassociated with lengthy training times resulting from independent training of\nsubarrays. To achieve a faster CSIindependent solution, inspired by the\ncorrelation between the beamfocusing matrices of the subarrays, we leverage\ntransfer learning techniques. First, we introduce a novel similarity criterion\nbased on the Phase Distribution Image of subarray apertures. Then we devise a\nsubarray policy propagation scheme that transfers the knowledge from trained to\nuntrained subarrays. We further enhance learning by introducing\nQuasi-Liquid-Layers as a revised version of the adaptive policy reuse\ntechnique. We show through simulations that the proposed scheme improves the\ntraining speed about 5 times. Furthermore, for dynamic DFP management, we\ndevised a DFP policy blending process, which augments the convergence rate up\nto 8-fold.\n","authors":["Mohammad Amir Fallah","Mehdi Monemi","Mehdi Rasti","Matti Latva-Aho"],"pdf_url":"https://arxiv.org/pdf/2405.19347v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20988v4","updated":"2024-11-21T10:04:19Z","published":"2024-05-31T16:34:11Z","title":"Communication-Efficient Distributed Deep Learning via Federated Dynamic\n  Averaging","summary":"  The ever-growing volume and decentralized nature of data, coupled with the\nneed to harness it and extract knowledge, have led to the extensive use of\ndistributed deep learning (DDL) techniques for training. These techniques rely\non local training performed at distributed nodes using locally collected data,\nfollowed by a periodic synchronization process that combines these models to\ncreate a unified global model. However, the frequent synchronization of deep\nlearning models, encompassing millions to many billions of parameters, creates\na communication bottleneck, severely hindering scalability. Worse yet, DDL\nalgorithms typically waste valuable bandwidth and render themselves less\npractical in bandwidth-constrained federated settings by relying on overly\nsimplistic, periodic, and rigid synchronization schedules. These inefficiencies\nmake the training process increasingly impractical as they demand excessive\ntime for data communication. To address these shortcomings, we propose\nFederated Dynamic Averaging (FDA), a communication-efficient DDL strategy that\ndynamically triggers synchronization based on the value of the model variance.\nIn essence, the costly synchronization step is triggered only if the local\nmodels -- initialized from a common global model after each synchronization --\nhave significantly diverged. This decision is facilitated by the transmission\nof a small local state from each distributed node. Through extensive\nexperiments across a wide range of learning tasks we demonstrate that FDA\nreduces communication cost by orders of magnitude, compared to both traditional\nand cutting-edge communication-efficient algorithms. Additionally, we show that\nFDA maintains robust performance across diverse data heterogeneity settings.\n","authors":["Michail Theologitis","Georgios Frangias","Georgios Anestis","Vasilis Samoladas","Antonios Deligiannakis"],"pdf_url":"https://arxiv.org/pdf/2405.20988v4.pdf","comment":"Accepted as research paper at EDBT 2025"},{"id":"http://arxiv.org/abs/2403.12116v4","updated":"2024-11-21T09:50:30Z","published":"2024-03-18T16:14:28Z","title":"Unsupervised End-to-End Training with a Self-Defined Target","summary":"  Designing algorithms for versatile AI hardware that can learn on the edge\nusing both labeled and unlabeled data is challenging. Deep end-to-end training\nmethods incorporating phases of self-supervised and supervised learning are\naccurate and adaptable to input data but self-supervised learning requires even\nmore computational and memory resources than supervised learning, too high for\ncurrent embedded hardware. Conversely, unsupervised layer-by-layer training,\nsuch as Hebbian learning, is more compatible with existing hardware but does\nnot integrate well with supervised learning. To address this, we propose a\nmethod enabling networks or hardware designed for end-to-end supervised\nlearning to also perform high-performance unsupervised learning by adding two\nsimple elements to the output layer: Winner-Take-All (WTA) selectivity and\nhomeostasis regularization. These mechanisms introduce a \"self-defined target\"\nfor unlabeled data, allowing purely unsupervised training for both\nfully-connected and convolutional layers using backpropagation or equilibrium\npropagation on datasets like MNIST (up to 99.2%), Fashion-MNIST (up to 90.3%),\nand SVHN (up to 81.5%). We extend this method to semi-supervised learning,\nadjusting targets based on data type, achieving 96.6% accuracy with only 600\nlabeled MNIST samples in a multi-layer perceptron. Our results show that this\napproach can effectively enable networks and hardware initially dedicated to\nsupervised learning to also perform unsupervised learning, adapting to varying\navailability of labeled data.\n","authors":["Dongshu Liu","Jérémie Laydevant","Adrien Pontlevy","Damien Querlioz","Julie Grollier"],"pdf_url":"https://arxiv.org/pdf/2403.12116v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08032v3","updated":"2024-11-21T09:46:44Z","published":"2023-03-14T16:11:47Z","title":"Verifying the Robustness of Automatic Credibility Assessment","summary":"  Text classification methods have been widely investigated as a way to detect\ncontent of low credibility: fake news, social media bots, propaganda, etc.\nQuite accurate models (likely based on deep neural networks) help in moderating\npublic electronic platforms and often cause content creators to face rejection\nof their submissions or removal of already published texts. Having the\nincentive to evade further detection, content creators try to come up with a\nslightly modified version of the text (known as an attack with an adversarial\nexample) that exploit the weaknesses of classifiers and result in a different\noutput. Here we systematically test the robustness of common text classifiers\nagainst available attacking techniques and discover that, indeed,\nmeaning-preserving changes in input text can mislead the models. The approaches\nwe test focus on finding vulnerable spans in text and replacing individual\ncharacters or words, taking into account the similarity between the original\nand replacement content. We also introduce BODEGA: a benchmark for testing both\nvictim models and attack methods on four misinformation detection tasks in an\nevaluation framework designed to simulate real use-cases of content moderation.\nThe attacked tasks include (1) fact checking and detection of (2) hyperpartisan\nnews, (3) propaganda and (4) rumours. Our experimental results show that modern\nlarge language models are often more vulnerable to attacks than previous,\nsmaller solutions, e.g. attacks on GEMMA being up to 27\\% more successful than\nthose on BERT. Finally, we manually analyse a subset adversarial examples and\ncheck what kinds of modifications are used in successful attacks.\n","authors":["Piotr Przybyła","Alexander Shvets","Horacio Saggion"],"pdf_url":"https://arxiv.org/pdf/2303.08032v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13970v1","updated":"2024-11-21T09:34:48Z","published":"2024-11-21T09:34:48Z","title":"Movable Antenna-Equipped UAV for Data Collection in Backscatter Sensor\n  Networks: A Deep Reinforcement Learning-based Approach","summary":"  Backscatter communication (BC) becomes a promising energy-efficient solution\nfor future wireless sensor networks (WSNs). Unmanned aerial vehicles (UAVs)\nenable flexible data collection from remote backscatter devices (BDs), yet\nconventional UAVs rely on omni-directional fixed-position antennas (FPAs),\nlimiting channel gain and prolonging data collection time. To address this\nissue, we consider equipping a UAV with a directional movable antenna (MA) with\nhigh directivity and flexibility. The MA enhances channel gain by precisely\naiming its main lobe at each BD, focusing transmission power for efficient\ncommunication. Our goal is to minimize the total data collection time by\njointly optimizing the UAV's trajectory and the MA's orientation. We develop a\ndeep reinforcement learning (DRL)-based strategy using the azimuth angle and\ndistance between the UAV and each BD to simplify the agent's observation space.\nTo ensure stability during training, we adopt Soft Actor-Critic (SAC) algorithm\nthat balances exploration with reward maximization for efficient and reliable\nlearning. Simulation results demonstrate that our proposed MA-equipped UAV with\nSAC outperforms both FPA-equipped UAVs and other RL methods, achieving\nsignificant reductions in both data collection time and energy consumption.\n","authors":["Yu Bai","Boxuan Xie","Ruifan Zhu","Zheng Chang","Riku Jantti"],"pdf_url":"https://arxiv.org/pdf/2411.13970v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04165v2","updated":"2024-11-21T09:30:51Z","published":"2024-06-06T15:22:33Z","title":"Repurposing Language Models into Embedding Models: Finding the\n  Compute-Optimal Recipe","summary":"  Text embeddings are essential for many tasks, such as document retrieval,\nclustering, and semantic similarity assessment. In this paper, we study how to\ncontrastively train text embedding models in a compute-optimal fashion, given a\nsuite of pre-trained decoder-only language models. Our innovation is an\nalgorithm that produces optimal configurations of model sizes, data quantities,\nand fine-tuning methods for text-embedding models at different computational\nbudget levels. The resulting recipe, which we obtain through extensive\nexperiments, can be used by practitioners to make informed design choices for\ntheir embedding models. Specifically, our findings suggest that full\nfine-tuning and low-rank adaptation fine-tuning produce optimal models at lower\nand higher computational budgets respectively.\n","authors":["Alicja Ziarko","Albert Q. Jiang","Bartosz Piotrowski","Wenda Li","Mateja Jamnik","Piotr Miłoś"],"pdf_url":"https://arxiv.org/pdf/2406.04165v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.13366v2","updated":"2024-11-21T09:27:08Z","published":"2024-11-20T14:42:53Z","title":"Predicting Wall Thickness Changes in Cold Forging Processes: An\n  Integrated FEM and Neural Network approach","summary":"  This study presents a novel approach for predicting wall thickness changes in\ntubes during the nosing process. Specifically, we first provide a thorough\nanalysis of nosing processes and the influencing parameters. We further set-up\na Finite Element Method (FEM) simulation to better analyse the effects of\nvarying process parameters. As however traditional FEM simulations, while\naccurate, are time-consuming and computationally intensive, which renders them\ninapplicable for real-time application, we present a novel modeling framework\nbased on specifically designed graph neural networks as surrogate models. To\nthis end, we extend the neural network architecture by directly incorporating\ninformation about the nosing process by adding different types of edges and\ntheir corresponding encoders to model object interactions. This augmentation\nenhances model accuracy and opens the possibility for employing precise\nsurrogate models within closed-loop production processes. The proposed approach\nis evaluated using a new evaluation metric termed area between thickness curves\n(ABTC). The results demonstrate promising performance and highlight the\npotential of neural networks as surrogate models in predicting wall thickness\nchanges during nosing forging processes.\n","authors":["Sasa Ilic","Abdulkerim Karaman","Johannes Pöppelbaum","Jan Niclas Reimann","Michael Marré","Andreas Schwung"],"pdf_url":"https://arxiv.org/pdf/2411.13366v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16098v2","updated":"2024-11-21T09:24:12Z","published":"2024-09-24T13:52:15Z","title":"The Digital Transformation in Health: How AI Can Improve the Performance\n  of Health Systems","summary":"  Mobile health has the potential to revolutionize health care delivery and\npatient engagement. In this work, we discuss how integrating Artificial\nIntelligence into digital health applications-focused on supply chain, patient\nmanagement, and capacity building, among other use cases-can improve the health\nsystem and public health performance. We present an Artificial Intelligence and\nReinforcement Learning platform that allows the delivery of adaptive\ninterventions whose impact can be optimized through experimentation and\nreal-time monitoring. The system can integrate multiple data sources and\ndigital health applications. The flexibility of this platform to connect to\nvarious mobile health applications and digital devices and send personalized\nrecommendations based on past data and predictions can significantly improve\nthe impact of digital tools on health system outcomes. The potential for\nresource-poor settings, where the impact of this approach on health outcomes\ncould be more decisive, is discussed specifically. This framework is, however,\nsimilarly applicable to improving efficiency in health systems where scarcity\nis not an issue.\n","authors":["África Periáñez","Ana Fernández del Río","Ivan Nazarov","Enric Jané","Moiz Hassan","Aditya Rastogi","Dexian Tang"],"pdf_url":"https://arxiv.org/pdf/2409.16098v2.pdf","comment":"This is an original manuscript of an article published by Taylor &\n  Francis in Health Systems & Reform on 22 Oct 2024, available online:\n  https://www.tandfonline.com/doi/10.1080/23288604.2024.2387138"},{"id":"http://arxiv.org/abs/2411.13953v1","updated":"2024-11-21T09:06:33Z","published":"2024-11-21T09:06:33Z","title":"Material synthesis through simulations guided by machine learning: a\n  position paper","summary":"  In this position paper, we propose an approach for sustainable data\ncollection in the field of optimal mix design for marble sludge reuse. Marble\nsludge, a calcium-rich residual from stone-cutting processes, can be repurposed\nby mixing it with various ingredients. However, determining the optimal mix\ndesign is challenging due to the variability in sludge composition and the\ncostly, time-consuming nature of experimental data collection. Also, we\ninvestigate the possibility of using machine learning models using\nmeta-learning as an optimization tool to estimate the correct quantity of\nstone-cutting sludge to be used in aggregates to obtain a mix design with\nspecific mechanical properties that can be used successfully in the building\nindustry. Our approach offers two key advantages: (i) through simulations, a\nlarge dataset can be generated, saving time and money during the data\ncollection phase, and (ii) Utilizing machine learning models, with performance\nenhancement through hyper-parameter optimization via meta-learning, to estimate\noptimal mix designs reducing the need for extensive manual experimentation,\nlowering costs, minimizing environmental impact, and accelerating the\nprocessing of quarry sludge. Our idea promises to streamline the marble sludge\nreuse process by leveraging collective data and advanced machine learning,\npromoting sustainability and efficiency in the stonecutting sector.\n","authors":["Usman Syed","Federico Cunico","Uzair Khan","Eros Radicchi","Francesco Setti","Adolfo Speghini","Paolo Marone","Filiberto Semenzin","Marco Cristani"],"pdf_url":"https://arxiv.org/pdf/2411.13953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13951v1","updated":"2024-11-21T09:03:12Z","published":"2024-11-21T09:03:12Z","title":"A Dataset for Evaluating Online Anomaly Detection Approaches for\n  Discrete Multivariate Time Series","summary":"  Benchmarking anomaly detection approaches for multivariate time series is\nchallenging due to the lack of high-quality datasets. Current publicly\navailable datasets are too small, not diverse and feature trivial anomalies,\nwhich hinders measurable progress in this research area. We propose a solution:\na diverse, extensive, and non-trivial dataset generated via state-of-the-art\nsimulation tools that reflects realistic behaviour of an automotive powertrain,\nincluding its multivariate, dynamic and variable-state properties. To cater for\nboth unsupervised and semi-supervised anomaly detection settings, as well as\ntime series generation and forecasting, we make different versions of the\ndataset available, where training and test subsets are offered in contaminated\nand clean versions, depending on the task. We also provide baseline results\nfrom a small selection of approaches based on deterministic and variational\nautoencoders, as well as a non-parametric approach. As expected, the baseline\nexperimentation shows that the approaches trained on the semi-supervised\nversion of the dataset outperform their unsupervised counterparts, highlighting\na need for approaches more robust to contaminated training data.\n","authors":["Lucas Correia","Jan-Christoph Goos","Thomas Bäck","Anna V. Kononova"],"pdf_url":"https://arxiv.org/pdf/2411.13951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02611v2","updated":"2024-11-21T08:57:44Z","published":"2023-12-05T09:39:04Z","title":"Privacy-Aware Data Acquisition under Data Similarity in Regression\n  Markets","summary":"  Data markets facilitate decentralized data exchange for applications such as\nprediction, learning, or inference. The design of these markets is challenged\nby varying privacy preferences as well as data similarity among data owners.\nRelated works have often overlooked how data similarity impacts pricing and\ndata value through statistical information leakage. We demonstrate that data\nsimilarity and privacy preferences are integral to market design and propose a\nquery-response protocol using local differential privacy for a two-party data\nacquisition mechanism. In our regression data market model, we analyze\nstrategic interactions between privacy-aware owners and the learner as a\nStackelberg game over the asked price and privacy factor. Finally, we\nnumerically evaluate how data similarity affects market participation and\ntraded data value.\n","authors":["Shashi Raj Pandey","Pierre Pinson","Petar Popovski"],"pdf_url":"https://arxiv.org/pdf/2312.02611v2.pdf","comment":"Submitted to IEEE Transactions on Neural Networks and Learning\n  Systems"},{"id":"http://arxiv.org/abs/2411.13187v2","updated":"2024-11-21T08:56:57Z","published":"2024-11-20T10:40:08Z","title":"Engagement-Driven Content Generation with Large Language Models","summary":"  Large Language Models (LLMs) exhibit significant persuasion capabilities in\none-on-one interactions, but their influence within social networks remains\nunderexplored. This study investigates the potential social impact of LLMs in\nthese environments, where interconnected users and complex opinion dynamics\npose unique challenges. In particular, we address the following research\nquestion: can LLMs learn to generate meaningful content that maximizes user\nengagement on social networks?\n  To answer this question, we define a pipeline to guide the LLM-based content\ngeneration which employs reinforcement learning with simulated feedback. In our\nframework, the reward is based on an engagement model borrowed from the\nliterature on opinion dynamics and information propagation. Moreover, we force\nthe text generated by the LLM to be aligned with a given topic and to satisfy a\nminimum fluency requirement.\n  Using our framework, we analyze the capabilities and limitations of LLMs in\ntackling the given task, specifically considering the relative positions of the\nLLM as an agent within the social network and the distribution of opinions in\nthe network on the given topic. Our findings show the full potential of LLMs in\ncreating social engagement. Notable properties of our approach are that the\nlearning procedure is adaptive to the opinion distribution of the underlying\nnetwork and agnostic to the specifics of the engagement model, which is\nembedded as a plug-and-play component. In this regard, our approach can be\neasily refined for more complex engagement tasks and interventions in\ncomputational social science.\n  The code used for the experiments is publicly available at\nhttps://anonymous.4open.science/r/EDCG/.\n","authors":["Erica Coppolillo","Federico Cinus","Marco Minici","Francesco Bonchi","Giuseppe Manco"],"pdf_url":"https://arxiv.org/pdf/2411.13187v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13945v1","updated":"2024-11-21T08:54:45Z","published":"2024-11-21T08:54:45Z","title":"Neuromorphic Attitude Estimation and Control","summary":"  The real-world application of small drones is mostly hampered by energy\nlimitations. Neuromorphic computing promises extremely energy-efficient AI for\nautonomous flight, but is still challenging to train and deploy on real robots.\nIn order to reap the maximal benefits from neuromorphic computing, it is\ndesired to perform all autonomy functions end-to-end on a single neuromorphic\nchip, from low-level attitude control to high-level navigation. This research\npresents the first neuromorphic control system using a spiking neural network\n(SNN) to effectively map a drone's raw sensory input directly to motor\ncommands. We apply this method to low-level attitude estimation and control for\na quadrotor, deploying the SNN on a tiny Crazyflie. We propose a modular SNN,\nseparately training and then merging estimation and control sub-networks. The\nSNN is trained with imitation learning, using a flight dataset of sensory-motor\npairs. Post-training, the network is deployed on the Crazyflie, issuing control\ncommands from sensor inputs at $500$Hz. Furthermore, for the training procedure\nwe augmented training data by flying a controller with additional excitation\nand time-shifting the target data to enhance the predictive capabilities of the\nSNN. On the real drone the perception-to-control SNN tracks attitude commands\nwith an average error of $3$ degrees, compared to $2.5$ degrees for the regular\nflight stack. We also show the benefits of the proposed learning modifications\nfor reducing the average tracking error and reducing oscillations. Our work\nshows the feasibility of performing neuromorphic end-to-end control, laying the\nbasis for highly energy-efficient and low-latency neuromorphic autopilots.\n","authors":["Stein Stroobants","Christophe de Wagter","Guido C. H. E. De Croon"],"pdf_url":"https://arxiv.org/pdf/2411.13945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08944v2","updated":"2024-11-21T08:50:56Z","published":"2023-10-13T08:19:31Z","title":"A Confidence-based Acquisition Model for Self-supervised Active Learning\n  and Label Correction","summary":"  Supervised neural approaches are hindered by their dependence on large,\nmeticulously annotated datasets, a requirement that is particularly cumbersome\nfor sequential tasks. The quality of annotations tends to deteriorate with the\ntransition from expert-based to crowd-sourced labelling. To address these\nchallenges, we present CAMEL (Confidence-based Acquisition Model for Efficient\nself-supervised active Learning), a pool-based active learning framework\ntailored to sequential multi-output problems. CAMEL possesses two core\nfeatures: (1) it requires expert annotators to label only a fraction of a\nchosen sequence, and (2) it facilitates self-supervision for the remainder of\nthe sequence. By deploying a label correction mechanism, CAMEL can also be\nutilised for data cleaning. We evaluate CAMEL on two sequential tasks, with a\nspecial emphasis on dialogue belief tracking, a task plagued by the constraints\nof limited and noisy datasets. Our experiments demonstrate that CAMEL\nsignificantly outperforms the baselines in terms of efficiency. Furthermore,\nthe data corrections suggested by our method contribute to an overall\nimprovement in the quality of the resulting datasets.\n","authors":["Carel van Niekerk","Christian Geishauser","Michael Heck","Shutong Feng","Hsien-chin Lin","Nurul Lubis","Benjamin Ruppik","Renato Vukovic","Milica Gašić"],"pdf_url":"https://arxiv.org/pdf/2310.08944v2.pdf","comment":"Accepted at TACL"},{"id":"http://arxiv.org/abs/2411.02853v2","updated":"2024-11-21T08:44:23Z","published":"2024-11-05T06:57:47Z","title":"ADOPT: Modified Adam Can Converge with Any $β_2$ with the Optimal\n  Rate","summary":"  Adam is one of the most popular optimization algorithms in deep learning.\nHowever, it is known that Adam does not converge in theory unless choosing a\nhyperparameter, i.e., $\\beta_2$, in a problem-dependent manner. There have been\nmany attempts to fix the non-convergence (e.g., AMSGrad), but they require an\nimpractical assumption that the gradient noise is uniformly bounded. In this\npaper, we propose a new adaptive gradient method named ADOPT, which achieves\nthe optimal convergence rate of $\\mathcal{O} ( 1 / \\sqrt{T} )$ with any choice\nof $\\beta_2$ without depending on the bounded noise assumption. ADOPT addresses\nthe non-convergence issue of Adam by removing the current gradient from the\nsecond moment estimate and changing the order of the momentum update and the\nnormalization by the second moment estimate. We also conduct intensive\nnumerical experiments, and verify that our ADOPT achieves superior results\ncompared to Adam and its variants across a wide range of tasks, including image\nclassification, generative modeling, natural language processing, and deep\nreinforcement learning. The implementation is available at\nhttps://github.com/iShohei220/adopt.\n","authors":["Shohei Taniguchi","Keno Harada","Gouki Minegishi","Yuta Oshima","Seong Cheol Jeong","Go Nagahara","Tomoshi Iiyama","Masahiro Suzuki","Yusuke Iwasawa","Yutaka Matsuo"],"pdf_url":"https://arxiv.org/pdf/2411.02853v2.pdf","comment":"Accepted at Neural Information Processing Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2411.13934v1","updated":"2024-11-21T08:36:17Z","published":"2024-11-21T08:36:17Z","title":"Learning to Cooperate with Humans using Generative Agents","summary":"  Training agents that can coordinate zero-shot with humans is a key mission in\nmulti-agent reinforcement learning (MARL). Current algorithms focus on training\nsimulated human partner policies which are then used to train a Cooperator\nagent. The simulated human is produced either through behavior cloning over a\ndataset of human cooperation behavior, or by using MARL to create a population\nof simulated agents. However, these approaches often struggle to produce a\nCooperator that can coordinate well with real humans, since the simulated\nhumans fail to cover the diverse strategies and styles employed by people in\nthe real world. We show \\emph{learning a generative model of human partners}\ncan effectively address this issue. Our model learns a latent variable\nrepresentation of the human that can be regarded as encoding the human's unique\nstrategy, intention, experience, or style. This generative model can be\nflexibly trained from any (human or neural policy) agent interaction data. By\nsampling from the latent space, we can use the generative model to produce\ndifferent partners to train Cooperator agents. We evaluate our method --\n\\textbf{G}enerative \\textbf{A}gent \\textbf{M}odeling for \\textbf{M}ulti-agent\n\\textbf{A}daptation (GAMMA) -- on Overcooked, a challenging cooperative cooking\ngame that has become a standard benchmark for zero-shot coordination. We\nconduct an evaluation with real human teammates, and the results show that\nGAMMA consistently improves performance, whether the generative model is\ntrained on simulated populations or human datasets. Further, we propose a\nmethod for posterior sampling from the generative model that is biased towards\nthe human data, enabling us to efficiently improve performance with only a\nsmall amount of expensive human interaction data.\n","authors":["Yancheng Liang","Daphne Chen","Abhishek Gupta","Simon S. Du","Natasha Jaques"],"pdf_url":"https://arxiv.org/pdf/2411.13934v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07462v2","updated":"2024-11-21T08:32:27Z","published":"2024-08-27T14:51:11Z","title":"S-MolSearch: 3D Semi-supervised Contrastive Learning for Bioactive\n  Molecule Search","summary":"  Virtual Screening is an essential technique in the early phases of drug\ndiscovery, aimed at identifying promising drug candidates from vast molecular\nlibraries. Recently, ligand-based virtual screening has garnered significant\nattention due to its efficacy in conducting extensive database screenings\nwithout relying on specific protein-binding site information. Obtaining binding\naffinity data for complexes is highly expensive, resulting in a limited amount\nof available data that covers a relatively small chemical space. Moreover,\nthese datasets contain a significant amount of inconsistent noise. It is\nchallenging to identify an inductive bias that consistently maintains the\nintegrity of molecular activity during data augmentation. To tackle these\nchallenges, we propose S-MolSearch, the first framework to our knowledge, that\nleverages molecular 3D information and affinity information in semi-supervised\ncontrastive learning for ligand-based virtual screening. Drawing on the\nprinciples of inverse optimal transport, S-MolSearch efficiently processes both\nlabeled and unlabeled data, training molecular structural encoders while\ngenerating soft labels for the unlabeled data. This design allows S-MolSearch\nto adaptively utilize unlabeled data within the learning process. Empirically,\nS-MolSearch demonstrates superior performance on widely-used benchmarks\nLIT-PCBA and DUD-E. It surpasses both structure-based and ligand-based virtual\nscreening methods for AUROC, BEDROC and EF.\n","authors":["Gengmo Zhou","Zhen Wang","Feng Yu","Guolin Ke","Zhewei Wei","Zhifeng Gao"],"pdf_url":"https://arxiv.org/pdf/2409.07462v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13922v1","updated":"2024-11-21T08:18:04Z","published":"2024-11-21T08:18:04Z","title":"Exponentially Consistent Nonparametric Clustering of Data Streams","summary":"  In this paper, we consider nonparametric clustering of $M$ independent and\nidentically distributed (i.i.d.) data streams generated from unknown\ndistributions. The distributions of the $M$ data streams belong to $K$\nunderlying distribution clusters. Existing results on exponentially consistent\nnonparametric clustering algorithms, like single linkage-based (SLINK)\nclustering and $k$-medoids distribution clustering, assume that the maximum\nintra-cluster distance ($d_L$) is smaller than the minimum inter-cluster\ndistance ($d_H$). First, in the fixed sample size (FSS) setting, we show that\nexponential consistency can be achieved for SLINK clustering under a less\nstrict assumption, $d_I < d_H$, where $d_I$ is the maximum distance between any\ntwo sub-clusters of a cluster that partition the cluster. Note that $d_I < d_L$\nin general. Our results show that SLINK is exponentially consistent for a\nlarger class of problems than $k$-medoids distribution clustering. We also\nidentify examples where $k$-medoids clustering is unable to find the true\nclusters, but SLINK is exponentially consistent. Then, we propose a sequential\nclustering algorithm, named SLINK-SEQ, based on SLINK and prove that it is also\nexponentially consistent. Simulation results show that the SLINK-SEQ algorithm\nrequires fewer expected number of samples than the FSS SLINK algorithm for the\nsame probability of error.\n","authors":["Bhupender Singh","Ananth Ram Rajagopalan","Srikrishna Bhashyam"],"pdf_url":"https://arxiv.org/pdf/2411.13922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13921v1","updated":"2024-11-21T08:17:53Z","published":"2024-11-21T08:17:53Z","title":"NBMLSS: probabilistic forecasting of electricity prices via Neural Basis\n  Models for Location Scale and Shape","summary":"  Forecasters using flexible neural networks (NN) in multi-horizon\ndistributional regression setups often struggle to gain detailed insights into\nthe underlying mechanisms that lead to the predicted feature-conditioned\ndistribution parameters. In this work, we deploy a Neural Basis Model for\nLocation, Scale and Shape, that blends the principled interpretability of\nGAMLSS with a computationally scalable shared basis decomposition, combined by\nlinear projections supporting dedicated stepwise and parameter-wise feature\nshape functions aggregations. Experiments have been conducted on multiple\nmarket regions, achieving probabilistic forecasting performance comparable to\nthat of distributional neural networks, while providing more insights into the\nmodel behavior through the learned nonlinear feature level maps to the\ndistribution parameters across the prediction steps.\n","authors":["Alessandro Brusaferri","Danial Ramin","Andrea Ballarino"],"pdf_url":"https://arxiv.org/pdf/2411.13921v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2411.13919v1","updated":"2024-11-21T08:14:26Z","published":"2024-11-21T08:14:26Z","title":"Predictive Maintenance Study for High-Pressure Industrial Compressors:\n  Hybrid Clustering Models","summary":"  This study introduces a predictive maintenance strategy for high pressure\nindustrial compressors using sensor data and features derived from unsupervised\nclustering integrated into classification models. The goal is to enhance model\naccuracy and efficiency in detecting compressor failures. After data pre\nprocessing, sensitive clustering parameters were tuned to identify algorithms\nthat best capture the dataset's temporal and operational characteristics.\nClustering algorithms were evaluated using quality metrics like Normalized\nMutual Information (NMI) and Adjusted Rand Index (ARI), selecting those most\neffective at distinguishing between normal and non normal conditions. These\nfeatures enriched regression models, improving failure detection accuracy by\n4.87 percent on average. Although training time was reduced by 22.96 percent,\nthe decrease was not statistically significant, varying across algorithms.\nCross validation and key performance metrics confirmed the benefits of\nclustering based features in predictive maintenance models.\n","authors":["Alessandro Costa","Emilio Mastriani","Federico Incardona","Kevin Munari","Sebastiano Spinello"],"pdf_url":"https://arxiv.org/pdf/2411.13919v1.pdf","comment":"10 pages, 9 figures, 2 tables, HICSS58 conference"},{"id":"http://arxiv.org/abs/2411.13914v1","updated":"2024-11-21T07:57:59Z","published":"2024-11-21T07:57:59Z","title":"ICODE: Modeling Dynamical Systems with Extrinsic Input Information","summary":"  Learning models of dynamical systems with external inputs, that may be, for\nexample, nonsmooth or piecewise, is crucial for studying complex phenomena and\npredicting future state evolution, which is essential for applications such as\nsafety guarantees and decision-making. In this work, we introduce \\emph{Input\nConcomitant Neural ODEs (ICODEs)}, which incorporate precise real-time input\ninformation into the learning process of the models, rather than treating the\ninputs as hidden parameters to be learned. The sufficient conditions to ensure\nthe model's contraction property are provided to guarantee that system\ntrajectories of the trained model converge to a fixed point, regardless of\ninitial conditions across different training processes. We validate our method\nthrough experiments on several representative real dynamics: Single-link robot,\nDC-to-DC converter, motion dynamics of a rigid body, Rabinovich-Fabrikant\nequation, Glycolytic-glycogenolytic pathway model, and heat conduction\nequation. The experimental results demonstrate that our proposed ICODEs\nefficiently learn the ground truth systems, achieving superior prediction\nperformance under both typical and atypical inputs. This work offers a valuable\nclass of neural ODE models for understanding physical systems with explicit\nexternal input information, with potential promising applications in fields\nsuch as physics and robotics.\n","authors":["Zhaoyi Li","Wenjie Mei","Ke Yu","Yang Bai","Shihua Li"],"pdf_url":"https://arxiv.org/pdf/2411.13914v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13907v1","updated":"2024-11-21T07:46:01Z","published":"2024-11-21T07:46:01Z","title":"Split Federated Learning Over Heterogeneous Edge Devices: Algorithm and\n  Optimization","summary":"  Split Learning (SL) is a promising collaborative machine learning approach,\nenabling resource-constrained devices to train models without sharing raw data,\nwhile reducing computational load and preserving privacy simultaneously.\nHowever, current SL algorithms face limitations in training efficiency and\nsuffer from prolonged latency, particularly in sequential settings, where the\nslowest device can bottleneck the entire process due to heterogeneous resources\nand frequent data exchanges between clients and servers. To address these\nchallenges, we propose the Heterogeneous Split Federated Learning (HSFL)\nframework, which allows resource-constrained clients to train their\npersonalized client-side models in parallel, utilizing different cut layers.\nAiming to mitigate the impact of heterogeneous environments and accelerate the\ntraining process, we formulate a latency minimization problem that optimizes\ncomputational and transmission resources jointly. Additionally, we design a\nresource allocation algorithm that combines the Sample Average Approximation\n(SAA), Genetic Algorithm (GA), Lagrangian relaxation and Branch and Bound\n(B\\&B) methods to efficiently solve this problem. Simulation results\ndemonstrate that HSFL outperforms other frameworks in terms of both convergence\nrate and model accuracy on heterogeneous devices with non-iid data, while the\noptimization algorithm is better than other baseline methods in reducing\nlatency.\n","authors":["Yunrui Sun","Gang Hu","Yinglei Teng","Dunbo Cai"],"pdf_url":"https://arxiv.org/pdf/2411.13907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13903v1","updated":"2024-11-21T07:28:24Z","published":"2024-11-21T07:28:24Z","title":"AmpliNetECG12: A lightweight SoftMax-based relativistic amplitude\n  amplification architecture for 12 lead ECG classification","summary":"  The urgent need to promptly detect cardiac disorders from 12-lead\nElectrocardiograms using limited computations is motivated by the heart's fast\nand complex electrical activity and restricted computational power of portable\ndevices. Timely and precise diagnoses are crucial since delays might\nsignificantly impact patient health outcomes. This research presents a novel\ndeep-learning architecture that aims to diagnose heart abnormalities quickly\nand accurately. We devised a new activation function called aSoftMax, designed\nto improve the visibility of ECG deflections. The proposed activation function\nis used with Convolutional Neural Network architecture to includes kernel\nweight sharing across the ECG's various leads. This innovative method\nthoroughly generalizes the global 12-lead ECG features and minimizes the\nmodel's complexity by decreasing the trainable parameters. aSoftMax, combined\nwith enhanced CNN architecture yielded AmpliNetECG12, we obtain exceptional\naccuracy of 84% in diagnosing cardiac disorders. AmpliNetECG12 shows\noutstanding prediction ability when used with the CPSC2018 dataset for\narrhythmia classification. The model attains an F1-score of 80.71% and a\nROC-AUC score of 96.00%, with 280,000 trainable parameters which signifies the\nlightweight yet efficient nature of AmpliNetECG12. The stochastic\ncharacteristics of aSoftMax, a fundamental element of AmpliNetECG12, improve\nprediction accuracy and also increasse the model's interpretability. This\nfeature enhances comprehension of important ECG segments in different forms of\narrhythmias, establishing a new standard of explainable architecture for\ncardiac disorder classification.\n","authors":["Shreya Srivastava"],"pdf_url":"https://arxiv.org/pdf/2411.13903v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13899v1","updated":"2024-11-21T07:21:59Z","published":"2024-11-21T07:21:59Z","title":"Schemato -- An LLM for Netlist-to-Schematic Conversion","summary":"  Machine learning models are advancing circuit design, particularly in analog\ncircuits. They typically generate netlists that lack human interpretability.\nThis is a problem as human designers heavily rely on the interpretability of\ncircuit diagrams or schematics to intuitively understand, troubleshoot, and\ndevelop designs. Hence, to integrate domain knowledge effectively, it is\ncrucial to translate ML-generated netlists into interpretable schematics\nquickly and accurately. We propose Schemato, a large language model (LLM) for\nnetlist-to-schematic conversion. In particular, we consider our approach in the\ntwo settings of converting netlists to .asc files for LTSpice and LATEX files\nfor CircuiTikz schematics. Experiments on our circuit dataset show that\nSchemato achieves up to 93% compilation success rate for the netlist-to-LaTeX\nconversion task, surpassing the 26% rate scored by the state-of-the-art LLMs.\nFurthermore, our experiments show that Schemato generates schematics with a\nmean structural similarity index measure that is 3xhigher than the best\nperforming LLMs, therefore closer to the reference human design.\n","authors":["Ryoga Matsuo","Stefan Uhlich","Arun Venkitaraman","Andrea Bonetti","Chia-Yu Hsieh","Ali Momeni","Lukas Mauch","Augusto Capone","Eisaku Ohbuchi","Lorenzo Servadei"],"pdf_url":"https://arxiv.org/pdf/2411.13899v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02592v4","updated":"2024-11-21T07:16:05Z","published":"2024-10-03T15:34:41Z","title":"IC3M: In-Car Multimodal Multi-object Monitoring for Abnormal Status of\n  Both Driver and Passengers","summary":"  Recently, in-car monitoring has emerged as a promising technology for\ndetecting early-stage abnormal status of the driver and providing timely alerts\nto prevent traffic accidents. Although training models with multimodal data\nenhances the reliability of abnormal status detection, the scarcity of labeled\ndata and the imbalance of class distribution impede the extraction of critical\nabnormal state features, significantly deteriorating training performance.\nFurthermore, missing modalities due to environment and hardware limitations\nfurther exacerbate the challenge of abnormal status identification. More\nimportantly, monitoring abnormal health conditions of passengers, particularly\nin elderly care, is of paramount importance but remains underexplored. To\naddress these challenges, we introduce our IC3M, an efficient\ncamera-rotation-based multimodal framework for monitoring both driver and\npassengers in a car. Our IC3M comprises two key modules: an adaptive threshold\npseudo-labeling strategy and a missing modality reconstruction. The former\ncustomizes pseudo-labeling thresholds for different classes based on the class\ndistribution, generating class-balanced pseudo labels to guide model training\neffectively, while the latter leverages crossmodality relationships learned\nfrom limited labels to accurately recover missing modalities by distribution\ntransferring from available modalities. Extensive experimental results\ndemonstrate that IC3M outperforms state-of-the-art benchmarks in accuracy,\nprecision, and recall while exhibiting superior robustness under limited\nlabeled data and severe missing modality.\n","authors":["Zihan Fang","Zheng Lin","Senkang Hu","Hangcheng Cao","Yiqin Deng","Xianhao Chen","Yuguang Fang"],"pdf_url":"https://arxiv.org/pdf/2410.02592v4.pdf","comment":"16 pages, 17 figures"},{"id":"http://arxiv.org/abs/2310.00616v2","updated":"2024-11-21T07:11:21Z","published":"2023-10-01T08:35:46Z","title":"Towards Understanding Adversarial Transferability in Federated Learning","summary":"  We investigate a specific security risk in FL: a group of malicious clients\nhas impacted the model during training by disguising their identities and\nacting as benign clients but later switching to an adversarial role. They use\ntheir data, which was part of the training set, to train a substitute model and\nconduct transferable adversarial attacks against the federated model. This type\nof attack is subtle and hard to detect because these clients initially appear\nto be benign.\n  The key question we address is: How robust is the FL system to such covert\nattacks, especially compared to traditional centralized learning systems? We\nempirically show that the proposed attack imposes a high security risk to\ncurrent FL systems. By using only 3\\% of the client's data, we achieve the\nhighest attack rate of over 80\\%. To further offer a full understanding of the\nchallenges the FL system faces in transferable attacks, we provide a\ncomprehensive analysis over the transfer robustness of FL across a spectrum of\nconfigurations. Surprisingly, FL systems show a higher level of robustness than\ntheir centralized counterparts, especially when both systems are equally good\nat handling regular, non-malicious data.\n  We attribute this increased robustness to two main factors: 1) Decentralized\nData Training: Each client trains the model on its own data, reducing the\noverall impact of any single malicious client. 2) Model Update Averaging: The\nupdates from each client are averaged together, further diluting any malicious\nalterations. Both practical experiments and theoretical analysis support our\nconclusions. This research not only sheds light on the resilience of FL systems\nagainst hidden attacks but also raises important considerations for their\nfuture application and development.\n","authors":["Yijiang Li","Ying Gao","Haohan Wang"],"pdf_url":"https://arxiv.org/pdf/2310.00616v2.pdf","comment":"Published in Transactions on Machine Learning Research (TMLR)\n  (11/2024)"},{"id":"http://arxiv.org/abs/2411.01589v2","updated":"2024-11-21T07:11:20Z","published":"2024-11-03T14:49:11Z","title":"BiT-MamSleep: Bidirectional Temporal Mamba for EEG Sleep Staging","summary":"  In this paper, we address the challenges in automatic sleep stage\nclassification, particularly the high computational cost, inadequate modeling\nof bidirectional temporal dependencies, and class imbalance issues faced by\nTransformer-based models. To address these limitations, we propose\nBiT-MamSleep, a novel architecture that integrates the Triple-Resolution CNN\n(TRCNN) for efficient multi-scale feature extraction with the Bidirectional\nMamba (BiMamba) mechanism, which models both short- and long-term temporal\ndependencies through bidirectional processing of EEG data. Additionally,\nBiT-MamSleep incorporates an Adaptive Feature Recalibration (AFR) module and a\ntemporal enhancement block to dynamically refine feature importance, optimizing\nclassification accuracy without increasing computational complexity. To further\nimprove robustness, we apply optimization techniques such as Focal Loss and\nSMOTE to mitigate class imbalance. Extensive experiments on four public\ndatasets demonstrate that BiT-MamSleep significantly outperforms\nstate-of-the-art methods, particularly in handling long EEG sequences and\naddressing class imbalance, leading to more accurate and scalable sleep stage\nclassification.\n","authors":["Xinliang Zhou","Yuzhe Han","Zhisheng Chen","Chenyu Liu","Yi Ding","Ziyu Jia","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2411.01589v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13890v1","updated":"2024-11-21T07:08:48Z","published":"2024-11-21T07:08:48Z","title":"GraCo -- A Graph Composer for Integrated Circuits","summary":"  Designing integrated circuits involves substantial complexity, posing\nchallenges in revealing its potential applications - from custom digital cells\nto analog circuits. Despite extensive research over the past decades in\nbuilding versatile and automated frameworks, there remains open room to explore\nmore computationally efficient AI-based solutions. This paper introduces the\ngraph composer GraCo, a novel method for synthesizing integrated circuits using\nreinforcement learning (RL). GraCo learns to construct a graph step-by-step,\nwhich is then converted into a netlist and simulated with SPICE. We demonstrate\nthat GraCo is highly configurable, enabling the incorporation of prior design\nknowledge into the framework. We formalize how this prior knowledge can be\nutilized and, in particular, show that applying consistency checks enhances the\nefficiency of the sampling process. To evaluate its performance, we compare\nGraCo to a random baseline, which is known to perform well for smaller design\nspace problems. We demonstrate that GraCo can discover circuits for tasks such\nas generating standard cells, including the inverter and the two-input NAND\n(NAND2) gate. Compared to a random baseline, GraCo requires 5x fewer sampling\nsteps to design an inverter and successfully synthesizes a NAND2 gate that is\n2.5x faster.\n","authors":["Stefan Uhlich","Andrea Bonetti","Arun Venkitaraman","Ali Momeni","Ryoga Matsuo","Chia-Yu Hsieh","Eisaku Ohbuchi","Lorenzo Servadei"],"pdf_url":"https://arxiv.org/pdf/2411.13890v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13503v3","updated":"2024-11-21T06:56:49Z","published":"2024-09-20T13:44:00Z","title":"SatFed: A Resource-Efficient LEO Satellite-Assisted Heterogeneous\n  Federated Learning Framework","summary":"  Traditional federated learning (FL) frameworks rely heavily on terrestrial\nnetworks, where coverage limitations and increasing bandwidth congestion\nsignificantly hinder model convergence. Fortunately, the advancement of\nlow-Earth orbit (LEO) satellite networks offers promising new communication\navenues to augment traditional terrestrial FL. Despite this potential, the\nlimited satellite-ground communication bandwidth and the heterogeneous\noperating environments of ground devices-including variations in data,\nbandwidth, and computing power-pose substantial challenges for effective and\nrobust satellite-assisted FL. To address these challenges, we propose SatFed, a\nresource-efficient satellite-assisted heterogeneous FL framework. SatFed\nimplements freshness-based model prioritization queues to optimize the use of\nhighly constrained satellite-ground bandwidth, ensuring the transmission of the\nmost critical models. Additionally, a multigraph is constructed to capture\nreal-time heterogeneous relationships between devices, including data\ndistribution, terrestrial bandwidth, and computing capability. This multigraph\nenables SatFed to aggregate satellite-transmitted models into peer guidance,\nenhancing local training in heterogeneous environments. Extensive experiments\nwith real-world LEO satellite networks demonstrate that SatFed achieves\nsuperior performance and robustness compared to state-of-the-art benchmarks.\n","authors":["Yuxin Zhang","Zheng Lin","Zhe Chen","Zihan Fang","Wenjun Zhu","Xianhao Chen","Jin Zhao","Yue Gao"],"pdf_url":"https://arxiv.org/pdf/2409.13503v3.pdf","comment":"10 pages, 12 figures"},{"id":"http://arxiv.org/abs/2411.13883v1","updated":"2024-11-21T06:47:53Z","published":"2024-11-21T06:47:53Z","title":"When Online Algorithms Influence the Environment: A Dynamical Systems\n  Analysis of the Unintended Consequences","summary":"  We analyze the effect that online algorithms have on the environment that\nthey are learning. As a motivation, consider recommendation systems that use\nonline algorithms to learn optimal product recommendations based on user and\nproduct attributes. It is well known that the sequence of recommendations\naffects user preferences. However, typical learning algorithms treat the user\nattributes as static and disregard the impact of their recommendations on user\npreferences. Our interest is to analyze the effect of this mismatch between the\nmodel assumption of a static environment, and the reality of an evolving\nenvironment affected by the recommendations. To perform this analysis, we first\nintroduce a model for a generic coupled evolution of the parameters that are\nbeing learned, and the environment that is affected by it. We then frame a\nlinear bandit recommendation system (RS) into this generic model where the\nusers are characterized by a state variable that evolves based on the sequence\nof recommendations. The learning algorithm of the RS does not explicitly\naccount for this evolution and assumes that the users are static. A dynamical\nsystem model that captures the coupled evolution of the population state and\nthe learning algorithm is described, and its equilibrium behavior is analyzed.\nWe show that when the recommendation algorithm is able to learn the population\npreferences in the presence of this mismatch, the algorithm induces similarity\nin the preferences of the user population. In particular, we present results on\nhow different properties of the recommendation algorithm, namely the user\nattribute space and the exploration-exploitation tradeoff, effect the\npopulation preferences when they are learned by the algorithm. We demonstrate\nthese results using model simulations.\n","authors":["Prabhat Lankireddy","Jayakrishnan Nair","D Manjunath"],"pdf_url":"https://arxiv.org/pdf/2411.13883v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.04022v4","updated":"2024-11-21T06:47:48Z","published":"2024-09-06T04:26:57Z","title":"Heterogeneity-Aware Cooperative Federated Edge Learning with Adaptive\n  Computation and Communication Compression","summary":"  Motivated by the drawbacks of cloud-based federated learning (FL),\ncooperative federated edge learning (CFEL) has been proposed to improve\nefficiency for FL over mobile edge networks, where multiple edge servers\ncollaboratively coordinate the distributed model training across a large number\nof edge devices. However, CFEL faces critical challenges arising from dynamic\nand heterogeneous device properties, which slow down the convergence and\nincrease resource consumption. This paper proposes a heterogeneity-aware CFEL\nscheme called \\textit{Heterogeneity-Aware Cooperative Edge-based Federated\nAveraging} (HCEF) that aims to maximize the model accuracy while minimizing the\ntraining time and energy consumption via adaptive computation and communication\ncompression in CFEL. By theoretically analyzing how local update frequency and\ngradient compression affect the convergence error bound in CFEL, we develop an\nefficient online control algorithm for HCEF to dynamically determine local\nupdate frequencies and compression ratios for heterogeneous devices.\nExperimental results show that compared with prior schemes, the proposed HCEF\nscheme can maintain higher model accuracy while reducing training latency and\nimproving energy efficiency simultaneously.\n","authors":["Zhenxiao Zhang","Zhidong Gao","Yuanxiong Guo","Yanmin Gong"],"pdf_url":"https://arxiv.org/pdf/2409.04022v4.pdf","comment":"20 pages, 8 figures, accepted by IEEE Transactions on Mobile\n  Computing"},{"id":"http://arxiv.org/abs/2410.09747v3","updated":"2024-11-21T06:46:57Z","published":"2024-10-13T06:53:58Z","title":"t-READi: Transformer-Powered Robust and Efficient Multimodal Inference\n  for Autonomous Driving","summary":"  Given the wide adoption of multimodal sensors (e.g., camera, lidar, radar) by\nautonomous vehicles (AVs), deep analytics to fuse their outputs for a robust\nperception become imperative. However, existing fusion methods often make two\nassumptions rarely holding in practice: i) similar data distributions for all\ninputs and ii) constant availability for all sensors. Because, for example,\nlidars have various resolutions and failures of radars may occur, such\nvariability often results in significant performance degradation in fusion. To\nthis end, we present tREADi, an adaptive inference system that accommodates the\nvariability of multimodal sensory data and thus enables robust and efficient\nperception. t-READi identifies variation-sensitive yet structure-specific model\nparameters; it then adapts only these parameters while keeping the rest intact.\nt-READi also leverages a cross-modality contrastive learning method to\ncompensate for the loss from missing modalities. Both functions are implemented\nto maintain compatibility with existing multimodal deep fusion methods. The\nextensive experiments evidently demonstrate that compared with the status quo\napproaches, t-READi not only improves the average inference accuracy by more\nthan 6% but also reduces the inference latency by almost 15x with the cost of\nonly 5% extra memory overhead in the worst case under realistic data and modal\nvariations.\n","authors":["Pengfei Hu","Yuhang Qian","Tianyue Zheng","Ang Li","Zhe Chen","Yue Gao","Xiuzhen Cheng","Jun Luo"],"pdf_url":"https://arxiv.org/pdf/2410.09747v3.pdf","comment":"14 pages, 16 figures"},{"id":"http://arxiv.org/abs/2411.13881v1","updated":"2024-11-21T06:41:39Z","published":"2024-11-21T06:41:39Z","title":"Exploring applications of topological data analysis in stock index\n  movement prediction","summary":"  Topological Data Analysis (TDA) has recently gained significant attention in\nthe field of financial prediction. However, the choice of point cloud\nconstruction methods, topological feature representations, and classification\nmodels has a substantial impact on prediction results. This paper addresses the\nclassification problem of stock index movement. First, we construct point\nclouds for stock indices using three different methods. Next, we apply TDA to\nextract topological structures from the point clouds. Four distinct topological\nfeatures are computed to represent the patterns in the data, and 15\ncombinations of these features are enumerated and input into six different\nmachine learning models. We evaluate the predictive performance of various TDA\nconfigurations by conducting index movement classification tasks on datasets\nsuch as CSI, DAX, HSI and FTSE providing insights into the efficiency of\ndifferent TDA setups.\n","authors":["Dazhi Huang","Pengcheng Xu","Xiaocheng Huang","Jiayi Chen"],"pdf_url":"https://arxiv.org/pdf/2411.13881v1.pdf","comment":"20 pages, 10 figures"},{"id":"http://arxiv.org/abs/2311.16141v3","updated":"2024-11-21T06:20:46Z","published":"2023-11-05T12:20:29Z","title":"Brain-Inspired Efficient Pruning: Exploiting Criticality in Spiking\n  Neural Networks","summary":"  Spiking Neural Networks (SNNs) have gained significant attention due to the\nenergy-efficient and multiplication-free characteristics. Despite these\nadvantages, deploying large-scale SNNs on edge hardware is challenging due to\nlimited resource availability. Network pruning offers a viable approach to\ncompress the network scale and reduce hardware resource requirements for model\ndeployment. However, existing SNN pruning methods cause high pruning costs and\nperformance loss because they lack efficiency in processing the sparse spike\nrepresentation of SNNs. In this paper, inspired by the critical brain\nhypothesis in neuroscience and the high biological plausibility of SNNs, we\nexplore and leverage criticality to facilitate efficient pruning in deep SNNs.\nWe firstly explain criticality in SNNs from the perspective of maximizing\nfeature information entropy. Second, We propose a low-cost metric for assess\nneuron criticality in feature transmission and design a pruning-regeneration\nmethod that incorporates this criticality into the pruning process.\nExperimental results demonstrate that our method achieves higher performance\nthan the current state-of-the-art (SOTA) method with up to 95.26\\% reduction of\npruning cost. The criticality-based regeneration process efficiently selects\npotential structures and facilitates consistent feature representation.\n","authors":["Shuo Chen","Boxiao Liu","Zeshi Liu","Haihang You"],"pdf_url":"https://arxiv.org/pdf/2311.16141v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11340v2","updated":"2024-11-21T06:16:16Z","published":"2024-10-15T07:12:57Z","title":"Toward a Well-Calibrated Discrimination via Survival Outcome-Aware\n  Contrastive Learning","summary":"  Previous deep learning approaches for survival analysis have primarily relied\non ranking losses to improve discrimination performance, which often comes at\nthe expense of calibration performance. To address such an issue, we propose a\nnovel contrastive learning approach specifically designed to enhance\ndiscrimination \\textit{without} sacrificing calibration. Our method employs\nweighted sampling within a contrastive learning framework, assigning lower\npenalties to samples with similar survival outcomes. This aligns well with the\nassumption that patients with similar event times share similar clinical\nstatuses. Consequently, when augmented with the commonly used negative\nlog-likelihood loss, our approach significantly improves discrimination\nperformance without directly manipulating the model outputs, thereby achieving\nbetter calibration. Experiments on multiple real-world clinical datasets\ndemonstrate that our method outperforms state-of-the-art deep survival models\nin both discrimination and calibration. Through comprehensive ablation studies,\nwe further validate the effectiveness of our approach through quantitative and\nqualitative analyses.\n","authors":["Dongjoon Lee","Hyeryn Park","Changhee Lee"],"pdf_url":"https://arxiv.org/pdf/2410.11340v2.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.13869v1","updated":"2024-11-21T06:07:36Z","published":"2024-11-21T06:07:36Z","title":"Topology optimization of periodic lattice structures for specified\n  mechanical properties using machine learning considering member connectivity","summary":"  This study proposes a methodology to utilize machine learning (ML) for\ntopology optimization of periodic lattice structures. In particular, we\ninvestigate data representation of lattice structures used as input data for ML\nmodels to improve the performance of the models, focusing on the filtering\nprocess and feature selection. We use the filtering technique to explicitly\nconsider the connectivity of lattice members and perform feature selection to\nreduce the input data size. In addition, we propose a convolution approach to\napply pre-trained models for small structures to structures of larger sizes.\nThe computational cost for obtaining optimal topologies by a heuristic method\nis reduced by incorporating the prediction of the trained ML model into the\noptimization process. In the numerical examples, a response prediction model is\nconstructed for a lattice structure of 4x4 units, and topology optimization of\n4x4-unit and 8x8-unit structures is performed by simulated annealing assisted\nby the trained ML model. The example demonstrates that ML models perform higher\naccuracy by using the filtered data as input than by solely using the data\nrepresenting the existence of each member. It is also demonstrated that a\nsmall-scale prediction model can be constructed with sufficient accuracy by\nfeature selection. Additionally, the proposed method can find the optimal\nstructure in less computation time than the pure simulated annealing.\n","authors":["Tomoya Matsuoka","Makoto Ohsaki","Kazuki Hayashi"],"pdf_url":"https://arxiv.org/pdf/2411.13869v1.pdf","comment":"Presented at Asian Congress of Structural and Multidisciplinary\n  Optimization (ACSMO 2024)"},{"id":"http://arxiv.org/abs/2411.13868v1","updated":"2024-11-21T06:06:04Z","published":"2024-11-21T06:06:04Z","title":"Robust Detection of Watermarks for Large Language Models Under Human\n  Edits","summary":"  Watermarking has offered an effective approach to distinguishing text\ngenerated by large language models (LLMs) from human-written text. However, the\npervasive presence of human edits on LLM-generated text dilutes watermark\nsignals, thereby significantly degrading detection performance of existing\nmethods. In this paper, by modeling human edits through mixture model\ndetection, we introduce a new method in the form of a truncated goodness-of-fit\ntest for detecting watermarked text under human edits, which we refer to as\nTr-GoF. We prove that the Tr-GoF test achieves optimality in robust detection\nof the Gumbel-max watermark in a certain asymptotic regime of substantial text\nmodifications and vanishing watermark signals. Importantly, Tr-GoF achieves\nthis optimality \\textit{adaptively} as it does not require precise knowledge of\nhuman edit levels or probabilistic specifications of the LLMs, in contrast to\nthe optimal but impractical (Neyman--Pearson) likelihood ratio test. Moreover,\nwe establish that the Tr-GoF test attains the highest detection efficiency rate\nin a certain regime of moderate text modifications. In stark contrast, we show\nthat sum-based detection rules, as employed by existing methods, fail to\nachieve optimal robustness in both regimes because the additive nature of their\nstatistics is less resilient to edit-induced noise. Finally, we demonstrate the\ncompetitive and sometimes superior empirical performance of the Tr-GoF test on\nboth synthetic data and open-source LLMs in the OPT and LLaMA families.\n","authors":["Xiang Li","Feng Ruan","Huiyuan Wang","Qi Long","Weijie J. Su"],"pdf_url":"https://arxiv.org/pdf/2411.13868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13865v1","updated":"2024-11-21T06:01:47Z","published":"2024-11-21T06:01:47Z","title":"HARec: Hyperbolic Graph-LLM Alignment for Exploration and Exploitation\n  in Recommender Systems","summary":"  Modern recommendation systems often create information cocoons, limiting\nusers' exposure to diverse content. To enhance user experience, a crucial\nchallenge is developing systems that can balance content exploration and\nexploitation, allowing users to adjust their recommendation preferences.\nIntuitively, this balance can be achieved through a tree-structured\nrepresentation, where depth search facilitates exploitation and breadth search\nenables exploration. However, current works face two challenges to achieve this\ntarget: (1) Euclidean methods fail to fully capture hierarchical structures and\nlack flexibility in balancing exploration-exploitation, while (2) hyperbolic\napproaches, despite better hierarchical modeling, suffer from insufficient\nsemantic alignment due to their reliance on Euclidean text encoders. To address\nthese challenges, we propose HARec, a hyperbolic representation learning\nframework that jointly aligns user-item collaborative information with textual\ndescriptions in hyperbolic space. Our framework introduces two key technique\nnovelty: (1) a hierarchical-aware graph-llm alignment mechanism that enables\nbetter hierarchical representation, and (2) a hyperbolic hierarchical tree\nstructure that facilitates user-adjustable exploration-exploitation trade-offs.\nExtensive experiments demonstrate that HARec consistently outperforms both\nEuclidean and hyperbolic baselines, achieving up to 5.49% improvement in\nutility metrics and 11.39% increase in diversity metrics.\n","authors":["Qiyao Ma","Menglin Yang","Mingxuan Ju","Tong Zhao","Neil Shah","Rex Ying"],"pdf_url":"https://arxiv.org/pdf/2411.13865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17335v2","updated":"2024-11-21T05:42:02Z","published":"2024-06-25T07:45:00Z","title":"A Thorough Performance Benchmarking on Lightweight Embedding-based\n  Recommender Systems","summary":"  Since the creation of the Web, recommender systems (RSs) have been an\nindispensable mechanism in information filtering. State-of-the-art RSs\nprimarily depend on categorical features, which ecoded by embedding vectors,\nresulting in excessively large embedding tables. To prevent over-parameterized\nembedding tables from harming scalability, both academia and industry have seen\nincreasing efforts in compressing RS embeddings. However, despite the\nprosperity of lightweight embedding-based RSs (LERSs), a wide diversity is seen\nin evaluation protocols, resulting in obstacles when relating LERS performance\nto real-world usability. Moreover, despite the common goal of lightweight\nembeddings, LERSs are evaluated with a single choice between the two main\nrecommendation tasks -- collaborative filtering and content-based\nrecommendation. This lack of discussions on cross-task transferability hinders\nthe development of unified, more scalable solutions. Motivated by these issues,\nthis study investigates various LERSs' performance, efficiency, and cross-task\ntransferability via a thorough benchmarking process. Additionally, we propose\nan efficient embedding compression method using magnitude pruning, which is an\neasy-to-deploy yet highly competitive baseline that outperforms various complex\nLERSs. Our study reveals the distinct performance of LERSs across the two\ntasks, shedding light on their effectiveness and generalizability. To support\nedge-based recommendations, we tested all LERSs on a Raspberry Pi 4, where the\nefficiency bottleneck is exposed. Finally, we conclude this paper with critical\nsummaries of LERS performance, model selection suggestions, and underexplored\nchallenges around LERSs for future research. To encourage future research, we\npublish source codes and artifacts at \\href{this\nlink}{https://github.com/chenxing1999/recsys-benchmark}.\n","authors":["Hung Vinh Tran","Tong Chen","Quoc Viet Hung Nguyen","Zi Huang","Lizhen Cui","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2406.17335v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07249v3","updated":"2024-11-21T05:39:36Z","published":"2024-10-26T21:27:53Z","title":"SPDIM: Source-Free Unsupervised Conditional and Label Shift Adaptation\n  in EEG","summary":"  The non-stationary nature of electroencephalography (EEG) introduces\ndistribution shifts across domains (e.g., days and subjects), posing a\nsignificant challenge to EEG-based neurotechnology generalization. Without\nlabeled calibration data for target domains, the problem is a source-free\nunsupervised domain adaptation (SFUDA) problem. For scenarios with constant\nlabel distribution, Riemannian geometry-aware statistical alignment frameworks\non the symmetric positive definite (SPD) manifold are considered\nstate-of-the-art. However, many practical scenarios, including EEG-based sleep\nstaging, exhibit label shifts. Here, we propose a geometric deep learning\nframework for SFUDA problems under specific distribution shifts, including\nlabel shifts. We introduce a novel, realistic generative model and show that\nprior Riemannian statistical alignment methods on the SPD manifold can\ncompensate for specific marginal and conditional distribution shifts but hurt\ngeneralization under label shifts. As a remedy, we propose a\nparameter-efficient manifold optimization strategy termed SPDIM. SPDIM uses the\ninformation maximization principle to learn a single SPD-manifold-constrained\nparameter per target domain. In simulations, we demonstrate that SPDIM can\ncompensate for the shifts under our generative model. Moreover, using public\nEEG-based brain-computer interface and sleep staging datasets, we show that\nSPDIM outperforms prior approaches.\n","authors":["Shanglin Li","Motoaki Kawanabe","Reinmar J. Kobler"],"pdf_url":"https://arxiv.org/pdf/2411.07249v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.09955v2","updated":"2024-11-21T05:28:10Z","published":"2024-11-15T05:18:15Z","title":"Instruction-Guided Editing Controls for Images and Multimedia: A Survey\n  in LLM era","summary":"  The rapid advancement of large language models (LLMs) and multimodal learning\nhas transformed digital content creation and manipulation. Traditional visual\nediting tools require significant expertise, limiting accessibility. Recent\nstrides in instruction-based editing have enabled intuitive interaction with\nvisual content, using natural language as a bridge between user intent and\ncomplex editing operations. This survey provides an overview of these\ntechniques, focusing on how LLMs and multimodal models empower users to achieve\nprecise visual modifications without deep technical knowledge. By synthesizing\nover 100 publications, we explore methods from generative adversarial networks\nto diffusion models, examining multimodal integration for fine-grained content\ncontrol. We discuss practical applications across domains such as fashion, 3D\nscene manipulation, and video synthesis, highlighting increased accessibility\nand alignment with human intuition. Our survey compares existing literature,\nemphasizing LLM-empowered editing, and identifies key challenges to stimulate\nfurther research. We aim to democratize powerful visual editing across various\nindustries, from entertainment to education. Interested readers are encouraged\nto access our repository at\nhttps://github.com/tamlhp/awesome-instruction-editing.\n","authors":["Thanh Tam Nguyen","Zhao Ren","Trinh Pham","Thanh Trung Huynh","Phi Le Nguyen","Hongzhi Yin","Quoc Viet Hung Nguyen"],"pdf_url":"https://arxiv.org/pdf/2411.09955v2.pdf","comment":"Fixed a serious error in author information"},{"id":"http://arxiv.org/abs/2411.13855v1","updated":"2024-11-21T05:27:42Z","published":"2024-11-21T05:27:42Z","title":"A Multimodal Approach to The Detection and Classification of Skin\n  Diseases","summary":"  According to PBS, nearly one-third of Americans lack access to primary care\nservices, and another forty percent delay going to avoid medical costs. As a\nresult, many diseases are left undiagnosed and untreated, even if the disease\nshows many physical symptoms on the skin. With the rise of AI, self-diagnosis\nand improved disease recognition have become more promising than ever; in spite\nof that, existing methods suffer from a lack of large-scale patient databases\nand outdated methods of study, resulting in studies being limited to only a few\ndiseases or modalities. This study incorporates readily available and easily\naccessible patient information via image and text for skin disease\nclassification on a new dataset of 26 skin disease types that includes both\nskin disease images (37K) and associated patient narratives. Using this\ndataset, baselines for various image models were established that outperform\nexisting methods. Initially, the Resnet-50 model was only able to achieve an\naccuracy of 70% but, after various optimization techniques, the accuracy was\nimproved to 80%. In addition, this study proposes a novel fine-tuning strategy\nfor sequence classification Large Language Models (LLMs), Chain of Options,\nwhich breaks down a complex reasoning task into intermediate steps at training\ntime instead of inference. With Chain of Options and preliminary disease\nrecommendations from the image model, this method achieves state of the art\naccuracy 91% in diagnosing patient skin disease given just an image of the\nafflicted area as well as a patient description of the symptoms (such as\nitchiness or dizziness). Through this research, an earlier diagnosis of skin\ndiseases can occur, and clinicians can work with deep learning models to give a\nmore accurate diagnosis, improving quality of life and saving lives.\n","authors":["Allen Yang","Edward Yang"],"pdf_url":"https://arxiv.org/pdf/2411.13855v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13852v1","updated":"2024-11-21T05:24:35Z","published":"2024-11-21T05:24:35Z","title":"Dealing with Synthetic Data Contamination in Online Continual Learning","summary":"  Image generation has shown remarkable results in generating high-fidelity\nrealistic images, in particular with the advancement of diffusion-based models.\nHowever, the prevalence of AI-generated images may have side effects for the\nmachine learning community that are not clearly identified. Meanwhile, the\nsuccess of deep learning in computer vision is driven by the massive dataset\ncollected on the Internet. The extensive quantity of synthetic data being added\nto the Internet would become an obstacle for future researchers to collect\n\"clean\" datasets without AI-generated content. Prior research has shown that\nusing datasets contaminated by synthetic images may result in performance\ndegradation when used for training. In this paper, we investigate the potential\nimpact of contaminated datasets on Online Continual Learning (CL) research. We\nexperimentally show that contaminated datasets might hinder the training of\nexisting online CL methods. Also, we propose Entropy Selection with\nReal-synthetic similarity Maximization (ESRM), a method to alleviate the\nperformance deterioration caused by synthetic images when training online CL\nmodels. Experiments show that our method can significantly alleviate\nperformance deterioration, especially when the contamination is severe. For\nreproducibility, the source code of our work is available at\nhttps://github.com/maorong-wang/ESRM.\n","authors":["Maorong Wang","Nicolas Michel","Jiafeng Mao","Toshihiko Yamasaki"],"pdf_url":"https://arxiv.org/pdf/2411.13852v1.pdf","comment":"Accepted to NeurIPS'24"},{"id":"http://arxiv.org/abs/2406.11919v2","updated":"2024-11-21T05:24:27Z","published":"2024-06-17T04:00:41Z","title":"Graph Knowledge Distillation to Mixture of Experts","summary":"  In terms of accuracy, Graph Neural Networks (GNNs) are the best architectural\nchoice for the node classification task. Their drawback in real-world\ndeployment is the latency that emerges from the neighbourhood processing\noperation. One solution to the latency issue is to perform knowledge\ndistillation from a trained GNN to a Multi-Layer Perceptron (MLP), where the\nMLP processes only the features of the node being classified (and possibly some\npre-computed structural information). However, the performance of such MLPs in\nboth transductive and inductive settings remains inconsistent for existing\nknowledge distillation techniques. We propose to address the performance\nconcerns by using a specially-designed student model instead of an MLP. Our\nmodel, named Routing-by-Memory (RbM), is a form of Mixture-of-Experts (MoE),\nwith a design that enforces expert specialization. By encouraging each expert\nto specialize on a certain region on the hidden representation space, we\ndemonstrate experimentally that it is possible to derive considerably more\nconsistent performance across multiple datasets. Code available at\nhttps://github.com/Rufaim/routing-by-memory.\n","authors":["Pavel Rumiantsev","Mark Coates"],"pdf_url":"https://arxiv.org/pdf/2406.11919v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14107v3","updated":"2024-11-21T05:19:42Z","published":"2024-10-18T01:26:04Z","title":"Transfer Learning on Transformers for Building Energy Consumption\n  Forecasting -- A Comparative Study","summary":"  This study investigates the application of Transfer Learning (TL) on\nTransformer architectures to enhance building energy consumption forecasting.\nTransformers are a relatively new deep learning architecture, which has served\nas the foundation for groundbreaking technologies such as ChatGPT. While TL has\nbeen studied in the past, prior studies considered either one data-centric TL\nstrategy or used older deep learning models such as Recurrent Neural Networks\nor Convolutional Neural Networks. Here, we carry out an extensive empirical\nstudy on six different data-centric TL strategies and analyse their performance\nunder varying feature spaces. In addition to the vanilla Transformer\narchitecture, we also experiment with Informer and PatchTST, specifically\ndesigned for time series forecasting. We use 16 datasets from the Building Data\nGenome Project 2 to create building energy consumption forecasting models.\nExperimental results reveal that while TL is generally beneficial, especially\nwhen the target domain has no data, careful selection of the exact TL strategy\nshould be made to gain the maximum benefit. This decision largely depends on\nthe feature space properties such as the recorded weather features. We also\nnote that PatchTST outperforms the other two Transformer variants (vanilla\nTransformer and Informer). Our findings advance the building energy consumption\nforecasting using advanced approaches like TL and Transformer architectures.\n","authors":["Robert Spencer","Surangika Ranathunga","Mikael Boulic","Andries van Heerden","Teo Susnjak"],"pdf_url":"https://arxiv.org/pdf/2410.14107v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05666v8","updated":"2024-11-21T05:19:04Z","published":"2024-06-09T06:49:22Z","title":"Distribution Learning and Its Application in Deep Learning","summary":"  This paper introduces a novel theoretical learning framework, termed\nprobability distribution learning (PD learning). Departing from the traditional\nstatistical learning framework, PD learning focuses on learning the underlying\nprobability distribution, which is modeled as a random variable within the\nprobability simplex. Within this framework, the optimization objective is\nlearning error, which quantifies the posterior expected discrepancy between the\nmodel's predicted distribution and the underlying true distribution, given\navailable sample data and prior knowledge. To optimize the learning error, this\npaper proposes the necessary conditions for loss functions, models, and\noptimization algorithms, ensuring that these conditions are all satisfied in\nreal-world machine learning scenarios. Based on these conditions, the\nnon-convex optimization mechanism corresponding to model training can be\ntheoretically resolved. Moreover, the paper provides both model-dependent and\nmodel-independent bounds on learning error, offering new insights into the\nmodel's fitting ability and generalization capabilities. Furthermore, the paper\napplies the PD learning framework to elucidate the mechanisms by which various\ntechniques, including random parameter initialization, over-parameterization,\nand dropout, influence deep model training. Finally, the paper substantiates\nthe key conclusions of the proposed framework through experimental results.\n","authors":["Binchuan Qi","Wei Gong","Li Li"],"pdf_url":"https://arxiv.org/pdf/2406.05666v8.pdf","comment":"arXiv admin note: text overlap with arXiv:2105.04026 by other\n  authors. arXiv admin note: text overlap with arXiv:2105.04026 by other\n  authors"},{"id":"http://arxiv.org/abs/2411.13848v1","updated":"2024-11-21T05:15:28Z","published":"2024-11-21T05:15:28Z","title":"Exact and approximate error bounds for physics-informed neural networks","summary":"  The use of neural networks to solve differential equations, as an alternative\nto traditional numerical solvers, has increased recently. However, error bounds\nfor the obtained solutions have only been developed for certain equations. In\nthis work, we report important progress in calculating error bounds of\nphysics-informed neural networks (PINNs) solutions of nonlinear first-order\nODEs. We give a general expression that describes the error of the solution\nthat the PINN-based method provides for a nonlinear first-order ODE. In\naddition, we propose a technique to calculate an approximate bound for the\ngeneral case and an exact bound for a particular case. The error bounds are\ncomputed using only the residual information and the equation structure. We\napply the proposed methods to particular cases and show that they can\nsuccessfully provide error bounds without relying on the numerical solution.\n","authors":["Augusto T. Chantada","Pavlos Protopapas","Luca Gomez Bachar","Susana J. Landau","Claudia G. Scóccola"],"pdf_url":"https://arxiv.org/pdf/2411.13848v1.pdf","comment":"10 pages, 1 figure, accepted to NeurIPS 2024 Workshop on Machine\n  Learning and the Physical Sciences"},{"id":"http://arxiv.org/abs/2410.08109v2","updated":"2024-11-21T04:39:13Z","published":"2024-10-10T16:56:05Z","title":"A Closer Look at Machine Unlearning for Large Language Models","summary":"  Large language models (LLMs) may memorize sensitive or copyrighted content,\nraising privacy and legal concerns. Due to the high cost of retraining from\nscratch, researchers attempt to employ machine unlearning to remove specific\ncontent from LLMs while preserving the overall performance. In this paper, we\ndiscuss several issues in machine unlearning for LLMs and provide our insights\non possible approaches. To address the issue of inadequate evaluation of model\noutputs after unlearning, we introduce three additional metrics to evaluate\ntoken diversity, sentence semantics, and factual correctness. We then\ncategorize unlearning methods into untargeted and targeted, and discuss their\nissues respectively. Specifically, the behavior that untargeted unlearning\nattempts to approximate is unpredictable and may involve hallucinations, and\nexisting regularization is insufficient for targeted unlearning. To alleviate\nthese issues, we propose using the objective of maximizing entropy (ME) for\nuntargeted unlearning and incorporate answer preservation (AP) loss as\nregularization for targeted unlearning. Experimental results across three\nscenarios, i.e., fictitious unlearning, continual unlearning, and real-world\nunlearning, demonstrate the effectiveness of our approaches. The code is\navailable at https://github.com/sail-sg/closer-look-LLM-unlearning.\n","authors":["Xiaojian Yuan","Tianyu Pang","Chao Du","Kejiang Chen","Weiming Zhang","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2410.08109v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13826v1","updated":"2024-11-21T04:23:17Z","published":"2024-11-21T04:23:17Z","title":"Interactive and Expressive Code-Augmented Planning with Large Language\n  Models","summary":"  Large Language Models (LLMs) demonstrate strong abilities in common-sense\nreasoning and interactive decision-making, but often struggle with complex,\nlong-horizon planning tasks. Recent techniques have sought to structure LLM\noutputs using control flow and other code-adjacent techniques to improve\nplanning performance. These techniques include using variables (to track\nimportant information) and functions (to divide complex tasks into smaller\nre-usable sub-tasks). However, purely code-based approaches can be error-prone\nand insufficient for handling ambiguous or unstructured data. To address these\nchallenges, we propose REPL-Plan, an LLM planning approach that is fully\ncode-expressive (it can utilize all the benefits of code) while also being\ndynamic (it can flexibly adapt from errors and use the LLM for fuzzy\nsituations). In REPL-Plan, an LLM solves tasks by interacting with a\nRead-Eval-Print Loop (REPL), which iteratively executes and evaluates code,\nsimilar to language shells or interactive code notebooks, allowing the model to\nflexibly correct errors and handle tasks dynamically. We demonstrate that\nREPL-Plan achieves strong results across various planning domains compared to\nprevious methods.\n","authors":["Anthony Z. Liu","Xinhe Wang","Jacob Sansom","Yao Fu","Jongwook Choi","Sungryull Sohn","Jaekyeom Kim","Honglak Lee"],"pdf_url":"https://arxiv.org/pdf/2411.13826v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13714v4","updated":"2024-11-21T04:22:12Z","published":"2024-10-17T16:14:49Z","title":"Generation through the lens of learning theory","summary":"  We study generation through the lens of statistical learning theory. First,\nwe abstract and formalize the results of Gold [1967], Angluin [1979], Angluin\n[1980] and Kleinberg and Mullainathan [2024] in terms of a binary hypothesis\nclass defined over an abstract example space. Then, we extend the notion of\n\"generation\" from Kleinberg and Mullainathan [2024] to two new settings, we\ncall \"uniform\" and \"non-uniform\" generation, and provide a characterization of\nwhich hypothesis classes are uniformly and non-uniformly generatable. As is\nstandard in learning theory, our characterizations are in terms of the\nfiniteness of a new combinatorial dimension termed the Closure dimension. By\ndoing so, we are able to compare generatability with predictability (captured\nvia PAC and online learnability) and show that these two properties of\nhypothesis classes are incompatible -- there are classes that are generatable\nbut not predictable and vice versa. Finally, we extend our results to capture\nprompted generation and give a complete characterization of which classes are\nprompt generatable, generalizing some of the work by Kleinberg and Mullainathan\n[2024].\n","authors":["Jiaxun Li","Vinod Raman","Ambuj Tewari"],"pdf_url":"https://arxiv.org/pdf/2410.13714v4.pdf","comment":"28 pages, 2 figures"},{"id":"http://arxiv.org/abs/2311.13159v2","updated":"2024-11-21T04:02:46Z","published":"2023-11-22T04:49:16Z","title":"Multi-Objective Optimization via Wasserstein-Fisher-Rao Gradient Flow","summary":"  Multi-objective optimization (MOO) aims to optimize multiple, possibly\nconflicting objectives with widespread applications. We introduce a novel\ninteracting particle method for MOO inspired by molecular dynamics simulations.\nOur approach combines overdamped Langevin and birth-death dynamics,\nincorporating a \"dominance potential\" to steer particles toward global Pareto\noptimality. In contrast to previous methods, our method is able to relocate\ndominated particles, making it particularly adept at managing Pareto fronts of\ncomplicated geometries. Our method is also theoretically grounded as a\nWasserstein-Fisher-Rao gradient flow with convergence guarantees. Extensive\nexperiments confirm that our approach outperforms state-of-the-art methods on\nchallenging synthetic and real-world datasets.\n","authors":["Yinuo Ren","Tesi Xiao","Tanmay Gangwani","Anshuka Rangi","Holakou Rahmanian","Lexing Ying","Subhajit Sanyal"],"pdf_url":"https://arxiv.org/pdf/2311.13159v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13821v1","updated":"2024-11-21T03:59:07Z","published":"2024-11-21T03:59:07Z","title":"Heterophilic Graph Neural Networks Optimization with Causal\n  Message-passing","summary":"  In this work, we discover that causal inference provides a promising approach\nto capture heterophilic message-passing in Graph Neural Network (GNN). By\nleveraging cause-effect analysis, we can discern heterophilic edges based on\nasymmetric node dependency. The learned causal structure offers more accurate\nrelationships among nodes. To reduce the computational complexity, we introduce\nintervention-based causal inference in graph learning. We first simplify causal\nanalysis on graphs by formulating it as a structural learning model and define\nthe optimization problem within the Bayesian scheme. We then present an\nanalysis of decomposing the optimization target into a consistency penalty and\na structure modification based on cause-effect relations. We then estimate this\ntarget by conditional entropy and present insights into how conditional entropy\nquantifies the heterophily. Accordingly, we propose CausalMP, a causal\nmessage-passing discovery network for heterophilic graph learning, that\niteratively learns the explicit causal structure of input graphs. We conduct\nextensive experiments in both heterophilic and homophilic graph settings. The\nresult demonstrates that the our model achieves superior link prediction\nperformance. Training on causal structure can also enhance node representation\nin classification task across different base models.\n","authors":["Botao Wang","Jia Li","Heng Chang","Keli Zhang","Fugee Tsung"],"pdf_url":"https://arxiv.org/pdf/2411.13821v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.09010v5","updated":"2024-11-21T03:41:39Z","published":"2022-12-18T04:44:38Z","title":"Risk-Sensitive Reinforcement Learning with Exponential Criteria","summary":"  While reinforcement learning has shown experimental success in a number of\napplications, it is known to be sensitive to noise and perturbations in the\nparameters of the system, leading to high variance in the total reward amongst\ndifferent episodes in slightly different environments. To introduce robustness,\nas well as sample efficiency, risk-sensitive reinforcement learning methods are\nbeing thoroughly studied. In this work, we provide a definition of robust\nreinforcement learning policies and formulate a risk-sensitive reinforcement\nlearning problem to approximate them, by solving an optimization problem with\nrespect to a modified objective based on exponential criteria. In particular,\nwe study a model-free risk-sensitive variation of the widely-used Monte Carlo\nPolicy Gradient algorithm and introduce a novel risk-sensitive online\nActor-Critic algorithm based on solving a multiplicative Bellman equation using\nstochastic approximation updates. Analytical results suggest that the use of\nexponential criteria generalizes commonly used ad-hoc regularization\napproaches, improves sample efficiency, and introduces robustness with respect\nto perturbations in the model parameters and the environment. The\nimplementation, performance, and robustness properties of the proposed methods\nare evaluated in simulated experiments.\n","authors":["Erfaun Noorani","Christos Mavridis","John Baras"],"pdf_url":"https://arxiv.org/pdf/2212.09010v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13815v1","updated":"2024-11-21T03:40:34Z","published":"2024-11-21T03:40:34Z","title":"FLRNet: A Deep Learning Method for Regressive Reconstruction of Flow\n  Field From Limited Sensor Measurements","summary":"  Many applications in computational and experimental fluid mechanics require\neffective methods for reconstructing the flow fields from limited sensor data.\nHowever, this task remains a significant challenge because the measurement\noperator, which provides the punctual sensor measurement for a given state of\nthe flow field, is often ill-conditioned and non-invertible. This issue impedes\nthe feasibility of identifying the forward map, theoretically the inverse of\nthe measurement operator, for field reconstruction purposes. While data-driven\nmethods are available, their generalizability across different flow conditions\n(\\textit{e.g.,} different Reynold numbers) remains questioned. Moreover, they\nfrequently face the problem of spectral bias, which leads to smooth and blurry\nreconstructed fields, thereby decreasing the accuracy of reconstruction. We\nintroduce FLRNet, a deep learning method for flow field reconstruction from\nsparse sensor measurements. FLRNet employs an variational autoencoder with\nFourier feature layers and incorporates an extra perceptual loss term during\ntraining to learn a rich, low-dimensional latent representation of the flow\nfield. The learned latent representation is then correlated to the sensor\nmeasurement using a fully connected (dense) network. We validated the\nreconstruction capability and the generalizability of FLRNet under various\nfluid flow conditions and sensor configurations, including different sensor\ncounts and sensor layouts. Numerical experiments show that in all tested\nscenarios, FLRNet consistently outperformed other baselines, delivering the\nmost accurate reconstructed flow field and being the most robust to noise.\n","authors":["Phong C. H. Nguyen","Joseph B. Choi","Quang-Trung Luu"],"pdf_url":"https://arxiv.org/pdf/2411.13815v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13814v1","updated":"2024-11-21T03:35:07Z","published":"2024-11-21T03:35:07Z","title":"AutoMixQ: Self-Adjusting Quantization for High Performance\n  Memory-Efficient Fine-Tuning","summary":"  Fine-tuning large language models (LLMs) under resource constraints is a\nsignificant challenge in deep learning. Low-Rank Adaptation (LoRA), pruning,\nand quantization are all effective methods for improving resource efficiency.\nHowever, combining them directly often results in suboptimal performance,\nespecially with uniform quantization across all model layers. This is due to\nthe complex, uneven interlayer relationships introduced by pruning,\nnecessitating more refined quantization strategies. To address this, we propose\nAutoMixQ, an end-to-end optimization framework that selects optimal\nquantization configurations for each LLM layer. AutoMixQ leverages lightweight\nperformance models to guide the selection process, significantly reducing time\nand computational resources compared to exhaustive search methods. By\nincorporating Pareto optimality, AutoMixQ balances memory usage and\nperformance, approaching the upper bounds of model capability under strict\nresource constraints. Our experiments on widely used benchmarks show that\nAutoMixQ reduces memory consumption while achieving superior performance. For\nexample, at a 30\\% pruning rate in LLaMA-7B, AutoMixQ achieved 66.21\\% on BoolQ\ncompared to 62.45\\% for LoRA and 58.96\\% for LoftQ, while reducing memory\nconsumption by 35.5\\% compared to LoRA and 27.5\\% compared to LoftQ.\n","authors":["Changhai Zhou","Shiyang Zhang","Yuhua Zhou","Zekai Liu","Shichao Weng"],"pdf_url":"https://arxiv.org/pdf/2411.13814v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22649v2","updated":"2024-11-21T03:34:44Z","published":"2024-10-30T02:36:55Z","title":"WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting","summary":"  In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA.\n","authors":["Aobo Liang","Yan Sun","Nadra Guizani"],"pdf_url":"https://arxiv.org/pdf/2410.22649v2.pdf","comment":"Model architecture changed"},{"id":"http://arxiv.org/abs/2407.17438v3","updated":"2024-11-21T03:26:54Z","published":"2024-07-24T17:15:58Z","title":"HumanVid: Demystifying Training Data for Camera-controllable Human Image\n  Animation","summary":"  Human image animation involves generating videos from a character photo,\nallowing user control and unlocking the potential for video and movie\nproduction. While recent approaches yield impressive results using high-quality\ntraining data, the inaccessibility of these datasets hampers fair and\ntransparent benchmarking. Moreover, these approaches prioritize 2D human motion\nand overlook the significance of camera motions in videos, leading to limited\ncontrol and unstable video generation. To demystify the training data, we\npresent HumanVid, the first large-scale high-quality dataset tailored for human\nimage animation, which combines crafted real-world and synthetic data. For the\nreal-world data, we compile a vast collection of real-world videos from the\ninternet. We developed and applied careful filtering rules to ensure video\nquality, resulting in a curated collection of 20K high-resolution (1080P)\nhuman-centric videos. Human and camera motion annotation is accomplished using\na 2D pose estimator and a SLAM-based method. To expand our synthetic dataset,\nwe collected 10K 3D avatar assets and leveraged existing assets of body shapes,\nskin textures and clothings. Notably, we introduce a rule-based camera\ntrajectory generation method, enabling the synthetic pipeline to incorporate\ndiverse and precise camera motion annotation, which can rarely be found in\nreal-world data. To verify the effectiveness of HumanVid, we establish a\nbaseline model named CamAnimate, short for Camera-controllable Human Animation,\nthat considers both human and camera motions as conditions. Through extensive\nexperimentation, we demonstrate that such simple baseline training on our\nHumanVid achieves state-of-the-art performance in controlling both human pose\nand camera motions, setting a new benchmark. Demo, data and code could be found\nin the project website: https://humanvid.github.io/.\n","authors":["Zhenzhi Wang","Yixuan Li","Yanhong Zeng","Youqing Fang","Yuwei Guo","Wenran Liu","Jing Tan","Kai Chen","Tianfan Xue","Bo Dai","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2407.17438v3.pdf","comment":"NeurIPS D&B Track 2024 camera ready version, TL;DR: the first\n  large-scale dataset for camera controllable human image animation task, and a\n  baseline method"},{"id":"http://arxiv.org/abs/2411.13787v1","updated":"2024-11-21T02:18:06Z","published":"2024-11-21T02:18:06Z","title":"Edge-Cloud Routing for Text-to-Image Model with Token-Level Multi-Metric\n  Prediction","summary":"  Large text-to-image models demonstrate impressive generation capabilities;\nhowever, their substantial size necessitates expensive cloud servers for\ndeployment. Conversely, light-weight models can be deployed on edge devices at\nlower cost but often with inferior generation quality for complex user prompts.\nTo strike a balance between performance and cost, we propose a routing\nframework, called \\texttt{RouteT2I}, which dynamically selects either the large\ncloud model or the light-weight edge model for each user prompt. Since\ngenerated image quality is challenging to measure directly, \\texttt{RouteT2I}\nestablishes multi-dimensional quality metrics, particularly, by evaluating the\nsimilarity between the generated images and both positive and negative texts\nthat describe each specific quality metric. \\texttt{RouteT2I} then predicts the\nexpected quality of the generated images by identifying key tokens in the\nprompt and comparing their impact on the quality. \\texttt{RouteT2I} further\nintroduces the Pareto relative superiority to compare the multi-metric quality\nof the generated images. Based on this comparison and predefined cost\nconstraints, \\texttt{RouteT2I} allocates prompts to either the edge or the\ncloud. Evaluation reveals that \\texttt{RouteT2I} significantly reduces the\nnumber of requesting large cloud model while maintaining high-quality image\ngeneration.\n","authors":["Zewei Xin","Qinya Li","Chaoyue Niu","Fan Wu"],"pdf_url":"https://arxiv.org/pdf/2411.13787v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03896v2","updated":"2024-11-21T02:17:39Z","published":"2024-06-06T09:36:05Z","title":"Data-driven discovery of self-similarity using neural networks","summary":"  Finding self-similarity is a key step for understanding the governing law\nbehind complex physical phenomena. Traditional methods for identifying\nself-similarity often rely on specific models, which can introduce significant\nbias. In this paper, we present a novel neural network-based approach that\ndiscovers self-similarity directly from observed data, without presupposing any\nmodels. The presence of self-similar solutions in a physical problem signals\nthat the governing law contains a function whose arguments are given by\npower-law monomials of physical parameters, which are characterized by\npower-law exponents. The basic idea is to enforce such particular forms\nstructurally in a neural network in a parametrized way. We train the neural\nnetwork model using the observed data, and when the training is successful, we\ncan extract the power exponents that characterize scale-transformation\nsymmetries of the physical problem. We demonstrate the effectiveness of our\nmethod with both synthetic and experimental data, validating its potential as a\nrobust, model-independent tool for exploring self-similarity in complex\nsystems.\n","authors":["Ryota Watanabe","Takanori Ishii","Yuji Hirono","Hirokazu Maruoka"],"pdf_url":"https://arxiv.org/pdf/2406.03896v2.pdf","comment":"21 pages, 18 figures, 5 tables"},{"id":"http://arxiv.org/abs/2411.13786v1","updated":"2024-11-21T02:15:52Z","published":"2024-11-21T02:15:52Z","title":"Adaptable Embeddings Network (AEN)","summary":"  Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems.\n","authors":["Stan Loosmore","Alexander Titus"],"pdf_url":"https://arxiv.org/pdf/2411.13786v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2307.16680v6","updated":"2024-11-21T01:41:01Z","published":"2023-07-31T13:57:05Z","title":"On the Trustworthiness Landscape of State-of-the-art Generative Models:\n  A Survey and Outlook","summary":"  Diffusion models and large language models have emerged as leading-edge\ngenerative models, revolutionizing various aspects of human life. However, the\npractical implementations of these models have also exposed inherent risks,\nbringing to the forefront their evil sides and sparking concerns regarding\ntheir trustworthiness. Despite the wealth of literature on this subject, a\ncomprehensive survey specifically delving into the intersection of large-scale\ngenerative models and their trustworthiness remains largely absent. To bridge\nthis gap, this paper investigates both the long-standing and emerging threats\nassociated with these models across four fundamental dimensions: 1) privacy, 2)\nsecurity, 3) fairness, and 4) responsibility. Based on the investigation\nresults, we develop an extensive map outlining the trustworthiness of large\ngenerative models. After that, we provide practical recommendations and\npotential research directions for future secure applications equipped with\nlarge generative models, ultimately promoting the trustworthiness of the models\nand benefiting the society as a whole.\n","authors":["Mingyuan Fan","Chengyu Wang","Cen Chen","Yang Liu","Jun Huang"],"pdf_url":"https://arxiv.org/pdf/2307.16680v6.pdf","comment":"draft"},{"id":"http://arxiv.org/abs/2411.13779v1","updated":"2024-11-21T01:37:38Z","published":"2024-11-21T01:37:38Z","title":"NewsInterview: a Dataset and a Playground to Evaluate LLMs' Ground Gap\n  via Informational Interviews","summary":"  Large Language Models (LLMs) have demonstrated impressive capabilities in\ngenerating coherent text but often struggle with grounding language and\nstrategic dialogue. To address this gap, we focus on journalistic interviews, a\ndomain rich in grounding communication and abundant in data. We curate a\ndataset of 40,000 two-person informational interviews from NPR and CNN, and\nreveal that LLMs are significantly less likely than human interviewers to use\nacknowledgements and to pivot to higher-level questions. Realizing that a\nfundamental deficit exists in multi-turn planning and strategic thinking, we\ndevelop a realistic simulated environment, incorporating source personas and\npersuasive elements, in order to facilitate the development of agents with\nlonger-horizon rewards. Our experiments show that while source LLMs mimic human\nbehavior in information sharing, interviewer LLMs struggle with recognizing\nwhen questions are answered and engaging persuasively, leading to suboptimal\ninformation extraction across model size and capability. These findings\nunderscore the need for enhancing LLMs' strategic dialogue capabilities.\n","authors":["Michael Lu","Hyundong Justin Cho","Weiyan Shi","Jonathan May","Alexander Spangher"],"pdf_url":"https://arxiv.org/pdf/2411.13779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13778v1","updated":"2024-11-21T01:26:52Z","published":"2024-11-21T01:26:52Z","title":"A Survey on Adversarial Robustness of LiDAR-based Machine Learning\n  Perception in Autonomous Vehicles","summary":"  In autonomous driving, the combination of AI and vehicular technology offers\ngreat potential. However, this amalgamation comes with vulnerabilities to\nadversarial attacks. This survey focuses on the intersection of Adversarial\nMachine Learning (AML) and autonomous systems, with a specific focus on\nLiDAR-based systems. We comprehensively explore the threat landscape,\nencompassing cyber-attacks on sensors and adversarial perturbations.\nAdditionally, we investigate defensive strategies employed in countering these\nthreats. This paper endeavors to present a concise overview of the challenges\nand advances in securing autonomous driving systems against adversarial\nthreats, emphasizing the need for robust defenses to ensure safety and\nsecurity.\n","authors":["Junae Kim","Amardeep Kaur"],"pdf_url":"https://arxiv.org/pdf/2411.13778v1.pdf","comment":"20 pages, 2 figures"},{"id":"http://arxiv.org/abs/2304.03365v3","updated":"2024-11-21T01:06:21Z","published":"2023-04-06T20:47:09Z","title":"Decision-Focused Model-based Reinforcement Learning for Reward Transfer","summary":"  Model-based reinforcement learning (MBRL) provides a way to learn a\ntransition model of the environment, which can then be used to plan\npersonalized policies for different patient cohorts and to understand the\ndynamics involved in the decision-making process. However, standard MBRL\nalgorithms are either sensitive to changes in the reward function or achieve\nsuboptimal performance on the task when the transition model is restricted.\nMotivated by the need to use simple and interpretable models in critical\ndomains such as healthcare, we propose a novel robust decision-focused (RDF)\nalgorithm that learns a transition model that achieves high returns while being\nrobust to changes in the reward function. We demonstrate our RDF algorithm can\nbe used with several model classes and planning algorithms. We also provide\ntheoretical and empirical evidence, on a variety of simulators and real patient\ndata, that RDF can learn simple yet effective models that can be used to plan\npersonalized policies.\n","authors":["Abhishek Sharma","Sonali Parbhoo","Omer Gottesman","Finale Doshi-Velez"],"pdf_url":"https://arxiv.org/pdf/2304.03365v3.pdf","comment":"Machine Learning for Healthcare (MLHC) 2024"},{"id":"http://arxiv.org/abs/2404.10351v2","updated":"2024-11-21T00:57:22Z","published":"2024-04-16T07:39:54Z","title":"On the Use of Relative Validity Indices for Comparing Clustering\n  Approaches","summary":"  Relative Validity Indices (RVIs) such as the Silhouette Width Criterion and\nDavies Bouldin indices are the most widely used tools for evaluating and\noptimising clustering outcomes. Traditionally, their ability to rank\ncollections of candidate dataset partitions has been used to guide the\nselection of the number of clusters, and to compare partitions from different\nclustering algorithms. However, there is a growing trend in the literature to\nuse RVIs when selecting a Similarity Paradigm (SP) for clustering - the\ncombination of normalisation procedure, representation method, and distance\nmeasure which affects the computation of object dissimilarities used in\nclustering. Despite the growing prevalence of this practice, there has been no\nempirical or theoretical investigation into the suitability of RVIs for this\npurpose. Moreover, since RVIs are computed using object dissimilarities, it\nremains unclear how they would need to be implemented for fair comparisons of\ndifferent SPs. This study presents the first comprehensive investigation into\nthe reliability of RVIs for SP selection. We conducted extensive experiments\nwith seven popular RVIs on over 2.7 million clustering partitions of synthetic\nand real-world datasets, encompassing feature-vector and time-series data. We\nidentified fundamental conceptual limitations undermining the use of RVIs for\nSP selection, and our empirical findings confirmed this predicted\nunsuitability. Among our recommendations, we suggest instead that practitioners\nselect SPs by using external validation on high quality labelled datasets or\ncarefully designed outcome-oriented objective criteria, both of which should be\ninformed by careful consideration of dataset characteristics, and domain\nrequirements. Our findings have important implications for clustering\nmethodology and evaluation, suggesting the need for more rigorous approaches to\nSP selection.\n","authors":["Luke W. Yerbury","Ricardo J. G. B. Campello","G. C. Livingston Jr","Mark Goldsworthy","Lachlan O'Neil"],"pdf_url":"https://arxiv.org/pdf/2404.10351v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11374v4","updated":"2024-11-21T00:19:27Z","published":"2024-01-21T02:29:12Z","title":"Language Models as Hierarchy Encoders","summary":"  Interpreting hierarchical structures latent in language is a key limitation\nof current language models (LMs). While previous research has implicitly\nleveraged these hierarchies to enhance LMs, approaches for their explicit\nencoding are yet to be explored. To address this, we introduce a novel approach\nto re-train transformer encoder-based LMs as Hierarchy Transformer encoders\n(HiTs), harnessing the expansive nature of hyperbolic space. Our method\nsituates the output embedding space of pre-trained LMs within a Poincar\\'e ball\nwith a curvature that adapts to the embedding dimension, followed by training\non hyperbolic clustering and centripetal losses. These losses are designed to\neffectively cluster related entities (input as texts) and organise them\nhierarchically. We evaluate HiTs against pre-trained LMs, standard fine-tuned\nLMs, and several hyperbolic embedding baselines, focusing on their capabilities\nin simulating transitive inference, predicting subsumptions, and transferring\nknowledge across hierarchies. The results demonstrate that HiTs consistently\noutperform all baselines in these tasks, underscoring the effectiveness and\ntransferability of our re-trained hierarchy encoders.\n","authors":["Yuan He","Zhangdie Yuan","Jiaoyan Chen","Ian Horrocks"],"pdf_url":"https://arxiv.org/pdf/2401.11374v4.pdf","comment":"Accept at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.14034v1","updated":"2024-11-21T11:37:05Z","published":"2024-11-21T11:37:05Z","title":"Assessing data-driven predictions of band gap and electrical\n  conductivity for transparent conducting materials","summary":"  Machine Learning (ML) has offered innovative perspectives for accelerating\nthe discovery of new functional materials, leveraging the increasing\navailability of material databases. Despite the promising advances, data-driven\nmethods face constraints imposed by the quantity and quality of available data.\nMoreover, ML is often employed in tandem with simulated datasets originating\nfrom density functional theory (DFT), and assessed through in-sample evaluation\nschemes. This scenario raises questions about the practical utility of ML in\nuncovering new and significant material classes for industrial applications.\nHere, we propose a data-driven framework aimed at accelerating the discovery of\nnew transparent conducting materials (TCMs), an important category of\nsemiconductors with a wide range of applications. To mitigate the shortage of\navailable data, we create and validate unique experimental databases,\ncomprising several examples of existing TCMs. We assess state-of-the-art (SOTA)\nML models for property prediction from the stoichiometry alone. We propose a\nbespoke evaluation scheme to provide empirical evidence on the ability of ML to\nuncover new, previously unseen materials of interest. We test our approach on a\nlist of 55 compositions containing typical elements of known TCMs. Although our\nstudy indicates that ML tends to identify new TCMs compositionally similar to\nthose in the training data, we empirically demonstrate that it can highlight\nmaterial candidates that may have been previously overlooked, offering a\nsystematic approach to identify materials that are likely to display TCMs\ncharacteristics.\n","authors":["Federico Ottomano","John Y. Goulermas","Vladimir Gusev","Rahul Savani","Michael W. Gaultois","Troy D. Manning","Hai Lin","Teresa P. Manzanera","Emmeline G. Poole","Matthew S. Dyer","John B. Claridge","Jon Alaria","Luke M. Daniels","Su Varma","David Rimmer","Kevin Sanderson","Matthew J. Rosseinsky"],"pdf_url":"https://arxiv.org/pdf/2411.14034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13867v1","updated":"2024-11-21T06:03:25Z","published":"2024-11-21T06:03:25Z","title":"Generative Fuzzy System for Sequence Generation","summary":"  Generative Models (GMs), particularly Large Language Models (LLMs), have\ngarnered significant attention in machine learning and artificial intelligence\nfor their ability to generate new data by learning the statistical properties\nof training data and creating data that resemble the original. This capability\noffers a wide range of applications across various domains. However, the\ncomplex structures and numerous model parameters of GMs make the input-output\nprocesses opaque, complicating the understanding and control of outputs.\nMoreover, the purely data-driven learning mechanism limits GM's ability to\nacquire broader knowledge. There remains substantial potential for enhancing\nthe robustness and generalization capabilities of GMs. In this work, we\nintroduce the fuzzy system, a classical modeling method that combines data and\nknowledge-driven mechanisms, to generative tasks. We propose a novel Generative\nFuzzy System framework, named GenFS, which integrates the deep learning\ncapabilities of GM with the interpretability and dual-driven mechanisms of\nfuzzy systems. Specifically, we propose an end-to-end GenFS-based model for\nsequence generation, called FuzzyS2S. A series of experimental studies were\nconducted on 12 datasets, covering three distinct categories of generative\ntasks: machine translation, code generation, and summary generation. The\nresults demonstrate that FuzzyS2S outperforms the Transformer in terms of\naccuracy and fluency. Furthermore, it exhibits better performance on some\ndatasets compared to state-of-the-art models T5 and CodeT5.\n","authors":["Hailong Yang","Zhaohong Deng","Wei Zhang","Zhuangzhuang Zhao","Guanjin Wang","Kup-sze Choi"],"pdf_url":"https://arxiv.org/pdf/2411.13867v1.pdf","comment":"12 pages, 5 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2411.14207v1","updated":"2024-11-21T15:16:48Z","published":"2024-11-21T15:16:48Z","title":"HARP: A Large-Scale Higher-Order Ambisonic Room Impulse Response Dataset","summary":"  This contribution introduces a dataset of 7th-order Ambisonic Room Impulse\nResponses (HOA-RIRs), created using the Image Source Method. By employing\nhigher-order Ambisonics, our dataset enables precise spatial audio\nreproduction, a critical requirement for realistic immersive audio\napplications. Leveraging the virtual simulation, we present a unique microphone\nconfiguration, based on the superposition principle, designed to optimize sound\nfield coverage while addressing the limitations of traditional microphone\narrays. The presented 64-microphone configuration allows us to capture RIRs\ndirectly in the Spherical Harmonics domain. The dataset features a wide range\nof room configurations, encompassing variations in room geometry, acoustic\nabsorption materials, and source-receiver distances. A detailed description of\nthe simulation setup is provided alongside for an accurate reproduction. The\ndataset serves as a vital resource for researchers working on spatial audio,\nparticularly in applications involving machine learning to improve room\nacoustics modeling and sound field synthesis. It further provides a very high\nlevel of spatial resolution and realism crucial for tasks such as source\nlocalization, reverberation prediction, and immersive sound reproduction.\n","authors":["Shivam Saini","Jürgen Peissig"],"pdf_url":"https://arxiv.org/pdf/2411.14207v1.pdf","comment":"Submitted to ICASSP 2025 Workshop Dataset and code to be uploaded at:\n  https://github.com/whojavumusic/HARP"},{"id":"http://arxiv.org/abs/2411.14135v1","updated":"2024-11-21T14:01:33Z","published":"2024-11-21T14:01:33Z","title":"Compact Visual Data Representation for Green Multimedia -- A Human\n  Visual System Perspective","summary":"  The Human Visual System (HVS), with its intricate sophistication, is capable\nof achieving ultra-compact information compression for visual signals. This\nremarkable ability is coupled with high generalization capability and energy\nefficiency. By contrast, the state-of-the-art Versatile Video Coding (VVC)\nstandard achieves a compression ratio of around 1,000 times for raw visual\ndata. This notable disparity motivates the research community to draw\ninspiration to effectively handle the immense volume of visual data in a green\nway. Therefore, this paper provides a survey of how visual data can be\nefficiently represented for green multimedia, in particular when the ultimate\ntask is knowledge extraction instead of visual signal reconstruction. We\nintroduce recent research efforts that promote green, sustainable, and\nefficient multimedia in this field. Moreover, we discuss how the deep\nunderstanding of the HVS can benefit the research community, and envision the\ndevelopment of future green multimedia technologies.\n","authors":["Peilin Chen","Xiaohan Fang","Meng Wang","Shiqi Wang","Siwei Ma"],"pdf_url":"https://arxiv.org/pdf/2411.14135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13917v1","updated":"2024-11-21T08:07:26Z","published":"2024-11-21T08:07:26Z","title":"SpikEmo: Enhancing Emotion Recognition With Spiking Temporal Dynamics in\n  Conversations","summary":"  In affective computing, the task of Emotion Recognition in Conversations\n(ERC) has emerged as a focal area of research. The primary objective of this\ntask is to predict emotional states within conversations by analyzing\nmultimodal data including text, audio, and video. While existing studies have\nprogressed in extracting and fusing representations from multimodal data, they\noften overlook the temporal dynamics in the data during conversations. To\naddress this challenge, we have developed the SpikEmo framework, which is based\non spiking neurons and employs a Semantic & Dynamic Two-stage Modeling approach\nto more precisely capture the complex temporal features of multimodal emotional\ndata. Additionally, to tackle the class imbalance and emotional semantic\nsimilarity problems in the ERC tasks, we have devised an innovative combination\nof loss functions that significantly enhances the model's performance when\ndealing with ERC data characterized by long-tail distributions. Extensive\nexperiments conducted on multiple ERC benchmark datasets demonstrate that\nSpikEmo significantly outperforms existing state-of-the-art methods in ERC\ntasks. Our code is available at https://github.com/Yu-xm/SpikEmo.git.\n","authors":["Xiaomin Yu","Feiyang Wang","Ziyue Qiao"],"pdf_url":"https://arxiv.org/pdf/2411.13917v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.09955v2","updated":"2024-11-21T05:28:10Z","published":"2024-11-15T05:18:15Z","title":"Instruction-Guided Editing Controls for Images and Multimedia: A Survey\n  in LLM era","summary":"  The rapid advancement of large language models (LLMs) and multimodal learning\nhas transformed digital content creation and manipulation. Traditional visual\nediting tools require significant expertise, limiting accessibility. Recent\nstrides in instruction-based editing have enabled intuitive interaction with\nvisual content, using natural language as a bridge between user intent and\ncomplex editing operations. This survey provides an overview of these\ntechniques, focusing on how LLMs and multimodal models empower users to achieve\nprecise visual modifications without deep technical knowledge. By synthesizing\nover 100 publications, we explore methods from generative adversarial networks\nto diffusion models, examining multimodal integration for fine-grained content\ncontrol. We discuss practical applications across domains such as fashion, 3D\nscene manipulation, and video synthesis, highlighting increased accessibility\nand alignment with human intuition. Our survey compares existing literature,\nemphasizing LLM-empowered editing, and identifies key challenges to stimulate\nfurther research. We aim to democratize powerful visual editing across various\nindustries, from entertainment to education. Interested readers are encouraged\nto access our repository at\nhttps://github.com/tamlhp/awesome-instruction-editing.\n","authors":["Thanh Tam Nguyen","Zhao Ren","Trinh Pham","Thanh Trung Huynh","Phi Le Nguyen","Hongzhi Yin","Quoc Viet Hung Nguyen"],"pdf_url":"https://arxiv.org/pdf/2411.09955v2.pdf","comment":"Fixed a serious error in author information"},{"id":"http://arxiv.org/abs/2411.13819v1","updated":"2024-11-21T03:49:18Z","published":"2024-11-21T03:49:18Z","title":"Robust Steganography with Boundary-Preserving Overflow Alleviation and\n  Adaptive Error Correction","summary":"  With the rapid evolution of the Internet, the vast amount of data has created\nopportunities for fostering the development of steganographic techniques.\nHowever, traditional steganographic techniques encounter challenges due to\ndistortions in online social networks, such as JPEG recompression. Presently,\nresearch into the lossy operations of spatial truncation in JPEG recompression\nremains limited. Existing methods aim to ensure the stability of the quantized\ncoefficients by reducing the effects of spatial truncation. Nevertheless, these\napproaches may induce notable alterations to image pixels, potentially\ncompromising anti-steganalysis performance. In this study, we analyzed the\noverflow characteristics of spatial blocks and observed that pixel values at\nthe boundaries of spatial blocks are more prone to overflow. Building upon this\nobservation, we proposed a preprocessing method that performs overflow removal\noperations based on the actual overflow conditions of spatial blocks. After\npreprocessing, our algorithm enhances coefficient stability while minimizing\nmodifications to spatial block boundaries, favoring image quality preservation.\nSubsequently, we employed adaptive error correction coding to reduce coding\nredundancy, thereby augmenting robustness and mitigating its impact on\nanti-steganalysis performance. The experimental results indicate that the\nproposed method possesses a strong embedding capacity, maintaining a high level\nof robustness while enhancing security.\n","authors":["Yu Cheng","Zhenlin Luo","Zhaoxia Yin"],"pdf_url":"https://arxiv.org/pdf/2411.13819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13811v1","updated":"2024-11-21T03:21:42Z","published":"2024-11-21T03:21:42Z","title":"X-CrossNet: A complex spectral mapping approach to target speaker\n  extraction with cross attention speaker embedding fusion","summary":"  Target speaker extraction (TSE) is a technique for isolating a target\nspeaker's voice from mixed speech using auxiliary features associated with the\ntarget speaker. This approach addresses the cocktail party problem and is\ngenerally considered more promising for practical applications than\nconventional speech separation methods. Although academic research in this area\nhas achieved high accuracy and evaluation scores on public datasets, most\nmodels exhibit significantly reduced performance in real-world noisy or\nreverberant conditions. To address this limitation, we propose a novel TSE\nmodel, X-CrossNet, which leverages CrossNet as its backbone. CrossNet is a\nspeech separation network specifically optimized for challenging noisy and\nreverberant environments, achieving state-of-the-art performance in tasks such\nas speaker separation under these conditions. Additionally, to enhance the\nnetwork's ability to capture and utilize auxiliary features of the target\nspeaker, we integrate a Cross-Attention mechanism into the global multi-head\nself-attention (GMHSA) module within each CrossNet block. This facilitates more\neffective integration of target speaker features with mixed speech features.\nExperimental results show that our method performs superior separation on the\nWSJ0-2mix and WHAMR! datasets, demonstrating strong robustness and stability.\n","authors":["Chang Sun","Bo Qin"],"pdf_url":"https://arxiv.org/pdf/2411.13811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14627v1","updated":"2024-11-21T23:02:12Z","published":"2024-11-21T23:02:12Z","title":"Generative AI for Music and Audio","summary":"  Generative AI has been transforming the way we interact with technology and\nconsume content. In the next decade, AI technology will reshape how we create\naudio content in various media, including music, theater, films, games,\npodcasts, and short videos. In this dissertation, I introduce the three main\ndirections of my research centered around generative AI for music and audio: 1)\nmultitrack music generation, 2) assistive music creation tools, and 3)\nmultimodal learning for audio and music. Through my research, I aim to answer\nthe following two fundamental questions: 1) How can AI help professionals or\namateurs create music and audio content? 2) Can AI learn to create music in a\nway similar to how humans learn music? My long-term goal is to lower the\nbarrier of entry for music composition and democratize audio content creation\n","authors":["Hao-Wen Dong"],"pdf_url":"https://arxiv.org/pdf/2411.14627v1.pdf","comment":"PhD Dissertation"},{"id":"http://arxiv.org/abs/2411.14613v1","updated":"2024-11-21T22:29:15Z","published":"2024-11-21T22:29:15Z","title":"Optimal Transcoding Preset Selection for Live Video Streaming","summary":"  In today's digital landscape, video content dominates internet traffic,\nunderscoring the need for efficient video processing to support seamless live\nstreaming experiences on platforms like YouTube Live, Twitch, and Facebook\nLive. This paper introduces a comprehensive framework designed to optimize\nvideo transcoding parameters, with a specific focus on preset and bitrate\nselection to minimize distortion while respecting constraints on bitrate and\ntranscoding time. The framework comprises three main steps: feature extraction,\nprediction, and optimization. It leverages extracted features to predict\ntranscoding time and rate-distortion, employing both supervised and\nunsupervised methods. By utilizing integer linear programming, it identifies\nthe optimal sequence of presets and bitrates for video segments, ensuring\nreal-time application feasibility under set constraints. The results\ndemonstrate the framework's effectiveness in enhancing video quality for live\nstreaming, maintaining high standards of video delivery while managing\ncomputational resources efficiently. This optimization approach meets the\nevolving demands of video delivery by offering a solution for real-time\ntranscoding optimization. Evaluation using the User Generated Content dataset\nshowed an average PSNR improvement of 1.5 dB over the default Twitch\nconfiguration, highlighting significant PSNR gains. Additionally, subsequent\nexperiments demonstrated a BD-rate reduction of -49.60%, reinforcing the\nframework's superior performance over Twitch's default configuration.\n","authors":["Zahra Nabizadeh","Maedeh Jamali","Nader Karimi","Shadrokh Samavi","Shahram Shirani"],"pdf_url":"https://arxiv.org/pdf/2411.14613v1.pdf","comment":"23 pages, 10 figures"}]},"2024-11-22T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2411.15129v1","updated":"2024-11-22T18:55:21Z","published":"2024-11-22T18:55:21Z","title":"Measuring Bullshit in the Language Games played by ChatGPT","summary":"  Generative large language models (LLMs), which create text without direct\ncorrespondence to truth value, are widely understood to resemble the uses of\nlanguage described in Frankfurt's popular monograph On Bullshit. In this paper,\nwe offer a rigorous investigation of this topic, identifying how the phenomenon\nhas arisen, and how it might be analysed. In this paper, we elaborate on this\nargument to propose that LLM-based chatbots play the 'language game of\nbullshit'. We use statistical text analysis to investigate the features of this\nWittgensteinian language game, based on a dataset constructed to contrast the\nlanguage of 1,000 scientific publications with typical pseudo-scientific text\ngenerated by ChatGPT. We then explore whether the same language features can be\ndetected in two well-known contexts of social dysfunction: George Orwell's\ncritique of politics and language, and David Graeber's characterisation of\nbullshit jobs. Using simple hypothesis-testing methods, we demonstrate that a\nstatistical model of the language of bullshit can reliably relate the\nFrankfurtian artificial bullshit of ChatGPT to the political and workplace\nfunctions of bullshit as observed in natural human language.\n","authors":["Alessandro Trevisan","Harry Giddens","Sarah Dillon","Alan F. Blackwell"],"pdf_url":"https://arxiv.org/pdf/2411.15129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15124v1","updated":"2024-11-22T18:44:04Z","published":"2024-11-22T18:44:04Z","title":"TÜLU 3: Pushing Frontiers in Open Language Model Post-Training","summary":"  Language model post-training is applied to refine behaviors and unlock new\nskills across a wide range of recent language models, but open recipes for\napplying these techniques lag behind proprietary ones. The underlying training\ndata and recipes for post-training are simultaneously the most important pieces\nof the puzzle and the portion with the least transparency. To bridge this gap,\nwe introduce T\\\"ULU 3, a family of fully-open state-of-the-art post-trained\nmodels, alongside its data, code, and training recipes, serving as a\ncomprehensive guide for modern post-training techniques. T\\\"ULU 3, which builds\non Llama 3.1 base models, achieves results surpassing the instruct versions of\nLlama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and\nClaude 3.5-Haiku. The training algorithms for our models include supervised\nfinetuning (SFT), Direct Preference Optimization (DPO), and a novel method we\ncall Reinforcement Learning with Verifiable Rewards (RLVR). With T\\\"ULU 3, we\nintroduce a multi-task evaluation scheme for post-training recipes with\ndevelopment and unseen evaluations, standard benchmark implementations, and\nsubstantial decontamination of existing open datasets on said benchmarks. We\nconclude with analysis and discussion of training methods that did not reliably\nimprove performance.\n  In addition to the T\\\"ULU 3 model weights and demo, we release the complete\nrecipe -- including datasets for diverse core skills, a robust toolkit for data\ncuration and evaluation, the training code and infrastructure, and, most\nimportantly, a detailed report for reproducing and further adapting the T\\\"ULU\n3 approach to more domains.\n","authors":["Nathan Lambert","Jacob Morrison","Valentina Pyatkin","Shengyi Huang","Hamish Ivison","Faeze Brahman","Lester James V. Miranda","Alisa Liu","Nouha Dziri","Shane Lyu","Yuling Gu","Saumya Malik","Victoria Graf","Jena D. Hwang","Jiangjiang Yang","Ronan Le Bras","Oyvind Tafjord","Chris Wilhelm","Luca Soldaini","Noah A. Smith","Yizhong Wang","Pradeep Dasigi","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2411.15124v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15122v1","updated":"2024-11-22T18:40:02Z","published":"2024-11-22T18:40:02Z","title":"ReXrank: A Public Leaderboard for AI-Powered Radiology Report Generation","summary":"  AI-driven models have demonstrated significant potential in automating\nradiology report generation for chest X-rays. However, there is no standardized\nbenchmark for objectively evaluating their performance. To address this, we\npresent ReXrank, https://rexrank.ai, a public leaderboard and challenge for\nassessing AI-powered radiology report generation. Our framework incorporates\nReXGradient, the largest test dataset consisting of 10,000 studies, and three\npublic datasets (MIMIC-CXR, IU-Xray, CheXpert Plus) for report generation\nassessment. ReXrank employs 8 evaluation metrics and separately assesses models\ncapable of generating only findings sections and those providing both findings\nand impressions sections. By providing this standardized evaluation framework,\nReXrank enables meaningful comparisons of model performance and offers crucial\ninsights into their robustness across diverse clinical settings. Beyond its\ncurrent focus on chest X-rays, ReXrank's framework sets the stage for\ncomprehensive evaluation of automated reporting across the full spectrum of\nmedical imaging.\n","authors":["Xiaoman Zhang","Hong-Yu Zhou","Xiaoli Yang","Oishi Banerjee","Julián N. Acosta","Josh Miller","Ouwen Huang","Pranav Rajpurkar"],"pdf_url":"https://arxiv.org/pdf/2411.15122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15115v1","updated":"2024-11-22T18:31:47Z","published":"2024-11-22T18:31:47Z","title":"VideoRepair: Improving Text-to-Video Generation via Misalignment\n  Evaluation and Localized Refinement","summary":"  Recent text-to-video (T2V) diffusion models have demonstrated impressive\ngeneration capabilities across various domains. However, these models often\ngenerate videos that have misalignments with text prompts, especially when the\nprompts describe complex scenes with multiple objects and attributes. To\naddress this, we introduce VideoRepair, a novel model-agnostic, training-free\nvideo refinement framework that automatically identifies fine-grained\ntext-video misalignments and generates explicit spatial and textual feedback,\nenabling a T2V diffusion model to perform targeted, localized refinements.\nVideoRepair consists of four stages: In (1) video evaluation, we detect\nmisalignments by generating fine-grained evaluation questions and answering\nthose questions with MLLM. In (2) refinement planning, we identify accurately\ngenerated objects and then create localized prompts to refine other areas in\nthe video. Next, in (3) region decomposition, we segment the correctly\ngenerated area using a combined grounding module. We regenerate the video by\nadjusting the misaligned regions while preserving the correct regions in (4)\nlocalized refinement. On two popular video generation benchmarks (EvalCrafter\nand T2V-CompBench), VideoRepair substantially outperforms recent baselines\nacross various text-video alignment metrics. We provide a comprehensive\nanalysis of VideoRepair components and qualitative examples.\n","authors":["Daeun Lee","Jaehong Yoon","Jaemin Cho","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2411.15115v1.pdf","comment":"Project page: https://video-repair.github.io"},{"id":"http://arxiv.org/abs/2411.15113v1","updated":"2024-11-22T18:29:37Z","published":"2024-11-22T18:29:37Z","title":"Efficient Pruning of Text-to-Image Models: Insights from Pruning Stable\n  Diffusion","summary":"  As text-to-image models grow increasingly powerful and complex, their\nburgeoning size presents a significant obstacle to widespread adoption,\nespecially on resource-constrained devices. This paper presents a pioneering\nstudy on post-training pruning of Stable Diffusion 2, addressing the critical\nneed for model compression in text-to-image domain. Our study tackles the\npruning techniques for the previously unexplored multi-modal generation models,\nand particularly examines the pruning impact on the textual component and the\nimage generation component separately. We conduct a comprehensive comparison on\npruning the model or the single component of the model in various sparsities.\nOur results yield previously undocumented findings. For example, contrary to\nestablished trends in language model pruning, we discover that simple magnitude\npruning outperforms more advanced techniques in text-to-image context.\nFurthermore, our results show that Stable Diffusion 2 can be pruned to 38.5%\nsparsity with minimal quality loss, achieving a significant reduction in model\nsize. We propose an optimal pruning configuration that prunes the text encoder\nto 47.5% and the diffusion generator to 35%. This configuration maintains image\ngeneration quality while substantially reducing computational requirements. In\naddition, our work uncovers intriguing questions about information encoding in\ntext-to-image models: we observe that pruning beyond certain thresholds leads\nto sudden performance drops (unreadable images), suggesting that specific\nweights encode critical semantics information. This finding opens new avenues\nfor future research in model compression, interoperability, and bias\nidentification in text-to-image models. By providing crucial insights into the\npruning behavior of text-to-image models, our study lays the groundwork for\ndeveloping more efficient and accessible AI-driven image generation systems\n","authors":["Samarth N Ramesh","Zhixue Zhao"],"pdf_url":"https://arxiv.org/pdf/2411.15113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15100v1","updated":"2024-11-22T18:01:37Z","published":"2024-11-22T18:01:37Z","title":"XGrammar: Flexible and Efficient Structured Generation Engine for Large\n  Language Models","summary":"  The applications of LLM Agents are becoming increasingly complex and diverse,\nleading to a high demand for structured outputs that can be parsed into code,\nstructured function calls, and embodied agent commands. These developments\nbring significant demands for structured generation in LLM inference.\nContext-free grammar is a flexible approach to enable structured generation via\nconstrained decoding. However, executing context-free grammar requires going\nthrough several stack states over all tokens in vocabulary during runtime,\nbringing non-negligible overhead for structured generation. In this paper, we\npropose XGrammar, a flexible and efficient structure generation engine for\nlarge language models. XGrammar accelerates context-free grammar execution by\ndividing the vocabulary into context-independent tokens that can be prechecked\nand context-dependent tokens that need to be interpreted during runtime. We\nfurther build transformations to expand the grammar context and reduce the\nnumber of context-independent tokens. Additionally, we build an efficient\npersistent stack to accelerate the context-dependent token checks. Finally, we\nco-design the grammar engine with LLM inference engine to overlap grammar\ncomputation with GPU executions. Evaluation results show that XGrammar can\nachieve up to 100x speedup over existing solutions. Combined with an LLM\ninference engine, it can generate near-zero overhead structure generation in\nend-to-end low-LLM serving.\n","authors":["Yixin Dong","Charlie F. Ruan","Yaxing Cai","Ruihang Lai","Ziyi Xu","Yilong Zhao","Tianqi Chen"],"pdf_url":"https://arxiv.org/pdf/2411.15100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15099v1","updated":"2024-11-22T17:55:39Z","published":"2024-11-22T17:55:39Z","title":"Context-Aware Multimodal Pretraining","summary":"  Large-scale multimodal representation learning successfully optimizes for\nzero-shot transfer at test time. Yet the standard pretraining paradigm\n(contrastive learning on large amounts of image-text data) does not explicitly\nencourage representations to support few-shot adaptation. In this work, we\npropose a simple, but carefully designed extension to multimodal pretraining\nwhich enables representations to accommodate additional context. Using this\nobjective, we show that vision-language models can be trained to exhibit\nsignificantly increased few-shot adaptation: across 21 downstream tasks, we\nfind up to four-fold improvements in test-time sample efficiency, and average\nfew-shot adaptation gains of over 5%, while retaining zero-shot generalization\nperformance across model scales and training durations. In particular, equipped\nwith simple, training-free, metric-based adaptation mechanisms, our\nrepresentations easily surpass more complex and expensive optimization-based\nschemes, vastly simplifying generalization to new domains.\n","authors":["Karsten Roth","Zeynep Akata","Dima Damen","Ivana Balažević","Olivier J. Hénaff"],"pdf_url":"https://arxiv.org/pdf/2411.15099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12043v2","updated":"2024-11-22T17:48:57Z","published":"2024-07-02T07:12:51Z","title":"The Art of Saying No: Contextual Noncompliance in Language Models","summary":"  Chat-based language models are designed to be helpful, yet they should not\ncomply with every user request. While most existing work primarily focuses on\nrefusal of \"unsafe\" queries, we posit that the scope of noncompliance should be\nbroadened. We introduce a comprehensive taxonomy of contextual noncompliance\ndescribing when and how models should not comply with user requests. Our\ntaxonomy spans a wide range of categories including incomplete, unsupported,\nindeterminate, and humanizing requests (in addition to unsafe requests). To\ntest noncompliance capabilities of language models, we use this taxonomy to\ndevelop a new evaluation suite of 1000 noncompliance prompts. We find that most\nexisting models show significantly high compliance rates in certain previously\nunderstudied categories with models like GPT-4 incorrectly complying with as\nmany as 30% of requests. To address these gaps, we explore different training\nstrategies using a synthetically-generated training set of requests and\nexpected noncompliant responses. Our experiments demonstrate that while direct\nfinetuning of instruction-tuned models can lead to both over-refusal and a\ndecline in general capabilities, using parameter efficient methods like low\nrank adapters helps to strike a good balance between appropriate noncompliance\nand other capabilities.\n","authors":["Faeze Brahman","Sachin Kumar","Vidhisha Balachandran","Pradeep Dasigi","Valentina Pyatkin","Abhilasha Ravichander","Sarah Wiegreffe","Nouha Dziri","Khyathi Chandu","Jack Hessel","Yulia Tsvetkov","Noah A. Smith","Yejin Choi","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2407.12043v2.pdf","comment":"The first two authors are co-first authors; Accepted at NeurIPS 2024\n  Track on Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2411.15087v1","updated":"2024-11-22T17:28:43Z","published":"2024-11-22T17:28:43Z","title":"Instance-Aware Generalized Referring Expression Segmentation","summary":"  Recent works on Generalized Referring Expression Segmentation (GRES) struggle\nwith handling complex expressions referring to multiple distinct objects. This\nis because these methods typically employ an end-to-end foreground-background\nsegmentation and lack a mechanism to explicitly differentiate and associate\ndifferent object instances to the text query. To this end, we propose\nInstAlign, a method that incorporates object-level reasoning into the\nsegmentation process. Our model leverages both text and image inputs to extract\na set of object-level tokens that capture both the semantic information in the\ninput prompt and the objects within the image. By modeling the text-object\nalignment via instance-level supervision, each token uniquely represents an\nobject segment in the image, while also aligning with relevant semantic\ninformation from the text. Extensive experiments on the gRefCOCO and Ref-ZOM\nbenchmarks demonstrate that our method significantly advances state-of-the-art\nperformance, setting a new standard for precise and flexible GRES.\n","authors":["E-Ro Nguyen","Hieu Le","Dimitris Samaras","Michael Ryoo"],"pdf_url":"https://arxiv.org/pdf/2411.15087v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2411.15068v1","updated":"2024-11-22T16:56:38Z","published":"2024-11-22T16:56:38Z","title":"Locating the Leading Edge of Cultural Change","summary":"  Measures of textual similarity and divergence are increasingly used to study\ncultural change. But which measures align, in practice, with social evidence\nabout change? We apply three different representations of text (topic models,\ndocument embeddings, and word-level perplexity) to three different corpora\n(literary studies, economics, and fiction). In every case, works by\nhighly-cited authors and younger authors are textually ahead of the curve. We\ndon't find clear evidence that one representation of text is to be preferred\nover the others. But alignment with social evidence is strongest when texts are\nrepresented through the top quartile of passages, suggesting that a text's\nimpact may depend more on its most forward-looking moments than on sustaining a\nhigh level of innovation throughout.\n","authors":["Sarah Griebel","Becca Cohen","Lucian Li","Jaihyun Park","Jiayu Liu","Jana Perkins","Ted Underwood"],"pdf_url":"https://arxiv.org/pdf/2411.15068v1.pdf","comment":"Accepted CHR 2024"},{"id":"http://arxiv.org/abs/2407.21753v2","updated":"2024-11-22T16:39:04Z","published":"2024-07-31T17:18:25Z","title":"Characterizing User Archetypes and Discussions on Scored.co","summary":"  In recent years, the proliferation of social platforms has drastically\ntransformed the way individuals interact, organize, and share information. In\nthis scenario, we experience an unprecedented increase in the scale and\ncomplexity of interactions and, at the same time, little to no research about\nsome fringe social platforms. In this paper, we present a multi-dimensional\nframework for characterizing nodes and hyperedges in social hypernetworks, with\na focus on the understudied alt-right platform Scored.co. Our approach\nintegrates the possibility of studying higher-order interactions, thanks to the\nhypernetwork representation, and various node features such as user activity,\nsentiment, and toxicity, with the aim to define distinct user archetypes and\nunderstand their roles within the network. Utilizing a comprehensive dataset\nfrom Scored.co, we analyze the dynamics of these archetypes over time and\nexplore their interactions and influence within the community. The framework's\nversatility allows for detailed analysis of both individual user behaviors and\nbroader social structures. Our findings highlight the importance of\nhigher-order interactions in understanding social dynamics, offering new\ninsights into the roles and behaviors that emerge in complex online\nenvironments.\n","authors":["Andrea Failla","Salvatore Citraro","Giulio Rossetti","Francesco Cauteruccio"],"pdf_url":"https://arxiv.org/pdf/2407.21753v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15051v1","updated":"2024-11-22T16:38:03Z","published":"2024-11-22T16:38:03Z","title":"Fantastic Biases (What are They) and Where to Find Them","summary":"  Deep Learning models tend to learn correlations of patterns on huge datasets.\nThe bigger these systems are, the more complex are the phenomena they can\ndetect, and the more data they need for this. The use of Artificial\nIntelligence (AI) is becoming increasingly ubiquitous in our society, and its\nimpact is growing everyday. The promises it holds strongly depend on their fair\nand universal use, such as access to information or education for all. In a\nworld of inequalities, they can help to reach the most disadvantaged areas.\nHowever, such a universal systems must be able to represent society, without\nbenefiting some at the expense of others. We must not reproduce the\ninequalities observed throughout the world, but educate these IAs to go beyond\nthem. We have seen cases where these systems use gender, race, or even class\ninformation in ways that are not appropriate for resolving their tasks. Instead\nof real causal reasoning, they rely on spurious correlations, which is what we\nusually call a bias. In this paper, we first attempt to define what is a bias\nin general terms. It helps us to demystify the concept of bias, to understand\nwhy we can find them everywhere and why they are sometimes useful. Second, we\nfocus over the notion of what is generally seen as negative bias, the one we\nwant to avoid in machine learning, before presenting a general zoology\ncontaining the most common of these biases. We finally conclude by looking at\nclassical methods to detect them, by means of specially crafted datasets of\ntemplates and specific algorithms, and also classical methods to mitigate them.\n","authors":["Valentin Barriere"],"pdf_url":"https://arxiv.org/pdf/2411.15051v1.pdf","comment":"Publication in Spanish in the Journal Bits de Ciencias:\n  https://www.dcc.uchile.cl/media/bits/pdfs/bits26.2-sesgos-fantasticos.pdf"},{"id":"http://arxiv.org/abs/2312.03720v2","updated":"2024-11-22T16:34:12Z","published":"2023-11-26T08:44:58Z","title":"Negotiating with LLMS: Prompt Hacks, Skill Gaps, and Reasoning Deficits","summary":"  Large language models LLMs like ChatGPT have reached the 100 Mio user barrier\nin record time and might increasingly enter all areas of our life leading to a\ndiverse set of interactions between those Artificial Intelligence models and\nhumans. While many studies have discussed governance and regulations\ndeductively from first-order principles, few studies provide an inductive,\ndata-driven lens based on observing dialogues between humans and LLMs\nespecially when it comes to non-collaborative, competitive situations that have\nthe potential to pose a serious threat to people. In this work, we conduct a\nuser study engaging over 40 individuals across all age groups in price\nnegotiations with an LLM. We explore how people interact with an LLM,\ninvestigating differences in negotiation outcomes and strategies. Furthermore,\nwe highlight shortcomings of LLMs with respect to their reasoning capabilities\nand, in turn, susceptiveness to prompt hacking, which intends to manipulate the\nLLM to make agreements that are against its instructions or beyond any\nrationality. We also show that the negotiated prices humans manage to achieve\nspan a broad range, which points to a literacy gap in effectively interacting\nwith LLMs.\n","authors":["Johannes Schneider","Steffi Haag","Leona Chandra Kruse"],"pdf_url":"https://arxiv.org/pdf/2312.03720v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00903v2","updated":"2024-11-22T16:31:25Z","published":"2024-10-01T17:46:21Z","title":"Causal Representation Learning with Generative Artificial Intelligence:\n  Application to Texts as Treatments","summary":"  In this paper, we demonstrate how to enhance the validity of causal inference\nwith unstructured high-dimensional treatments like texts, by leveraging the\npower of generative Artificial Intelligence. Specifically, we propose to use a\ndeep generative model such as large language models (LLMs) to efficiently\ngenerate treatments and use their internal representation for subsequent causal\neffect estimation. We show that the knowledge of this true internal\nrepresentation helps disentangle the treatment features of interest, such as\nspecific sentiments and certain topics, from other possibly unknown confounding\nfeatures. Unlike the existing methods, our proposed approach eliminates the\nneed to learn causal representation from the data and hence produces more\naccurate and efficient estimates. We formally establish the conditions required\nfor the nonparametric identification of the average treatment effect, propose\nan estimation strategy that avoids the violation of the overlap assumption, and\nderive the asymptotic properties of the proposed estimator through the\napplication of double machine learning. Finally, using an instrumental\nvariables approach, we extend the proposed methodology to the settings, in\nwhich the treatment feature is based on human perception rather than is assumed\nto be fixed given the treatment object. The proposed methodology is also\napplicable to text reuse where an LLM is used to regenerate the existing texts.\nWe conduct simulation and empirical studies, using the generated text data from\nan open-source LLM, Llama 3, to illustrate the advantages of our estimator over\nthe state-of-the-art causal representation learning algorithms.\n","authors":["Kosuke Imai","Kentaro Nakamura"],"pdf_url":"https://arxiv.org/pdf/2410.00903v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.08977v2","updated":"2024-11-22T16:22:35Z","published":"2024-11-13T19:08:23Z","title":"Robustness and Confounders in the Demographic Alignment of LLMs with\n  Human Perceptions of Offensiveness","summary":"  Large language models (LLMs) are known to exhibit demographic biases, yet few\nstudies systematically evaluate these biases across multiple datasets or\naccount for confounding factors. In this work, we examine LLM alignment with\nhuman annotations in five offensive language datasets, comprising approximately\n220K annotations. Our findings reveal that while demographic traits,\nparticularly race, influence alignment, these effects are inconsistent across\ndatasets and often entangled with other factors. Confounders -- such as\ndocument difficulty, annotator sensitivity, and within-group agreement --\naccount for more variation in alignment patterns than demographic traits alone.\nSpecifically, alignment increases with higher annotator sensitivity and group\nagreement, while greater document difficulty corresponds to reduced alignment.\nOur results underscore the importance of multi-dataset analyses and\nconfounder-aware methodologies in developing robust measures of demographic\nbias in LLMs.\n","authors":["Shayan Alipour","Indira Sen","Mattia Samory","Tanushree Mitra"],"pdf_url":"https://arxiv.org/pdf/2411.08977v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15041v1","updated":"2024-11-22T16:15:50Z","published":"2024-11-22T16:15:50Z","title":"mR$^2$AG: Multimodal Retrieval-Reflection-Augmented Generation for\n  Knowledge-Based VQA","summary":"  Advanced Multimodal Large Language Models (MLLMs) struggle with recent\nKnowledge-based VQA tasks, such as INFOSEEK and Encyclopedic-VQA, due to their\nlimited and frozen knowledge scope, often leading to ambiguous and inaccurate\nresponses. Thus, multimodal Retrieval-Augmented Generation (mRAG) is naturally\nintroduced to provide MLLMs with comprehensive and up-to-date knowledge,\neffectively expanding the knowledge scope. However, current mRAG methods have\ninherent drawbacks, including: 1) Performing retrieval even when external\nknowledge is not needed. 2) Lacking of identification of evidence that supports\nthe query. 3) Increasing model complexity due to additional information\nfiltering modules or rules. To address these shortcomings, we propose a novel\ngeneralized framework called \\textbf{m}ultimodal\n\\textbf{R}etrieval-\\textbf{R}eflection-\\textbf{A}ugmented \\textbf{G}eneration\n(mR$^2$AG), which achieves adaptive retrieval and useful information\nlocalization to enable answers through two easy-to-implement reflection\noperations, preventing high model complexity. In mR$^2$AG, Retrieval-Reflection\nis designed to distinguish different user queries and avoids redundant\nretrieval calls, and Relevance-Reflection is introduced to guide the MLLM in\nlocating beneficial evidence of the retrieved content and generating answers\naccordingly. In addition, mR$^2$AG can be integrated into any well-trained MLLM\nwith efficient fine-tuning on the proposed mR$^2$AG Instruction-Tuning dataset\n(mR$^2$AG-IT). mR$^2$AG significantly outperforms state-of-the-art MLLMs (e.g.,\nGPT-4v/o) and RAG-based MLLMs on INFOSEEK and Encyclopedic-VQA, while\nmaintaining the exceptional capabilities of base MLLMs across a wide range of\nVisual-dependent tasks.\n","authors":["Tao Zhang","Ziqi Zhang","Zongyang Ma","Yuxin Chen","Zhongang Qi","Chunfeng Yuan","Bing Li","Junfu Pu","Yuxuan Zhao","Zehua Xie","Jin Ma","Ying Shan","Weiming Hu"],"pdf_url":"https://arxiv.org/pdf/2411.15041v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23054v2","updated":"2024-11-22T16:04:44Z","published":"2024-10-30T14:21:33Z","title":"Controlling Language and Diffusion Models by Transporting Activations","summary":"  The increasing capabilities of large generative models and their ever more\nwidespread deployment have raised concerns about their reliability, safety, and\npotential misuse. To address these issues, recent works have proposed to\ncontrol model generation by steering model activations in order to effectively\ninduce or prevent the emergence of concepts or behaviors in the generated\noutput. In this paper we introduce Activation Transport (AcT), a general\nframework to steer activations guided by optimal transport theory that\ngeneralizes many previous activation-steering works. AcT is modality-agnostic\nand provides fine-grained control over the model behavior with negligible\ncomputational overhead, while minimally impacting model abilities. We\nexperimentally show the effectiveness and versatility of our approach by\naddressing key challenges in large language models (LLMs) and text-to-image\ndiffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate\ntoxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is,\nwe show how AcT enables fine-grained style control and concept negation.\n","authors":["Pau Rodriguez","Arno Blaas","Michal Klein","Luca Zappella","Nicholas Apostoloff","Marco Cuturi","Xavier Suau"],"pdf_url":"https://arxiv.org/pdf/2410.23054v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.06077v4","updated":"2024-11-22T15:58:28Z","published":"2023-06-05T17:22:54Z","title":"Semantically-Prompted Language Models Improve Visual Descriptions","summary":"  Language-vision models like CLIP have made significant strides in vision\ntasks, such as zero-shot image classification (ZSIC). However, generating\nspecific and expressive visual descriptions remains challenging; descriptions\nproduced by current methods are often ambiguous and lacking in granularity. To\ntackle these issues, we propose V-GLOSS: Visual Glosses, a novel method built\nupon two key ideas. The first is Semantic Prompting, which conditions a\nlanguage model on structured semantic knowledge. The second is a new\ncontrastive algorithm that elicits fine-grained distinctions between similar\nconcepts. With both ideas, we demonstrate that V-GLOSS improves visual\ndescriptions and achieves strong results in the zero-shot setting on general\nand fine-grained image-classification datasets, including ImageNet, STL-10,\nFGVC Aircraft, and Flowers 102. Moreover, these descriptive capabilities\ncontribute to enhancing image-generation performance. Finally, we introduce a\nquality-tested silver dataset with descriptions generated with V-GLOSS for all\nImageNet classes.\n","authors":["Michael Ogezi","Bradley Hauer","Grzegorz Kondrak"],"pdf_url":"https://arxiv.org/pdf/2306.06077v4.pdf","comment":"Published at NAACL 2024. See\n  https://aclanthology.org/2024.findings-naacl.267/"},{"id":"http://arxiv.org/abs/2405.05966v3","updated":"2024-11-22T15:36:32Z","published":"2024-05-09T17:59:32Z","title":"Natural Language Processing RELIES on Linguistics","summary":"  Large Language Models (LLMs) have become capable of generating highly fluent\ntext in certain languages, without modules specially designed to capture\ngrammar or semantic coherence. What does this mean for the future of linguistic\nexpertise in NLP? We highlight several aspects in which NLP (still) relies on\nlinguistics, or where linguistic thinking can illuminate new directions. We\nargue our case around the acronym RELIES that encapsulates six major facets\nwhere linguistics contributes to NLP: Resources, Evaluation, Low-resource\nsettings, Interpretability, Explanation, and the Study of language. This list\nis not exhaustive, nor is linguistics the main point of reference for every\neffort under these themes; but at a macro level, these facets highlight the\nenduring importance of studying machine systems vis-\\`a-vis systems of human\nlanguage.\n","authors":["Juri Opitz","Shira Wein","Nathan Schneider"],"pdf_url":"https://arxiv.org/pdf/2405.05966v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15008v1","updated":"2024-11-22T15:31:50Z","published":"2024-11-22T15:31:50Z","title":"Evolutionary Automata and Deep Evolutionary Computation","summary":"  Evolution by natural selection, which is one of the most compelling themes of\nmodern science, brought forth evolutionary algorithms and evolutionary\ncomputation, applying mechanisms of evolution in nature to various problems\nsolved by computers. In this paper we concentrate on evolutionary automata that\nconstitute an analogous model of evolutionary computation compared to\nwell-known evolutionary algorithms. Evolutionary automata provide a more\ncomplete dual model of evolutionary computation, similar like abstract automata\n(e.g., Turing machines) form a more formal and precise model compared to\nrecursive algorithms and their subset - evolutionary algorithms. An\nevolutionary automaton is an automaton that evolves performing evolutionary\ncomputation perhaps using an infinite number of generations. This model allows\nfor a direct modeling evolution of evolution, and leads to tremendous\nexpressiveness of evolutionary automata and evolutionary computation. This also\ngives the hint to the power of natural evolution that is self-evolving by\ninteractive feedback with the environment.\n","authors":["Eugene Eberbach"],"pdf_url":"https://arxiv.org/pdf/2411.15008v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15004v1","updated":"2024-11-22T15:26:23Z","published":"2024-11-22T15:26:23Z","title":"ScribeAgent: Towards Specialized Web Agents Using Production-Scale\n  Workflow Data","summary":"  Large Language Model (LLM) agents are rapidly improving to handle\nincreasingly complex web-based tasks. Most of these agents rely on\ngeneral-purpose, proprietary models like GPT-4 and focus on designing better\nprompts to improve their planning abilities. However, general-purpose LLMs are\nnot specifically trained to understand specialized web contexts such as HTML,\nand they often struggle with long-horizon planning. We explore an alternative\napproach that fine-tunes open-source LLMs using production-scale workflow data\ncollected from over 250 domains corresponding to 6 billion tokens. This simple\nyet effective approach shows substantial gains over prompting-based agents on\nexisting benchmarks -- ScribeAgent achieves state-of-the-art direct generation\nperformance on Mind2Web and improves the task success rate by 14.1% over the\nprevious best text-only web agents on WebArena. We further perform detailed\nablation studies on various fine-tuning design choices and provide insights\ninto LLM selection, training recipes, context window optimization, and effect\nof dataset sizes.\n","authors":["Junhong Shen","Atishay Jain","Zedian Xiao","Ishan Amlekar","Mouad Hadji","Aaron Podolny","Ameet Talwalkar"],"pdf_url":"https://arxiv.org/pdf/2411.15004v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13485v2","updated":"2024-11-22T15:24:07Z","published":"2024-11-20T17:35:21Z","title":"Utilizing Large Language Models to Synthesize Product Desirability\n  Datasets","summary":"  This research explores the application of large language models (LLMs) to\ngenerate synthetic datasets for Product Desirability Toolkit (PDT) testing, a\nkey component in evaluating user sentiment and product experience. Utilizing\ngpt-4o-mini, a cost-effective alternative to larger commercial LLMs, three\nmethods, Word+Review, Review+Word, and Supply-Word, were each used to\nsynthesize 1000 product reviews. The generated datasets were assessed for\nsentiment alignment, textual diversity, and data generation cost. Results\ndemonstrated high sentiment alignment across all methods, with Pearson\ncorrelations ranging from 0.93 to 0.97. Supply-Word exhibited the highest\ndiversity and coverage of PDT terms, although with increased generation costs.\nDespite minor biases toward positive sentiments, in situations with limited\ntest data, LLM-generated synthetic data offers significant advantages,\nincluding scalability, cost savings, and flexibility in dataset production.\n","authors":["John D. Hastings","Sherri Weitl-Harms","Joseph Doty","Zachary J. Myers","Warren Thompson"],"pdf_url":"https://arxiv.org/pdf/2411.13485v2.pdf","comment":"9 pages, 2 figures, 6 tables, updated author list"},{"id":"http://arxiv.org/abs/2411.14982v1","updated":"2024-11-22T14:41:36Z","published":"2024-11-22T14:41:36Z","title":"Large Multi-modal Models Can Interpret Features in Large Multi-modal\n  Models","summary":"  Recent advances in Large Multimodal Models (LMMs) lead to significant\nbreakthroughs in both academia and industry. One question that arises is how\nwe, as humans, can understand their internal neural representations. This paper\ntakes an initial step towards addressing this question by presenting a\nversatile framework to identify and interpret the semantics within LMMs.\nSpecifically, 1) we first apply a Sparse Autoencoder(SAE) to disentangle the\nrepresentations into human understandable features. 2) We then present an\nautomatic interpretation framework to interpreted the open-semantic features\nlearned in SAE by the LMMs themselves. We employ this framework to analyze the\nLLaVA-NeXT-8B model using the LLaVA-OV-72B model, demonstrating that these\nfeatures can effectively steer the model's behavior. Our results contribute to\na deeper understanding of why LMMs excel in specific tasks, including EQ tests,\nand illuminate the nature of their mistakes along with potential strategies for\ntheir rectification. These findings offer new insights into the internal\nmechanisms of LMMs and suggest parallels with the cognitive processes of the\nhuman brain.\n","authors":["Kaichen Zhang","Yifei Shen","Bo Li","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2411.14982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14967v1","updated":"2024-11-22T14:23:07Z","published":"2024-11-22T14:23:07Z","title":"SwissADT: An Audio Description Translation System for Swiss Languages","summary":"  Audio description (AD) is a crucial accessibility service provided to blind\npersons and persons with visual impairment, designed to convey visual\ninformation in acoustic form. Despite recent advancements in multilingual\nmachine translation research, the lack of well-crafted and time-synchronized AD\ndata impedes the development of audio description translation (ADT) systems\nthat address the needs of multilingual countries such as Switzerland.\nFurthermore, since the majority of ADT systems rely solely on text, uncertainty\nexists as to whether incorporating visual information from the corresponding\nvideo clips can enhance the quality of ADT outputs. In this work, we present\nSwissADT, the first ADT system implemented for three main Swiss languages and\nEnglish. By collecting well-crafted AD data augmented with video clips in\nGerman, French, Italian, and English, and leveraging the power of Large\nLanguage Models (LLMs), we aim to enhance information accessibility for diverse\nlanguage populations in Switzerland by automatically translating AD scripts to\nthe desired Swiss language. Our extensive experimental ADT results, composed of\nboth automatic and human evaluations of ADT quality, demonstrate the promising\ncapability of SwissADT for the ADT task. We believe that combining human\nexpertise with the generation power of LLMs can further enhance the performance\nof ADT systems, ultimately benefiting a larger multilingual target population.\n","authors":["Lukas Fischer","Yingqiang Gao","Alexa Lintner","Sarah Ebling"],"pdf_url":"https://arxiv.org/pdf/2411.14967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14962v1","updated":"2024-11-22T14:21:18Z","published":"2024-11-22T14:21:18Z","title":"LLM for Barcodes: Generating Diverse Synthetic Data for Identity\n  Documents","summary":"  Accurate barcode detection and decoding in Identity documents is crucial for\napplications like security, healthcare, and education, where reliable data\nextraction and verification are essential. However, building robust detection\nmodels is challenging due to the lack of diverse, realistic datasets an issue\noften tied to privacy concerns and the wide variety of document formats.\nTraditional tools like Faker rely on predefined templates, making them less\neffective for capturing the complexity of real-world identity documents. In\nthis paper, we introduce a new approach to synthetic data generation that uses\nLLMs to create contextually rich and realistic data without relying on\npredefined field. Using the vast knowledge LLMs have about different documents\nand content, our method creates data that reflects the variety found in real\nidentity documents. This data is then encoded into barcode and overlayed on\ntemplates for documents such as Driver's licenses, Insurance cards, Student\nIDs. Our approach simplifies the process of dataset creation, eliminating the\nneed for extensive domain knowledge or predefined fields. Compared to\ntraditional methods like Faker, data generated by LLM demonstrates greater\ndiversity and contextual relevance, leading to improved performance in barcode\ndetection models. This scalable, privacy-first solution is a big step forward\nin advancing machine learning for automated document processing and identity\nverification.\n","authors":["Hitesh Laxmichand Patel","Amit Agarwal","Bhargava Kumar","Karan Gupta","Priyaranjan Pattnayak"],"pdf_url":"https://arxiv.org/pdf/2411.14962v1.pdf","comment":"5 pages, 1 figures"},{"id":"http://arxiv.org/abs/2411.14957v1","updated":"2024-11-22T14:16:09Z","published":"2024-11-22T14:16:09Z","title":"Information Extraction from Heterogenous Documents without Ground Truth\n  Labels using Synthetic Label Generation and Knowledge Distillation","summary":"  Invoices and receipts submitted by employees are visually rich documents\n(VRDs) with textual, visual and layout information. To protect against the risk\nof fraud and abuse, it is crucial for organizations to efficiently extract\ndesired information from submitted receipts. This helps in the assessment of\nkey factors such as appropriateness of the expense claim, adherence to spending\nand transaction policies, the validity of the receipt, as well as downstream\nanomaly detection at various levels. These documents are heterogenous, with\nmultiple formats and languages, uploaded with different image qualities, and\noften do not contain ground truth labels for the efficient training of models.\nIn this paper we propose Task Aware Instruction-based Labelling (TAIL), a\nmethod for synthetic label generation in VRD corpuses without labels, and\nfine-tune a multimodal Visually Rich Document Understanding Model (VRDU) on\nTAIL labels using response-based knowledge distillation without using the\nteacher model's weights or training dataset to conditionally generate\nannotations in the appropriate format. Using a benchmark external dataset where\nground truth labels are available, we demonstrate conditions under which our\napproach performs at par with Claude 3 Sonnet through empirical studies. We\nthen show that the resulting model performs at par or better on the internal\nexpense documents of a large multinational organization than state-of-the-art\nLMM (large multimodal model) Claude 3 Sonnet while being 85% less costly and\n~5X faster, and outperforms layout-aware baselines by more than 10% in Average\nNormalized Levenshtein Similarity (ANLS) scores due to its ability to reason\nand extract information from rare formats. Finally, we illustrate the usage of\nour approach in overpayment prevention.\n","authors":["Aniket Bhattacharyya","Anurag Tripathi"],"pdf_url":"https://arxiv.org/pdf/2411.14957v1.pdf","comment":"Accepted to WACV 2025"},{"id":"http://arxiv.org/abs/2411.02451v2","updated":"2024-11-22T14:11:13Z","published":"2024-11-03T10:06:14Z","title":"High-performance automated abstract screening with large language model\n  ensembles","summary":"  Large language models (LLMs) excel in tasks requiring processing and\ninterpretation of input text. Abstract screening is a labour-intensive\ncomponent of systematic review involving repetitive application of inclusion\nand exclusion criteria on a large volume of studies identified by a literature\nsearch. Here, LLMs (GPT-3.5 Turbo, GPT-4 Turbo, GPT-4o, Llama 3 70B, Gemini 1.5\nPro, and Claude Sonnet 3.5) were trialled on systematic reviews in a full issue\nof the Cochrane Library to evaluate their accuracy in zero-shot binary\nclassification for abstract screening. Trials over a subset of 800 records\nidentified optimal prompting strategies and demonstrated superior performance\nof LLMs to human researchers in terms of sensitivity (LLM-max = 1.000,\nhuman-max = 0.775), precision (LLM-max = 0.927, human-max = 0.911), and\nbalanced accuracy (LLM-max = 0.904, human-max = 0.865). The best performing\nLLM-prompt combinations were trialled across every replicated search result (n\n= 119,691), and exhibited consistent sensitivity (range 0.756-1.000) but\ndiminished precision (range 0.004-0.096). 66 LLM-human and LLM-LLM ensembles\nexhibited perfect sensitivity with a maximal precision of 0.458, with less\nobserved performance drop in larger trials. Significant variation in\nperformance was observed between reviews, highlighting the importance of\ndomain-specific validation before deployment. LLMs may reduce the human labour\ncost of systematic review with maintained or improved accuracy and sensitivity.\nSystematic review is the foundation of evidence synthesis across academic\ndisciplines, including evidence-based medicine, and LLMs may increase the\nefficiency and quality of this mode of research.\n","authors":["Rohan Sanghera","Arun James Thirunavukarasu","Marc El Khoury","Jessica O'Logbon","Yuqing Chen","Archie Watt","Mustafa Mahmood","Hamid Butt","George Nishimura","Andrew Soltan"],"pdf_url":"https://arxiv.org/pdf/2411.02451v2.pdf","comment":"RS and AJT are joint-first authors"},{"id":"http://arxiv.org/abs/2411.14901v1","updated":"2024-11-22T12:46:50Z","published":"2024-11-22T12:46:50Z","title":"ReVisionLLM: Recursive Vision-Language Model for Temporal Grounding in\n  Hour-Long Videos","summary":"  Large language models (LLMs) excel at retrieving information from lengthy\ntext, but their vision-language counterparts (VLMs) face difficulties with\nhour-long videos, especially for temporal grounding. Specifically, these VLMs\nare constrained by frame limitations, often losing essential temporal details\nneeded for accurate event localization in extended video content. We propose\nReVisionLLM, a recursive vision-language model designed to locate events in\nhour-long videos. Inspired by human search strategies, our model initially\ntargets broad segments of interest, progressively revising its focus to\npinpoint exact temporal boundaries. Our model can seamlessly handle videos of\nvastly different lengths, from minutes to hours. We also introduce a\nhierarchical training strategy that starts with short clips to capture distinct\nevents and progressively extends to longer videos. To our knowledge,\nReVisionLLM is the first VLM capable of temporal grounding in hour-long videos,\noutperforming previous state-of-the-art methods across multiple datasets by a\nsignificant margin (+2.6% R1@0.1 on MAD). The code is available at\nhttps://github.com/Tanveer81/ReVisionLLM.\n","authors":["Tanveer Hannan","Md Mohaiminul Islam","Jindong Gu","Thomas Seidl","Gedas Bertasius"],"pdf_url":"https://arxiv.org/pdf/2411.14901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.00925v6","updated":"2024-11-22T12:42:14Z","published":"2023-07-03T10:53:05Z","title":"Automatic Design of Semantic Similarity Ensembles Using Grammatical\n  Evolution","summary":"  Semantic similarity measures are widely used in natural language processing\nto catalyze various computer-related tasks. However, no single semantic\nsimilarity measure is the most appropriate for all tasks, and researchers often\nuse ensemble strategies to ensure performance. This research work proposes a\nmethod for automatically designing semantic similarity ensembles. In fact, our\nproposed method uses grammatical evolution, for the first time, to\nautomatically select and aggregate measures from a pool of candidates to create\nan ensemble that maximizes correlation to human judgment. The method is\nevaluated on several benchmark datasets and compared to state-of-the-art\nensembles, showing that it can significantly improve similarity assessment\naccuracy and outperform existing methods in some cases. As a result, our\nresearch demonstrates the potential of using grammatical evolution to\nautomatically compare text and prove the benefits of using ensembles for\nsemantic similarity tasks. The source code that illustrates our approach can be\ndownloaded from https://github.com/jorge-martinez-gil/sesige.\n","authors":["Jorge Martinez-Gil"],"pdf_url":"https://arxiv.org/pdf/2307.00925v6.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2411.14896v1","updated":"2024-11-22T12:37:41Z","published":"2024-11-22T12:37:41Z","title":"Evaluating LLM Prompts for Data Augmentation in Multi-label\n  Classification of Ecological Texts","summary":"  Large language models (LLMs) play a crucial role in natural language\nprocessing (NLP) tasks, improving the understanding, generation, and\nmanipulation of human language across domains such as translating, summarizing,\nand classifying text. Previous studies have demonstrated that instruction-based\nLLMs can be effectively utilized for data augmentation to generate diverse and\nrealistic text samples. This study applied prompt-based data augmentation to\ndetect mentions of green practices in Russian social media. Detecting green\npractices in social media aids in understanding their prevalence and helps\nformulate recommendations for scaling eco-friendly actions to mitigate\nenvironmental issues. We evaluated several prompts for augmenting texts in a\nmulti-label classification task, either by rewriting existing datasets using\nLLMs, generating new data, or combining both approaches. Our results revealed\nthat all strategies improved classification performance compared to the models\nfine-tuned only on the original dataset, outperforming baselines in most cases.\nThe best results were obtained with the prompt that paraphrased the original\ntext while clearly indicating the relevant categories.\n","authors":["Anna Glazkova","Olga Zakharova"],"pdf_url":"https://arxiv.org/pdf/2411.14896v1.pdf","comment":"Ivannikov ISPRAS Open Conference (ISPRAS) 2024"},{"id":"http://arxiv.org/abs/2406.13677v2","updated":"2024-11-22T12:03:34Z","published":"2024-06-19T16:30:58Z","title":"Leveraging Large Language Models to Measure Gender Representation Bias\n  in Gendered Language Corpora","summary":"  Gender bias in text corpora that are used for a variety of natural language\nprocessing (NLP) tasks, such as for training large language models (LLMs), can\nlead to the perpetuation and amplification of societal inequalities. This\nphenomenon is particularly pronounced in gendered languages like Spanish or\nFrench, where grammatical structures inherently encode gender, making the bias\nanalysis more challenging. A first step in quantifying gender bias in text\nentails computing biases in gender representation, i.e., differences in the\nprevalence of words referring to males vs. females. Existing methods to measure\ngender representation bias in text corpora have mainly been proposed for\nEnglish and do not generalize to gendered languages due to the intrinsic\nlinguistic differences between English and gendered languages. This paper\nintroduces a novel methodology that leverages the contextual understanding\ncapabilities of LLMs to quantitatively measure gender representation bias in\nSpanish corpora. By utilizing LLMs to identify and classify gendered nouns and\npronouns in relation to their reference to human entities, our approach\nprovides a robust analysis of gender representation bias in gendered languages.\nWe empirically validate our method on four widely-used benchmark datasets,\nuncovering significant gender prevalence disparities with a male-to-female\nratio ranging from 4:1 to 6:1. These findings demonstrate the value of our\nmethodology for bias quantification in gendered language corpora and suggest\nits application in NLP, contributing to the development of more equitable\nlanguage technologies.\n","authors":["Erik Derner","Sara Sansalvador de la Fuente","Yoan Gutiérrez","Paloma Moreda","Nuria Oliver"],"pdf_url":"https://arxiv.org/pdf/2406.13677v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14880v1","updated":"2024-11-22T12:01:04Z","published":"2024-11-22T12:01:04Z","title":"Leveraging Hierarchical Prototypes as the Verbalizer for Implicit\n  Discourse Relation Recognition","summary":"  Implicit discourse relation recognition involves determining relationships\nthat hold between spans of text that are not linked by an explicit discourse\nconnective. In recent years, the pre-train, prompt, and predict paradigm has\nemerged as a promising approach for tackling this task. However, previous work\nsolely relied on manual verbalizers for implicit discourse relation\nrecognition, which suffer from issues of ambiguity and even incorrectness. To\novercome these limitations, we leverage the prototypes that capture certain\nclass-level semantic features and the hierarchical label structure for\ndifferent classes as the verbalizer. We show that our method improves on\ncompetitive baselines. Besides, our proposed approach can be extended to enable\nzero-shot cross-lingual learning, facilitating the recognition of discourse\nrelations in languages with scarce resources. These advancement validate the\npracticality and versatility of our approach in addressing the issues of\nimplicit discourse relation recognition across different languages.\n","authors":["Wanqiu Long","Bonnie Webber"],"pdf_url":"https://arxiv.org/pdf/2411.14880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14877v1","updated":"2024-11-22T11:59:15Z","published":"2024-11-22T11:59:15Z","title":"Astro-HEP-BERT: A bidirectional language model for studying the meanings\n  of concepts in astrophysics and high energy physics","summary":"  I present Astro-HEP-BERT, a transformer-based language model specifically\ndesigned for generating contextualized word embeddings (CWEs) to study the\nmeanings of concepts in astrophysics and high-energy physics. Built on a\ngeneral pretrained BERT model, Astro-HEP-BERT underwent further training over\nthree epochs using the Astro-HEP Corpus, a dataset I curated from 21.84 million\nparagraphs extracted from more than 600,000 scholarly articles on arXiv, all\nbelonging to at least one of these two scientific domains. The project\ndemonstrates both the effectiveness and feasibility of adapting a bidirectional\ntransformer for applications in the history, philosophy, and sociology of\nscience (HPSS). The entire training process was conducted using freely\navailable code, pretrained weights, and text inputs, completed on a single\nMacBook Pro Laptop (M2/96GB). Preliminary evaluations indicate that\nAstro-HEP-BERT's CWEs perform comparably to domain-adapted BERT models trained\nfrom scratch on larger datasets for domain-specific word sense disambiguation\nand induction and related semantic change analyses. This suggests that\nretraining general language models for specific scientific domains can be a\ncost-effective and efficient strategy for HPSS researchers, enabling high\nperformance without the need for extensive training from scratch.\n","authors":["Arno Simons"],"pdf_url":"https://arxiv.org/pdf/2411.14877v1.pdf","comment":"7 pages, 4 figures, 1 table"},{"id":"http://arxiv.org/abs/2411.14871v1","updated":"2024-11-22T11:45:33Z","published":"2024-11-22T11:45:33Z","title":"Prioritize Denoising Steps on Diffusion Model Preference Alignment via\n  Explicit Denoised Distribution Estimation","summary":"  Diffusion models have shown remarkable success in text-to-image generation,\nmaking alignment methods for these models increasingly important. A key\nchallenge is the sparsity of preference labels, which are typically available\nonly at the terminal of denoising trajectories. This raises the issue of how to\nassign credit across denoising steps based on these sparse labels. In this\npaper, we propose Denoised Distribution Estimation (DDE), a novel method for\ncredit assignment. Unlike previous approaches that rely on auxiliary models or\nhand-crafted schemes, DDE derives its strategy more explicitly. The proposed\nDDE directly estimates the terminal denoised distribution from the perspective\nof each step. It is equipped with two estimation strategies and capable of\nrepresenting the entire denoising trajectory with a single model inference.\nTheoretically and empirically, we show that DDE prioritizes optimizing the\nmiddle part of the denoising trajectory, resulting in a novel and effective\ncredit assignment scheme. Extensive experiments demonstrate that our approach\nachieves superior performance, both quantitatively and qualitatively.\n","authors":["Dingyuan Shi","Yong Wang","Hangyu Li","Xiangxiang Chu"],"pdf_url":"https://arxiv.org/pdf/2411.14871v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08713v2","updated":"2024-11-22T11:25:34Z","published":"2024-07-11T17:50:09Z","title":"GTA: A Benchmark for General Tool Agents","summary":"  Significant focus has been placed on integrating large language models (LLMs)\nwith various tools in developing general-purpose agents. This poses a challenge\nto LLMs' tool-use capabilities. However, there are evident gaps between\nexisting tool-use evaluations and real-world scenarios. Current evaluations\noften use AI-generated queries, single-step tasks, dummy tools, and text-only\ninteractions, failing to reveal the agents' real-world problem-solving\nabilities effectively. To address this, we propose GTA, a benchmark for General\nTool Agents, featuring three main aspects: (i) Real user queries: human-written\nqueries with simple real-world objectives but implicit tool-use, requiring the\nLLM to reason the suitable tools and plan the solution steps. (ii) Real\ndeployed tools: an evaluation platform equipped with tools across perception,\noperation, logic, and creativity categories to evaluate the agents' actual task\nexecution performance. (iii) Real multimodal inputs: authentic image files,\nsuch as spatial scenes, web page screenshots, tables, code snippets, and\nprinted/handwritten materials, used as the query contexts to align with\nreal-world scenarios closely. We design 229 real-world tasks and executable\ntool chains to evaluate mainstream LLMs. Our findings show that real-world user\nqueries are challenging for existing LLMs, with GPT-4 completing less than 50%\nof the tasks and most LLMs achieving below 25%. This evaluation reveals the\nbottlenecks in the tool-use capabilities of current LLMs in real-world\nscenarios, which provides future direction for advancing general-purpose tool\nagents. The code and dataset are available at\nhttps://github.com/open-compass/GTA.\n","authors":["Jize Wang","Zerun Ma","Yining Li","Songyang Zhang","Cailian Chen","Kai Chen","Xinyi Le"],"pdf_url":"https://arxiv.org/pdf/2407.08713v2.pdf","comment":"Github repo: https://github.com/open-compass/GTA"},{"id":"http://arxiv.org/abs/2409.15371v4","updated":"2024-11-22T10:40:35Z","published":"2024-09-19T10:26:42Z","title":"Bone: Block-Affine Adaptation of Large Language Models","summary":"  Low-Rank Adaptation (LoRA) has achieved remarkable training results by\nfreezing the original weights and training only low-rank matrices, establishing\nitself as the predominant fine-tuning method for LLMs. In pursuit of\nperformance closer to full-parameter training, a series of LoRA variants have\nemerged, such as LoRA+, PISSA, Olora, and LoRA-GA. This paper introduces a\nnovel PEFT technique distinct from LoRA, called Block-Affine Adaptation (Bone).\nBy dividing the original weights into multiple subspaces that share a single\nmatrix for weight updates, Bone simplifies the process by requiring the\ntrainable matrix to be initialized to zero, eliminating the need for complex\ninitialization as in some LoRA variants. Compared to LoRA, Bone significantly\nreduces memory usage and achieves faster computation. Evaluation of both NLU\nand NLG tasks demonstrates that Bone substantially outperforms LoRA and its\nvariants. Inspired by Pissa, we further proposed the ``Weight Guide'' theory to\nbetter utilize the information from the original weights. By integrating\n``Weight Guide'' with Bone, we developed a new structure called Block-Affine\nTransformation (Bat), and ablation experiments confirmed the effectiveness of\n``Weight Guide''.\n","authors":["Jiale Kang"],"pdf_url":"https://arxiv.org/pdf/2409.15371v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03817v2","updated":"2024-11-22T10:24:44Z","published":"2024-11-06T10:35:11Z","title":"From Novice to Expert: LLM Agent Policy Optimization via Step-wise\n  Reinforcement Learning","summary":"  The outstanding capabilities of large language models (LLMs) render them a\ncrucial component in various autonomous agent systems. While traditional\nmethods depend on the inherent knowledge of LLMs without fine-tuning, more\nrecent approaches have shifted toward the reinforcement learning strategy to\nfurther enhance agents' ability to solve complex interactive tasks with\nenvironments and tools. However, previous approaches are constrained by the\nsparse reward issue, where existing datasets solely provide a final scalar\nreward for each multi-step reasoning chain, potentially leading to\nineffectiveness and inefficiency in policy learning. In this paper, we\nintroduce StepAgent, which utilizes step-wise reward to optimize the agent's\nreinforcement learning process. Inheriting the spirit of novice-to-expert\ntheory, we first compare the actions of the expert and the agent to\nautomatically generate intermediate rewards for fine-grained optimization.\nAdditionally, we propose implicit-reward and inverse reinforcement learning\ntechniques to facilitate agent reflection and policy adjustment. Further\ntheoretical analysis demonstrates that the action distribution of the agent can\nconverge toward the expert action distribution over multiple training cycles.\nExperimental results across various datasets indicate that StepAgent\noutperforms existing baseline methods.\n","authors":["Zhirui Deng","Zhicheng Dou","Yutao Zhu","Ji-Rong Wen","Ruibin Xiong","Mang Wang","Weipeng Chen"],"pdf_url":"https://arxiv.org/pdf/2411.03817v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14832v1","updated":"2024-11-22T10:10:53Z","published":"2024-11-22T10:10:53Z","title":"VisGraphVar: A Benchmark Generator for Assessing Variability in Graph\n  Analysis Using Large Vision-Language Models","summary":"  The fast advancement of Large Vision-Language Models (LVLMs) has shown\nimmense potential. These models are increasingly capable of tackling abstract\nvisual tasks. Geometric structures, particularly graphs with their inherent\nflexibility and complexity, serve as an excellent benchmark for evaluating\nthese models' predictive capabilities. While human observers can readily\nidentify subtle visual details and perform accurate analyses, our investigation\nreveals that state-of-the-art LVLMs exhibit consistent limitations in specific\nvisual graph scenarios, especially when confronted with stylistic variations.\nIn response to these challenges, we introduce VisGraphVar (Visual Graph\nVariability), a customizable benchmark generator able to produce graph images\nfor seven distinct task categories (detection, classification, segmentation,\npattern recognition, link prediction, reasoning, matching), designed to\nsystematically evaluate the strengths and limitations of individual LVLMs. We\nuse VisGraphVar to produce 990 graph images and evaluate six LVLMs, employing\ntwo distinct prompting strategies, namely zero-shot and chain-of-thought. The\nfindings demonstrate that variations in visual attributes of images (e.g., node\nlabeling and layout) and the deliberate inclusion of visual imperfections, such\nas overlapping nodes, significantly affect model performance. This research\nemphasizes the importance of a comprehensive evaluation across graph-related\ntasks, extending beyond reasoning alone. VisGraphVar offers valuable insights\nto guide the development of more reliable and robust systems capable of\nperforming advanced visual graph analysis.\n","authors":["Camilo Chacón Sartori","Christian Blum","Filippo Bistaffa"],"pdf_url":"https://arxiv.org/pdf/2411.14832v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14811v1","updated":"2024-11-22T09:12:02Z","published":"2024-11-22T09:12:02Z","title":"Fine-Grained Alignment in Vision-and-Language Navigation through\n  Bayesian Optimization","summary":"  This paper addresses the challenge of fine-grained alignment in\nVision-and-Language Navigation (VLN) tasks, where robots navigate realistic 3D\nenvironments based on natural language instructions. Current approaches use\ncontrastive learning to align language with visual trajectory sequences.\nNevertheless, they encounter difficulties with fine-grained vision negatives.\nTo enhance cross-modal embeddings, we introduce a novel Bayesian\nOptimization-based adversarial optimization framework for creating fine-grained\ncontrastive vision samples. To validate the proposed methodology, we conduct a\nseries of experiments to assess the effectiveness of the enriched embeddings on\nfine-grained vision negatives. We conduct experiments on two common VLN\nbenchmarks R2R and REVERIE, experiments on the them demonstrate that these\nembeddings benefit navigation, and can lead to a promising performance\nenhancement. Our source code and trained models are available at:\nhttps://anonymous.4open.science/r/FGVLN.\n","authors":["Yuhang Song","Mario Gianni","Chenguang Yang","Kunyang Lin","Te-Chuan Chiu","Anh Nguyen","Chun-Yi Lee"],"pdf_url":"https://arxiv.org/pdf/2411.14811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14807v1","updated":"2024-11-22T09:08:36Z","published":"2024-11-22T09:08:36Z","title":"Harlequin: Color-driven Generation of Synthetic Data for Referring\n  Expression Comprehension","summary":"  Referring Expression Comprehension (REC) aims to identify a particular object\nin a scene by a natural language expression, and is an important topic in\nvisual language understanding. State-of-the-art methods for this task are based\non deep learning, which generally requires expensive and manually labeled\nannotations. Some works tackle the problem with limited-supervision learning or\nrelying on Large Vision and Language Models. However, the development of\ntechniques to synthesize labeled data is overlooked. In this paper, we propose\na novel framework that generates artificial data for the REC task, taking into\naccount both textual and visual modalities. At first, our pipeline processes\nexisting data to create variations in the annotations. Then, it generates an\nimage using altered annotations as guidance. The result of this pipeline is a\nnew dataset, called Harlequin, made by more than 1M queries. This approach\neliminates manual data collection and annotation, enabling scalability and\nfacilitating arbitrary complexity. We pre-train three REC models on Harlequin,\nthen fine-tuned and evaluated on human-annotated datasets. Our experiments show\nthat the pre-training on artificial data is beneficial for performance.\n","authors":["Luca Parolari","Elena Izzo","Lamberto Ballan"],"pdf_url":"https://arxiv.org/pdf/2411.14807v1.pdf","comment":"Accepted to ICPR 2024"},{"id":"http://arxiv.org/abs/2410.18808v2","updated":"2024-11-22T09:00:10Z","published":"2024-10-24T14:55:09Z","title":"Delving into the Reversal Curse: How Far Can Large Language Models\n  Generalize?","summary":"  While large language models (LLMs) showcase unprecedented capabilities, they\nalso exhibit certain inherent limitations when facing seemingly trivial tasks.\nA prime example is the recently debated \"reversal curse\", which surfaces when\nmodels, having been trained on the fact \"A is B\", struggle to generalize this\nknowledge to infer that \"B is A\". In this paper, we examine the manifestation\nof the reversal curse across various tasks and delve into both the\ngeneralization abilities and the problem-solving mechanisms of LLMs. This\ninvestigation leads to a series of significant insights: (1) LLMs are able to\ngeneralize to \"B is A\" when both A and B are presented in the context as in the\ncase of a multiple-choice question. (2) This generalization ability is highly\ncorrelated to the structure of the fact \"A is B\" in the training documents. For\nexample, this generalization only applies to biographies structured in \"[Name]\nis [Description]\" but not to \"[Description] is [Name]\". (3) We propose and\nverify the hypothesis that LLMs possess an inherent bias in fact recalling\nduring knowledge application, which explains and underscores the importance of\nthe document structure to successful learning. (4) The negative impact of this\nbias on the downstream performance of LLMs can hardly be mitigated through\ntraining alone. These findings offer a novel perspective on interpreting LLMs'\ngeneralization through their intrinsic mechanisms and provide insights for\ndeveloping more effective learning methods. Our code and data are available at\nhttps://github.com/alibaba/thinking_bias.git.\n","authors":["Zhengkai Lin","Zhihang Fu","Kai Liu","Liang Xie","Binbin Lin","Wenxiao Wang","Deng Cai","Yue Wu","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2410.18808v2.pdf","comment":"Accepted at NeurIPS 2024. Our code and data are available at\n  https://github.com/alibaba/thinking_bias.git"},{"id":"http://arxiv.org/abs/2411.10083v2","updated":"2024-11-22T08:57:42Z","published":"2024-11-15T10:01:52Z","title":"Xmodel-1.5: An 1B-scale Multilingual LLM","summary":"  We introduce Xmodel-1.5, a 1-billion-parameter multilingual large language\nmodel pretrained on 2 trillion tokens, designed for balanced performance and\nscalability. Unlike most large models that use the BPE tokenizer, Xmodel-1.5\nemploys a custom unigram tokenizer with 65,280 tokens, optimizing both\nefficiency and accuracy. The model delivers competitive results across multiple\nlanguages, including Thai, Arabic, French, Chinese, and English, outperforming\nAlibaba's PolyLM-1.7B on respective evaluation datasets. Xmodel-1.5 excels in\nbenchmarks like mMMLU and PIQA, and achieves state-of-the-art results in Thai.\nTo support low-resource language research, we release Xdata_Thai, a\nThai-specific evaluation dataset featuring unique linguistic challenges such as\ngendered particles and idioms. While the model demonstrates strong performance,\nthere is still room for improvement in handling culturally specific nuances. We\nhope this work contributes to advancements in multilingual AI research. Models\nand code are publicly available on GitHub at\nhttps://github.com/XiaoduoAILab/XmodelLM-1.5\n","authors":["Wang Qun","Liu Yang","Lin Qingquan","Jiang Ling"],"pdf_url":"https://arxiv.org/pdf/2411.10083v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06387v2","updated":"2024-11-22T08:54:17Z","published":"2024-11-10T08:11:05Z","title":"Self-Training Meets Consistency: Improving LLMs' Reasoning With\n  Consistency-Driven Rationale Evaluation","summary":"  Self-training approach for large language models (LLMs) improves reasoning\nabilities by training the models on their self-generated rationales. Previous\napproaches have labeled rationales that produce correct answers for a given\nquestion as appropriate for training. However, a single measure risks\nmisjudging rationale quality, leading the models to learn flawed reasoning\npatterns. To address this issue, we propose CREST (Consistency-driven Rationale\nEvaluation for Self-Training), a self-training framework that further evaluates\neach rationale through follow-up questions and leverages this evaluation to\nguide its training. Specifically, we introduce two methods: (1) filtering out\nrationales that frequently result in incorrect answers on follow-up questions\nand (2) preference learning based on mixed preferences from rationale\nevaluation results of both original and follow-up questions. Experiments on\nthree question-answering datasets using open LLMs show that CREST not only\nimproves the logical robustness and correctness of rationales but also improves\nreasoning abilities compared to previous self-training approaches.\n","authors":["Jaehyeok Lee","Keisuke Sakaguchi","JinYeong Bak"],"pdf_url":"https://arxiv.org/pdf/2411.06387v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2411.14797v1","updated":"2024-11-22T08:48:30Z","published":"2024-11-22T08:48:30Z","title":"Continual SFT Matches Multimodal RLHF with Negative Supervision","summary":"  Multimodal RLHF usually happens after supervised finetuning (SFT) stage to\ncontinually improve vision-language models' (VLMs) comprehension. Conventional\nwisdom holds its superiority over continual SFT during this preference\nalignment stage. In this paper, we observe that the inherent value of\nmultimodal RLHF lies in its negative supervision, the logit of the rejected\nresponses. We thus propose a novel negative supervised finetuning (nSFT)\napproach that fully excavates these information resided. Our nSFT disentangles\nthis negative supervision in RLHF paradigm, and continually aligns VLMs with a\nsimple SFT loss. This is more memory efficient than multimodal RLHF where 2\n(e.g., DPO) or 4 (e.g., PPO) large VLMs are strictly required. The\neffectiveness of nSFT is rigorously proved by comparing it with various\nmultimodal RLHF approaches, across different dataset sources, base VLMs and\nevaluation metrics. Besides, fruitful of ablations are provided to support our\nhypothesis. We hope this paper will stimulate further research to properly\nalign large vision language models.\n","authors":["Ke Zhu","Yu Wang","Yanpeng Sun","Qiang Chen","Jiangjiang Liu","Gang Zhang","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2411.14797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14795v1","updated":"2024-11-22T08:35:35Z","published":"2024-11-22T08:35:35Z","title":"De-biased Multimodal Electrocardiogram Analysis","summary":"  Multimodal large language models (MLLMs) are increasingly being applied in\nthe medical field, particularly in medical imaging. However, developing MLLMs\nfor ECG signals, which are crucial in clinical settings, has been a significant\nchallenge beyond medical imaging. Previous studies have attempted to address\nthis by converting ECGs into several text tags using an external classifier in\na training-free manner. However, this approach significantly compresses the\ninformation in ECGs and underutilizes the reasoning capabilities of LLMs. In\nthis work, we directly feed the embeddings of ECGs into the LLM through a\nprojection layer, retaining more information about ECGs and better leveraging\nthe reasoning abilities of LLMs. Our method can also effectively handle a\ncommon situation in clinical practice where it is necessary to compare two ECGs\ntaken at different times. Recent studies found that MLLMs may rely solely on\ntext input to provide answers, ignoring inputs from other modalities. We\nanalyzed this phenomenon from a causal perspective in the context of ECG MLLMs\nand discovered that the confounder, severity of illness, introduces a spurious\ncorrelation between the question and answer, leading the model to rely on this\nspurious correlation and ignore the ECG input. Such models do not comprehend\nthe ECG input and perform poorly in adversarial tests where different\nexpressions of the same question are used in the training and testing sets. We\ndesigned a de-biased pre-training method to eliminate the confounder's effect\naccording to the theory of backdoor adjustment. Our model performed well on the\nECG-QA task under adversarial testing and demonstrated zero-shot capabilities.\nAn interesting random ECG test further validated that our model effectively\nunderstands and utilizes the input ECG signal.\n","authors":["Haitao Li","Ziyu Li","Yiheng Mao","Ziyi Liu","Zhoujian Sun","Zhengxing Huang"],"pdf_url":"https://arxiv.org/pdf/2411.14795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14794v1","updated":"2024-11-22T08:33:36Z","published":"2024-11-22T08:33:36Z","title":"VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained\n  Video Reasoning via Core Frame Selection","summary":"  The advancement of Large Vision Language Models (LVLMs) has significantly\nimproved multimodal understanding, yet challenges remain in video reasoning\ntasks due to the scarcity of high-quality, large-scale datasets. Existing video\nquestion-answering (VideoQA) datasets often rely on costly manual annotations\nwith insufficient granularity or automatic construction methods with redundant\nframe-by-frame analysis, limiting their scalability and effectiveness for\ncomplex reasoning. To address these challenges, we introduce VideoEspresso, a\nnovel dataset that features VideoQA pairs preserving essential spatial details\nand temporal coherence, along with multimodal annotations of intermediate\nreasoning steps. Our construction pipeline employs a semantic-aware method to\nreduce redundancy, followed by generating QA pairs using GPT-4o. We further\ndevelop video Chain-of-Thought (CoT) annotations to enrich reasoning processes,\nguiding GPT-4o in extracting logical relationships from QA pairs and video\ncontent. To exploit the potential of high-quality VideoQA pairs, we propose a\nHybrid LVLMs Collaboration framework, featuring a Frame Selector and a\ntwo-stage instruction fine-tuned reasoning LVLM. This framework adaptively\nselects core frames and performs CoT reasoning using multimodal evidence.\nEvaluated on our proposed benchmark with 14 tasks against 9 popular LVLMs, our\nmethod outperforms existing baselines on most tasks, demonstrating superior\nvideo reasoning capabilities. Our code and dataset will be released at:\nhttps://github.com/hshjerry/VideoEspresso\n","authors":["Songhao Han","Wei Huang","Hairong Shi","Le Zhuo","Xiu Su","Shifeng Zhang","Xu Zhou","Xiaojuan Qi","Yue Liao","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2411.14794v1.pdf","comment":"14 pages, 14 figures"},{"id":"http://arxiv.org/abs/2411.14790v1","updated":"2024-11-22T08:21:03Z","published":"2024-11-22T08:21:03Z","title":"KBAda: Efficient Self Adaptation on Specific Knowledge Bases","summary":"  Humans can utilize techniques to quickly acquire knowledge from specific\nmaterials in advance, such as creating self-assessment questions, enabling us\nto achieving related tasks more efficiently. In contrast, large language models\n(LLMs) usually relies on retrieval-augmented generation to exploit knowledge\nmaterials in an instant manner, or requires external signals such as human\npreference data and stronger LLM annotations to conduct knowledge adaptation.\nTo unleash the self-learning potential of LLMs, we propose KBAda, an approach\ndesigned for efficient adaptation to downstream tasks involving knowledge\nbases. Our method utilizes iterative training with self-annotated data such as\nQ&A pairs and revision suggestions, enabling the model to grasp the knowledge\ncontent efficiently. Experimental results on multiple datasets demonstrate the\neffectiveness of our approach, significantly boosting model performance in\ndownstream tasks that require specific knowledge at a low cost. Notably, our\napproach achieves over 90% of the performance improvement that can be obtained\nby using GPT-4-turbo annotation, while relying entirely on self-supervision. We\nrelease our experimental data, models, and process analyses to the community\nfor further exploration (https://github.com/thunlp/KBAda).\n","authors":["Zheni Zeng","Yuxuan Chen","Shi Yu","Yukun Yan","Zhenghao Liu","Shuo Wang","Xu Han","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2411.14790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.09944v2","updated":"2024-11-22T06:44:22Z","published":"2024-11-15T04:44:34Z","title":"SlimLM: An Efficient Small Language Model for On-Device Document\n  Assistance","summary":"  While small language models (SLMs) show promises for mobile deployment, their\nreal-world performance and applications on smartphones remains underexplored.\nWe present SlimLM, a series of SLMs optimized for document assistance tasks on\nmobile devices. Through extensive experiments on a Samsung Galaxy S24, we\nidentify the optimal trade-offs between model size (ranging from 125M to 7B\nparameters), context length, and inference time for efficient on-device\nprocessing. SlimLM is pre-trained on SlimPajama-627B and fine-tuned on\nDocAssist, our constructed dataset for summarization, question answering and\nsuggestion tasks. Our smallest model demonstrates efficient performance on S24,\nwhile larger variants offer enhanced capabilities within mobile constraints. We\nevaluate SlimLM against existing SLMs, showing comparable or superior\nperformance and offering a benchmark for future research in on-device language\nmodels. We also provide an Android application, offering practical insights\ninto SLM deployment. Our findings provide valuable insights and illuminate the\ncapabilities of running advanced language models on high-end smartphones,\npotentially reducing server costs and enhancing privacy through on-device\nprocessing.\n","authors":["Thang M. Pham","Phat T. Nguyen","Seunghyun Yoon","Viet Dac Lai","Franck Dernoncourt","Trung Bui"],"pdf_url":"https://arxiv.org/pdf/2411.09944v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00627v2","updated":"2024-11-22T06:19:35Z","published":"2024-06-02T06:09:56Z","title":"Prompt Framework for Role-playing: Generation and Evaluation","summary":"  Large language models (LLMs) exhibit impressive proficiency in natural\nlanguage generation, understanding user instructions, and emulating human-like\nlanguage use, which has led to significant interest in their application to\nrole-playing scenarios. However, the manual collection of role-specific script\ndata and the evaluation of model performance are resource-intensive processes.\nThis project introduces a prompt-based framework designed to leverage GPT's\ncapabilities for the generation of role-playing dialogue datasets and the\nevaluation of role-playing performance. To validate the effectiveness of the\nGPT-based generation and evaluation, we further incorporate the recall-oriented\nRouge-L metric, providing an additional quantitative measure of performance.\n","authors":["Xun Liu","Zhengwei Ni"],"pdf_url":"https://arxiv.org/pdf/2406.00627v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19544v3","updated":"2024-11-22T05:55:58Z","published":"2024-05-29T22:12:52Z","title":"One-Shot Safety Alignment for Large Language Models via Optimal\n  Dualization","summary":"  The growing safety concerns surrounding large language models raise an urgent\nneed to align them with diverse human preferences to simultaneously enhance\ntheir helpfulness and safety. A promising approach is to enforce safety\nconstraints through Reinforcement Learning from Human Feedback (RLHF). For such\nconstrained RLHF, typical Lagrangian-based primal-dual policy optimization\nmethods are computationally expensive and often unstable. This paper presents a\nperspective of dualization that reduces constrained alignment to an equivalent\nunconstrained alignment problem. We do so by pre-optimizing a smooth and convex\ndual function that has a closed form. This shortcut eliminates the need for\ncumbersome primal-dual policy iterations, greatly reducing the computational\nburden and improving training stability. Our strategy leads to two practical\nalgorithms in model-based and preference-based settings (MoCAN and PeCAN,\nrespectively). A broad range of experiments demonstrate the effectiveness and\nmerits of our algorithms.\n","authors":["Xinmeng Huang","Shuo Li","Edgar Dobriban","Osbert Bastani","Hamed Hassani","Dongsheng Ding"],"pdf_url":"https://arxiv.org/pdf/2405.19544v3.pdf","comment":"32 pages, 6 figures, 8 tables"},{"id":"http://arxiv.org/abs/2411.14739v1","updated":"2024-11-22T05:18:35Z","published":"2024-11-22T05:18:35Z","title":"IRLab@iKAT24: Learned Sparse Retrieval with Multi-aspect LLM Query\n  Generation for Conversational Search","summary":"  The Interactive Knowledge Assistant Track (iKAT) 2024 focuses on advancing\nconversational assistants, able to adapt their interaction and responses from\npersonalized user knowledge. The track incorporates a Personal Textual\nKnowledge Base (PTKB) alongside Conversational AI tasks, such as passage\nranking and response generation. Query Rewrite being an effective approach for\nresolving conversational context, we explore Large Language Models (LLMs), as\nquery rewriters. Specifically, our submitted runs explore multi-aspect query\ngeneration using the MQ4CS framework, which we further enhance with Learned\nSparse Retrieval via the SPLADE architecture, coupled with robust cross-encoder\nmodels. We also propose an alternative to the previous interleaving strategy,\naggregating multiple aspects during the reranking phase. Our findings indicate\nthat multi-aspect query generation is effective in enhancing performance when\nintegrated with advanced retrieval and reranking models. Our results also lead\nthe way for better personalization in Conversational Search, relying on LLMs to\nintegrate personalization within query rewrite, and outperforming human rewrite\nperformance.\n","authors":["Simon Lupart","Zahra Abbasiantaeb","Mohammad Aliannejadi"],"pdf_url":"https://arxiv.org/pdf/2411.14739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14738v1","updated":"2024-11-22T05:17:18Z","published":"2024-11-22T05:17:18Z","title":"Universal and Context-Independent Triggers for Precise Control of LLM\n  Outputs","summary":"  Large language models (LLMs) have been widely adopted in applications such as\nautomated content generation and even critical decision-making systems.\nHowever, the risk of prompt injection allows for potential manipulation of LLM\noutputs. While numerous attack methods have been documented, achieving full\ncontrol over these outputs remains challenging, often requiring experienced\nattackers to make multiple attempts and depending heavily on the prompt\ncontext. Recent advancements in gradient-based white-box attack techniques have\nshown promise in tasks like jailbreaks and system prompt leaks. Our research\ngeneralizes gradient-based attacks to find a trigger that is (1) Universal:\neffective irrespective of the target output; (2) Context-Independent: robust\nacross diverse prompt contexts; and (3) Precise Output: capable of manipulating\nLLM inputs to yield any specified output with high accuracy. We propose a novel\nmethod to efficiently discover such triggers and assess the effectiveness of\nthe proposed attack. Furthermore, we discuss the substantial threats posed by\nsuch attacks to LLM-based applications, highlighting the potential for\nadversaries to taking over the decisions and actions made by AI agents.\n","authors":["Jiashuo Liang","Guancheng Li","Yang Yu"],"pdf_url":"https://arxiv.org/pdf/2411.14738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15647v2","updated":"2024-11-22T05:02:26Z","published":"2023-03-28T00:06:38Z","title":"Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning","summary":"  This paper presents a systematic overview of parameter-efficient fine-tuning\nmethods, covering over 50 papers published between early 2019 and mid-2024.\nThese methods aim to address the challenges of fine-tuning large language\nmodels by training only a small subset of parameters. We provide a taxonomy\nthat covers a broad range of methods and present a detailed method comparison\nwith a specific focus on real-life efficiency in fine-tuning multibillion-scale\nlanguage models. We also conduct an extensive head-to-head experimental\ncomparison of 15 diverse PEFT methods, evaluating their performance and\nefficiency on models up to 11B parameters. Our findings reveal that methods\npreviously shown to surpass a strong LoRA baseline face difficulties in\nresource-constrained settings, where hyperparameter optimization is limited and\nthe network is fine-tuned only for a few epochs. Finally, we provide a set of\npractical recommendations for using PEFT methods and outline potential future\nresearch directions.\n","authors":["Vladislav Lialin","Vijeta Deshpande","Xiaowei Yao","Anna Rumshisky"],"pdf_url":"https://arxiv.org/pdf/2303.15647v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14725v1","updated":"2024-11-22T04:41:20Z","published":"2024-11-22T04:41:20Z","title":"Evaluating and Advancing Multimodal Large Language Models in Ability\n  Lens","summary":"  As multimodal large language models (MLLMs) advance rapidly, rigorous\nevaluation has become essential, providing further guidance for their\ndevelopment. In this work, we focus on a unified and robust evaluation of\n\\textbf{vision perception} abilities, the foundational skill of MLLMs. We find\nthat existing perception benchmarks, each focusing on different question types,\ndomains, and evaluation metrics, introduce significant evaluation variance,\ncomplicating comprehensive assessments of perception abilities when relying on\nany single benchmark. To address this, we introduce \\textbf{AbilityLens}, a\nunified benchmark designed to evaluate MLLMs across six key perception\nabilities, focusing on both accuracy and stability, with each ability\nencompassing diverse question types, domains, and metrics. With the assistance\nof AbilityLens, we: (1) identify the strengths and weaknesses of current\nmodels, highlighting stability patterns and revealing a notable performance gap\nbetween open-source and closed-source models; (2) introduce an online\nevaluation mode, which uncovers interesting ability conflict and early\nconvergence phenomena during MLLM training; and (3) design a simple\nability-specific model merging method that combines the best ability checkpoint\nfrom early training stages, effectively mitigating performance decline due to\nability conflict. The benchmark and online leaderboard will be released soon.\n","authors":["Feng Chen","Chenhui Gou","Jing Liu","Yang Yang","Zhaoyang Li","Jiyuan Zhang","Zhenbang Sun","Bohan Zhuang","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2411.14725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14721v1","updated":"2024-11-22T04:28:56Z","published":"2024-11-22T04:28:56Z","title":"MolReFlect: Towards In-Context Fine-grained Alignments between Molecules\n  and Texts","summary":"  Molecule discovery is a pivotal research field, impacting everything from the\nmedicines we take to the materials we use. Recently, Large Language Models\n(LLMs) have been widely adopted in molecule understanding and generation, yet\nthe alignments between molecules and their corresponding captions remain a\nsignificant challenge. Previous endeavours often treat the molecule as a\ngeneral SMILES string or molecular graph, neglecting the fine-grained\nalignments between the molecular sub-structures and the descriptive textual\nphrases, which are crucial for accurate and explainable predictions. In this\ncase, we introduce MolReFlect, a novel teacher-student framework designed to\ncontextually perform the molecule-caption alignments in a fine-grained way. Our\napproach initially leverages a larger teacher LLM to label the detailed\nalignments by directly extracting critical phrases from molecule captions or\nSMILES strings and implying them to corresponding sub-structures or\ncharacteristics. To refine these alignments, we propose In-Context Selective\nReflection, which retrieves previous extraction results as context examples for\nteacher LLM to reflect and lets a smaller student LLM select from in-context\nreflection and previous extraction results. Finally, we enhance the learning\nprocess of the student LLM through Chain-of-Thought In-Context Molecule Tuning,\nintegrating the fine-grained alignments and the reasoning processes within the\nChain-of-Thought format. Our experimental results demonstrate that MolReFlect\nenables LLMs like Mistral-7B to significantly outperform the previous\nbaselines, achieving SOTA performance on the ChEBI-20 dataset. This advancement\nnot only enhances the generative capabilities of LLMs in the molecule-caption\ntranslation task, but also contributes to a more explainable framework.\n","authors":["Jiatong Li","Yunqing Liu","Wei Liu","Jingdi Le","Di Zhang","Wenqi Fan","Dongzhan Zhou","Yuqiang Li","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2411.14721v1.pdf","comment":"22 pages, 12 figures"},{"id":"http://arxiv.org/abs/2411.14720v1","updated":"2024-11-22T04:19:32Z","published":"2024-11-22T04:19:32Z","title":"Optimizing Social Media Annotation of HPV Vaccine Skepticism and\n  Misinformation Using Large Language Models: An Experimental Evaluation of\n  In-Context Learning and Fine-Tuning Stance Detection Across Multiple Models","summary":"  This paper leverages large-language models (LLMs) to experimentally determine\noptimal strategies for scaling up social media content annotation for stance\ndetection on HPV vaccine-related tweets. We examine both conventional\nfine-tuning and emergent in-context learning methods, systematically varying\nstrategies of prompt engineering across widely used LLMs and their variants\n(e.g., GPT4, Mistral, and Llama3, etc.). Specifically, we varied prompt\ntemplate design, shot sampling methods, and shot quantity to detect stance on\nHPV vaccination. Our findings reveal that 1) in general, in-context learning\noutperforms fine-tuning in stance detection for HPV vaccine social media\ncontent; 2) increasing shot quantity does not necessarily enhance performance\nacross models; and 3) different LLMs and their variants present differing\nsensitivity to in-context learning conditions. We uncovered that the optimal\nin-context learning configuration for stance detection on HPV vaccine tweets\ninvolves six stratified shots paired with detailed contextual prompts. This\nstudy highlights the potential and provides an applicable approach for applying\nLLMs to research on social media stance and skepticism detection.\n","authors":["Luhang Sun","Varsha Pendyala","Yun-Shiuan Chuang","Shanglin Yang","Jonathan Feldman","Andrew Zhao","Munmun De Choudhury","Sijia Yang","Dhavan Shah"],"pdf_url":"https://arxiv.org/pdf/2411.14720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14717v1","updated":"2024-11-22T04:09:23Z","published":"2024-11-22T04:09:23Z","title":"FedMLLM: Federated Fine-tuning MLLM on Multimodal Heterogeneity Data","summary":"  Multimodal Large Language Models (MLLMs) have made significant advancements,\ndemonstrating powerful capabilities in processing and understanding multimodal\ndata. Fine-tuning MLLMs with Federated Learning (FL) allows for expanding the\ntraining data scope by including private data sources, thereby enhancing their\npractical applicability in privacy-sensitive domains. However, current research\nremains in the early stage, particularly in addressing the \\textbf{multimodal\nheterogeneities} in real-world applications. In this paper, we introduce a\nbenchmark for evaluating various downstream tasks in the federated fine-tuning\nof MLLMs within multimodal heterogeneous scenarios, laying the groundwork for\nthe research in the field. Our benchmark encompasses two datasets, five\ncomparison baselines, and four multimodal scenarios, incorporating over ten\ntypes of modal heterogeneities. To address the challenges posed by modal\nheterogeneity, we develop a general FedMLLM framework that integrates four\nrepresentative FL methods alongside two modality-agnostic strategies. Extensive\nexperimental results show that our proposed FL paradigm improves the\nperformance of MLLMs by broadening the range of training data and mitigating\nmultimodal heterogeneity. Code is available at https://github.com/1xbq1/FedMLLM\n","authors":["Binqian Xu","Xiangbo Shu","Haiyang Mei","Guosen Xie","Basura Fernando","Mike Zheng Shou","Jinhui Tang"],"pdf_url":"https://arxiv.org/pdf/2411.14717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14708v1","updated":"2024-11-22T03:33:51Z","published":"2024-11-22T03:33:51Z","title":"Understanding LLM Embeddings for Regression","summary":"  With the rise of large language models (LLMs) for flexibly processing\ninformation as strings, a natural application is regression, specifically by\npreprocessing string representations into LLM embeddings as downstream features\nfor metric prediction. In this paper, we provide one of the first comprehensive\ninvestigations into embedding-based regression and demonstrate that LLM\nembeddings as features can be better for high-dimensional regression tasks than\nusing traditional feature engineering. This regression performance can be\nexplained in part due to LLM embeddings over numeric data inherently preserving\nLipschitz continuity over the feature space. Furthermore, we quantify the\ncontribution of different model effects, most notably model size and language\nunderstanding, which we find surprisingly do not always improve regression\nperformance.\n","authors":["Eric Tang","Bangding Yang","Xingyou Song"],"pdf_url":"https://arxiv.org/pdf/2411.14708v1.pdf","comment":"15 pages, 13 figures"},{"id":"http://arxiv.org/abs/2411.14698v1","updated":"2024-11-22T03:12:39Z","published":"2024-11-22T03:12:39Z","title":"Improving Mathematical Reasoning Capabilities of Small Language Models\n  via Feedback-Driven Distillation","summary":"  Large Language Models (LLMs) demonstrate exceptional reasoning capabilities,\noften achieving state-of-the-art performance in various tasks. However, their\nsubstantial computational and memory demands, due to billions of parameters,\nhinder deployment in resource-constrained environments. A promising solution is\nknowledge distillation, where LLMs transfer reasoning capabilities to Small\nLanguage Models (SLMs, $\\le$ 1B parameters), enabling wider deployment on\nlow-resource devices. Existing methods primarily focus on generating\nhigh-quality reasoning rationales for distillation datasets but often neglect\nthe critical role of data quantity and quality. To address these challenges, we\npropose a Feedback-Driven Distillation (FDD) framework to enhance SLMs'\nmathematical reasoning capabilities. In the initialization stage, a\ndistillation dataset is constructed by prompting LLMs to pair mathematical\nproblems with corresponding reasoning rationales. We classify problems into\neasy and hard categories based on SLM performance. For easy problems, LLMs\ngenerate more complex variations, while for hard problems, new questions of\nsimilar complexity are synthesized. In addition, we propose a multi-round\ndistillation paradigm to iteratively enrich the distillation datasets, thereby\nprogressively improving the mathematical reasoning abilities of SLMs.\nExperimental results demonstrate that our method can make SLMs achieve SOTA\nmathematical reasoning performance.\n","authors":["Xunyu Zhu","Jian Li","Can Ma","Weiping Wang"],"pdf_url":"https://arxiv.org/pdf/2411.14698v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14688v1","updated":"2024-11-22T02:46:44Z","published":"2024-11-22T02:46:44Z","title":"Whats in a Video: Factorized Autoregressive Decoding for Online Dense\n  Video Captioning","summary":"  Generating automatic dense captions for videos that accurately describe their\ncontents remains a challenging area of research. Most current models require\nprocessing the entire video at once. Instead, we propose an efficient, online\napproach which outputs frequent, detailed and temporally aligned captions,\nwithout access to future frames. Our model uses a novel autoregressive\nfactorized decoding architecture, which models the sequence of visual features\nfor each time segment, outputting localized descriptions and efficiently\nleverages the context from the previous video segments. This allows the model\nto output frequent, detailed captions to more comprehensively describe the\nvideo, according to its actual local content, rather than mimic the training\ndata. Second, we propose an optimization for efficient training and inference,\nwhich enables scaling to longer videos. Our approach shows excellent\nperformance compared to both offline and online methods, and uses 20\\% less\ncompute. The annotations produced are much more comprehensive and frequent, and\ncan further be utilized in automatic video tagging and in large-scale video\ndata harvesting.\n","authors":["AJ Piergiovanni","Dahun Kim","Michael S. Ryoo","Isaac Noble","Anelia Angelova"],"pdf_url":"https://arxiv.org/pdf/2411.14688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13802v2","updated":"2024-11-22T02:32:09Z","published":"2024-11-21T03:05:38Z","title":"SemiKong: Curating, Training, and Evaluating A Semiconductor\n  Industry-Specific Large Language Model","summary":"  Large Language Models (LLMs) have demonstrated the potential to address some\nissues within the semiconductor industry. However, they are often\ngeneral-purpose models that lack the specialized knowledge needed to tackle the\nunique challenges of this sector, such as the intricate physics and chemistry\nof semiconductor devices and processes. SemiKong, the first industry-specific\nLLM for the semiconductor domain, provides a foundation that can be used to\ndevelop tailored proprietary models. With SemiKong 1.0, we aim to develop a\nfoundational model capable of understanding etching problems at an expert\nlevel. Our key contributions include (a) curating a comprehensive corpus of\nsemiconductor-related texts, (b) creating a foundational model with in-depth\nsemiconductor knowledge, and (c) introducing a framework for integrating expert\nknowledge, thereby advancing the evaluation process of domain-specific AI\nmodels. Through fine-tuning a pre-trained LLM using our curated dataset, we\nhave shown that SemiKong outperforms larger, general-purpose LLMs in various\nsemiconductor manufacturing and design tasks. Our extensive experiments\nunderscore the importance of developing domain-specific LLMs as a foundation\nfor company- or tool-specific proprietary models, paving the way for further\nresearch and applications in the semiconductor domain. Code and dataset will be\navailable at https://github.com/aitomatic/semikong\n","authors":["Christopher Nguyen","William Nguyen","Atsushi Suzuki","Daisuke Oku","Hong An Phan","Sang Dinh","Zooey Nguyen","Anh Ha","Shruti Raghavan","Huy Vo","Thang Nguyen","Lan Nguyen","Yoshikuni Hirayama"],"pdf_url":"https://arxiv.org/pdf/2411.13802v2.pdf","comment":"On-going work"},{"id":"http://arxiv.org/abs/2411.14672v1","updated":"2024-11-22T02:11:37Z","published":"2024-11-22T02:11:37Z","title":"Multiverse of Greatness: Generating Story Branches with LLMs","summary":"  This paper presents Dynamic Context Prompting/Programming (DCP/P), a novel\nframework for interacting with LLMs to generate graph-based content with a\ndynamic context window history. While there is an existing study utilizing LLMs\nto generate a visual novel game, the previous study involved a manual process\nof output extraction and did not provide flexibility in generating a longer,\ncoherent story. We evaluate DCP/P against our baseline, which does not provide\ncontext history to an LLM and only relies on the initial story data. Through\nobjective evaluation, we show that simply providing the LLM with a summary\nleads to a subpar story compared to additionally providing the LLM with the\nproper context of the story. We also provide an extensive qualitative analysis\nand discussion. We qualitatively examine the quality of the objectively\nbest-performing generated game from each approach. In addition, we examine\nbiases in word choices and word sentiment of the generated content. We find a\nconsistent observation with previous studies that LLMs are biased towards\ncertain words, even with a different LLM family. Finally, we provide a\ncomprehensive discussion on opportunities for future studies.\n","authors":["Pittawat Taveekitworachai","Chollakorn Nimpattanavong","Mustafa Can Gursesli","Antonio Lanata","Andrea Guazzini","Ruck Thawonmas"],"pdf_url":"https://arxiv.org/pdf/2411.14672v1.pdf","comment":"12 pages, 14 figures"},{"id":"http://arxiv.org/abs/2302.04391v9","updated":"2024-11-22T01:41:55Z","published":"2023-02-09T01:09:57Z","title":"The Re-Label Method For Data-Centric Machine Learning","summary":"  In industry deep learning application, our manually labeled data has a\ncertain number of noisy data. To solve this problem and achieve more than 90\nscore in dev dataset, we present a simple method to find the noisy data and\nre-label the noisy data by human, given the model predictions as references in\nhuman labeling. In this paper, we illustrate our idea for a broad set of deep\nlearning tasks, includes classification, sequence tagging, object detection,\nsequence generation, click-through rate prediction. The dev dataset evaluation\nresults and human evaluation results verify our idea.\n","authors":["Tong Guo"],"pdf_url":"https://arxiv.org/pdf/2302.04391v9.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14654v1","updated":"2024-11-22T00:59:25Z","published":"2024-11-22T00:59:25Z","title":"Comparative Analysis of Pooling Mechanisms in LLMs: A Sentiment Analysis\n  Perspective","summary":"  Large Language Models (LLMs) have revolutionized natural language processing\n(NLP) by delivering state-of-the-art performance across a variety of tasks.\nAmong these, Transformer-based models like BERT and GPT rely on pooling layers\nto aggregate token-level embeddings into sentence-level representations. Common\npooling mechanisms such as Mean, Max, and Weighted Sum play a pivotal role in\nthis aggregation process. Despite their widespread use, the comparative\nperformance of these strategies on different LLM architectures remains\nunderexplored. To address this gap, this paper investigates the effects of\nthese pooling mechanisms on two prominent LLM families -- BERT and GPT, in the\ncontext of sentence-level sentiment analysis. Comprehensive experiments reveal\nthat each pooling mechanism exhibits unique strengths and weaknesses depending\non the task's specific requirements. Our findings underline the importance of\nselecting pooling methods tailored to the demands of particular applications,\nprompting a re-evaluation of common assumptions regarding pooling operations.\nBy offering actionable insights, this study contributes to the optimization of\nLLM-based models for downstream tasks.\n","authors":["Jinming Xing","Ruilin Xing","Yan Sun"],"pdf_url":"https://arxiv.org/pdf/2411.14654v1.pdf","comment":"4 figures"},{"id":"http://arxiv.org/abs/2411.14647v1","updated":"2024-11-22T00:37:49Z","published":"2024-11-22T00:37:49Z","title":"Benchmarking Multimodal Models for Ukrainian Language Understanding\n  Across Academic and Cultural Domains","summary":"  While the evaluation of multimodal English-centric models is an active area\nof research with numerous benchmarks, there is a profound lack of benchmarks or\nevaluation suites for low- and mid-resource languages. We introduce ZNO-Vision,\na comprehensive multimodal Ukrainian-centric benchmark derived from\nstandardized university entrance examination (ZNO). The benchmark consists of\nover 4,300 expert-crafted questions spanning 12 academic disciplines, including\nmathematics, physics, chemistry, and humanities. We evaluated the performance\nof both open-source models and API providers, finding that only a handful of\nmodels performed above baseline. Alongside the new benchmark, we performed the\nfirst evaluation study of multimodal text generation for the Ukrainian\nlanguage: we measured caption generation quality on the Multi30K-UK dataset,\ntranslated the VQA benchmark into Ukrainian, and measured performance\ndegradation relative to original English versions. Lastly, we tested a few\nmodels from a cultural perspective on knowledge of national cuisine. We believe\nour work will advance multimodal generation capabilities for the Ukrainian\nlanguage and our approach could be useful for other low-resource languages.\n","authors":["Yurii Paniv","Artur Kiulian","Dmytro Chaplynskyi","Mykola Khandoga","Anton Polishko","Tetiana Bas","Guillermo Gabrielli"],"pdf_url":"https://arxiv.org/pdf/2411.14647v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2411.15139v1","updated":"2024-11-22T18:59:47Z","published":"2024-11-22T18:59:47Z","title":"DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous\n  Driving","summary":"  Recently, the diffusion model has emerged as a powerful generative technique\nfor robotic policy learning, capable of modeling multi-mode action\ndistributions. Leveraging its capability for end-to-end autonomous driving is a\npromising direction. However, the numerous denoising steps in the robotic\ndiffusion policy and the more dynamic, open-world nature of traffic scenes pose\nsubstantial challenges for generating diverse driving actions at a real-time\nspeed. To address these challenges, we propose a novel truncated diffusion\npolicy that incorporates prior multi-mode anchors and truncates the diffusion\nschedule, enabling the model to learn denoising from anchored Gaussian\ndistribution to the multi-mode driving action distribution. Additionally, we\ndesign an efficient cascade diffusion decoder for enhanced interaction with\nconditional scene context. The proposed model, DiffusionDrive, demonstrates\n10$\\times$ reduction in denoising steps compared to vanilla diffusion policy,\ndelivering superior diversity and quality in just 2 steps. On the\nplanning-oriented NAVSIM dataset, with the aligned ResNet-34 backbone,\nDiffusionDrive achieves 88.1 PDMS without bells and whistles, setting a new\nrecord, while running at a real-time speed of 45 FPS on an NVIDIA 4090.\nQualitative results on challenging scenarios further confirm that\nDiffusionDrive can robustly generate diverse plausible driving actions. Code\nand model will be available at https://github.com/hustvl/DiffusionDrive.\n","authors":["Bencheng Liao","Shaoyu Chen","Haoran Yin","Bo Jiang","Cheng Wang","Sixu Yan","Xinbang Zhang","Xiangyu Li","Ying Zhang","Qian Zhang","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2411.15139v1.pdf","comment":"Work in progress. Code & demo & model will be available at\n  https://github.com/hustvl/DiffusionDrive"},{"id":"http://arxiv.org/abs/2411.15138v1","updated":"2024-11-22T18:59:39Z","published":"2024-11-22T18:59:39Z","title":"Material Anything: Generating Materials for Any 3D Object via Diffusion","summary":"  We present Material Anything, a fully-automated, unified diffusion framework\ndesigned to generate physically-based materials for 3D objects. Unlike existing\nmethods that rely on complex pipelines or case-specific optimizations, Material\nAnything offers a robust, end-to-end solution adaptable to objects under\ndiverse lighting conditions. Our approach leverages a pre-trained image\ndiffusion model, enhanced with a triple-head architecture and rendering loss to\nimprove stability and material quality. Additionally, we introduce confidence\nmasks as a dynamic switcher within the diffusion model, enabling it to\neffectively handle both textured and texture-less objects across varying\nlighting conditions. By employing a progressive material generation strategy\nguided by these confidence masks, along with a UV-space material refiner, our\nmethod ensures consistent, UV-ready material outputs. Extensive experiments\ndemonstrate our approach outperforms existing methods across a wide range of\nobject categories and lighting conditions.\n","authors":["Xin Huang","Tengfei Wang","Ziwei Liu","Qing Wang"],"pdf_url":"https://arxiv.org/pdf/2411.15138v1.pdf","comment":"Project page: https://xhuangcv.github.io/MaterialAnything/"},{"id":"http://arxiv.org/abs/2411.15131v1","updated":"2024-11-22T18:56:56Z","published":"2024-11-22T18:56:56Z","title":"WildLMa: Long Horizon Loco-Manipulation in the Wild","summary":"  `In-the-wild' mobile manipulation aims to deploy robots in diverse real-world\nenvironments, which requires the robot to (1) have skills that generalize\nacross object configurations; (2) be capable of long-horizon task execution in\ndiverse environments; and (3) perform complex manipulation beyond\npick-and-place. Quadruped robots with manipulators hold promise for extending\nthe workspace and enabling robust locomotion, but existing results do not\ninvestigate such a capability. This paper proposes WildLMa with three\ncomponents to address these issues: (1) adaptation of learned low-level\ncontroller for VR-enabled whole-body teleoperation and traversability; (2)\nWildLMa-Skill -- a library of generalizable visuomotor skills acquired via\nimitation learning or heuristics and (3) WildLMa-Planner -- an interface of\nlearned skills that allow LLM planners to coordinate skills for long-horizon\ntasks. We demonstrate the importance of high-quality training data by achieving\nhigher grasping success rate over existing RL baselines using only tens of\ndemonstrations. WildLMa exploits CLIP for language-conditioned imitation\nlearning that empirically generalizes to objects unseen in training\ndemonstrations. Besides extensive quantitative evaluation, we qualitatively\ndemonstrate practical robot applications, such as cleaning up trash in\nuniversity hallways or outdoor terrains, operating articulated objects, and\nrearranging items on a bookshelf.\n","authors":["Ri-Zhao Qiu","Yuchen Song","Xuanbin Peng","Sai Aneesh Suryadevara","Ge Yang","Minghuan Liu","Mazeyu Ji","Chengzhe Jia","Ruihan Yang","Xueyan Zou","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2411.15131v1.pdf","comment":"Website: https://wildlma.github.io/"},{"id":"http://arxiv.org/abs/2411.15128v1","updated":"2024-11-22T18:51:51Z","published":"2024-11-22T18:51:51Z","title":"Health AI Developer Foundations","summary":"  Robust medical Machine Learning (ML) models have the potential to\nrevolutionize healthcare by accelerating clinical research, improving workflows\nand outcomes, and producing novel insights or capabilities. Developing such ML\nmodels from scratch is cost prohibitive and requires substantial compute, data,\nand time (e.g., expert labeling). To address these challenges, we introduce\nHealth AI Developer Foundations (HAI-DEF), a suite of pre-trained,\ndomain-specific foundation models, tools, and recipes to accelerate building ML\nfor health applications. The models cover various modalities and domains,\nincluding radiology (X-rays and computed tomography), histopathology,\ndermatological imaging, and audio. These models provide domain specific\nembeddings that facilitate AI development with less labeled data, shorter\ntraining times, and reduced computational costs compared to traditional\napproaches. In addition, we utilize a common interface and style across these\nmodels, and prioritize usability to enable developers to integrate HAI-DEF\nefficiently. We present model evaluations across various tasks and conclude\nwith a discussion of their application and evaluation, covering the importance\nof ensuring efficacy, fairness, and equity. Finally, while HAI-DEF and\nspecifically the foundation models lower the barrier to entry for ML in\nhealthcare, we emphasize the importance of validation with problem- and\npopulation-specific data for each desired usage setting. This technical report\nwill be updated over time as more modalities and features are added.\n","authors":["Atilla P. Kiraly","Sebastien Baur","Kenneth Philbrick","Fereshteh Mahvar","Liron Yatziv","Tiffany Chen","Bram Sterling","Nick George","Fayaz Jamil","Jing Tang","Kai Bailey","Faruk Ahmed","Akshay Goel","Abbi Ward","Lin Yang","Andrew Sellergren","Yossi Matias","Avinatan Hassidim","Shravya Shetty","Daniel Golden","Shekoofeh Azizi","David F. Steiner","Yun Liu","Tim Thelin","Rory Pilgrim","Can Kirmizibayrak"],"pdf_url":"https://arxiv.org/pdf/2411.15128v1.pdf","comment":"16 pages, 8 figures"},{"id":"http://arxiv.org/abs/2411.15122v1","updated":"2024-11-22T18:40:02Z","published":"2024-11-22T18:40:02Z","title":"ReXrank: A Public Leaderboard for AI-Powered Radiology Report Generation","summary":"  AI-driven models have demonstrated significant potential in automating\nradiology report generation for chest X-rays. However, there is no standardized\nbenchmark for objectively evaluating their performance. To address this, we\npresent ReXrank, https://rexrank.ai, a public leaderboard and challenge for\nassessing AI-powered radiology report generation. Our framework incorporates\nReXGradient, the largest test dataset consisting of 10,000 studies, and three\npublic datasets (MIMIC-CXR, IU-Xray, CheXpert Plus) for report generation\nassessment. ReXrank employs 8 evaluation metrics and separately assesses models\ncapable of generating only findings sections and those providing both findings\nand impressions sections. By providing this standardized evaluation framework,\nReXrank enables meaningful comparisons of model performance and offers crucial\ninsights into their robustness across diverse clinical settings. Beyond its\ncurrent focus on chest X-rays, ReXrank's framework sets the stage for\ncomprehensive evaluation of automated reporting across the full spectrum of\nmedical imaging.\n","authors":["Xiaoman Zhang","Hong-Yu Zhou","Xiaoli Yang","Oishi Banerjee","Julián N. Acosta","Josh Miller","Ouwen Huang","Pranav Rajpurkar"],"pdf_url":"https://arxiv.org/pdf/2411.15122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15115v1","updated":"2024-11-22T18:31:47Z","published":"2024-11-22T18:31:47Z","title":"VideoRepair: Improving Text-to-Video Generation via Misalignment\n  Evaluation and Localized Refinement","summary":"  Recent text-to-video (T2V) diffusion models have demonstrated impressive\ngeneration capabilities across various domains. However, these models often\ngenerate videos that have misalignments with text prompts, especially when the\nprompts describe complex scenes with multiple objects and attributes. To\naddress this, we introduce VideoRepair, a novel model-agnostic, training-free\nvideo refinement framework that automatically identifies fine-grained\ntext-video misalignments and generates explicit spatial and textual feedback,\nenabling a T2V diffusion model to perform targeted, localized refinements.\nVideoRepair consists of four stages: In (1) video evaluation, we detect\nmisalignments by generating fine-grained evaluation questions and answering\nthose questions with MLLM. In (2) refinement planning, we identify accurately\ngenerated objects and then create localized prompts to refine other areas in\nthe video. Next, in (3) region decomposition, we segment the correctly\ngenerated area using a combined grounding module. We regenerate the video by\nadjusting the misaligned regions while preserving the correct regions in (4)\nlocalized refinement. On two popular video generation benchmarks (EvalCrafter\nand T2V-CompBench), VideoRepair substantially outperforms recent baselines\nacross various text-video alignment metrics. We provide a comprehensive\nanalysis of VideoRepair components and qualitative examples.\n","authors":["Daeun Lee","Jaehong Yoon","Jaemin Cho","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2411.15115v1.pdf","comment":"Project page: https://video-repair.github.io"},{"id":"http://arxiv.org/abs/2411.05771v2","updated":"2024-11-22T18:29:47Z","published":"2024-11-08T18:33:03Z","title":"Sketched Equivariant Imaging Regularization and Deep Internal Learning\n  for Inverse Problems","summary":"  Equivariant Imaging (EI) regularization has become the de-facto technique for\nunsupervised training of deep imaging networks, without any need of\nground-truth data. Observing that the EI-based unsupervised training paradigm\ncurrently has significant computational redundancy leading to inefficiency in\nhigh-dimensional applications, we propose a sketched EI regularization which\nleverages the randomized sketching techniques for acceleration. We then extend\nour sketched EI regularization to develop an accelerated deep internal learning\nframework -- Sketched Equivariant Deep Image Prior (Sk-EI-DIP), which can be\nefficiently applied for single-image and task-adapted reconstruction.\nAdditionally, for network adaptation tasks, we propose a parameter-efficient\napproach for accelerating both EI-DIP and Sk-EI-DIP via optimizing only the\nnormalization layers. Our numerical study on X-ray CT image reconstruction\ntasks demonstrate that our approach can achieve order-of-magnitude\ncomputational acceleration over standard EI-based counterpart in single-input\nsetting, and network adaptation at test time.\n","authors":["Guixian Xu","Jinglai Li","Junqi Tang"],"pdf_url":"https://arxiv.org/pdf/2411.05771v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15113v1","updated":"2024-11-22T18:29:37Z","published":"2024-11-22T18:29:37Z","title":"Efficient Pruning of Text-to-Image Models: Insights from Pruning Stable\n  Diffusion","summary":"  As text-to-image models grow increasingly powerful and complex, their\nburgeoning size presents a significant obstacle to widespread adoption,\nespecially on resource-constrained devices. This paper presents a pioneering\nstudy on post-training pruning of Stable Diffusion 2, addressing the critical\nneed for model compression in text-to-image domain. Our study tackles the\npruning techniques for the previously unexplored multi-modal generation models,\nand particularly examines the pruning impact on the textual component and the\nimage generation component separately. We conduct a comprehensive comparison on\npruning the model or the single component of the model in various sparsities.\nOur results yield previously undocumented findings. For example, contrary to\nestablished trends in language model pruning, we discover that simple magnitude\npruning outperforms more advanced techniques in text-to-image context.\nFurthermore, our results show that Stable Diffusion 2 can be pruned to 38.5%\nsparsity with minimal quality loss, achieving a significant reduction in model\nsize. We propose an optimal pruning configuration that prunes the text encoder\nto 47.5% and the diffusion generator to 35%. This configuration maintains image\ngeneration quality while substantially reducing computational requirements. In\naddition, our work uncovers intriguing questions about information encoding in\ntext-to-image models: we observe that pruning beyond certain thresholds leads\nto sudden performance drops (unreadable images), suggesting that specific\nweights encode critical semantics information. This finding opens new avenues\nfor future research in model compression, interoperability, and bias\nidentification in text-to-image models. By providing crucial insights into the\npruning behavior of text-to-image models, our study lays the groundwork for\ndeveloping more efficient and accessible AI-driven image generation systems\n","authors":["Samarth N Ramesh","Zhixue Zhao"],"pdf_url":"https://arxiv.org/pdf/2411.15113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13490v2","updated":"2024-11-22T18:26:22Z","published":"2024-11-20T17:38:34Z","title":"Efficient Brain Imaging Analysis for Alzheimer's and Dementia Detection\n  Using Convolution-Derivative Operations","summary":"  Alzheimer's disease (AD) is characterized by progressive neurodegeneration\nand results in detrimental structural changes in human brains. Detecting these\nchanges is crucial for early diagnosis and timely intervention of disease\nprogression. Jacobian maps, derived from spatial normalization in voxel-based\nmorphometry (VBM), have been instrumental in interpreting volume alterations\nassociated with AD. However, the computational cost of generating Jacobian maps\nlimits its clinical adoption. In this study, we explore alternative methods and\npropose Sobel kernel angle difference (SKAD) as a computationally efficient\nalternative. SKAD is a derivative operation that offers an optimized approach\nto quantifying volumetric alterations through localized analysis of the\ngradients. By efficiently extracting gradient amplitude changes at critical\nspatial regions, this derivative operation captures regional volume variations\nEvaluation of SKAD over various medical datasets demonstrates that it is 6.3x\nfaster than Jacobian maps while still maintaining comparable accuracy. This\nmakes it an efficient and competitive approach in neuroimaging research and\nclinical practice.\n","authors":["Yasmine Mustafa","Mohamed Elmahallawy","Tie Luo"],"pdf_url":"https://arxiv.org/pdf/2411.13490v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15110v1","updated":"2024-11-22T18:21:20Z","published":"2024-11-22T18:21:20Z","title":"A Real-Time DETR Approach to Bangladesh Road Object Detection for\n  Autonomous Vehicles","summary":"  In the recent years, we have witnessed a paradigm shift in the field of\nComputer Vision, with the forthcoming of the transformer architecture.\nDetection Transformers has become a state of the art solution to object\ndetection and is a potential candidate for Road Object Detection in Autonomous\nVehicles. Despite the abundance of object detection schemes, real-time DETR\nmodels are shown to perform significantly better on inference times, with\nminimal loss of accuracy and performance. In our work, we used Real-Time DETR\n(RTDETR) object detection on the BadODD Road Object Detection dataset based in\nBangladesh, and performed necessary experimentation and testing. Our results\ngave a mAP50 score of 0.41518 in the public 60% test set, and 0.28194 in the\nprivate 40% test set.\n","authors":["Irfan Nafiz Shahan","Arban Hossain","Saadman Sakib","Al-Mubin Nabil"],"pdf_url":"https://arxiv.org/pdf/2411.15110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01593v2","updated":"2024-11-22T18:20:26Z","published":"2024-06-03T17:59:51Z","title":"MaGS: Reconstructing and Simulating Dynamic 3D Objects with\n  Mesh-adsorbed Gaussian Splatting","summary":"  3D reconstruction and simulation, although interrelated, have distinct\nobjectives: reconstruction requires a flexible 3D representation that can adapt\nto diverse scenes, while simulation needs a structured representation to model\nmotion principles effectively. This paper introduces the Mesh-adsorbed Gaussian\nSplatting (MaGS) method to address this challenge. MaGS constrains 3D Gaussians\nto roam near the mesh, creating a mutually adsorbed mesh-Gaussian 3D\nrepresentation. Such representation harnesses both the rendering flexibility of\n3D Gaussians and the structured property of meshes. To achieve this, we\nintroduce RMD-Net, a network that learns motion priors from video data to\nrefine mesh deformations, alongside RGD-Net, which models the relative\ndisplacement between the mesh and Gaussians to enhance rendering fidelity under\nmesh constraints. To generalize to novel, user-defined deformations beyond\ninput video without reliance on temporal data, we propose MPE-Net, which\nleverages inherent mesh information to bootstrap RMD-Net and RGD-Net. Due to\nthe universality of meshes, MaGS is compatible with various deformation priors\nsuch as ARAP, SMPL, and soft physics simulation. Extensive experiments on the\nD-NeRF, DG-Mesh, and PeopleSnapshot datasets demonstrate that MaGS achieves\nstate-of-the-art performance in both reconstruction and simulation.\n","authors":["Shaojie Ma","Yawei Luo","Wei Yang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2406.01593v2.pdf","comment":"Project Page: see https://wcwac.github.io/MaGS-page/"},{"id":"http://arxiv.org/abs/2411.15106v1","updated":"2024-11-22T18:09:27Z","published":"2024-11-22T18:09:27Z","title":"About Time: Advances, Challenges, and Outlooks of Action Understanding","summary":"  We have witnessed impressive advances in video action understanding.\nIncreased dataset sizes, variability, and computation availability have enabled\nleaps in performance and task diversification. Current systems can provide\ncoarse- and fine-grained descriptions of video scenes, extract segments\ncorresponding to queries, synthesize unobserved parts of videos, and predict\ncontext. This survey comprehensively reviews advances in uni- and multi-modal\naction understanding across a range of tasks. We focus on prevalent challenges,\noverview widely adopted datasets, and survey seminal works with an emphasis on\nrecent advances. We broadly distinguish between three temporal scopes: (1)\nrecognition tasks of actions observed in full, (2) prediction tasks for ongoing\npartially observed actions, and (3) forecasting tasks for subsequent unobserved\naction. This division allows us to identify specific action modeling and video\nrepresentation challenges. Finally, we outline future directions to address\ncurrent shortcomings.\n","authors":["Alexandros Stergiou","Ronald Poppe"],"pdf_url":"https://arxiv.org/pdf/2411.15106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.08363v2","updated":"2024-11-22T17:56:26Z","published":"2024-05-14T07:05:18Z","title":"UnMarker: A Universal Attack on Defensive Image Watermarking","summary":"  Reports regarding the misuse of Generative AI (GenAI) to create deepfakes are\nfrequent. Defensive watermarking enables GenAI providers to hide fingerprints\nin their images and use them later for deepfake detection. Yet, its potential\nhas not been fully explored. We present UnMarker -- the first practical\nuniversal attack on defensive watermarking. Unlike existing attacks, UnMarker\nrequires no detector feedback, no unrealistic knowledge of the watermarking\nscheme or similar models, and no advanced denoising pipelines that may not be\navailable. Instead, being the product of an in-depth analysis of the\nwatermarking paradigm revealing that robust schemes must construct their\nwatermarks in the spectral amplitudes, UnMarker employs two novel adversarial\noptimizations to disrupt the spectra of watermarked images, erasing the\nwatermarks. Evaluations against SOTA schemes prove UnMarker's effectiveness. It\nnot only defeats traditional schemes while retaining superior quality compared\nto existing attacks but also breaks semantic watermarks that alter an image's\nstructure, reducing the best detection rate to $43\\%$ and rendering them\nuseless. To our knowledge, UnMarker is the first practical attack on semantic\nwatermarks, which have been deemed the future of defensive watermarking. Our\nfindings show that defensive watermarking is not a viable defense against\ndeepfakes, and we urge the community to explore alternatives.\n","authors":["Andre Kassis","Urs Hengartner"],"pdf_url":"https://arxiv.org/pdf/2405.08363v2.pdf","comment":"To appear at IEEE S&P 2025"},{"id":"http://arxiv.org/abs/2411.15099v1","updated":"2024-11-22T17:55:39Z","published":"2024-11-22T17:55:39Z","title":"Context-Aware Multimodal Pretraining","summary":"  Large-scale multimodal representation learning successfully optimizes for\nzero-shot transfer at test time. Yet the standard pretraining paradigm\n(contrastive learning on large amounts of image-text data) does not explicitly\nencourage representations to support few-shot adaptation. In this work, we\npropose a simple, but carefully designed extension to multimodal pretraining\nwhich enables representations to accommodate additional context. Using this\nobjective, we show that vision-language models can be trained to exhibit\nsignificantly increased few-shot adaptation: across 21 downstream tasks, we\nfind up to four-fold improvements in test-time sample efficiency, and average\nfew-shot adaptation gains of over 5%, while retaining zero-shot generalization\nperformance across model scales and training durations. In particular, equipped\nwith simple, training-free, metric-based adaptation mechanisms, our\nrepresentations easily surpass more complex and expensive optimization-based\nschemes, vastly simplifying generalization to new domains.\n","authors":["Karsten Roth","Zeynep Akata","Dima Damen","Ivana Balažević","Olivier J. Hénaff"],"pdf_url":"https://arxiv.org/pdf/2411.15099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15098v1","updated":"2024-11-22T17:55:15Z","published":"2024-11-22T17:55:15Z","title":"OminiControl: Minimal and Universal Control for Diffusion Transformer","summary":"  In this paper, we introduce OminiControl, a highly versatile and\nparameter-efficient framework that integrates image conditions into pre-trained\nDiffusion Transformer (DiT) models. At its core, OminiControl leverages a\nparameter reuse mechanism, enabling the DiT to encode image conditions using\nitself as a powerful backbone and process them with its flexible multi-modal\nattention processors. Unlike existing methods, which rely heavily on additional\nencoder modules with complex architectures, OminiControl (1) effectively and\nefficiently incorporates injected image conditions with only ~0.1% additional\nparameters, and (2) addresses a wide range of image conditioning tasks in a\nunified manner, including subject-driven generation and spatially-aligned\nconditions such as edges, depth, and more. Remarkably, these capabilities are\nachieved by training on images generated by the DiT itself, which is\nparticularly beneficial for subject-driven generation. Extensive evaluations\ndemonstrate that OminiControl outperforms existing UNet-based and DiT-adapted\nmodels in both subject-driven and spatially-aligned conditional generation.\nAdditionally, we release our training dataset, Subjects200K, a diverse\ncollection of over 200,000 identity-consistent images, along with an efficient\ndata synthesis pipeline to advance research in subject-consistent generation.\n","authors":["Zhenxiong Tan","Songhua Liu","Xingyi Yang","Qiaochu Xue","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2411.15098v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15095v1","updated":"2024-11-22T17:50:27Z","published":"2024-11-22T17:50:27Z","title":"Dimension-independent rates for structured neural density estimation","summary":"  We show that deep neural networks achieve dimension-independent rates of\nconvergence for learning structured densities such as those arising in image,\naudio, video, and text applications. More precisely, we demonstrate that neural\nnetworks with a simple $L^2$-minimizing loss achieve a rate of $n^{-1/(4+r)}$\nin nonparametric density estimation when the underlying density is Markov to a\ngraph whose maximum clique size is at most $r$, and we provide evidence that in\nthe aforementioned applications, this size is typically constant, i.e.,\n$r=O(1)$. We then establish that the optimal rate in $L^1$ is $n^{-1/(2+r)}$\nwhich, compared to the standard nonparametric rate of $n^{-1/(2+d)}$, reveals\nthat the effective dimension of such problems is the size of the largest clique\nin the Markov random field. These rates are independent of the data's ambient\ndimension, making them applicable to realistic models of image, sound, video,\nand text data. Our results provide a novel justification for deep learning's\nability to circumvent the curse of dimensionality, demonstrating\ndimension-independent convergence rates in these contexts.\n","authors":["Robert A. Vandermeulen","Wai Ming Tai","Bryon Aragam"],"pdf_url":"https://arxiv.org/pdf/2411.15095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08748v3","updated":"2024-11-22T17:42:10Z","published":"2024-04-12T18:21:08Z","title":"Multi-Branch Generative Models for Multichannel Imaging with an\n  Application to PET/CT Synergistic Reconstruction","summary":"  This paper presents a novel approach for learned synergistic reconstruction\nof medical images using multi-branch generative models. Leveraging variational\nautoencoders (VAEs), our model learns from pairs of images simultaneously,\nenabling effective denoising and reconstruction. Synergistic image\nreconstruction is achieved by incorporating the trained models in a regularizer\nthat evaluates the distance between the images and the model. We demonstrate\nthe efficacy of our approach on both Modified National Institute of Standards\nand Technology (MNIST) and positron emission tomography (PET)/computed\ntomography (CT) datasets, showcasing improved image quality for low-dose\nimaging. Despite challenges such as patch decomposition and model limitations,\nour results underscore the potential of generative models for enhancing medical\nimaging reconstruction.\n","authors":["Noel Jeffrey Pinton","Alexandre Bousse","Catherine Cheze-Le-Rest","Dimitris Visvikis"],"pdf_url":"https://arxiv.org/pdf/2404.08748v3.pdf","comment":"12 pages, 17 figures, 2 tables, submitted to IEEE TRPMS"},{"id":"http://arxiv.org/abs/2411.15087v1","updated":"2024-11-22T17:28:43Z","published":"2024-11-22T17:28:43Z","title":"Instance-Aware Generalized Referring Expression Segmentation","summary":"  Recent works on Generalized Referring Expression Segmentation (GRES) struggle\nwith handling complex expressions referring to multiple distinct objects. This\nis because these methods typically employ an end-to-end foreground-background\nsegmentation and lack a mechanism to explicitly differentiate and associate\ndifferent object instances to the text query. To this end, we propose\nInstAlign, a method that incorporates object-level reasoning into the\nsegmentation process. Our model leverages both text and image inputs to extract\na set of object-level tokens that capture both the semantic information in the\ninput prompt and the objects within the image. By modeling the text-object\nalignment via instance-level supervision, each token uniquely represents an\nobject segment in the image, while also aligning with relevant semantic\ninformation from the text. Extensive experiments on the gRefCOCO and Ref-ZOM\nbenchmarks demonstrate that our method significantly advances state-of-the-art\nperformance, setting a new standard for precise and flexible GRES.\n","authors":["E-Ro Nguyen","Hieu Le","Dimitris Samaras","Michael Ryoo"],"pdf_url":"https://arxiv.org/pdf/2411.15087v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2411.15086v1","updated":"2024-11-22T17:28:07Z","published":"2024-11-22T17:28:07Z","title":"Quantum-enhanced unsupervised image segmentation for medical images\n  analysis","summary":"  Breast cancer remains the leading cause of cancer-related mortality among\nwomen worldwide, necessitating the meticulous examination of mammograms by\nradiologists to characterize abnormal lesions. This manual process demands high\naccuracy and is often time-consuming, costly, and error-prone. Automated image\nsegmentation using artificial intelligence offers a promising alternative to\nstreamline this workflow. However, most existing methods are supervised,\nrequiring large, expertly annotated datasets that are not always available, and\nthey experience significant generalization issues. Thus, unsupervised learning\nmodels can be leveraged for image segmentation, but they come at a cost of\nreduced accuracy, or require extensive computational resourcess. In this paper,\nwe propose the first end-to-end quantum-enhanced framework for unsupervised\nmammography medical images segmentation that balances between performance\naccuracy and computational requirements. We first introduce a quantum-inspired\nimage representation that serves as an initial approximation of the\nsegmentation mask. The segmentation task is then formulated as a QUBO problem,\naiming to maximize the contrast between the background and the tumor region\nwhile ensuring a cohesive segmentation mask with minimal connected components.\nWe conduct an extensive evaluation of quantum and quantum-inspired methods for\nimage segmentation, demonstrating that quantum annealing and variational\nquantum circuits achieve performance comparable to classical optimization\ntechniques. Notably, quantum annealing is shown to be an order of magnitude\nfaster than the classical optimization method in our experiments. Our findings\ndemonstrate that this framework achieves performance comparable to\nstate-of-the-art supervised methods, including UNet-based architectures,\noffering a viable unsupervised alternative for breast cancer image\nsegmentation.\n","authors":["Laia Domingo","Mahdi Chehimi"],"pdf_url":"https://arxiv.org/pdf/2411.15086v1.pdf","comment":"16 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.15205v3","updated":"2024-11-22T17:27:31Z","published":"2024-08-27T17:06:22Z","title":"Leveraging Hallucinations to Reduce Manual Prompt Dependency in\n  Promptable Segmentation","summary":"  Promptable segmentation typically requires instance-specific manual prompts\nto guide the segmentation of each desired object. To minimize such a need,\ntask-generic promptable segmentation has been introduced, which employs a\nsingle task-generic prompt to segment various images of different objects in\nthe same task. Current methods use Multimodal Large Language Models (MLLMs) to\nreason detailed instance-specific prompts from a task-generic prompt for\nimproving segmentation accuracy. The effectiveness of this segmentation heavily\ndepends on the precision of these derived prompts. However, MLLMs often suffer\nhallucinations during reasoning, resulting in inaccurate prompting. While\nexisting methods focus on eliminating hallucinations to improve a model, we\nargue that MLLM hallucinations can reveal valuable contextual insights when\nleveraged correctly, as they represent pre-trained large-scale knowledge beyond\nindividual images. In this paper, we utilize hallucinations to mine\ntask-related information from images and verify its accuracy for enhancing\nprecision of the generated prompts. Specifically, we introduce an iterative\nPrompt-Mask Cycle generation framework (ProMaC) with a prompt generator and a\nmask generator.The prompt generator uses a multi-scale chain of thought\nprompting, initially exploring hallucinations for extracting extended\ncontextual knowledge on a test image.These hallucinations are then reduced to\nformulate precise instance-specific prompts, directing the mask generator to\nproduce masks that are consistent with task semantics by mask semantic\nalignment. The generated masks iteratively induce the prompt generator to focus\nmore on task-relevant image areas and reduce irrelevant hallucinations,\nresulting jointly in better prompts and masks. Experiments on 5 benchmarks\ndemonstrate the effectiveness of ProMaC. Code given in\nhttps://lwpyh.github.io/ProMaC/.\n","authors":["Jian Hu","Jiayi Lin","Junchi Yan","Shaogang Gong"],"pdf_url":"https://arxiv.org/pdf/2408.15205v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.15084v1","updated":"2024-11-22T17:19:58Z","published":"2024-11-22T17:19:58Z","title":"Leapfrog Latent Consistency Model (LLCM) for Medical Images Generation","summary":"  The scarcity of accessible medical image data poses a significant obstacle in\neffectively training deep learning models for medical diagnosis, as hospitals\nrefrain from sharing their data due to privacy concerns. In response, we\ngathered a diverse dataset named MedImgs, which comprises over 250,127 images\nspanning 61 disease types and 159 classes of both humans and animals from\nopen-source repositories. We propose a Leapfrog Latent Consistency Model (LLCM)\nthat is distilled from a retrained diffusion model based on the collected\nMedImgs dataset, which enables our model to generate real-time high-resolution\nimages. We formulate the reverse diffusion process as a probability flow\nordinary differential equation (PF-ODE) and solve it in latent space using the\nLeapfrog algorithm. This formulation enables rapid sampling without\nnecessitating additional iterations. Our model demonstrates state-of-the-art\nperformance in generating medical images. Furthermore, our model can be\nfine-tuned with any custom medical image datasets, facilitating the generation\nof a vast array of images. Our experimental results outperform those of\nexisting models on unseen dog cardiac X-ray images. Source code is available at\nhttps://github.com/lskdsjy/LeapfrogLCM.\n","authors":["Lakshmikar R. Polamreddy","Kalyan Roy","Sheng-Han Yueh","Deepshikha Mahato","Shilpa Kuppili","Jialu Li","Youshan Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.15084v1.pdf","comment":"Total 16 pages including 5 figures and 36 references"},{"id":"http://arxiv.org/abs/2411.15076v1","updated":"2024-11-22T17:08:28Z","published":"2024-11-22T17:08:28Z","title":"RankByGene: Gene-Guided Histopathology Representation Learning Through\n  Cross-Modal Ranking Consistency","summary":"  Spatial transcriptomics (ST) provides essential spatial context by mapping\ngene expression within tissue, enabling detailed study of cellular\nheterogeneity and tissue organization. However, aligning ST data with histology\nimages poses challenges due to inherent spatial distortions and\nmodality-specific variations. Existing methods largely rely on direct\nalignment, which often fails to capture complex cross-modal relationships. To\naddress these limitations, we propose a novel framework that aligns gene and\nimage features using a ranking-based alignment loss, preserving relative\nsimilarity across modalities and enabling robust multi-scale alignment. To\nfurther enhance the alignment's stability, we employ self-supervised knowledge\ndistillation with a teacher-student network architecture, effectively\nmitigating disruptions from high dimensionality, sparsity, and noise in gene\nexpression data. Extensive experiments on gene expression prediction and\nsurvival analysis demonstrate our framework's effectiveness, showing improved\nalignment and predictive performance over existing methods and establishing a\nrobust tool for gene-guided image representation learning in digital pathology.\n","authors":["Wentao Huang","Meilong Xu","Xiaoling Hu","Shahira Abousamra","Aniruddha Ganguly","Saarthak Kapse","Alisa Yurovsky","Prateek Prasanna","Tahsin Kurc","Joel Saltz","Michael L. Miller","Chao Chen"],"pdf_url":"https://arxiv.org/pdf/2411.15076v1.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2411.15074v1","updated":"2024-11-22T17:03:25Z","published":"2024-11-22T17:03:25Z","title":"Learning to Stabilize Faces","summary":"  Nowadays, it is possible to scan faces and automatically register them with\nhigh quality. However, the resulting face meshes often need further processing:\nwe need to stabilize them to remove unwanted head movement. Stabilization is\nimportant for tasks like game development or movie making which require facial\nexpressions to be cleanly separated from rigid head motion. Since manual\nstabilization is labor-intensive, there have been attempts to automate it.\nHowever, previous methods remain impractical: they either still require some\nmanual input, produce imprecise alignments, rely on dubious heuristics and slow\noptimization, or assume a temporally ordered input. Instead, we present a new\nlearning-based approach that is simple and fully automatic. We treat\nstabilization as a regression problem: given two face meshes, our network\ndirectly predicts the rigid transform between them that brings their skulls\ninto alignment. We generate synthetic training data using a 3D Morphable Model\n(3DMM), exploiting the fact that 3DMM parameters separate skull motion from\nfacial skin motion. Through extensive experiments we show that our approach\noutperforms the state-of-the-art both quantitatively and qualitatively on the\ntasks of stabilizing discrete sets of facial expressions as well as dynamic\nfacial performances. Furthermore, we provide an ablation study detailing the\ndesign choices and best practices to help others adopt our approach for their\nown uses. Supplementary videos can be found on the project webpage\nsyntec-research.github.io/FaceStab.\n","authors":["Jan Bednarik","Erroll Wood","Vasileios Choutas","Timo Bolkart","Daoye Wang","Chenglei Wu","Thabo Beeler"],"pdf_url":"https://arxiv.org/pdf/2411.15074v1.pdf","comment":"Eurographics 2024"},{"id":"http://arxiv.org/abs/2405.13278v2","updated":"2024-11-22T16:59:38Z","published":"2024-05-22T01:17:27Z","title":"Single color digital H&E staining with In-and-Out Net","summary":"  Virtual staining streamlines traditional staining procedures by digitally\ngenerating stained images from unstained or differently stained images. While\nconventional staining methods involve time-consuming chemical processes,\nvirtual staining offers an efficient and low infrastructure alternative.\nLeveraging microscopy-based techniques, such as confocal microscopy,\nresearchers can expedite tissue analysis without the need for physical\nsectioning. However, interpreting grayscale or pseudo-color microscopic images\nremains a challenge for pathologists and surgeons accustomed to traditional\nhistologically stained images. To fill this gap, various studies explore\ndigitally simulating staining to mimic targeted histological stains. This paper\nintroduces a novel network, In-and-Out Net, specifically designed for virtual\nstaining tasks. Based on Generative Adversarial Networks (GAN), our model\nefficiently transforms Reflectance Confocal Microscopy (RCM) images into\nHematoxylin and Eosin (H&E) stained images. We enhance nuclei contrast in RCM\nimages using aluminum chloride preprocessing for skin tissues. Training the\nmodel with virtual H\\&E labels featuring two fluorescence channels eliminates\nthe need for image registration and provides pixel-level ground truth. Our\ncontributions include proposing an optimal training strategy, conducting a\ncomparative analysis demonstrating state-of-the-art performance, validating the\nmodel through an ablation study, and collecting perfectly matched input and\nground truth images without registration. In-and-Out Net showcases promising\nresults, offering a valuable tool for virtual staining tasks and advancing the\nfield of histological image analysis.\n","authors":["Mengkun Chen","Yen-Tung Liu","Fadeel Sher Khan","Matthew C. Fox","Jason S. Reichenberg","Fabiana C. P. S. Lopes","Katherine R. Sebastian","Mia K. Markey","James W. Tunnell"],"pdf_url":"https://arxiv.org/pdf/2405.13278v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15066v1","updated":"2024-11-22T16:54:17Z","published":"2024-11-22T16:54:17Z","title":"SPAC-Net: Rethinking Point Cloud Completion with Structural Prior","summary":"  Point cloud completion aims to infer a complete shape from its partial\nobservation. Many approaches utilize a pure encoderdecoder paradigm in which\ncomplete shape can be directly predicted by shape priors learned from partial\nscans, however, these methods suffer from the loss of details inevitably due to\nthe feature abstraction issues. In this paper, we propose a novel\nframework,termed SPAC-Net, that aims to rethink the completion task under the\nguidance of a new structural prior, we call it interface. Specifically, our\nmethod first investigates Marginal Detector (MAD) module to localize the\ninterface, defined as the intersection between the known observation and the\nmissing parts. Based on the interface, our method predicts the coarse shape by\nlearning the displacement from the points in interface move to their\ncorresponding position in missing parts. Furthermore, we devise an additional\nStructure Supplement(SSP) module before the upsampling stage to enhance the\nstructural details of the coarse shape, enabling the upsampling module to focus\nmore on the upsampling task. Extensive experiments have been conducted on\nseveral challenging benchmarks, and the results demonstrate that our method\noutperforms existing state-of-the-art approaches.\n","authors":["Zizhao Wu","Jian Shi","Xuan Deng","Cheng Zhang","Genfu Yang","Ming Zeng","Yunhai Wang"],"pdf_url":"https://arxiv.org/pdf/2411.15066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15060v1","updated":"2024-11-22T16:46:00Z","published":"2024-11-22T16:46:00Z","title":"Detecting Hallucinations in Virtual Histology with Neural Precursors","summary":"  Significant biomedical research and clinical care rely on the histopathologic\nexamination of tissue structure using microscopy of stained tissue. Virtual\nstaining (VS) offers a promising alternative with the potential to reduce cost\nand eliminate the use of toxic reagents. However, the critical challenge of\nhallucinations limits confidence in its use, necessitating a VS co-pilot to\ndetect these hallucinations. Here, we first formally establish the problem of\nhallucination detection in VS. Next, we introduce a scalable, post-hoc\nhallucination detection method that identifies a Neural Hallucination Precursor\n(NHP) from VS model embeddings for test-time detection. We report extensive\nvalidation across diverse and challenging VS settings to demonstrate NHP's\neffectiveness and robustness. Furthermore, we show that VS models with fewer\nhallucinations do not necessarily disclose them better, risking a false sense\nof security when reporting just the former metric. This highlights the need for\na reassessment of current VS evaluation practices.\n","authors":["Ji-Hun Oh","Kianoush Falahkheirkhah","Rohit Bhargava"],"pdf_url":"https://arxiv.org/pdf/2411.15060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15051v1","updated":"2024-11-22T16:38:03Z","published":"2024-11-22T16:38:03Z","title":"Fantastic Biases (What are They) and Where to Find Them","summary":"  Deep Learning models tend to learn correlations of patterns on huge datasets.\nThe bigger these systems are, the more complex are the phenomena they can\ndetect, and the more data they need for this. The use of Artificial\nIntelligence (AI) is becoming increasingly ubiquitous in our society, and its\nimpact is growing everyday. The promises it holds strongly depend on their fair\nand universal use, such as access to information or education for all. In a\nworld of inequalities, they can help to reach the most disadvantaged areas.\nHowever, such a universal systems must be able to represent society, without\nbenefiting some at the expense of others. We must not reproduce the\ninequalities observed throughout the world, but educate these IAs to go beyond\nthem. We have seen cases where these systems use gender, race, or even class\ninformation in ways that are not appropriate for resolving their tasks. Instead\nof real causal reasoning, they rely on spurious correlations, which is what we\nusually call a bias. In this paper, we first attempt to define what is a bias\nin general terms. It helps us to demystify the concept of bias, to understand\nwhy we can find them everywhere and why they are sometimes useful. Second, we\nfocus over the notion of what is generally seen as negative bias, the one we\nwant to avoid in machine learning, before presenting a general zoology\ncontaining the most common of these biases. We finally conclude by looking at\nclassical methods to detect them, by means of specially crafted datasets of\ntemplates and specific algorithms, and also classical methods to mitigate them.\n","authors":["Valentin Barriere"],"pdf_url":"https://arxiv.org/pdf/2411.15051v1.pdf","comment":"Publication in Spanish in the Journal Bits de Ciencias:\n  https://www.dcc.uchile.cl/media/bits/pdfs/bits26.2-sesgos-fantasticos.pdf"},{"id":"http://arxiv.org/abs/2411.15043v1","updated":"2024-11-22T16:25:05Z","published":"2024-11-22T16:25:05Z","title":"OVO-SLAM: Open-Vocabulary Online Simultaneous Localization and Mapping","summary":"  This paper presents the first Open-Vocabulary Online 3D semantic SLAM\npipeline, that we denote as OVO-SLAM. Our primary contribution is in the\npipeline itself, particularly in the mapping thread. Given a set of posed RGB-D\nframes, we detect and track 3D segments, which we describe using CLIP vectors,\ncalculated through a novel aggregation from the viewpoints where these 3D\nsegments are observed. Notably, our OVO-SLAM pipeline is not only faster but\nalso achieves better segmentation metrics compared to offline approaches in the\nliterature. Along with superior segmentation performance, we show experimental\nresults of our contributions integrated with Gaussian-SLAM, being the first\nones demonstrating end-to-end open-vocabulary online 3D reconstructions without\nrelying on ground-truth camera poses or scene geometry.\n","authors":["Tomas Berriel Martins","Martin R. Oswald","Javier Civera"],"pdf_url":"https://arxiv.org/pdf/2411.15043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15034v1","updated":"2024-11-22T16:08:03Z","published":"2024-11-22T16:08:03Z","title":"HeadRouter: A Training-free Image Editing Framework for MM-DiTs by\n  Adaptively Routing Attention Heads","summary":"  Diffusion Transformers (DiTs) have exhibited robust capabilities in image\ngeneration tasks. However, accurate text-guided image editing for multimodal\nDiTs (MM-DiTs) still poses a significant challenge. Unlike UNet-based\nstructures that could utilize self/cross-attention maps for semantic editing,\nMM-DiTs inherently lack support for explicit and consistent incorporated text\nguidance, resulting in semantic misalignment between the edited results and\ntexts. In this study, we disclose the sensitivity of different attention heads\nto different image semantics within MM-DiTs and introduce HeadRouter, a\ntraining-free image editing framework that edits the source image by adaptively\nrouting the text guidance to different attention heads in MM-DiTs. Furthermore,\nwe present a dual-token refinement module to refine text/image token\nrepresentations for precise semantic guidance and accurate region expression.\nExperimental results on multiple benchmarks demonstrate HeadRouter's\nperformance in terms of editing fidelity and image quality.\n","authors":["Yu Xu","Fan Tang","Juan Cao","Yuxin Zhang","Xiaoyu Kong","Jintao Li","Oliver Deussen","Tong-Yee Lee"],"pdf_url":"https://arxiv.org/pdf/2411.15034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05349v2","updated":"2024-11-22T16:07:35Z","published":"2024-02-08T02:01:36Z","title":"Scrapping The Web For Early Wildfire Detection: A New Annotated Dataset\n  of Images and Videos of Smoke Plumes In-the-wild","summary":"  Early wildfire detection is of the utmost importance to enable rapid response\nefforts, and thus minimize the negative impacts of wildfire spreads. To this\nend, we present PyroNear-2024, a new dataset composed of both images and\nvideos, allowing for the training and evaluation of smoke plume detection\nmodels, including sequential models. The data is sourced from: \\textit{(i)}\nweb-scraped videos of wildfires from public networks of cameras for wildfire\ndetection in-the-wild, \\text{(ii)} videos from our in-house network of cameras,\nand \\textit{(iii)} a small portion of synthetic and real images. This dataset\nincludes around 150,000 manual annotations on 50,000 images, covering 400\nwildfires, \\Pyro surpasses existing datasets in size and diversity. It includes\ndata from France, Spain, and the United States. Finally, it is composed of both\nimages and videos, allowing for the training and evaluation of smoke plume\ndetection models, including sequential models. We ran cross-dataset experiments\nusing a lightweight state-of-the-art object detection model and found out the\nproposed dataset is particularly challenging, with F1 score of around 60%, but\nmore stable than existing datasets. The video part of the dataset can be used\nto train a lightweight sequential model, improving global recall while\nmaintaining precision. Finally, its use in concordance with other public\ndataset helps to reach higher results overall. We will make both our code and\ndata available.\n","authors":["Mateo Lostanlen","Nicolas Isla","Jose Guillen","Felix Veith","Cristian Buc","Valentin Barriere"],"pdf_url":"https://arxiv.org/pdf/2402.05349v2.pdf","comment":"Preprint of ongoing work"},{"id":"http://arxiv.org/abs/2402.17298v2","updated":"2024-11-22T16:05:12Z","published":"2024-02-27T08:20:45Z","title":"ArcSin: Adaptive ranged cosine Similarity injected noise for\n  Language-Driven Visual Tasks","summary":"  \"A data scientist is tasked with developing a low-cost surgical VQA system\nfor a 2-month workshop. Due to data sensitivity, she collects 50 hours of\nsurgical video from a hospital, requiring two months for privacy approvals.\nPrivacy restrictions prevent uploading data to platforms like ChatGPT, so she\nassembles one annotator and a medical expert to manually create QA pairs. This\nprocess takes three weeks and costs over $10,000. The trained model provides\naccurate responses within the limited data scope but lacks broader\ngeneralizability, completing the project in 3 months.\"\n  To simplify the challenges presented in the scenario above. In this paper, we\nreplace the image input with text for Vision-language training. Inspired by\nprior noise injection methods to reduce modality gaps, we introduce Adaptive\nranged cosine Similarity injected noise (ArcSin). First, we introduce an\ninnovative adaptive noise scale that effectively generates the textual elements\nwith more variability while preserving the original text feature's integrity.\nSecond, a similarity pool strategy is employed, expanding the domain\ngeneralization potential by broadening the overall noise scale. This dual\nstrategy effectively broadens the scope of the original domain while\nsafeguarding content integrity. Our empirical results demonstrate that these\nmodels closely rival those trained on images in terms of performance.\nSpecifically, our method exhibits substantial improvements over the previous\nstate-of-the-art, achieving gains of 1.9 and 1.1 CIDEr points in S-Cap and\nM-Cap, respectively. Additionally, we observe increases of 0.5 percentage\npoints (pp), 1.4 pp, and 1.4 pp in accuracy for VQA, VQA-E, and VE,\nrespectively, pushing the boundaries of what is achievable within the\nconstraints of image-trained model benchmarks.\n","authors":["Yang Liu","Xiaomin Yu","Gongyu Zhang","Zhen Zhu","Christos Bergeles","Prokar Dasgupta","Alejandro Granados","Sebastien Ourselin"],"pdf_url":"https://arxiv.org/pdf/2402.17298v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23054v2","updated":"2024-11-22T16:04:44Z","published":"2024-10-30T14:21:33Z","title":"Controlling Language and Diffusion Models by Transporting Activations","summary":"  The increasing capabilities of large generative models and their ever more\nwidespread deployment have raised concerns about their reliability, safety, and\npotential misuse. To address these issues, recent works have proposed to\ncontrol model generation by steering model activations in order to effectively\ninduce or prevent the emergence of concepts or behaviors in the generated\noutput. In this paper we introduce Activation Transport (AcT), a general\nframework to steer activations guided by optimal transport theory that\ngeneralizes many previous activation-steering works. AcT is modality-agnostic\nand provides fine-grained control over the model behavior with negligible\ncomputational overhead, while minimally impacting model abilities. We\nexperimentally show the effectiveness and versatility of our approach by\naddressing key challenges in large language models (LLMs) and text-to-image\ndiffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate\ntoxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is,\nwe show how AcT enables fine-grained style control and concept negation.\n","authors":["Pau Rodriguez","Arno Blaas","Michal Klein","Luca Zappella","Nicholas Apostoloff","Marco Cuturi","Xavier Suau"],"pdf_url":"https://arxiv.org/pdf/2410.23054v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15028v1","updated":"2024-11-22T15:59:48Z","published":"2024-11-22T15:59:48Z","title":"FloAt: Flow Warping of Self-Attention for Clothing Animation Generation","summary":"  We propose a diffusion model-based approach, FloAtControlNet to generate\ncinemagraphs composed of animations of human clothing. We focus on human\nclothing like dresses, skirts and pants. The input to our model is a text\nprompt depicting the type of clothing and the texture of clothing like leopard,\nstriped, or plain, and a sequence of normal maps that capture the underlying\nanimation that we desire in the output. The backbone of our method is a\nnormal-map conditioned ControlNet which is operated in a training-free regime.\nThe key observation is that the underlying animation is embedded in the flow of\nthe normal maps. We utilize the flow thus obtained to manipulate the\nself-attention maps of appropriate layers. Specifically, the self-attention\nmaps of a particular layer and frame are recomputed as a linear combination of\nitself and the self-attention maps of the same layer and the previous frame,\nwarped by the flow on the normal maps of the two frames. We show that\nmanipulating the self-attention maps greatly enhances the quality of the\nclothing animation, making it look more natural as well as suppressing the\nbackground artifacts. Through extensive experiments, we show that the method\nproposed beats all baselines both qualitatively in terms of visual results and\nuser study. Specifically, our method is able to alleviate the background\nflickering that exists in other diffusion model-based baselines that we\nconsider. In addition, we show that our method beats all baselines in terms of\nRMSE and PSNR computed using the input normal map sequences and the normal map\nsequences obtained from the output RGB frames. Further, we show that\nwell-established evaluation metrics like LPIPS, SSIM, and CLIP scores that are\ngenerally for visual quality are not necessarily suitable for capturing the\nsubtle motions in human clothing animations.\n","authors":["Swasti Shreya Mishra","Kuldeep Kulkarni","Duygu Ceylan","Balaji Vasan Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2411.15028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.06077v4","updated":"2024-11-22T15:58:28Z","published":"2023-06-05T17:22:54Z","title":"Semantically-Prompted Language Models Improve Visual Descriptions","summary":"  Language-vision models like CLIP have made significant strides in vision\ntasks, such as zero-shot image classification (ZSIC). However, generating\nspecific and expressive visual descriptions remains challenging; descriptions\nproduced by current methods are often ambiguous and lacking in granularity. To\ntackle these issues, we propose V-GLOSS: Visual Glosses, a novel method built\nupon two key ideas. The first is Semantic Prompting, which conditions a\nlanguage model on structured semantic knowledge. The second is a new\ncontrastive algorithm that elicits fine-grained distinctions between similar\nconcepts. With both ideas, we demonstrate that V-GLOSS improves visual\ndescriptions and achieves strong results in the zero-shot setting on general\nand fine-grained image-classification datasets, including ImageNet, STL-10,\nFGVC Aircraft, and Flowers 102. Moreover, these descriptive capabilities\ncontribute to enhancing image-generation performance. Finally, we introduce a\nquality-tested silver dataset with descriptions generated with V-GLOSS for all\nImageNet classes.\n","authors":["Michael Ogezi","Bradley Hauer","Grzegorz Kondrak"],"pdf_url":"https://arxiv.org/pdf/2306.06077v4.pdf","comment":"Published at NAACL 2024. See\n  https://aclanthology.org/2024.findings-naacl.267/"},{"id":"http://arxiv.org/abs/2411.15024v1","updated":"2024-11-22T15:55:19Z","published":"2024-11-22T15:55:19Z","title":"DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models","summary":"  Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.\n","authors":["Keda Tao","Can Qin","Haoxuan You","Yang Sui","Huan Wang"],"pdf_url":"https://arxiv.org/pdf/2411.15024v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2404.14117v2","updated":"2024-11-22T15:51:52Z","published":"2024-04-22T12:07:10Z","title":"Hierarchical localization with panoramic views and triplet loss\n  functions","summary":"  The main objective of this paper is to tackle visual localization, which is\nessential for the safe navigation of mobile robots. The solution we propose\nemploys panoramic images and triplet convolutional neural networks. We seek to\nexploit the properties of such architectures to address both hierarchical and\nglobal localization in indoor environments, which are prone to visual aliasing\nand other phenomena. Considering their importance in these architectures, a\ncomplete comparative evaluation of different triplet loss functions is\nperformed. The experimental section proves that triplet networks can be trained\nwith a relatively low number of images captured under a specific lighting\ncondition and even so, the resulting networks are a robust tool to perform\nvisual localization under dynamic conditions. Our approach has been evaluated\nagainst some of these effects, such as changes in the lighting conditions,\nocclusions, noise and motion blurring. Furthermore, to explore the limits of\nour approach, triplet networks have been tested in different indoor\nenvironments simultaneously. In all the cases, these architectures have\ndemonstrated a great capability to generalize to diverse and challenging\nscenarios. The code used in the experiments is available at\nhttps://github.com/MarcosAlfaro/TripletNetworksIndoorLocalization.git.\n","authors":["Marcos Alfaro","Juan José Cabrera","María Flores","Óscar Reinoso","Luis Payá"],"pdf_url":"https://arxiv.org/pdf/2404.14117v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10745v2","updated":"2024-11-22T15:49:47Z","published":"2024-11-16T08:55:18Z","title":"TDSM: Triplet Diffusion for Skeleton-Text Matching in Zero-Shot Action\n  Recognition","summary":"  We firstly present a diffusion-based action recognition with zero-shot\nlearning for skeleton inputs. In zero-shot skeleton-based action recognition,\naligning skeleton features with the text features of action labels is essential\nfor accurately predicting unseen actions. Previous methods focus on direct\nalignment between skeleton and text latent spaces, but the modality gaps\nbetween these spaces hinder robust generalization learning. Motivated from the\nremarkable performance of text-to-image diffusion models, we leverage their\nalignment capabilities between different modalities mostly by focusing on the\ntraining process during reverse diffusion rather than using their generative\npower. Based on this, our framework is designed as a Triplet Diffusion for\nSkeleton-Text Matching (TDSM) method which aligns skeleton features with text\nprompts through reverse diffusion, embedding the prompts into the unified\nskeleton-text latent space to achieve robust matching. To enhance\ndiscriminative power, we introduce a novel triplet diffusion (TD) loss that\nencourages our TDSM to correct skeleton-text matches while pushing apart\nincorrect ones. Our TDSM significantly outperforms the very recent\nstate-of-the-art methods with large margins of 2.36%-point to 13.05%-point,\ndemonstrating superior accuracy and scalability in zero-shot settings through\neffective skeleton-text matching.\n","authors":["Jeonghyeok Do","Munchurl Kim"],"pdf_url":"https://arxiv.org/pdf/2411.10745v2.pdf","comment":"Please visit our project page at\n  https://kaist-viclab.github.io/TDSM_site/"},{"id":"http://arxiv.org/abs/2411.15018v1","updated":"2024-11-22T15:47:42Z","published":"2024-11-22T15:47:42Z","title":"Neural 4D Evolution under Large Topological Changes from 2D Images","summary":"  In the literature, it has been shown that the evolution of the known explicit\n3D surface to the target one can be learned from 2D images using the\ninstantaneous flow field, where the known and target 3D surfaces may largely\ndiffer in topology. We are interested in capturing 4D shapes whose topology\nchanges largely over time. We encounter that the straightforward extension of\nthe existing 3D-based method to the desired 4D case performs poorly.\n  In this work, we address the challenges in extending 3D neural evolution to\n4D under large topological changes by proposing two novel modifications. More\nprecisely, we introduce (i) a new architecture to discretize and encode the\ndeformation and learn the SDF and (ii) a technique to impose the temporal\nconsistency. (iii) Also, we propose a rendering scheme for color prediction\nbased on Gaussian splatting. Furthermore, to facilitate learning directly from\n2D images, we propose a learning framework that can disentangle the geometry\nand appearance from RGB images. This method of disentanglement, while also\nuseful for the 4D evolution problem that we are concentrating on, is also novel\nand valid for static scenes. Our extensive experiments on various data provide\nawesome results and, most importantly, open a new approach toward\nreconstructing challenging scenes with significant topological changes and\ndeformations. Our source code and the dataset are publicly available at\nhttps://github.com/insait-institute/N4DE.\n","authors":["AmirHossein Naghi Razlighi","Tiago Novello","Asen Nachkov","Thomas Probst","Danda Paudel"],"pdf_url":"https://arxiv.org/pdf/2411.15018v1.pdf","comment":"15 pages, 21 figures"},{"id":"http://arxiv.org/abs/2411.15016v1","updated":"2024-11-22T15:45:23Z","published":"2024-11-22T15:45:23Z","title":"MSSF: A 4D Radar and Camera Fusion Framework With Multi-Stage Sampling\n  for 3D Object Detection in Autonomous Driving","summary":"  As one of the automotive sensors that have emerged in recent years, 4D\nmillimeter-wave radar has a higher resolution than conventional 3D radar and\nprovides precise elevation measurements. But its point clouds are still sparse\nand noisy, making it challenging to meet the requirements of autonomous\ndriving. Camera, as another commonly used sensor, can capture rich semantic\ninformation. As a result, the fusion of 4D radar and camera can provide an\naffordable and robust perception solution for autonomous driving systems.\nHowever, previous radar-camera fusion methods have not yet been thoroughly\ninvestigated, resulting in a large performance gap compared to LiDAR-based\nmethods. Specifically, they ignore the feature-blurring problem and do not\ndeeply interact with image semantic information. To this end, we present a\nsimple but effective multi-stage sampling fusion (MSSF) network based on 4D\nradar and camera. On the one hand, we design a fusion block that can deeply\ninteract point cloud features with image features, and can be applied to\ncommonly used single-modal backbones in a plug-and-play manner. The fusion\nblock encompasses two types, namely, simple feature fusion (SFF) and multiscale\ndeformable feature fusion (MSDFF). The SFF is easy to implement, while the\nMSDFF has stronger fusion abilities. On the other hand, we propose a\nsemantic-guided head to perform foreground-background segmentation on voxels\nwith voxel feature re-weighting, further alleviating the problem of feature\nblurring. Extensive experiments on the View-of-Delft (VoD) and TJ4DRadset\ndatasets demonstrate the effectiveness of our MSSF. Notably, compared to\nstate-of-the-art methods, MSSF achieves a 7.0% and 4.0% improvement in 3D mean\naverage precision on the VoD and TJ4DRadSet datasets, respectively. It even\nsurpasses classical LiDAR-based methods on the VoD dataset.\n","authors":["Hongsi Liu","Jun Liu","Guangfeng Jiang","Xin Jin"],"pdf_url":"https://arxiv.org/pdf/2411.15016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.08508v2","updated":"2024-11-22T15:35:52Z","published":"2024-11-13T10:43:39Z","title":"BillBoard Splatting (BBSplat): Learnable Textured Primitives for Novel\n  View Synthesis","summary":"  We present billboard Splatting (BBSplat) - a novel approach for 3D scene\nrepresentation based on textured geometric primitives. BBSplat represents the\nscene as a set of optimizable textured planar primitives with learnable RGB\ntextures and alpha-maps to control their shape. BBSplat primitives can be used\nin any Gaussian Splatting pipeline as drop-in replacements for Gaussians. Our\nmethod's qualitative and quantitative improvements over 3D and 2D Gaussians are\nmost noticeable when fewer primitives are used, when BBSplat achieves over 1200\nFPS. Our novel regularization term encourages textures to have a sparser\nstructure, unlocking an efficient compression that leads to a reduction in\nstorage space of the model. Our experiments show the efficiency of BBSplat on\nstandard datasets of real indoor and outdoor scenes such as Tanks&Temples, DTU,\nand Mip-NeRF-360. We demonstrate improvements on PSNR, SSIM, and LPIPS metrics\ncompared to the state-of-the-art, especially for the case when fewer primitives\nare used, which, on the other hand, leads to up to 2 times inference speed\nimprovement for the same rendering quality.\n","authors":["David Svitov","Pietro Morerio","Lourdes Agapito","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2411.08508v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12168v2","updated":"2024-11-22T15:25:13Z","published":"2024-11-19T02:18:19Z","title":"Sketch-guided Cage-based 3D Gaussian Splatting Deformation","summary":"  3D Gaussian Splatting (GS) is one of the most promising novel 3D\nrepresentations that has received great interest in computer graphics and\ncomputer vision. While various systems have introduced editing capabilities for\n3D GS, such as those guided by text prompts, fine-grained control over\ndeformation remains an open challenge. In this work, we present a novel\nsketch-guided 3D GS deformation system that allows users to intuitively modify\nthe geometry of a 3D GS model by drawing a silhouette sketch from a single\nviewpoint. Our approach introduces a new deformation method that combines\ncage-based deformations with a variant of Neural Jacobian Fields, enabling\nprecise, fine-grained control. Additionally, it leverages large-scale 2D\ndiffusion priors and ControlNet to ensure the generated deformations are\nsemantically plausible. Through a series of experiments, we demonstrate the\neffectiveness of our method and showcase its ability to animate static 3D GS\nmodels as one of its key applications.\n","authors":["Tianhao Xie","Noam Aigerman","Eugene Belilovsky","Tiberiu Popa"],"pdf_url":"https://arxiv.org/pdf/2411.12168v2.pdf","comment":"10 pages, 9 figures, project page:\n  https://tianhaoxie.github.io/project/gs_deform/"},{"id":"http://arxiv.org/abs/2411.14992v1","updated":"2024-11-22T15:02:29Z","published":"2024-11-22T15:02:29Z","title":"Differentiable Biomechanics for Markerless Motion Capture in Upper Limb\n  Stroke Rehabilitation: A Comparison with Optical Motion Capture","summary":"  Marker-based Optical Motion Capture (OMC) paired with biomechanical modeling\nis currently considered the most precise and accurate method for measuring\nhuman movement kinematics. However, combining differentiable biomechanical\nmodeling with Markerless Motion Capture (MMC) offers a promising approach to\nmotion capture in clinical settings, requiring only minimal equipment, such as\nsynchronized webcams, and minimal effort for data collection. This study\ncompares key kinematic outcomes from biomechanically modeled MMC and OMC data\nin 15 stroke patients performing the drinking task, a functional task\nrecommended for assessing upper limb movement quality. We observed a high level\nof agreement in kinematic trajectories between MMC and OMC, as indicated by\nhigh correlations (median r above 0.95 for the majority of kinematic\ntrajectories) and median RMSE values ranging from 2-5 degrees for joint angles,\n0.04 m/s for end-effector velocity, and 6 mm for trunk displacement.\nTrial-to-trial biases between OMC and MMC were consistent within participant\nsessions, with interquartile ranges of bias around 1-3 degrees for joint\nangles, 0.01 m/s in end-effector velocity, and approximately 3mm for trunk\ndisplacement. Our findings indicate that our MMC for arm tracking is\napproaching the accuracy of marker-based methods, supporting its potential for\nuse in clinical settings. MMC could provide valuable insights into movement\nrehabilitation in stroke patients, potentially enhancing the effectiveness of\nrehabilitation strategies.\n","authors":["Tim Unger","Arash Sal Moslehian","J. D. Peiffer","Johann Ullrich","Roger Gassert","Olivier Lambercy","R. James Cotton","Chris Awai Easthope"],"pdf_url":"https://arxiv.org/pdf/2411.14992v1.pdf","comment":"7 pages, 4 figures, 3 tables, RehabWeek 2025 ICORR, first 3 authors\n  are shared-first and last two authors are shared last"},{"id":"http://arxiv.org/abs/2411.08127v2","updated":"2024-11-22T14:58:31Z","published":"2024-11-12T19:09:45Z","title":"TIPO: Text to Image with Text Presampling for Prompt Optimization","summary":"  TIPO (Text to Image with text pre-sampling for Prompt Optimization) is an\ninnovative framework designed to enhance text-to-image (T2I) generation by\nlanguage model (LM) for automatic prompt engineering. By refining and extending\nuser-provided prompts, TIPO bridges the gap between simple inputs and the\ndetailed prompts required for high-quality image generation. Unlike previous\napproaches that rely on Large Language Models (LLMs) or reinforcement learning\n(RL), TIPO adjusts user input prompts with the distribution of a trained prompt\ndataset, eliminating the need for complex runtime cost via lightweight model.\nThis pre-sampling approach enables efficient and scalable prompt optimization,\ngrounded in the model's training distribution. Experimental results demonstrate\nTIPO's effectiveness in improving aesthetic scores, reducing image corruption,\nand better aligning generated images with dataset distributions. These findings\nhighlight the critical role of prompt engineering in T2I systems and open\navenues for broader applications of automatic prompt refinement.\n","authors":["Shih-Ying Yeh","Sang-Hyun Park","Giyeong Oh","Min Song","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2411.08127v2.pdf","comment":"26 pages, 19 figures"},{"id":"http://arxiv.org/abs/2411.14982v1","updated":"2024-11-22T14:41:36Z","published":"2024-11-22T14:41:36Z","title":"Large Multi-modal Models Can Interpret Features in Large Multi-modal\n  Models","summary":"  Recent advances in Large Multimodal Models (LMMs) lead to significant\nbreakthroughs in both academia and industry. One question that arises is how\nwe, as humans, can understand their internal neural representations. This paper\ntakes an initial step towards addressing this question by presenting a\nversatile framework to identify and interpret the semantics within LMMs.\nSpecifically, 1) we first apply a Sparse Autoencoder(SAE) to disentangle the\nrepresentations into human understandable features. 2) We then present an\nautomatic interpretation framework to interpreted the open-semantic features\nlearned in SAE by the LMMs themselves. We employ this framework to analyze the\nLLaVA-NeXT-8B model using the LLaVA-OV-72B model, demonstrating that these\nfeatures can effectively steer the model's behavior. Our results contribute to\na deeper understanding of why LMMs excel in specific tasks, including EQ tests,\nand illuminate the nature of their mistakes along with potential strategies for\ntheir rectification. These findings offer new insights into the internal\nmechanisms of LMMs and suggest parallels with the cognitive processes of the\nhuman brain.\n","authors":["Kaichen Zhang","Yifei Shen","Bo Li","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2411.14982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14975v1","updated":"2024-11-22T14:34:04Z","published":"2024-11-22T14:34:04Z","title":"Exploring Foundation Models Fine-Tuning for Cytology Classification","summary":"  Cytology slides are essential tools in diagnosing and staging cancer, but\ntheir analysis is time-consuming and costly. Foundation models have shown great\npotential to assist in these tasks. In this paper, we explore how existing\nfoundation models can be applied to cytological classification. More\nparticularly, we focus on low-rank adaptation, a parameter-efficient\nfine-tuning method suited to few-shot learning. We evaluated five foundation\nmodels across four cytological classification datasets. Our results demonstrate\nthat fine-tuning the pre-trained backbones with LoRA significantly improves\nmodel performance compared to fine-tuning only the classifier head, achieving\nstate-of-the-art results on both simple and complex classification tasks while\nrequiring fewer data samples.\n","authors":["Manon Dausort","Tiffanie Godelaine","Maxime Zanella","Karim El Khoury","Isabelle Salmon","Benoît Macq"],"pdf_url":"https://arxiv.org/pdf/2411.14975v1.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2411.14974v1","updated":"2024-11-22T14:31:39Z","published":"2024-11-22T14:31:39Z","title":"3D Convex Splatting: Radiance Field Rendering with 3D Smooth Convexes","summary":"  Recent advances in radiance field reconstruction, such as 3D Gaussian\nSplatting (3DGS), have achieved high-quality novel view synthesis and fast\nrendering by representing scenes with compositions of Gaussian primitives.\nHowever, 3D Gaussians present several limitations for scene reconstruction.\nAccurately capturing hard edges is challenging without significantly increasing\nthe number of Gaussians, creating a large memory footprint. Moreover, they\nstruggle to represent flat surfaces, as they are diffused in space. Without\nhand-crafted regularizers, they tend to disperse irregularly around the actual\nsurface. To circumvent these issues, we introduce a novel method, named 3D\nConvex Splatting (3DCS), which leverages 3D smooth convexes as primitives for\nmodeling geometrically-meaningful radiance fields from multi-view images.\nSmooth convex shapes offer greater flexibility than Gaussians, allowing for a\nbetter representation of 3D scenes with hard edges and dense volumes using\nfewer primitives. Powered by our efficient CUDA-based rasterizer, 3DCS achieves\nsuperior performance over 3DGS on benchmarks such as Mip-NeRF360, Tanks and\nTemples, and Deep Blending. Specifically, our method attains an improvement of\nup to 0.81 in PSNR and 0.026 in LPIPS compared to 3DGS while maintaining high\nrendering speeds and reducing the number of required primitives. Our results\nhighlight the potential of 3D Convex Splatting to become the new standard for\nhigh-quality scene reconstruction and novel view synthesis. Project page:\nwww.convexsplatting.com.\n","authors":["Jan Held","Renaud Vandeghen","Abdullah Hamdi","Adrien Deliege","Anthony Cioppa","Silvio Giancola","Andrea Vedaldi","Bernard Ghanem","Marc Van Droogenbroeck"],"pdf_url":"https://arxiv.org/pdf/2411.14974v1.pdf","comment":"13 pages, 13 figures, 10 tables"},{"id":"http://arxiv.org/abs/2411.14967v1","updated":"2024-11-22T14:23:07Z","published":"2024-11-22T14:23:07Z","title":"SwissADT: An Audio Description Translation System for Swiss Languages","summary":"  Audio description (AD) is a crucial accessibility service provided to blind\npersons and persons with visual impairment, designed to convey visual\ninformation in acoustic form. Despite recent advancements in multilingual\nmachine translation research, the lack of well-crafted and time-synchronized AD\ndata impedes the development of audio description translation (ADT) systems\nthat address the needs of multilingual countries such as Switzerland.\nFurthermore, since the majority of ADT systems rely solely on text, uncertainty\nexists as to whether incorporating visual information from the corresponding\nvideo clips can enhance the quality of ADT outputs. In this work, we present\nSwissADT, the first ADT system implemented for three main Swiss languages and\nEnglish. By collecting well-crafted AD data augmented with video clips in\nGerman, French, Italian, and English, and leveraging the power of Large\nLanguage Models (LLMs), we aim to enhance information accessibility for diverse\nlanguage populations in Switzerland by automatically translating AD scripts to\nthe desired Swiss language. Our extensive experimental ADT results, composed of\nboth automatic and human evaluations of ADT quality, demonstrate the promising\ncapability of SwissADT for the ADT task. We believe that combining human\nexpertise with the generation power of LLMs can further enhance the performance\nof ADT systems, ultimately benefiting a larger multilingual target population.\n","authors":["Lukas Fischer","Yingqiang Gao","Alexa Lintner","Sarah Ebling"],"pdf_url":"https://arxiv.org/pdf/2411.14967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14961v1","updated":"2024-11-22T14:19:01Z","published":"2024-11-22T14:19:01Z","title":"LoRA-FAIR: Federated LoRA Fine-Tuning with Aggregation and\n  Initialization Refinement","summary":"  Foundation models (FMs) achieve strong performance across diverse tasks with\ntask-specific fine-tuning, yet full parameter fine-tuning is often\ncomputationally prohibitive for large models. Parameter-efficient fine-tuning\n(PEFT) methods like Low-Rank Adaptation (LoRA) reduce this cost by introducing\nlow-rank matrices for tuning fewer parameters. While LoRA allows for efficient\nfine-tuning, it requires significant data for adaptation, making Federated\nLearning (FL) an appealing solution due to its privacy-preserving collaborative\nframework. However, combining LoRA with FL introduces two key challenges: the\n\\textbf{Server-Side LoRA Aggregation Bias}, where server-side averaging of LoRA\nmatrices diverges from the ideal global update, and the \\textbf{Client-Side\nLoRA Initialization Drift}, emphasizing the need for consistent initialization\nacross rounds. Existing approaches address these challenges individually,\nlimiting their effectiveness. We propose LoRA-FAIR, a novel method that tackles\nboth issues by introducing a correction term on the server while keeping the\noriginal LoRA modules, enhancing aggregation efficiency and accuracy. LoRA-FAIR\nmaintains computational and communication efficiency, yielding superior\nperformance over state-of-the-art methods. Experimental results on ViT and\nMLP-Mixer models across large-scale datasets demonstrate that LoRA-FAIR\nconsistently achieves performance improvements in FL settings.\n","authors":["Jieming Bian","Lei Wang","Letian Zhang","Jie Xu"],"pdf_url":"https://arxiv.org/pdf/2411.14961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14959v1","updated":"2024-11-22T14:17:46Z","published":"2024-11-22T14:17:46Z","title":"Design-o-meter: Towards Evaluating and Refining Graphic Designs","summary":"  Graphic designs are an effective medium for visual communication. They range\nfrom greeting cards to corporate flyers and beyond. Off-late, machine learning\ntechniques are able to generate such designs, which accelerates the rate of\ncontent production. An automated way of evaluating their quality becomes\ncritical. Towards this end, we introduce Design-o-meter, a data-driven\nmethodology to quantify the goodness of graphic designs. Further, our approach\ncan suggest modifications to these designs to improve its visual appeal. To the\nbest of our knowledge, Design-o-meter is the first approach that scores and\nrefines designs in a unified framework despite the inherent subjectivity and\nambiguity of the setting. Our exhaustive quantitative and qualitative analysis\nof our approach against baselines adapted for the task (including recent\nMultimodal LLM-based approaches) brings out the efficacy of our methodology. We\nhope our work will usher more interest in this important and pragmatic problem\nsetting.\n","authors":["Sahil Goyal","Abhinav Mahajan","Swasti Mishra","Prateksha Udhayanan","Tripti Shukla","K J Joseph","Balaji Vasan Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2411.14959v1.pdf","comment":"Accepted to WACV 2025. Project page:\n  https://sahilg06.github.io/Design-o-meter/"},{"id":"http://arxiv.org/abs/2411.14953v1","updated":"2024-11-22T14:12:35Z","published":"2024-11-22T14:12:35Z","title":"Evaluating Vision Transformer Models for Visual Quality Control in\n  Industrial Manufacturing","summary":"  One of the most promising use-cases for machine learning in industrial\nmanufacturing is the early detection of defective products using a quality\ncontrol system. Such a system can save costs and reduces human errors due to\nthe monotonous nature of visual inspections. Today, a rich body of research\nexists which employs machine learning methods to identify rare defective\nproducts in unbalanced visual quality control datasets. These methods typically\nrely on two components: A visual backbone to capture the features of the input\nimage and an anomaly detection algorithm that decides if these features are\nwithin an expected distribution. With the rise of transformer architecture as\nvisual backbones of choice, there exists now a great variety of different\ncombinations of these two components, ranging all along the trade-off between\ndetection quality and inference time. Facing this variety, practitioners in the\nfield often have to spend a considerable amount of time on researching the\nright combination for their use-case at hand. Our contribution is to help\npractitioners with this choice by reviewing and evaluating current vision\ntransformer models together with anomaly detection methods. For this, we chose\nSotA models of both disciplines, combined them and evaluated them towards the\ngoal of having small, fast and efficient anomaly detection models suitable for\nindustrial manufacturing. We evaluated the results of our experiments on the\nwell-known MVTecAD and BTAD datasets. Moreover, we give guidelines for choosing\na suitable model architecture for a quality control system in practice,\nconsidering given use-case and hardware constraints.\n","authors":["Miriam Alber","Christoph Hönes","Patrick Baier"],"pdf_url":"https://arxiv.org/pdf/2411.14953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14951v1","updated":"2024-11-22T14:09:56Z","published":"2024-11-22T14:09:56Z","title":"Morph: A Motion-free Physics Optimization Framework for Human Motion\n  Generation","summary":"  Human motion generation plays a vital role in applications such as digital\nhumans and humanoid robot control. However, most existing approaches disregard\nphysics constraints, leading to the frequent production of physically\nimplausible motions with pronounced artifacts such as floating and foot\nsliding. In this paper, we propose \\textbf{Morph}, a\n\\textbf{Mo}tion-f\\textbf{r}ee \\textbf{ph}ysics optimization framework,\ncomprising a Motion Generator and a Motion Physics Refinement module, for\nenhancing physical plausibility without relying on costly real-world motion\ndata. Specifically, the Motion Generator is responsible for providing\nlarge-scale synthetic motion data, while the Motion Physics Refinement Module\nutilizes these synthetic data to train a motion imitator within a physics\nsimulator, enforcing physical constraints to project the noisy motions into a\nphysically-plausible space. These physically refined motions, in turn, are used\nto fine-tune the Motion Generator, further enhancing its capability.\nExperiments on both text-to-motion and music-to-dance generation tasks\ndemonstrate that our framework achieves state-of-the-art motion generation\nquality while improving physical plausibility drastically.\n","authors":["Zhuo Li","Mingshuang Luo","Ruibing Hou","Xin Zhao","Hao Liu","Hong Chang","Zimo Liu","Chen Li"],"pdf_url":"https://arxiv.org/pdf/2411.14951v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2211.00577v9","updated":"2024-11-22T14:01:43Z","published":"2022-11-01T16:48:04Z","title":"Fine-tuned Generative Adversarial Network-based Model for Medical Image\n  Super-Resolution","summary":"  In the field of medical image analysis, there is a substantial need for\nhigh-resolution (HR) images to improve diagnostic accuracy. However, it is a\nchallenging task to obtain HR medical images, as it requires advanced\ninstruments and significant time. Deep learning-based super-resolution methods\ncan help to improve the resolution and perceptual quality of low-resolution\n(LR) medical images. Recently, Generative Adversarial Network (GAN) based\nmethods have shown remarkable performance among deep learning-based\nsuper-resolution methods. Real-Enhanced Super-Resolution Generative Adversarial\nNetwork (Real-ESRGAN) is a practical model for recovering HR images from\nreal-world LR images. In our proposed approach, we use transfer learning\ntechnique and fine-tune the pre-trained Real-ESRGAN model using medical image\ndatasets. This technique helps in improving the performance of the model. We\nemploy the high-order degradation model of the Real-ESRGAN which better\nsimulates real-world image degradations. This adaptation allows for generating\nmore realistic degraded medical images, resulting in improved performance. The\nfocus of this paper is on enhancing the resolution and perceptual quality of\nchest X-ray and retinal images. We use the Tuberculosis chest X-ray (Shenzhen)\ndataset and the STARE dataset of retinal images for fine-tuning the model. The\nproposed model achieves superior perceptual quality compared to the Real-ESRGAN\nmodel, effectively preserving fine details and generating images with more\nrealistic textures.\n","authors":["Alireza Aghelan","Modjtaba Rouhani"],"pdf_url":"https://arxiv.org/pdf/2211.00577v9.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14946v1","updated":"2024-11-22T13:57:56Z","published":"2024-11-22T13:57:56Z","title":"Reliable Evaluation of Attribution Maps in CNNs: A Perturbation-Based\n  Approach","summary":"  In this paper, we present an approach for evaluating attribution maps, which\nplay a central role in interpreting the predictions of convolutional neural\nnetworks (CNNs). We show that the widely used insertion/deletion metrics are\nsusceptible to distribution shifts that affect the reliability of the ranking.\nOur method proposes to replace pixel modifications with adversarial\nperturbations, which provides a more robust evaluation framework. By using\nsmoothness and monotonicity measures, we illustrate the effectiveness of our\napproach in correcting distribution shifts. In addition, we conduct the most\ncomprehensive quantitative and qualitative assessment of attribution maps to\ndate. Introducing baseline attribution maps as sanity checks, we find that our\nmetric is the only contender to pass all checks. Using Kendall's $\\tau$ rank\ncorrelation coefficient, we show the increased consistency of our metric across\n15 dataset-architecture combinations. Of the 16 attribution maps tested, our\nresults clearly show SmoothGrad to be the best map currently available. This\nresearch makes an important contribution to the development of attribution maps\nby providing a reliable and consistent evaluation framework. To ensure\nreproducibility, we will provide the code along with our results.\n","authors":["Lars Nieradzik","Henrike Stephani","Janis Keuper"],"pdf_url":"https://arxiv.org/pdf/2411.14946v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14927v1","updated":"2024-11-22T13:34:29Z","published":"2024-11-22T13:34:29Z","title":"LiDAR-based End-to-end Temporal Perception for Vehicle-Infrastructure\n  Cooperation","summary":"  Temporal perception, the ability to detect and track objects over time, is\ncritical in autonomous driving for maintaining a comprehensive understanding of\ndynamic environments. However, this task is hindered by significant challenges,\nincluding incomplete perception caused by occluded objects and observational\nblind spots, which are common in single-vehicle perception systems. To address\nthese issues, we introduce LET-VIC, a LiDAR-based End-to-End Tracking framework\nfor Vehicle-Infrastructure Cooperation (VIC). LET-VIC leverages\nVehicle-to-Everything (V2X) communication to enhance temporal perception by\nfusing spatial and temporal data from both vehicle and infrastructure sensors.\nFirst, it spatially integrates Bird's Eye View (BEV) features from vehicle-side\nand infrastructure-side LiDAR data, creating a comprehensive view that\nmitigates occlusions and compensates for blind spots. Second, LET-VIC\nincorporates temporal context across frames, allowing the model to leverage\nhistorical data for enhanced tracking stability and accuracy. To further\nimprove robustness, LET-VIC includes a Calibration Error Compensation (CEC)\nmodule to address sensor misalignments and ensure precise feature alignment.\nExperiments on the V2X-Seq-SPD dataset demonstrate that LET-VIC significantly\noutperforms baseline models, achieving at least a 13.7% improvement in mAP and\na 13.1% improvement in AMOTA without considering communication delays. This\nwork offers a practical solution and a new research direction for advancing\ntemporal perception in autonomous driving through vehicle-infrastructure\ncooperation.\n","authors":["Zhenwei Yang","Jilei Mao","Wenxian Yang","Yibo Ai","Yu Kong","Haibao Yu","Weidong Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14927v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2411.11458v2","updated":"2024-11-22T13:32:01Z","published":"2024-11-18T10:46:05Z","title":"HistoEncoder: a digital pathology foundation model for prostate cancer","summary":"  Foundation models are trained on massive amounts of data to distinguish\ncomplex patterns and can be adapted to a wide range of downstream tasks with\nminimal computational resources. Here, we develop a foundation model for\nprostate cancer digital pathology called HistoEncoder by pre-training on 48\nmillion prostate tissue tile images. We demonstrate that HistoEncoder features\nextracted from tile images with similar histological patterns map closely\ntogether in the feature space. HistoEncoder outperforms models pre-trained with\nnatural images, even without fine-tuning or with 1000 times less training data.\nWe describe two use cases that leverage the capabilities of HistoEncoder by\nfine-tuning the model with a limited amount of data and computational\nresources. First, we show how HistoEncoder can be used to automatically\nannotate large-scale datasets with high accuracy. Second, we combine histomics\nwith commonly used clinical nomograms, significantly improving prostate\ncancer-specific death survival models. Foundation models such as HistoEncoder\ncan allow organizations with limited resources to build effective clinical\nsoftware tools without needing extensive datasets or significant amounts of\ncomputing.\n","authors":["Joona Pohjonen","Abderrahim-Oussama Batouche","Antti Rannikko","Kevin Sandeman","Andrew Erickson","Esa Pitkanen","Tuomas Mirtti"],"pdf_url":"https://arxiv.org/pdf/2411.11458v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14901v1","updated":"2024-11-22T12:46:50Z","published":"2024-11-22T12:46:50Z","title":"ReVisionLLM: Recursive Vision-Language Model for Temporal Grounding in\n  Hour-Long Videos","summary":"  Large language models (LLMs) excel at retrieving information from lengthy\ntext, but their vision-language counterparts (VLMs) face difficulties with\nhour-long videos, especially for temporal grounding. Specifically, these VLMs\nare constrained by frame limitations, often losing essential temporal details\nneeded for accurate event localization in extended video content. We propose\nReVisionLLM, a recursive vision-language model designed to locate events in\nhour-long videos. Inspired by human search strategies, our model initially\ntargets broad segments of interest, progressively revising its focus to\npinpoint exact temporal boundaries. Our model can seamlessly handle videos of\nvastly different lengths, from minutes to hours. We also introduce a\nhierarchical training strategy that starts with short clips to capture distinct\nevents and progressively extends to longer videos. To our knowledge,\nReVisionLLM is the first VLM capable of temporal grounding in hour-long videos,\noutperforming previous state-of-the-art methods across multiple datasets by a\nsignificant margin (+2.6% R1@0.1 on MAD). The code is available at\nhttps://github.com/Tanveer81/ReVisionLLM.\n","authors":["Tanveer Hannan","Md Mohaiminul Islam","Jindong Gu","Thomas Seidl","Gedas Bertasius"],"pdf_url":"https://arxiv.org/pdf/2411.14901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13548v4","updated":"2024-11-22T12:43:37Z","published":"2024-09-20T14:47:58Z","title":"Data Diet: Can Trimming PET/CT Datasets Enhance Lesion Segmentation?","summary":"  In this work, we describe our approach to compete in the autoPET3 datacentric\ntrack. While conventional wisdom suggests that larger datasets lead to better\nmodel performance, recent studies indicate that excluding certain training\nsamples can enhance model accuracy. We find that in the autoPETIII dataset, a\nmodel that is trained on the entire dataset exhibits undesirable\ncharacteristics by producing a large number of false positives particularly for\nPSMA-PETs. We counteract this by removing the easiest samples from the training\ndataset as measured by the model loss before retraining from scratch. Using the\nproposed approach we manage to drive down the false negative volume and improve\nupon the baseline model in both false negative volume and dice score on the\npreliminary test set. Code and pre-trained models are available at\ngithub.com/alexanderjaus/autopet3_datadiet.\n","authors":["Alexander Jaus","Simon Reiß","Jens Kleesiek","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2409.13548v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.08567v2","updated":"2024-11-22T12:37:50Z","published":"2024-11-13T12:27:21Z","title":"Saliency Map-based Image Retrieval using Invariant Krawtchouk Moments","summary":"  With the widespread adoption of digital devices equipped with cameras and the\nrapid development of Internet technology, numerous content-based image\nretrieval systems and novel image feature extraction techniques have emerged in\nrecent years. This paper introduces a saliency map-based image retrieval\napproach using invariant Krawtchouk moments (SM-IKM) to enhance retrieval speed\nand accuracy. The proposed method applies a global contrast-based salient\nregion detection algorithm to create a saliency map that effectively isolates\nthe foreground from the background. It then combines multiple orders of\ninvariant Krawtchouk moments (IKM) with local binary patterns (LBPs) and color\nhistograms to comprehensively represent the foreground and background.\nAdditionally, it incorporates LBPs derived from the saliency map to improve\ndiscriminative power, facilitating more precise image differentiation. A\nbag-of-visual-words (BoVW) model is employed to generate a codebook for\nclassification and discrimination. By using compact IKMs in the BoVW framework\nand integrating a range of region-based feature-including color histograms,\nLBPs, and saliency map-enhanced LBPs, our proposed SM-IKM achieves efficient\nand accurate image retrieval. Extensive experiments on publicly available\ndatasets, such as Caltech 101 and Wang, demonstrate that SM-IKM outperforms\nrecent state-of-the-art retrieval methods. The source code for SM-IKM is\navailable at github.com/arnejad/SMIKM.\n","authors":["Ashkan Nejad","Mohammad Reza Faraji","Xiaojun Qi"],"pdf_url":"https://arxiv.org/pdf/2411.08567v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14883v1","updated":"2024-11-22T12:06:24Z","published":"2024-11-22T12:06:24Z","title":"Boundless Across Domains: A New Paradigm of Adaptive Feature and\n  Cross-Attention for Domain Generalization in Medical Image Segmentation","summary":"  Domain-invariant representation learning is a powerful method for domain\ngeneralization. Previous approaches face challenges such as high computational\ndemands, training instability, and limited effectiveness with high-dimensional\ndata, potentially leading to the loss of valuable features. To address these\nissues, we hypothesize that an ideal generalized representation should exhibit\nsimilar pattern responses within the same channel across cross-domain images.\nBased on this hypothesis, we use deep features from the source domain as\nqueries, and deep features from the generated domain as keys and values.\nThrough a cross-channel attention mechanism, the original deep features are\nreconstructed into robust regularization representations, forming an explicit\nconstraint that guides the model to learn domain-invariant representations.\nAdditionally, style augmentation is another common method. However, existing\nmethods typically generate new styles through convex combinations of source\ndomains, which limits the diversity of training samples by confining the\ngenerated styles to the original distribution. To overcome this limitation, we\npropose an Adaptive Feature Blending (AFB) method that generates\nout-of-distribution samples while exploring the in-distribution space,\nsignificantly expanding the domain range. Extensive experimental results\ndemonstrate that our proposed methods achieve superior performance on two\nstandard domain generalization benchmarks for medical image segmentation.\n","authors":["Yuheng Xu","Taiping Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14883v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2408.01231v2","updated":"2024-11-22T12:04:05Z","published":"2024-08-02T12:44:07Z","title":"WaveMamba: Spatial-Spectral Wavelet Mamba for Hyperspectral Image\n  Classification","summary":"  Hyperspectral Imaging (HSI) has proven to be a powerful tool for capturing\ndetailed spectral and spatial information across diverse applications. Despite\nthe advancements in Deep Learning (DL) and Transformer architectures for HSI\nclassification, challenges such as computational efficiency and the need for\nextensive labeled data persist. This paper introduces WaveMamba, a novel\napproach that integrates wavelet transformation with the spatial-spectral Mamba\narchitecture to enhance HSI classification. WaveMamba captures both local\ntexture patterns and global contextual relationships in an end-to-end trainable\nmodel. The Wavelet-based enhanced features are then processed through the\nstate-space architecture to model spatial-spectral relationships and temporal\ndependencies. The experimental results indicate that WaveMamba surpasses\nexisting models, achieving an accuracy improvement of 4.5\\% on the University\nof Houston dataset and a 2.0\\% increase on the Pavia University dataset.\n","authors":["Muhammad Ahmad","Muhammad Usama","Manuel Mazzara","Salvatore Distefano"],"pdf_url":"https://arxiv.org/pdf/2408.01231v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07329v4","updated":"2024-11-22T11:57:08Z","published":"2024-06-11T15:00:24Z","title":"Cinematic Gaussians: Real-Time HDR Radiance Fields with Depth of Field","summary":"  Radiance field methods represent the state of the art in reconstructing\ncomplex scenes from multi-view photos. However, these reconstructions often\nsuffer from one or both of the following limitations: First, they typically\nrepresent scenes in low dynamic range (LDR), which restricts their use to\nevenly lit environments and hinders immersive viewing experiences. Secondly,\ntheir reliance on a pinhole camera model, assuming all scene elements are in\nfocus in the input images, presents practical challenges and complicates\nrefocusing during novel-view synthesis. Addressing these limitations, we\npresent a lightweight method based on 3D Gaussian Splatting that utilizes\nmulti-view LDR images of a scene with varying exposure times, apertures, and\nfocus distances as input to reconstruct a high-dynamic-range (HDR) radiance\nfield. By incorporating analytical convolutions of Gaussians based on a\nthin-lens camera model as well as a tonemapping module, our reconstructions\nenable the rendering of HDR content with flexible refocusing capabilities. We\ndemonstrate that our combined treatment of HDR and depth of field facilitates\nreal-time cinematic rendering, outperforming the state of the art.\n","authors":["Chao Wang","Krzysztof Wolski","Bernhard Kerbl","Ana Serrano","Mojtaba Bemana","Hans-Peter Seidel","Karol Myszkowski","Thomas Leimkühler"],"pdf_url":"https://arxiv.org/pdf/2406.07329v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14873v1","updated":"2024-11-22T11:53:33Z","published":"2024-11-22T11:53:33Z","title":"Implementation of Real-Time Lane Detection on Autonomous Mobile Robot","summary":"  This paper describes the implementation of a learning-based lane detection\nalgorithm on an Autonomous Mobile Robot. It aims to implement the Ultra Fast\nLane Detection algorithm for real-time application on the SEATER P2MC-BRIN\nprototype using a camera and optimize its performance on the Jetson Nano\nplatform. Preliminary experiments were conducted to evaluate the algorithm's\nperformance in terms of data processing speed and accuracy using two types of\ndatasets: outdoor using a public dataset and indoor using an internal dataset\nfrom the indoor area of the BRIN Workshop Building in Bandung. The experiments\nrevealed that the algorithm runs more optimally on the Jetson Nano platform\nafter conversion to TensorRT compared to the ONNX model, achieving processing\nspeeds of approximately 101 ms using CULane and 105 ms using TuSimple, which is\nabout 22 times faster than the previous model. While the algorithm demonstrates\ngood accuracy on the outdoor public dataset, its performance falls short on the\nindoor dataset. Future work should focus on transfer learning and fine-tuning\nto enhance indoor lane detection accuracy.\n","authors":["Midriem Mirdanies","Roni Permana Saputra","Edwar Yazid","Rozeha A. Rashid"],"pdf_url":"https://arxiv.org/pdf/2411.14873v1.pdf","comment":"4 pages, 9 figures 2 tables"},{"id":"http://arxiv.org/abs/2408.17135v2","updated":"2024-11-22T11:46:48Z","published":"2024-08-30T09:22:07Z","title":"TIMotion: Temporal and Interactive Framework for Efficient Human-Human\n  Motion Generation","summary":"  Human-human motion generation is essential for understanding humans as social\nbeings. Current methods fall into two main categories: single-person-based\nmethods and separate modeling-based methods. To delve into this field, we\nabstract the overall generation process into a general framework MetaMotion,\nwhich consists of two phases: temporal modeling and interaction mixing. For\ntemporal modeling, the single-person-based methods concatenate two people into\na single one directly, while the separate modeling-based methods skip the\nmodeling of interaction sequences. The inadequate modeling described above\nresulted in sub-optimal performance and redundant model parameters. In this\npaper, we introduce TIMotion (Temporal and Interactive Modeling), an efficient\nand effective framework for human-human motion generation. Specifically, we\nfirst propose Causal Interactive Injection to model two separate sequences as a\ncausal sequence leveraging the temporal and causal properties. Then we present\nRole-Evolving Scanning to adjust to the change in the active and passive roles\nthroughout the interaction. Finally, to generate smoother and more rational\nmotion, we design Localized Pattern Amplification to capture short-term motion\npatterns. Extensive experiments on InterHuman and InterX demonstrate that our\nmethod achieves superior performance. The project code will be released upon\nacceptance. Project page: https://aigc-explorer.github.io/TIMotion-page/\n","authors":["Yabiao Wang","Shuo Wang","Jiangning Zhang","Ke Fan","Jiafu Wu","Zhucun Xue","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2408.17135v2.pdf","comment":"Project page: https://aigc-explorer.github.io/TIMotion-page/"},{"id":"http://arxiv.org/abs/2411.14871v1","updated":"2024-11-22T11:45:33Z","published":"2024-11-22T11:45:33Z","title":"Prioritize Denoising Steps on Diffusion Model Preference Alignment via\n  Explicit Denoised Distribution Estimation","summary":"  Diffusion models have shown remarkable success in text-to-image generation,\nmaking alignment methods for these models increasingly important. A key\nchallenge is the sparsity of preference labels, which are typically available\nonly at the terminal of denoising trajectories. This raises the issue of how to\nassign credit across denoising steps based on these sparse labels. In this\npaper, we propose Denoised Distribution Estimation (DDE), a novel method for\ncredit assignment. Unlike previous approaches that rely on auxiliary models or\nhand-crafted schemes, DDE derives its strategy more explicitly. The proposed\nDDE directly estimates the terminal denoised distribution from the perspective\nof each step. It is equipped with two estimation strategies and capable of\nrepresenting the entire denoising trajectory with a single model inference.\nTheoretically and empirically, we show that DDE prioritizes optimizing the\nmiddle part of the denoising trajectory, resulting in a novel and effective\ncredit assignment scheme. Extensive experiments demonstrate that our approach\nachieves superior performance, both quantitatively and qualitatively.\n","authors":["Dingyuan Shi","Yong Wang","Hangyu Li","Xiangxiang Chu"],"pdf_url":"https://arxiv.org/pdf/2411.14871v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14869v1","updated":"2024-11-22T11:35:42Z","published":"2024-11-22T11:35:42Z","title":"BIP3D: Bridging 2D Images and 3D Perception for Embodied Intelligence","summary":"  In embodied intelligence systems, a key component is 3D perception algorithm,\nwhich enables agents to understand their surrounding environments. Previous\nalgorithms primarily rely on point cloud, which, despite offering precise\ngeometric information, still constrain perception performance due to inherent\nsparsity, noise, and data scarcity. In this work, we introduce a novel\nimage-centric 3D perception model, BIP3D, which leverages expressive image\nfeatures with explicit 3D position encoding to overcome the limitations of\npoint-centric methods. Specifically, we leverage pre-trained 2D vision\nfoundation models to enhance semantic understanding, and introduce a spatial\nenhancer module to improve spatial understanding. Together, these modules\nenable BIP3D to achieve multi-view, multi-modal feature fusion and end-to-end\n3D perception. In our experiments, BIP3D outperforms current state-of-the-art\nresults on the EmbodiedScan benchmark, achieving improvements of 5.69% in the\n3D detection task and 15.25% in the 3D visual grounding task.\n","authors":["Xuewu Lin","Tianwei Lin","Lichao Huang","Hongyu Xie","Zhizhong Su"],"pdf_url":"https://arxiv.org/pdf/2411.14869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14868v1","updated":"2024-11-22T11:34:18Z","published":"2024-11-22T11:34:18Z","title":"Defective Edge Detection Using Cascaded Ensemble Canny Operator","summary":"  Edge detection has been one of the most difficult challenges in computer\nvision because of the difficulty in identifying the borders and edges from the\nreal-world images including objects of varying kinds and sizes. Methods based\non ensemble learning, which use a combination of backbones and attention\nmodules, outperformed more conventional approaches, such as Sobel and Canny\nedge detection. Nevertheless, these algorithms are still challenged when faced\nwith complicated scene photos. In addition, the identified edges utilizing the\ncurrent methods are not refined and often include incorrect edges. In this\nwork, we used a Cascaded Ensemble Canny operator to solve these problems and\ndetect the object edges. The most difficult Fresh and Rotten and Berkeley\ndatasets are used to test the suggested approach in Python. In terms of\nperformance metrics and output picture quality, the acquired results outperform\nthe specified edge detection networks\n","authors":["Anjali Nambiyar Rajkumar Kannan"],"pdf_url":"https://arxiv.org/pdf/2411.14868v1.pdf","comment":"2 Pages and 2 Figures"},{"id":"http://arxiv.org/abs/2411.14865v1","updated":"2024-11-22T11:31:01Z","published":"2024-11-22T11:31:01Z","title":"Benchmarking the Robustness of Optical Flow Estimation to Corruptions","summary":"  Optical flow estimation is extensively used in autonomous driving and video\nediting. While existing models demonstrate state-of-the-art performance across\nvarious benchmarks, the robustness of these methods has been infrequently\ninvestigated. Despite some research focusing on the robustness of optical flow\nmodels against adversarial attacks, there has been a lack of studies\ninvestigating their robustness to common corruptions. Taking into account the\nunique temporal characteristics of optical flow, we introduce 7 temporal\ncorruptions specifically designed for benchmarking the robustness of optical\nflow models, in addition to 17 classical single-image corruptions, in which\nadvanced PSF Blur simulation method is performed. Two robustness benchmarks,\nKITTI-FC and GoPro-FC, are subsequently established as the first corruption\nrobustness benchmark for optical flow estimation, with Out-Of-Domain (OOD) and\nIn-Domain (ID) settings to facilitate comprehensive studies. Robustness\nmetrics, Corruption Robustness Error (CRE), Corruption Robustness Error ratio\n(CREr), and Relative Corruption Robustness Error (RCRE) are further introduced\nto quantify the optical flow estimation robustness. 29 model variants from 15\noptical flow methods are evaluated, yielding 10 intriguing observations, such\nas 1) the absolute robustness of the model is heavily dependent on the\nestimation performance; 2) the corruptions that diminish local information are\nmore serious than that reduce visual effects. We also give suggestions for the\ndesign and application of optical flow models. We anticipate that our benchmark\nwill serve as a foundational resource for advancing research in robust optical\nflow estimation. The benchmarks and source code will be released at\nhttps://github.com/ZhonghuaYi/optical_flow_robustness_benchmark.\n","authors":["Zhonghua Yi","Hao Shi","Qi Jiang","Yao Gao","Ze Wang","Yufan Zhang","Kailun Yang","Kaiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2411.14865v1.pdf","comment":"The benchmarks and source code will be released at\n  https://github.com/ZhonghuaYi/optical_flow_robustness_benchmark"},{"id":"http://arxiv.org/abs/2411.12620v2","updated":"2024-11-22T11:29:58Z","published":"2024-11-19T16:27:31Z","title":"Maps from Motion (MfM): Generating 2D Semantic Maps from Sparse\n  Multi-view Images","summary":"  World-wide detailed 2D maps require enormous collective efforts.\nOpenStreetMap is the result of 11 million registered users manually annotating\nthe GPS location of over 1.75 billion entries, including distinctive landmarks\nand common urban objects. At the same time, manual annotations can include\nerrors and are slow to update, limiting the map's accuracy. Maps from Motion\n(MfM) is a step forward to automatize such time-consuming map making procedure\nby computing 2D maps of semantic objects directly from a collection of\nuncalibrated multi-view images. From each image, we extract a set of object\ndetections, and estimate their spatial arrangement in a top-down local map\ncentered in the reference frame of the camera that captured the image. Aligning\nthese local maps is not a trivial problem, since they provide incomplete, noisy\nfragments of the scene, and matching detections across them is unreliable\nbecause of the presence of repeated pattern and the limited appearance\nvariability of urban objects. We address this with a novel graph-based\nframework, that encodes the spatial and semantic distribution of the objects\ndetected in each image, and learns how to combine them to predict the objects'\nposes in a global reference system, while taking into account all possible\ndetection matches and preserving the topology observed in each image. Despite\nthe complexity of the problem, our best model achieves global 2D registration\nwith an average accuracy within 4 meters (i.e., below GPS accuracy) even on\nsparse sequences with strong viewpoint change, on which COLMAP has an 80%\nfailure rate. We provide extensive evaluation on synthetic and real-world data,\nshowing how the method obtains a solution even in scenarios where standard\noptimization techniques fail.\n","authors":["Matteo Toso","Stefano Fiorini","Stuart James","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2411.12620v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19345v2","updated":"2024-11-22T11:24:39Z","published":"2024-09-28T13:24:11Z","title":"Unveil Benign Overfitting for Transformer in Vision: Training Dynamics,\n  Convergence, and Generalization","summary":"  Transformers have demonstrated great power in the recent development of large\nfoundational models. In particular, the Vision Transformer (ViT) has brought\nrevolutionary changes to the field of vision, achieving significant\naccomplishments on the experimental side. However, their theoretical\ncapabilities, particularly in terms of generalization when trained to overfit\ntraining data, are still not fully understood. To address this gap, this work\ndelves deeply into the benign overfitting perspective of transformers in\nvision. To this end, we study the optimization of a Transformer composed of a\nself-attention layer with softmax followed by a fully connected layer under\ngradient descent on a certain data distribution model. By developing techniques\nthat address the challenges posed by softmax and the interdependent nature of\nmultiple weights in transformer optimization, we successfully characterized the\ntraining dynamics and achieved generalization in post-training. Our results\nestablish a sharp condition that can distinguish between the small test error\nphase and the large test error regime, based on the signal-to-noise ratio in\nthe data model. The theoretical results are further verified by experimental\nsimulation. To the best of our knowledge, this is the first work to\ncharacterize benign overfitting for Transformers.\n","authors":["Jiarui Jiang","Wei Huang","Miao Zhang","Taiji Suzuki","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2409.19345v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14863v1","updated":"2024-11-22T11:24:14Z","published":"2024-11-22T11:24:14Z","title":"Latent Schrodinger Bridge: Prompting Latent Diffusion for Fast Unpaired\n  Image-to-Image Translation","summary":"  Diffusion models (DMs), which enable both image generation from noise and\ninversion from data, have inspired powerful unpaired image-to-image (I2I)\ntranslation algorithms. However, they often require a larger number of neural\nfunction evaluations (NFEs), limiting their practical applicability. In this\npaper, we tackle this problem with Schrodinger Bridges (SBs), which are\nstochastic differential equations (SDEs) between distributions with minimal\ntransport cost. We analyze the probability flow ordinary differential equation\n(ODE) formulation of SBs, and observe that we can decompose its vector field\ninto a linear combination of source predictor, target predictor, and noise\npredictor. Inspired by this observation, we propose Latent Schrodinger Bridges\n(LSBs) that approximate the SB ODE via pre-trained Stable Diffusion, and\ndevelop appropriate prompt optimization and change of variables formula to\nmatch the training and inference between distributions. We demonstrate that our\nalgorithm successfully conduct competitive I2I translation in unsupervised\nsetting with only a fraction of computation cost required by previous DM-based\nI2I methods.\n","authors":["Jeongsol Kim","Beomsu Kim","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2411.14863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12603v2","updated":"2024-11-22T11:21:50Z","published":"2024-11-19T16:06:32Z","title":"STREAM: A Universal State-Space Model for Sparse Geometric Data","summary":"  Handling sparse and unstructured geometric data, such as point clouds or\nevent-based vision, is a pressing challenge in the field of machine vision.\nRecently, sequence models such as Transformers and state-space models entered\nthe domain of geometric data. These methods require specialized preprocessing\nto create a sequential view of a set of points. Furthermore, prior works\ninvolving sequence models iterate geometric data with either uniform or learned\nstep sizes, implicitly relying on the model to infer the underlying geometric\nstructure. In this work, we propose to encode geometric structure explicitly\ninto the parameterization of a state-space model. State-space models are based\non linear dynamics governed by a one-dimensional variable such as time or a\nspatial coordinate. We exploit this dynamic variable to inject relative\ndifferences of coordinates into the step size of the state-space model. The\nresulting geometric operation computes interactions between all pairs of N\npoints in O(N) steps. Our model deploys the Mamba selective state-space model\nwith a modified CUDA kernel to efficiently map sparse geometric data to modern\nhardware. The resulting sequence model, which we call STREAM, achieves\ncompetitive results on a range of benchmarks from point-cloud classification to\nevent-based vision and audio classification. STREAM demonstrates a powerful\ninductive bias for sparse geometric data by improving the PointMamba baseline\nwhen trained from scratch on the ModelNet40 and ScanObjectNN point cloud\nanalysis datasets. It further achieves, for the first time, 100% test accuracy\non all 11 classes of the DVS128 Gestures dataset.\n","authors":["Mark Schöne","Yash Bhisikar","Karan Bania","Khaleelulla Khan Nazeer","Christian Mayr","Anand Subramoney","David Kappel"],"pdf_url":"https://arxiv.org/pdf/2411.12603v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14847v1","updated":"2024-11-22T10:47:47Z","published":"2024-11-22T10:47:47Z","title":"Dynamics-Aware Gaussian Splatting Streaming Towards Fast On-the-Fly\n  Training for 4D Reconstruction","summary":"  The recent development of 3D Gaussian Splatting (3DGS) has led to great\ninterest in 4D dynamic spatial reconstruction from multi-view visual inputs.\nWhile existing approaches mainly rely on processing full-length multi-view\nvideos for 4D reconstruction, there has been limited exploration of iterative\nonline reconstruction methods that enable on-the-fly training and per-frame\nstreaming. Current 3DGS-based streaming methods treat the Gaussian primitives\nuniformly and constantly renew the densified Gaussians, thereby overlooking the\ndifference between dynamic and static features and also neglecting the temporal\ncontinuity in the scene. To address these limitations, we propose a novel\nthree-stage pipeline for iterative streamable 4D dynamic spatial\nreconstruction. Our pipeline comprises a selective inheritance stage to\npreserve temporal continuity, a dynamics-aware shift stage for distinguishing\ndynamic and static primitives and optimizing their movements, and an\nerror-guided densification stage to accommodate emerging objects. Our method\nachieves state-of-the-art performance in online 4D reconstruction,\ndemonstrating a 20% improvement in on-the-fly training speed, superior\nrepresentation quality, and real-time rendering capability. Project page:\nhttps://www.liuzhening.top/DASS\n","authors":["Zhening Liu","Yingdong Hu","Xinjie Zhang","Jiawei Shao","Zehong Lin","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14847v1.pdf","comment":"Project page: https://www.liuzhening.top/DASS"},{"id":"http://arxiv.org/abs/2408.06047v2","updated":"2024-11-22T10:45:11Z","published":"2024-08-12T10:39:59Z","title":"BooW-VTON: Boosting In-the-Wild Virtual Try-On via Mask-Free Pseudo Data\n  Training","summary":"  Image-based virtual try-on is an increasingly popular and important task to\ngenerate realistic try-on images of the specific person. Recent methods model\nvirtual try-on as image mask-inpaint task, which requires masking the person\nimage and results in significant loss of spatial information. Especially, for\nin-the-wild try-on scenarios with complex poses and occlusions, mask-based\nmethods often introduce noticeable artifacts. Our research found that a\nmask-free approach can fully leverage spatial and lighting information from the\noriginal person image, enabling high-quality virtual try-on. Consequently, we\npropose a novel training paradigm for a mask-free try-on diffusion model. We\nensure the model's mask-free try-on capability by creating high-quality\npseudo-data and further enhance its handling of complex spatial information\nthrough effective in-the-wild data augmentation. Besides, a try-on localization\nloss is designed to concentrate on try-on area while suppressing garment\nfeatures in non-try-on areas, ensuring precise rendering of garments and\npreservation of fore/back-ground. In the end, we introduce BooW-VTON, the\nmask-free virtual try-on diffusion model, which delivers SOTA try-on quality\nwithout parsing cost. Extensive qualitative and quantitative experiments have\ndemonstrated superior performance in wild scenarios with such a low-demand\ninput.\n","authors":["Xuanpu Zhang","Dan Song","Pengxin Zhan","Tianyu Chang","Jianhao Zeng","Qingguo Chen","Weihua Luo","Anan Liu"],"pdf_url":"https://arxiv.org/pdf/2408.06047v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00144v2","updated":"2024-11-22T10:39:59Z","published":"2024-10-31T18:43:48Z","title":"Self-Ensembling Gaussian Splatting for Few-Shot Novel View Synthesis","summary":"  3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness for\nnovel view synthesis (NVS). However, the 3DGS model tends to overfit when\ntrained with sparse posed views, limiting its generalization ability to novel\nviews. In this paper, we alleviate the overfitting problem, presenting a\nSelf-Ensembling Gaussian Splatting (SE-GS) approach. Our method encompasses a\n$\\mathbf{\\Sigma}$-model and a $\\mathbf{\\Delta}$-model. The\n$\\mathbf{\\Sigma}$-model serves as an ensemble of 3DGS models that generates\nnovel-view images during inference. We achieve the self-ensembling by\nintroducing an uncertainty-aware perturbation strategy at the training state.\nWe complement the $\\mathbf{\\Sigma}$-model with the $\\mathbf{\\Delta}$-model,\nwhich is dynamically perturbed based on the uncertainties of novel-view\nrenderings across different training steps. The perturbation yields diverse\ntemporal samples in the Gaussian parameter space without additional training\ncosts. The geometry of the $\\mathbf{\\Sigma}$-model is regularized by penalizing\ndiscrepancies between the $\\mathbf{\\Sigma}$-model and these temporal samples.\nTherefore, our SE-GS conducts an effective and efficient regularization across\na large number of 3DGS models, resulting in a robust ensemble, the\n$\\mathbf{\\Sigma}$-model. Our experimental results on the LLFF, Mip-NeRF360,\nDTU, and MVImgNet datasets show that our approach improves NVS quality with\nfew-shot training views, outperforming existing state-of-the-art methods. The\ncode is released at: https://sailor-z.github.io/projects/SEGS.html.\n","authors":["Chen Zhao","Xuan Wang","Tong Zhang","Saqib Javed","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2411.00144v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11840v2","updated":"2024-11-22T10:38:42Z","published":"2024-06-17T17:59:59Z","title":"LLaNA: Large Language and NeRF Assistant","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated an excellent\nunderstanding of images and 3D data. However, both modalities have shortcomings\nin holistically capturing the appearance and geometry of objects. Meanwhile,\nNeural Radiance Fields (NeRFs), which encode information within the weights of\na simple Multi-Layer Perceptron (MLP), have emerged as an increasingly\nwidespread modality that simultaneously encodes the geometry and photorealistic\nappearance of objects. This paper investigates the feasibility and\neffectiveness of ingesting NeRF into MLLM. We create LLaNA, the first\ngeneral-purpose NeRF-language assistant capable of performing new tasks such as\nNeRF captioning and Q\\&A. Notably, our method directly processes the weights of\nthe NeRF's MLP to extract information about the represented objects without the\nneed to render images or materialize 3D data structures. Moreover, we build a\ndataset of NeRFs with text annotations for various NeRF-language tasks with no\nhuman intervention. Based on this dataset, we develop a benchmark to evaluate\nthe NeRF understanding capability of our method. Results show that processing\nNeRF weights performs favourably against extracting 2D or 3D representations\nfrom NeRFs.\n","authors":["Andrea Amaduzzi","Pierluigi Zama Ramirez","Giuseppe Lisanti","Samuele Salti","Luigi Di Stefano"],"pdf_url":"https://arxiv.org/pdf/2406.11840v2.pdf","comment":"Under review. Project page: https://andreamaduzzi.github.io/llana/"},{"id":"http://arxiv.org/abs/2411.12872v2","updated":"2024-11-22T10:26:27Z","published":"2024-11-19T21:34:50Z","title":"From Text to Pose to Image: Improving Diffusion Model Control and\n  Quality","summary":"  In the last two years, text-to-image diffusion models have become extremely\npopular. As their quality and usage increase, a major concern has been the need\nfor better output control. In addition to prompt engineering, one effective\nmethod to improve the controllability of diffusion models has been to condition\nthem on additional modalities such as image style, depth map, or keypoints.\nThis forms the basis of ControlNets or Adapters. When attempting to apply these\nmethods to control human poses in outputs of text-to-image diffusion models,\ntwo main challenges have arisen. The first challenge is generating poses\nfollowing a wide range of semantic text descriptions, for which previous\nmethods involved searching for a pose within a dataset of (caption, pose)\npairs. The second challenge is conditioning image generation on a specified\npose while keeping both high aesthetic and high pose fidelity. In this article,\nwe fix these two main issues by introducing a text-to-pose (T2P) generative\nmodel alongside a new sampling algorithm, and a new pose adapter that\nincorporates more pose keypoints for higher pose fidelity. Together, these two\nnew state-of-the-art models enable, for the first time, a generative\ntext-to-pose-to-image framework for higher pose control in diffusion models. We\nrelease all models and the code used for the experiments at\nhttps://github.com/clement-bonnet/text-to-pose.\n","authors":["Clément Bonnet","Ariel N. Lee","Franck Wertel","Antoine Tamano","Tanguy Cizain","Pablo Ducru"],"pdf_url":"https://arxiv.org/pdf/2411.12872v2.pdf","comment":"Published at the NeurIPS 2024 Workshop on Compositional Learning:\n  Perspectives, Methods, and Paths Forward"},{"id":"http://arxiv.org/abs/2406.01294v2","updated":"2024-11-22T10:25:03Z","published":"2024-06-03T13:04:42Z","title":"CE-VAE: Capsule Enhanced Variational AutoEncoder for Underwater Image\n  Enhancement","summary":"  Unmanned underwater image analysis for marine monitoring faces two key\nchallenges: (i) degraded image quality due to light attenuation and (ii)\nhardware storage constraints limiting high-resolution image collection.\nExisting methods primarily address image enhancement with approaches that hinge\non storing the full-size input. In contrast, we introduce the Capsule Enhanced\nVariational AutoEncoder (CE-VAE), a novel architecture designed to efficiently\ncompress and enhance degraded underwater images. Our attention-aware image\nencoder can project the input image onto a latent space representation while\nbeing able to run online on a remote device. The only information that needs to\nbe stored on the device or sent to a beacon is a compressed representation.\nThere is a dual-decoder module that performs offline, full-size enhanced image\ngeneration. One branch reconstructs spatial details from the compressed latent\nspace, while the second branch utilizes a capsule-clustering layer to capture\nentity-level structures and complex spatial relationships. This parallel\ndecoding strategy enables the model to balance fine-detail preservation with\ncontext-aware enhancements. CE-VAE achieves state-of-the-art performance in\nunderwater image enhancement on six benchmark datasets, providing up to 3x\nhigher compression efficiency than existing approaches. Code available at\n\\url{https://github.com/iN1k1/ce-vae-underwater-image-enhancement}.\n","authors":["Rita Pucci","Niki Martinel"],"pdf_url":"https://arxiv.org/pdf/2406.01294v2.pdf","comment":"Accepted for publication at IEEE/CVF Winter Conference on\n  Applications of Computer Vision (WACV)"},{"id":"http://arxiv.org/abs/2411.14833v1","updated":"2024-11-22T10:16:35Z","published":"2024-11-22T10:16:35Z","title":"Cell as Point: One-Stage Framework for Efficient Cell Tracking","summary":"  Cellular activities are dynamic and intricate, playing a crucial role in\nadvancing diagnostic and therapeutic techniques, yet they often require\nsubstantial resources for accurate tracking. Despite recent progress, the\nconventional multi-stage cell tracking approaches not only heavily rely on\ndetection or segmentation results as a prerequisite for the tracking stage,\ndemanding plenty of refined segmentation masks, but are also deteriorated by\nimbalanced and long sequence data, leading to under-learning in training and\nmissing cells in inference procedures. To alleviate the above issues, this\npaper proposes the novel end-to-end CAP framework, which leverages the idea of\nregarding Cell as Point to achieve efficient and stable cell tracking in one\nstage. CAP abandons detection or segmentation stages and simplifies the process\nby exploiting the correlation among the trajectories of cell points to track\ncells jointly, thus reducing the label demand and complexity of the pipeline.\nWith cell point trajectory and visibility to represent cell locations and\nlineage relationships, CAP leverages the key innovations of adaptive\nevent-guided (AEG) sampling for addressing data imbalance in cell division\nevents and the rolling-as-window (RAW) inference method to ensure continuous\ntracking of new cells in the long term. Eliminating the need for a prerequisite\ndetection or segmentation stage, CAP demonstrates strong cell tracking\nperformance while also being 10 to 55 times more efficient than existing\nmethods. The code and models will be released.\n","authors":["Yaxuan Song","Jianan Fan","Heng Huang","Mei Chen","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2411.14833v1.pdf","comment":"17 pages, 8 figures, 8 tables"},{"id":"http://arxiv.org/abs/2411.10074v2","updated":"2024-11-22T10:12:36Z","published":"2024-11-15T09:39:12Z","title":"Improving the accuracy of automated labeling of specimen images datasets\n  via a confidence-based process","summary":"  The digitization of natural history collections over the past three decades\nhas unlocked a treasure trove of specimen imagery and metadata. There is great\ninterest in making this data more useful by further labeling it with additional\ntrait data, and modern deep learning machine learning techniques utilizing\nconvolutional neural nets (CNNs) and similar networks show particular promise\nto reduce the amount of required manual labeling by human experts, making the\nprocess much faster and less expensive. However, in most cases, the accuracy of\nthese approaches is too low for reliable utilization of the automatic labeling,\ntypically in the range of 80-85% accuracy. In this paper, we present and\nvalidate an approach that can greatly improve this accuracy, essentially by\nexamining the confidence that the network has in the generated label as well as\nutilizing a user-defined threshold to reject labels that fall below a chosen\nlevel. We demonstrate that a naive model that produced 86% initial accuracy can\nachieve improved performance - over 95% accuracy (rejecting about 40% of the\nlabels) or over 99% accuracy (rejecting about 65%) by selecting higher\nconfidence thresholds. This gives flexibility to adapt existing models to the\nstatistical requirements of various types of research and has the potential to\nmove these automatic labeling approaches from being unusably inaccurate to\nbeing an invaluable new tool. After validating the approach in a number of\nways, we annotate the reproductive state of a large dataset of over 600,000\nherbarium specimens. The analysis of the results points at under-investigated\ncorrelations as well as general alignment with known trends. By sharing this\nnew dataset alongside this work, we want to allow ecologists to gather insights\nfor their own research questions, at their chosen point of accuracy/coverage\ntrade-off.\n","authors":["Quentin Bateux","Jonathan Koss","Patrick W. Sweeney","Erika Edwards","Nelson Rios","Aaron M. Dollar"],"pdf_url":"https://arxiv.org/pdf/2411.10074v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14832v1","updated":"2024-11-22T10:10:53Z","published":"2024-11-22T10:10:53Z","title":"VisGraphVar: A Benchmark Generator for Assessing Variability in Graph\n  Analysis Using Large Vision-Language Models","summary":"  The fast advancement of Large Vision-Language Models (LVLMs) has shown\nimmense potential. These models are increasingly capable of tackling abstract\nvisual tasks. Geometric structures, particularly graphs with their inherent\nflexibility and complexity, serve as an excellent benchmark for evaluating\nthese models' predictive capabilities. While human observers can readily\nidentify subtle visual details and perform accurate analyses, our investigation\nreveals that state-of-the-art LVLMs exhibit consistent limitations in specific\nvisual graph scenarios, especially when confronted with stylistic variations.\nIn response to these challenges, we introduce VisGraphVar (Visual Graph\nVariability), a customizable benchmark generator able to produce graph images\nfor seven distinct task categories (detection, classification, segmentation,\npattern recognition, link prediction, reasoning, matching), designed to\nsystematically evaluate the strengths and limitations of individual LVLMs. We\nuse VisGraphVar to produce 990 graph images and evaluate six LVLMs, employing\ntwo distinct prompting strategies, namely zero-shot and chain-of-thought. The\nfindings demonstrate that variations in visual attributes of images (e.g., node\nlabeling and layout) and the deliberate inclusion of visual imperfections, such\nas overlapping nodes, significantly affect model performance. This research\nemphasizes the importance of a comprehensive evaluation across graph-related\ntasks, extending beyond reasoning alone. VisGraphVar offers valuable insights\nto guide the development of more reliable and robust systems capable of\nperforming advanced visual graph analysis.\n","authors":["Camilo Chacón Sartori","Christian Blum","Filippo Bistaffa"],"pdf_url":"https://arxiv.org/pdf/2411.14832v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14827v1","updated":"2024-11-22T10:07:02Z","published":"2024-11-22T10:07:02Z","title":"Physically Interpretable Probabilistic Domain Characterization","summary":"  Characterizing domains is essential for models analyzing dynamic\nenvironments, as it allows them to adapt to evolving conditions or to hand the\ntask over to backup systems when facing conditions outside their operational\ndomain. Existing solutions typically characterize a domain by solving a\nregression or classification problem, which limits their applicability as they\nonly provide a limited summarized description of the domain. In this paper, we\npresent a novel approach to domain characterization by characterizing domains\nas probability distributions. Particularly, we develop a method to predict the\nlikelihood of different weather conditions from images captured by\nvehicle-mounted cameras by estimating distributions of physical parameters\nusing normalizing flows. To validate our proposed approach, we conduct\nexperiments within the context of autonomous vehicles, focusing on predicting\nthe distribution of weather parameters to characterize the operational domain.\nThis domain is characterized by physical parameters (absolute characterization)\nand arbitrarily predefined domains (relative characterization). Finally, we\nevaluate whether a system can safely operate in a target domain by comparing it\nto multiple source domains where safety has already been established. This\napproach holds significant potential, as accurate weather prediction and\neffective domain adaptation are crucial for autonomous systems to adjust to\ndynamic environmental conditions.\n","authors":["Anaïs Halin","Sébastien Piérard","Renaud Vandeghen","Benoît Gérin","Maxime Zanella","Martin Colot","Jan Held","Anthony Cioppa","Emmanuel Jean","Gianluca Bontempi","Saïd Mahmoudi","Benoît Macq","Marc Van Droogenbroeck"],"pdf_url":"https://arxiv.org/pdf/2411.14827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06067v7","updated":"2024-11-22T09:56:52Z","published":"2023-09-12T09:07:03Z","title":"Generalized Implicit Neural Representation for Efficient MRI Parallel\n  Imaging Reconstruction","summary":"  High-resolution magnetic resonance imaging (MRI) is essential in clinical\ndiagnosis. However, its long acquisition time remains a critical issue.\nParallel imaging (PI) is a common approach to reduce acquisition time by\nperiodically skipping specific k-space lines and reconstructing images from\nundersampled data. This study presents a generalized implicit neural\nrepresentation (INR)-based framework for MRI PI reconstruction, addressing\nlimitations commonly encountered in conventional methods, such as\nsubject-specific or undersampling scale-specific requirements and long\nreconstruction time. The proposed method overcomes these limitations by\nleveraging prior knowledge of voxel-specific features and integrating a novel\nscale-embedded encoder module. This encoder generates scale-independent\nvoxel-specific features from undersampled images, enabling robust\nreconstruction across various undersampling scales without requiring retraining\nfor each specific scale or subject. The framework's INR model treats fully\nsampled MR images as a continuous function of spatial coordinates and prior\nvoxel-specific features, efficiently reconstructing high-quality MR images from\nundersampled data. Extensive experiments on publicly available MRI datasets\ndemonstrate the superior performance of the proposed method in reconstructing\nimages at multiple acceleration factors (4x, 5x, and 6x), achieving higher\nevaluation metrics and visual fidelity compared to state-of-the-art methods. In\nterms of efficiency, this INR-based approach exhibits notable advantages,\nincluding reduced floating point operations and GPU usage, allowing for\naccelerated processing times while maintaining high reconstruction quality. The\ngeneralized design of the model significantly reduces computational resources\nand time consumption, making it more suitable for real-time clinical\napplications.\n","authors":["Hao Li","Yusheng Zhou","Jianan Liu","Xiling Liu","Tao Huang","Zhihan Lyu","Weidong Cai","Wei Chen"],"pdf_url":"https://arxiv.org/pdf/2309.06067v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14078v2","updated":"2024-11-22T09:46:23Z","published":"2024-11-21T12:43:19Z","title":"Self-supervised learning for radio-astronomy source classification: a\n  benchmark","summary":"  The upcoming Square Kilometer Array (SKA) telescope marks a significant step\nforward in radio astronomy, presenting new opportunities and challenges for\ndata analysis. Traditional visual models pretrained on optical photography\nimages may not perform optimally on radio interferometry images, which have\ndistinct visual characteristics.\n  Self-Supervised Learning (SSL) offers a promising approach to address this\nissue, leveraging the abundant unlabeled data in radio astronomy to train\nneural networks that learn useful representations from radio images. This study\nexplores the application of SSL to radio astronomy, comparing the performance\nof SSL-trained models with that of traditional models pretrained on natural\nimages, evaluating the importance of data curation for SSL, and assessing the\npotential benefits of self-supervision to different domain-specific radio\nastronomy datasets.\n  Our results indicate that, SSL-trained models achieve significant\nimprovements over the baseline in several downstream tasks, especially in the\nlinear evaluation setting; when the entire backbone is fine-tuned, the benefits\nof SSL are less evident but still outperform pretraining. These findings\nsuggest that SSL can play a valuable role in efficiently enhancing the analysis\nof radio astronomical data. The trained models and code is available at:\n\\url{https://github.com/dr4thmos/solo-learn-radio}\n","authors":["Thomas Cecconello","Simone Riggi","Ugo Becciani","Fabio Vitello","Andrew M. Hopkins","Giuseppe Vizzari","Concetto Spampinato","Simone Palazzo"],"pdf_url":"https://arxiv.org/pdf/2411.14078v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14823v1","updated":"2024-11-22T09:44:13Z","published":"2024-11-22T09:44:13Z","title":"Omni-IML: Towards Unified Image Manipulation Localization","summary":"  Image manipulation can lead to misinterpretation of visual content, posing\nsignificant risks to information security. Image Manipulation Localization\n(IML) has thus received increasing attention. However, existing IML methods\nrely heavily on task-specific designs, making them perform well only on one\ntarget image type but are mostly random guessing on other image types, and even\njoint training on multiple image types causes significant performance\ndegradation. This hinders the deployment for real applications as it notably\nincreases maintenance costs and the misclassification of image types leads to\nserious error accumulation. To this end, we propose Omni-IML, the first\ngeneralist model to unify diverse IML tasks. Specifically, Omni-IML achieves\ngeneralism by adopting the Modal Gate Encoder and the Dynamic Weight Decoder to\nadaptively determine the optimal encoding modality and the optimal decoder\nfilters for each sample. We additionally propose an Anomaly Enhancement module\nthat enhances the features of tampered regions with box supervision and helps\nthe generalist model to extract common features across different IML tasks. We\nvalidate our approach on IML tasks across three major scenarios: natural\nimages, document images, and face images. Without bells and whistles, our\nOmni-IML achieves state-of-the-art performance on all three tasks with a single\nunified model, providing valuable strategies and insights for real-world\napplication and future research in generalist image forensics. Our code will be\npublicly available.\n","authors":["Chenfan Qu","Yiwu Zhong","Fengjun Guo","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2411.14823v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07779v2","updated":"2024-11-22T09:28:01Z","published":"2024-09-12T06:25:44Z","title":"AFFSegNet: Adaptive Feature Fusion Segmentation Network for Microtumors\n  and Multi-Organ Segmentation","summary":"  Medical image segmentation, a crucial task in computer vision, facilitates\nthe automated delineation of anatomical structures and pathologies, supporting\nclinicians in diagnosis, treatment planning, and disease monitoring. Notably,\ntransformers employing shifted window-based self-attention have demonstrated\nexceptional performance. However, their reliance on local window attention\nlimits the fusion of local and global contextual information, crucial for\nsegmenting microtumors and miniature organs. To address this limitation, we\npropose the Adaptive Semantic Segmentation Network (ASSNet), a transformer\narchitecture that effectively integrates local and global features for precise\nmedical image segmentation. ASSNet comprises a transformer-based U-shaped\nencoder-decoder network. The encoder utilizes shifted window self-attention\nacross five resolutions to extract multi-scale features, which are then\npropagated to the decoder through skip connections. We introduce an augmented\nmulti-layer perceptron within the encoder to explicitly model long-range\ndependencies during feature extraction. Recognizing the constraints of\nconventional symmetrical encoder-decoder designs, we propose an Adaptive\nFeature Fusion (AFF) decoder to complement our encoder. This decoder\nincorporates three key components: the Long Range Dependencies (LRD) block, the\nMulti-Scale Feature Fusion (MFF) block, and the Adaptive Semantic Center (ASC)\nblock. These components synergistically facilitate the effective fusion of\nmulti-scale features extracted by the decoder while capturing long-range\ndependencies and refining object boundaries. Comprehensive experiments on\ndiverse medical image segmentation tasks, including multi-organ, liver tumor,\nand bladder tumor segmentation, demonstrate that ASSNet achieves\nstate-of-the-art results. Code and models are available at:\n\\url{https://github.com/lzeeorno/ASSNet}.\n","authors":["Fuchen Zheng","Xinyi Chen","Xuhang Chen","Haolun Li","Xiaojiao Guo","Guoheng Huang","Chi-Man Pun","Shoujun Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.07779v2.pdf","comment":"8 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2411.14816v1","updated":"2024-11-22T09:22:39Z","published":"2024-11-22T09:22:39Z","title":"Unsupervised Multi-view UAV Image Geo-localization via Iterative\n  Rendering","summary":"  Unmanned Aerial Vehicle (UAV) Cross-View Geo-Localization (CVGL) presents\nsignificant challenges due to the view discrepancy between oblique UAV images\nand overhead satellite images. Existing methods heavily rely on the supervision\nof labeled datasets to extract viewpoint-invariant features for cross-view\nretrieval. However, these methods have expensive training costs and tend to\noverfit the region-specific cues, showing limited generalizability to new\nregions. To overcome this issue, we propose an unsupervised solution that lifts\nthe scene representation to 3d space from UAV observations for satellite image\ngeneration, providing robust representation against view distortion. By\ngenerating orthogonal images that closely resemble satellite views, our method\nreduces view discrepancies in feature representation and mitigates shortcuts in\nregion-specific image pairing. To further align the rendered image's\nperspective with the real one, we design an iterative camera pose updating\nmechanism that progressively modulates the rendered query image with potential\nsatellite targets, eliminating spatial offsets relative to the reference\nimages. Additionally, this iterative refinement strategy enhances cross-view\nfeature invariance through view-consistent fusion across iterations. As such,\nour unsupervised paradigm naturally avoids the problem of region-specific\noverfitting, enabling generic CVGL for UAV images without feature fine-tuning\nor data-driven training. Experiments on the University-1652 and SUES-200\ndatasets demonstrate that our approach significantly improves geo-localization\naccuracy while maintaining robustness across diverse regions. Notably, without\nmodel fine-tuning or paired training, our method achieves competitive\nperformance with recent supervised methods.\n","authors":["Haoyuan Li","Chang Xu","Wen Yang","Li Mi","Huai Yu","Haijian Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14816v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2411.13152v2","updated":"2024-11-22T09:21:01Z","published":"2024-11-20T09:41:41Z","title":"AGLP: A Graph Learning Perspective for Semi-supervised Domain Adaptation","summary":"  In semi-supervised domain adaptation (SSDA), the model aims to leverage\npartially labeled target domain data along with a large amount of labeled\nsource domain data to enhance its generalization capability for the target\ndomain. A key advantage of SSDA is its ability to significantly reduce reliance\non labeled data, thereby lowering the costs and time associated with data\npreparation. Most existing SSDA methods utilize information from domain labels\nand class labels but overlook the structural information of the data. To\naddress this issue, this paper proposes a graph learning perspective (AGLP) for\nsemi-supervised domain adaptation. We apply the graph convolutional network to\nthe instance graph which allows structural information to propagate along the\nweighted graph edges. The proposed AGLP model has several advantages. First, to\nthe best of our knowledge, this is the first work to model structural\ninformation in SSDA. Second, the proposed model can effectively learn\ndomain-invariant and semantic representations, reducing domain discrepancies in\nSSDA. Extensive experimental results on multiple standard benchmarks\ndemonstrate that the proposed AGLP algorithm outperforms state-of-the-art\nsemi-supervised domain adaptation methods.\n","authors":["Houcheng Su","Mengzhu Wang","Jiao Li","Nan Yin","Liang Yang","Li Shen"],"pdf_url":"https://arxiv.org/pdf/2411.13152v2.pdf","comment":"8page"},{"id":"http://arxiv.org/abs/2411.13147v2","updated":"2024-11-22T09:18:20Z","published":"2024-11-20T09:24:46Z","title":"GraphCL: Graph-based Clustering for Semi-Supervised Medical Image\n  Segmentation","summary":"  Semi-supervised learning (SSL) has made notable advancements in medical image\nsegmentation (MIS), particularly in scenarios with limited labeled data and\nsignificantly enhancing data utilization efficiency. Previous methods primarily\nfocus on complex training strategies to utilize unlabeled data but neglect the\nimportance of graph structural information. Different from existing methods, we\npropose a graph-based clustering for semi-supervised medical image segmentation\n(GraphCL) by jointly modeling graph data structure in a unified deep model. The\nproposed GraphCL model enjoys several advantages. Firstly, to the best of our\nknowledge, this is the first work to model the data structure information for\nsemi-supervised medical image segmentation (SSMIS). Secondly, to get the\nclustered features across different graphs, we integrate both pairwise\naffinities between local image features and raw features as inputs. Extensive\nexperimental results on three standard benchmarks show that the proposed\nGraphCL algorithm outperforms state-of-the-art semi-supervised medical image\nsegmentation methods.\n","authors":["Mengzhu Wang","Jiao Li","Houcheng Su","Nan Yin","Liang Yang","Shen Li"],"pdf_url":"https://arxiv.org/pdf/2411.13147v2.pdf","comment":"9page"},{"id":"http://arxiv.org/abs/2403.07247v2","updated":"2024-11-22T09:17:02Z","published":"2024-03-12T02:09:39Z","title":"GuideGen: A Text-Guided Framework for Full-torso Anatomy and CT Volume\n  Generation","summary":"  The recently emerging conditional diffusion models seem promising for\nmitigating the labor and expenses in building large 3D medical imaging\ndatasets. However, previous studies on 3D CT generation have yet to fully\ncapitalize on semantic and textual conditions, and they have primarily focused\non specific organs characterized by a local structure and fixed contrast. In\nthis work, we present GuideGen, a controllable framework that generates\nanatomical masks and corresponding CT volumes for the entire torso-from chest\nto pelvis-based on free-form text prompts. Our approach includes three core\ncomponents: a text-conditional semantic synthesizer for creating realistic\nfull-torso anatomies; a contrast-aware autoencoder for detailed, high-fidelity\nfeature extraction across varying contrast levels; and a latent feature\ngenerator that ensures alignment between CT images, anatomical semantics and\ninput prompts. To train and evaluate GuideGen, we compile a multi-modality\ncancer imaging dataset with paired CT and clinical descriptions from 12 public\nTCIA datasets and one private real-world dataset. Comprehensive evaluations\nacross generation quality, cross-modality alignment, and data usability on\nmulti-organ and tumor segmentation tasks demonstrate GuideGen's superiority\nover existing CT generation methods.\n","authors":["Linrui Dai","Rongzhao Zhang","Yongrui Yu","Xiaofan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.07247v2.pdf","comment":"submitted to CVPR2025"},{"id":"http://arxiv.org/abs/2411.14811v1","updated":"2024-11-22T09:12:02Z","published":"2024-11-22T09:12:02Z","title":"Fine-Grained Alignment in Vision-and-Language Navigation through\n  Bayesian Optimization","summary":"  This paper addresses the challenge of fine-grained alignment in\nVision-and-Language Navigation (VLN) tasks, where robots navigate realistic 3D\nenvironments based on natural language instructions. Current approaches use\ncontrastive learning to align language with visual trajectory sequences.\nNevertheless, they encounter difficulties with fine-grained vision negatives.\nTo enhance cross-modal embeddings, we introduce a novel Bayesian\nOptimization-based adversarial optimization framework for creating fine-grained\ncontrastive vision samples. To validate the proposed methodology, we conduct a\nseries of experiments to assess the effectiveness of the enriched embeddings on\nfine-grained vision negatives. We conduct experiments on two common VLN\nbenchmarks R2R and REVERIE, experiments on the them demonstrate that these\nembeddings benefit navigation, and can lead to a promising performance\nenhancement. Our source code and trained models are available at:\nhttps://anonymous.4open.science/r/FGVLN.\n","authors":["Yuhang Song","Mario Gianni","Chenguang Yang","Kunyang Lin","Te-Chuan Chiu","Anh Nguyen","Chun-Yi Lee"],"pdf_url":"https://arxiv.org/pdf/2411.14811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14808v1","updated":"2024-11-22T09:08:58Z","published":"2024-11-22T09:08:58Z","title":"High-Resolution Image Synthesis via Next-Token Prediction","summary":"  Denoising with a Joint-Embedding Predictive Architecture (D-JEPA), an\nautoregressive model, has demonstrated outstanding performance in\nclass-conditional image generation. However, the application of next-token\nprediction in high-resolution text-to-image generation remains underexplored.\nIn this paper, we introduce D-JEPA$\\cdot$T2I, an extension of D-JEPA\nincorporating flow matching loss, designed to enable data-efficient continuous\nresolution learning. D-JEPA$\\cdot$T2I leverages a multimodal visual transformer\nto effectively integrate textual and visual features and adopts Visual Rotary\nPositional Embedding (VoPE) to facilitate continuous resolution learning.\nFurthermore, we devise a data feedback mechanism that significantly enhances\ndata utilization efficiency. For the first time, we achieve state-of-the-art\n\\textbf{high-resolution} image synthesis via next-token prediction.\n  The experimental code and pretrained models will be open-sourced at\n\\url{https://d-jepa.github.io/t2i}.\n","authors":["Dengsheng Chen","Jie Hu","Tiezhu Yue","Xiaoming Wei"],"pdf_url":"https://arxiv.org/pdf/2411.14808v1.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2411.14807v1","updated":"2024-11-22T09:08:36Z","published":"2024-11-22T09:08:36Z","title":"Harlequin: Color-driven Generation of Synthetic Data for Referring\n  Expression Comprehension","summary":"  Referring Expression Comprehension (REC) aims to identify a particular object\nin a scene by a natural language expression, and is an important topic in\nvisual language understanding. State-of-the-art methods for this task are based\non deep learning, which generally requires expensive and manually labeled\nannotations. Some works tackle the problem with limited-supervision learning or\nrelying on Large Vision and Language Models. However, the development of\ntechniques to synthesize labeled data is overlooked. In this paper, we propose\na novel framework that generates artificial data for the REC task, taking into\naccount both textual and visual modalities. At first, our pipeline processes\nexisting data to create variations in the annotations. Then, it generates an\nimage using altered annotations as guidance. The result of this pipeline is a\nnew dataset, called Harlequin, made by more than 1M queries. This approach\neliminates manual data collection and annotation, enabling scalability and\nfacilitating arbitrary complexity. We pre-train three REC models on Harlequin,\nthen fine-tuned and evaluated on human-annotated datasets. Our experiments show\nthat the pre-training on artificial data is beneficial for performance.\n","authors":["Luca Parolari","Elena Izzo","Lamberto Ballan"],"pdf_url":"https://arxiv.org/pdf/2411.14807v1.pdf","comment":"Accepted to ICPR 2024"},{"id":"http://arxiv.org/abs/2411.14798v1","updated":"2024-11-22T08:49:08Z","published":"2024-11-22T08:49:08Z","title":"Facial Features Matter: a Dynamic Watermark based Proactive Deepfake\n  Detection Approach","summary":"  Current passive deepfake face-swapping detection methods encounter\nsignificance bottlenecks in model generalization capabilities. Meanwhile,\nproactive detection methods often use fixed watermarks which lack a close\nrelationship with the content they protect and are vulnerable to security\nrisks. Dynamic watermarks based on facial features offer a promising solution,\nas these features provide unique identifiers. Therefore, this paper proposes a\nFacial Feature-based Proactive deepfake detection method (FaceProtect), which\nutilizes changes in facial characteristics during deepfake manipulation as a\nnovel detection mechanism. We introduce a GAN-based One-way Dynamic Watermark\nGenerating Mechanism (GODWGM) that uses 128-dimensional facial feature vectors\nas inputs. This method creates irreversible mappings from facial features to\nwatermarks, enhancing protection against various reverse inference attacks.\nAdditionally, we propose a Watermark-based Verification Strategy (WVS) that\ncombines steganography with GODWGM, allowing simultaneous transmission of the\nbenchmark watermark representing facial features within the image. Experimental\nresults demonstrate that our proposed method maintains exceptional detection\nperformance and exhibits high practicality on images altered by various\ndeepfake techniques.\n","authors":["Shulin Lan","Kanlin Liu","Yazhou Zhao","Chen Yang","Yingchao Wang","Xingshan Yao","Liehuang Zhu"],"pdf_url":"https://arxiv.org/pdf/2411.14798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14797v1","updated":"2024-11-22T08:48:30Z","published":"2024-11-22T08:48:30Z","title":"Continual SFT Matches Multimodal RLHF with Negative Supervision","summary":"  Multimodal RLHF usually happens after supervised finetuning (SFT) stage to\ncontinually improve vision-language models' (VLMs) comprehension. Conventional\nwisdom holds its superiority over continual SFT during this preference\nalignment stage. In this paper, we observe that the inherent value of\nmultimodal RLHF lies in its negative supervision, the logit of the rejected\nresponses. We thus propose a novel negative supervised finetuning (nSFT)\napproach that fully excavates these information resided. Our nSFT disentangles\nthis negative supervision in RLHF paradigm, and continually aligns VLMs with a\nsimple SFT loss. This is more memory efficient than multimodal RLHF where 2\n(e.g., DPO) or 4 (e.g., PPO) large VLMs are strictly required. The\neffectiveness of nSFT is rigorously proved by comparing it with various\nmultimodal RLHF approaches, across different dataset sources, base VLMs and\nevaluation metrics. Besides, fruitful of ablations are provided to support our\nhypothesis. We hope this paper will stimulate further research to properly\nalign large vision language models.\n","authors":["Ke Zhu","Yu Wang","Yanpeng Sun","Qiang Chen","Jiangjiang Liu","Gang Zhang","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2411.14797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18279v2","updated":"2024-11-22T08:45:55Z","published":"2024-06-26T12:05:49Z","title":"Improving EO Foundation Models with Confidence Assessment for enhanced\n  Semantic segmentation","summary":"  Confidence assessments of semantic segmentation algorithms are important.\nIdeally, deep learning models should have the ability to predict in advance\nwhether their output is likely to be incorrect. Assessing the confidence levels\nof model predictions in Earth Observation (EO) classification is essential, as\nit can enhance semantic segmentation performance and help prevent further\nexploitation of the results in case of erroneous prediction. The model we\ndeveloped, Confidence Assessment for enhanced Semantic segmentation (CAS),\nevaluates confidence at both the segment and pixel levels, providing both\nlabels and confidence scores as output. Our model, CAS, identifies segments\nwith incorrect predicted labels using the proposed combined confidence metric,\nrefines the model, and enhances its performance. This work has significant\napplications, particularly in evaluating EO Foundation Models on semantic\nsegmentation downstream tasks, such as land cover classification using\nSentinel-2 satellite data. The evaluation results show that this strategy is\neffective and that the proposed model CAS outperforms other baseline models.\n","authors":["Nikolaos Dionelis","Nicolas Longepe"],"pdf_url":"https://arxiv.org/pdf/2406.18279v2.pdf","comment":"5 pages, 7 figures, 4 tables, Accepted"},{"id":"http://arxiv.org/abs/2411.14796v1","updated":"2024-11-22T08:41:33Z","published":"2024-11-22T08:41:33Z","title":"Adaptive Hyper-Graph Convolution Network for Skeleton-based Human Action\n  Recognition with Virtual Connections","summary":"  The shared topology of human skeletons motivated the recent investigation of\ngraph convolutional network (GCN) solutions for action recognition. However,\nthe existing GCNs rely on the binary connection of two neighbouring vertices\n(joints) formed by an edge (bone), overlooking the potential of constructing\nmulti-vertex convolution structures. In this paper we address this oversight\nand explore the merits of a hyper-graph convolutional network (Hyper-GCN) to\nachieve the aggregation of rich semantic information conveyed by skeleton\nvertices. In particular, our Hyper-GCN adaptively optimises multi-scale\nhyper-graphs during training, revealing the action-driven multi-vertex\nrelations. Besides, virtual connections are often designed to support efficient\nfeature aggregation, implicitly extending the spectrum of dependencies within\nthe skeleton. By injecting virtual connections into hyper-graphs, the semantic\nclues of diverse action categories can be highlighted. The results of\nexperiments conducted on the NTU-60, NTU-120, and NW-UCLA datasets, demonstrate\nthe merits of our Hyper-GCN, compared to the state-of-the-art methods.\nSpecifically, we outperform the existing solutions on NTU-120, achieving 90.2\\%\nand 91.4\\% in terms of the top-1 recognition accuracy on X-Sub and X-Set.\n","authors":["Youwei Zhou","Tianyang Xu","Cong Wu","Xiaojun Wu","Josef Kittler"],"pdf_url":"https://arxiv.org/pdf/2411.14796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14794v1","updated":"2024-11-22T08:33:36Z","published":"2024-11-22T08:33:36Z","title":"VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained\n  Video Reasoning via Core Frame Selection","summary":"  The advancement of Large Vision Language Models (LVLMs) has significantly\nimproved multimodal understanding, yet challenges remain in video reasoning\ntasks due to the scarcity of high-quality, large-scale datasets. Existing video\nquestion-answering (VideoQA) datasets often rely on costly manual annotations\nwith insufficient granularity or automatic construction methods with redundant\nframe-by-frame analysis, limiting their scalability and effectiveness for\ncomplex reasoning. To address these challenges, we introduce VideoEspresso, a\nnovel dataset that features VideoQA pairs preserving essential spatial details\nand temporal coherence, along with multimodal annotations of intermediate\nreasoning steps. Our construction pipeline employs a semantic-aware method to\nreduce redundancy, followed by generating QA pairs using GPT-4o. We further\ndevelop video Chain-of-Thought (CoT) annotations to enrich reasoning processes,\nguiding GPT-4o in extracting logical relationships from QA pairs and video\ncontent. To exploit the potential of high-quality VideoQA pairs, we propose a\nHybrid LVLMs Collaboration framework, featuring a Frame Selector and a\ntwo-stage instruction fine-tuned reasoning LVLM. This framework adaptively\nselects core frames and performs CoT reasoning using multimodal evidence.\nEvaluated on our proposed benchmark with 14 tasks against 9 popular LVLMs, our\nmethod outperforms existing baselines on most tasks, demonstrating superior\nvideo reasoning capabilities. Our code and dataset will be released at:\nhttps://github.com/hshjerry/VideoEspresso\n","authors":["Songhao Han","Wei Huang","Hairong Shi","Le Zhuo","Xiu Su","Shifeng Zhang","Xu Zhou","Xiaojuan Qi","Yue Liao","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2411.14794v1.pdf","comment":"14 pages, 14 figures"},{"id":"http://arxiv.org/abs/2411.14793v1","updated":"2024-11-22T08:29:25Z","published":"2024-11-22T08:29:25Z","title":"Style-Friendly SNR Sampler for Style-Driven Generation","summary":"  Recent large-scale diffusion models generate high-quality images but struggle\nto learn new, personalized artistic styles, which limits the creation of unique\nstyle templates. Fine-tuning with reference images is the most promising\napproach, but it often blindly utilizes objectives and noise level\ndistributions used for pre-training, leading to suboptimal style alignment. We\npropose the Style-friendly SNR sampler, which aggressively shifts the\nsignal-to-noise ratio (SNR) distribution toward higher noise levels during\nfine-tuning to focus on noise levels where stylistic features emerge. This\nenables models to better capture unique styles and generate images with higher\nstyle alignment. Our method allows diffusion models to learn and share new\n\"style templates\", enhancing personalized content creation. We demonstrate the\nability to generate styles such as personal watercolor paintings, minimal flat\ncartoons, 3D renderings, multi-panel images, and memes with text, thereby\nbroadening the scope of style-driven generation.\n","authors":["Jooyoung Choi","Chaehun Shin","Yeongtak Oh","Heeseung Kim","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2411.14793v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10499v2","updated":"2024-11-22T08:19:48Z","published":"2024-11-15T11:02:23Z","title":"FitDiT: Advancing the Authentic Garment Details for High-fidelity\n  Virtual Try-on","summary":"  Although image-based virtual try-on has made considerable progress, emerging\napproaches still encounter challenges in producing high-fidelity and robust\nfitting images across diverse scenarios. These methods often struggle with\nissues such as texture-aware maintenance and size-aware fitting, which hinder\ntheir overall effectiveness. To address these limitations, we propose a novel\ngarment perception enhancement technique, termed FitDiT, designed for\nhigh-fidelity virtual try-on using Diffusion Transformers (DiT) allocating more\nparameters and attention to high-resolution features. First, to further improve\ntexture-aware maintenance, we introduce a garment texture extractor that\nincorporates garment priors evolution to fine-tune garment feature,\nfacilitating to better capture rich details such as stripes, patterns, and\ntext. Additionally, we introduce frequency-domain learning by customizing a\nfrequency distance loss to enhance high-frequency garment details. To tackle\nthe size-aware fitting issue, we employ a dilated-relaxed mask strategy that\nadapts to the correct length of garments, preventing the generation of garments\nthat fill the entire mask area during cross-category try-on. Equipped with the\nabove design, FitDiT surpasses all baselines in both qualitative and\nquantitative evaluations. It excels in producing well-fitting garments with\nphotorealistic and intricate details, while also achieving competitive\ninference times of 4.57 seconds for a single 1024x768 image after DiT structure\nslimming, outperforming existing methods.\n","authors":["Boyuan Jiang","Xiaobin Hu","Donghao Luo","Qingdong He","Chengming Xu","Jinlong Peng","Jiangning Zhang","Chengjie Wang","Yunsheng Wu","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2411.10499v2.pdf","comment":"Project page: https://byjiang.com/FitDiT/"},{"id":"http://arxiv.org/abs/2411.14789v1","updated":"2024-11-22T08:17:46Z","published":"2024-11-22T08:17:46Z","title":"Simplifying CLIP: Unleashing the Power of Large-Scale Models on\n  Consumer-level Computers","summary":"  Contrastive Language-Image Pre-training (CLIP) has attracted a surge of\nattention for its superior zero-shot performance and excellent transferability\nto downstream tasks. However, training such large-scale models usually requires\nsubstantial computation and storage, which poses barriers for general users\nwith consumer-level computers. Motivated by this observation, in this paper we\ninvestigate how to achieve competitive performance on only one Nvidia RTX3090\nGPU and with one terabyte for storing dataset. On one hand, we simplify the\ntransformer block structure and combine Weight Inheritance with multi-stage\nKnowledge Distillation (WIKD), thereby reducing the parameters and improving\nthe inference speed during training along with deployment. On the other hand,\nconfronted with the convergence challenge posed by small dataset, we generate\nsynthetic captions for each sample as data augmentation, and devise a novel\nPair Matching (PM) loss to fully exploit the distinguishment among positive and\nnegative image-text pairs. Extensive experiments demonstrate that our model can\nachieve a new state-of-the-art datascale-parameter-accuracy tradeoff, which\ncould further popularize the CLIP model in the related research community.\n","authors":["Hongbo Liu"],"pdf_url":"https://arxiv.org/pdf/2411.14789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14786v1","updated":"2024-11-22T08:06:32Z","published":"2024-11-22T08:06:32Z","title":"FastGrasp: Efficient Grasp Synthesis with Diffusion","summary":"  Effectively modeling the interaction between human hands and objects is\nchallenging due to the complex physical constraints and the requirement for\nhigh generation efficiency in applications. Prior approaches often employ\ncomputationally intensive two-stage approaches, which first generate an\nintermediate representation, such as contact maps, followed by an iterative\noptimization procedure that updates hand meshes to capture the hand-object\nrelation. However, due to the high computation complexity during the\noptimization stage, such strategies often suffer from low efficiency in\ninference. To address this limitation, this work introduces a novel\ndiffusion-model-based approach that generates the grasping pose in a one-stage\nmanner. This allows us to significantly improve generation speed and the\ndiversity of generated hand poses. In particular, we develop a Latent Diffusion\nModel with an Adaptation Module for object-conditioned hand pose generation and\na contact-aware loss to enforce the physical constraints between hands and\nobjects. Extensive experiments demonstrate that our method achieves faster\ninference, higher diversity, and superior pose quality than state-of-the-art\napproaches. Code is available at\n\\href{https://github.com/wuxiaofei01/FastGrasp}{https://github.com/wuxiaofei01/FastGrasp.}\n","authors":["Xiaofei Wu","Tao Liu","Caoji Li","Yuexin Ma","Yujiao Shi","Xuming He"],"pdf_url":"https://arxiv.org/pdf/2411.14786v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14781v1","updated":"2024-11-22T07:51:36Z","published":"2024-11-22T07:51:36Z","title":"Reconciling Semantic Controllability and Diversity for Remote Sensing\n  Image Synthesis with Hybrid Semantic Embedding","summary":"  Significant advancements have been made in semantic image synthesis in remote\nsensing. However, existing methods still face formidable challenges in\nbalancing semantic controllability and diversity. In this paper, we present a\nHybrid Semantic Embedding Guided Generative Adversarial Network (HySEGGAN) for\ncontrollable and efficient remote sensing image synthesis. Specifically,\nHySEGGAN leverages hierarchical information from a single source. Motivated by\nfeature description, we propose a hybrid semantic Embedding method, that\ncoordinates fine-grained local semantic layouts to characterize the geometric\nstructure of remote sensing objects without extra information. Besides, a\nSemantic Refinement Network (SRN) is introduced, incorporating a novel loss\nfunction to ensure fine-grained semantic feedback. The proposed approach\nmitigates semantic confusion and prevents geometric pattern collapse.\nExperimental results indicate that the method strikes an excellent balance\nbetween semantic controllability and diversity. Furthermore, HySEGGAN\nsignificantly improves the quality of synthesized images and achieves\nstate-of-the-art performance as a data augmentation technique across multiple\ndatasets for downstream tasks.\n","authors":["Junde Liu","Danpei Zhao","Bo Yuan","Wentao Li","Tian Li"],"pdf_url":"https://arxiv.org/pdf/2411.14781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14596v4","updated":"2024-11-22T07:43:21Z","published":"2024-06-20T17:45:02Z","title":"VLM Agents Generate Their Own Memories: Distilling Experience into\n  Embodied Programs of Thought","summary":"  Large-scale generative language and vision-language models (LLMs and VLMs)\nexcel in few-shot in-context learning for decision making and instruction\nfollowing. However, they require high-quality exemplar demonstrations in their\ncontext window. In this work, we ask: Can LLMs and VLMs generate their own\nexamples from generic, sub-optimal demonstrations? We propose In-Context\nAbstraction Learning (ICAL), a method that builds a memory of multimodal\nexperience from sub-optimal demonstrations and human feedback. Given a task\ndemonstration that may contain inefficiencies or mistakes, a VLM abstracts the\ntrajectory into a generalized program of thoughts by correcting inefficient\nactions and annotating cognitive abstractions: causal relationships, object\nstate changes, temporal subgoals, and task-relevant visual elements. These\nprograms of thought are iteratively improved through human feedback while the\nagent executes the trajectory in a similar environment. The resulting examples\nsignificantly improve decision-making in retrieval-augmented LLM and VLM\nagents. Moreover, as the agent's library of examples grows, it becomes more\nefficient, relying less on human feedback and requiring fewer environment\ninteractions per demonstration. Our ICAL agent surpasses the SOTA in\ndialogue-based instruction following in TEACh, multimodal web agents in\nVisualWebArena, and action anticipation in Ego4D. In TEACh, we achieve a 12.6%\nimprovement in goal-condition success. In VisualWebArena, our task success rate\nimproves over few-shot GPT4V. In Ego4D action forecasting, we improve over\nfew-shot GPT-4V and remain competitive with supervised models. We show\nfinetuning our retrieval-augmented in-context agent yields additional\nimprovements. Our approach significantly reduces reliance on manual prompt\nengineering and consistently outperforms in-context learning from action plans\nthat lack such programs of thought.\n","authors":["Gabriel Sarch","Lawrence Jang","Michael J. Tarr","William W. Cohen","Kenneth Marino","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2406.14596v4.pdf","comment":"Project website: http://ical-learning.github.io/"},{"id":"http://arxiv.org/abs/2411.14775v1","updated":"2024-11-22T07:33:33Z","published":"2024-11-22T07:33:33Z","title":"A Benchmark Dataset for Collaborative SLAM in Service Environments","summary":"  As service environments have become diverse, they have started to demand\ncomplicated tasks that are difficult for a single robot to complete. This\nchange has led to an interest in multiple robots instead of a single robot.\nC-SLAM, as a fundamental technique for multiple service robots, needs to handle\ndiverse challenges such as homogeneous scenes and dynamic objects to ensure\nthat robots operate smoothly and perform their tasks safely. However, existing\nC-SLAM datasets do not include the various indoor service environments with the\naforementioned challenges. To close this gap, we introduce a new multi-modal\nC-SLAM dataset for multiple service robots in various indoor service\nenvironments, called C-SLAM dataset in Service Environments (CSE). We use the\nNVIDIA Isaac Sim to generate data in various indoor service environments with\nthe challenges that may occur in real-world service environments. By using\nsimulation, we can provide accurate and precisely time-synchronized sensor\ndata, such as stereo RGB, stereo depth, IMU, and ground truth (GT) poses. We\nconfigure three common indoor service environments (Hospital, Office, and\nWarehouse), each of which includes various dynamic objects that perform motions\nsuitable to each environment. In addition, we drive three robots to mimic the\nactions of real service robots. Through these factors, we generate a more\nrealistic C-SLAM dataset for multiple service robots. We demonstrate our\ndataset by evaluating diverse state-of-the-art single-robot SLAM and\nmulti-robot SLAM methods. Our dataset is available at\nhttps://github.com/vision3d-lab/CSE_Dataset.\n","authors":["Harin Park","Inha Lee","Minje Kim","Hyungyu Park","Kyungdon Joo"],"pdf_url":"https://arxiv.org/pdf/2411.14775v1.pdf","comment":"8 pages, 6 figures, Accepted to IEEE RA-L"},{"id":"http://arxiv.org/abs/2411.14774v1","updated":"2024-11-22T07:32:11Z","published":"2024-11-22T07:32:11Z","title":"Resolution-Agnostic Transformer-based Climate Downscaling","summary":"  Understanding future weather changes at regional and local scales is crucial\nfor planning and decision-making, particularly in the context of extreme\nweather events, as well as for broader applications in agriculture, insurance,\nand infrastructure development. However, the computational cost of downscaling\nGlobal Climate Models (GCMs) to the fine resolutions needed for such\napplications presents a significant barrier. Drawing on advancements in weather\nforecasting models, this study introduces a cost-efficient downscaling method\nusing a pretrained Earth Vision Transformer (Earth ViT) model. Initially\ntrained on ERA5 data to downscale from 50 km to 25 km resolution, the model is\nthen tested on the higher resolution BARRA-SY dataset at a 3 km resolution.\nRemarkably, it performs well without additional training, demonstrating its\nability to generalize across different resolutions. This approach holds promise\nfor generating large ensembles of regional climate simulations by downscaling\nGCMs with varying input resolutions without incurring additional training\ncosts. Ultimately, this method could provide more comprehensive estimates of\npotential future changes in key climate variables, aiding in effective planning\nfor extreme weather events and climate change adaptation strategies.\n","authors":["Declan Curran","Hira Saleem","Flora Salim","Sanaa Hobeichi"],"pdf_url":"https://arxiv.org/pdf/2411.14774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.09145v2","updated":"2024-11-22T07:23:23Z","published":"2023-05-16T03:51:34Z","title":"Deep ReLU Networks Have Surprisingly Simple Polytopes","summary":"  A ReLU network is a piecewise linear function over polytopes. Figuring out\nthe properties of such polytopes is of fundamental importance for the research\nand development of neural networks. So far, either theoretical or empirical\nstudies on polytopes only stay at the level of counting their number, which is\nfar from a complete characterization. Here, we propose to study the shapes of\npolytopes via the number of faces of the polytope. Then, by computing and\nanalyzing the histogram of faces across polytopes, we find that a ReLU network\nhas relatively simple polytopes under both initialization and gradient descent,\nalthough these polytopes can be rather diverse and complicated by a specific\ndesign. This finding can be appreciated as a kind of generalized implicit bias,\nsubjected to the intrinsic geometric constraint in space partition of a ReLU\nnetwork. Next, we perform a combinatorial analysis to explain why adding depth\ndoes not generate a more complicated polytope by bounding the average number of\nfaces of polytopes with the dimensionality. Our results concretely reveal what\nkind of simple functions a network learns and what will happen when a network\ngoes deep. Also, by characterizing the shape of polytopes, the number of faces\ncan be a novel leverage for other problems, \\textit{e.g.}, serving as a generic\ntool to explain the power of popular shortcut networks such as ResNet and\nanalyzing the impact of different regularization strategies on a network's\nspace partition.\n","authors":["Feng-Lei Fan","Wei Huang","Xiangru Zhong","Lecheng Ruan","Tieyong Zeng","Huan Xiong","Fei Wang"],"pdf_url":"https://arxiv.org/pdf/2305.09145v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13909v2","updated":"2024-11-22T07:03:11Z","published":"2024-11-21T07:47:27Z","title":"Panther: Illuminate the Sight of Multimodal LLMs with Instruction-Guided\n  Visual Prompts","summary":"  Multimodal large language models (MLLMs) are closing the gap to human visual\nperception capability rapidly, while, still lag behind on attending to subtle\nimages details or locating small objects precisely, etc. Common schemes to\ntackle these issues include deploying multiple vision encoders or operating on\noriginal high-resolution images. Few studies have concentrated on taking the\ntextual instruction into improving visual representation, resulting in losing\nfocus in some vision-centric tasks, a phenomenon we herein termed as Amblyopia.\nIn this work, we introduce Panther, a MLLM that closely adheres to user\ninstruction and locates targets of interests precisely, with the finesse of a\nblack panther. Specifically, Panther comprises three integral components:\nPanther-VE, Panther-Bridge, and Panther-Decoder. Panther-VE integrates user\ninstruction information at the early stages of the vision encoder, thereby\nextracting the most relevant and useful visual representations. The\nPanther-Bridge module, equipped with powerful filtering capabilities,\nsignificantly reduces redundant visual information, leading to a substantial\nsavings in training costs. The Panther-Decoder is versatile and can be employed\nwith any decoder-only architecture of LLMs without discrimination. Experimental\nresults, particularly on vision-centric benchmarks, have demonstrated the\neffectiveness of Panther.\n","authors":["Honglin Li","Yuting Gao","Chenglu Zhu","Jingdong Chen","Ming Yang","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2411.13909v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14762v1","updated":"2024-11-22T06:50:44Z","published":"2024-11-22T06:50:44Z","title":"Efficient Long Video Tokenization via Coordinated-based Patch\n  Reconstruction","summary":"  Efficient tokenization of videos remains a challenge in training vision\nmodels that can process long videos. One promising direction is to develop a\ntokenizer that can encode long video clips, as it would enable the tokenizer to\nleverage the temporal coherence of videos better for tokenization. However,\ntraining existing tokenizers on long videos often incurs a huge training cost\nas they are trained to reconstruct all the frames at once. In this paper, we\nintroduce CoordTok, a video tokenizer that learns a mapping from\ncoordinate-based representations to the corresponding patches of input videos,\ninspired by recent advances in 3D generative models. In particular, CoordTok\nencodes a video into factorized triplane representations and reconstructs\npatches that correspond to randomly sampled $(x,y,t)$ coordinates. This allows\nfor training large tokenizer models directly on long videos without requiring\nexcessive training resources. Our experiments show that CoordTok can\ndrastically reduce the number of tokens for encoding long video clips. For\ninstance, CoordTok can encode a 128-frame video with 128$\\times$128 resolution\ninto 1280 tokens, while baselines need 6144 or 8192 tokens to achieve similar\nreconstruction quality. We further show that this efficient video tokenization\nenables memory-efficient training of a diffusion transformer that can generate\n128 frames at once.\n","authors":["Huiwon Jang","Sihyun Yu","Jinwoo Shin","Pieter Abbeel","Younggyo Seo"],"pdf_url":"https://arxiv.org/pdf/2411.14762v1.pdf","comment":"Code is available on the project webpage:\n  https://huiwon-jang.github.io/coordtok/"},{"id":"http://arxiv.org/abs/2410.22135v2","updated":"2024-11-22T06:41:07Z","published":"2024-10-29T15:31:27Z","title":"Lightweight Frequency Masker for Cross-Domain Few-Shot Semantic\n  Segmentation","summary":"  Cross-domain few-shot segmentation (CD-FSS) is proposed to first pre-train\nthe model on a large-scale source-domain dataset, and then transfer the model\nto data-scarce target-domain datasets for pixel-level segmentation. The\nsignificant domain gap between the source and target datasets leads to a sharp\ndecline in the performance of existing few-shot segmentation (FSS) methods in\ncross-domain scenarios. In this work, we discover an intriguing phenomenon:\nsimply filtering different frequency components for target domains can lead to\na significant performance improvement, sometimes even as high as 14% mIoU.\nThen, we delve into this phenomenon for an interpretation, and find such\nimprovements stem from the reduced inter-channel correlation in feature maps,\nwhich benefits CD-FSS with enhanced robustness against domain gaps and larger\nactivated regions for segmentation. Based on this, we propose a lightweight\nfrequency masker, which further reduces channel correlations by an\nAmplitude-Phase Masker (APM) module and an Adaptive Channel Phase Attention\n(ACPA) module. Notably, APM introduces only 0.01% additional parameters but\nimproves the average performance by over 10%, and ACPA imports only 2.5%\nparameters but further improves the performance by over 1.5%, which\nsignificantly surpasses the state-of-the-art CD-FSS methods.\n","authors":["Jintao Tong","Yixiong Zou","Yuhua Li","Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2410.22135v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2405.15549v3","updated":"2024-11-22T06:33:23Z","published":"2024-05-24T13:35:56Z","title":"SEP: Self-Enhanced Prompt Tuning for Visual-Language Model","summary":"  Prompt tuning based on Context Optimization (CoOp) effectively adapts\nvisual-language models (VLMs) to downstream tasks by inferring additional\nlearnable prompt tokens. However, these tokens are less discriminative as they\nare independent of the pre-trained tokens and fail to capture input-specific\nknowledge, such as class-aware textual or instance-aware visual knowledge.\nLeveraging the discriminative and generalization capabilities inherent in\npre-trained tokens, we introduce a novel approach named Self-Enhanced Prompt\nTuning (SEP). The core principle of SEP involves adapting the learnable prompt\ntokens at each encoder layer from the corresponding self-pretrained tokens,\nthereby explicitly incorporating discriminative prior knowledge to enhance both\ntextual-level and visual-level embeddings. Furthermore, SEP's self-enhanced\ntokens not only boost discrimination but also mitigate domain shifts in unseen\ndomains, enhancing generalization. In practice, SEP selects several\nrepresentative tokens from all pre-trained tokens for each input data at every\nlayer of the text/visual encoders. Subsequently, a Token Fusion Module (TFM) is\nintroduced to generate a self-enhanced token by merging these representative\ntokens with the learnable tokens using a cross-attention mechanism. This\nself-enhanced token is then concatenated with all pre-trained tokens, serving\nas input for subsequent encoder layers to produce the relevant embeddings.\nComprehensive evaluations across various benchmarks and tasks confirm SEP's\nefficacy in prompt tuning. Code: \\href{Code}{https://github.com/htyao89/SEP}.\n","authors":["Hantao Yao","Rui Zhang","Lu Yu","Yongdong Zhang","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2405.15549v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14755v1","updated":"2024-11-22T06:23:58Z","published":"2024-11-22T06:23:58Z","title":"FairAdapter: Detecting AI-generated Images with Improved Fairness","summary":"  The high-quality, realistic images generated by generative models pose\nsignificant challenges for exposing them.So far, data-driven deep neural\nnetworks have been justified as the most efficient forensics tools for the\nchallenges. However, they may be over-fitted to certain semantics, resulting in\nconsiderable inconsistency in detection performance across different contents\nof generated samples. It could be regarded as an issue of detection fairness.\nIn this paper, we propose a novel framework named Fairadapter to tackle the\nissue. In comparison with existing state-of-the-art methods, our model achieves\nimproved fairness performance. Our project:\nhttps://github.com/AppleDogDog/FairnessDetection\n","authors":["Feng Ding","Jun Zhang","Xinan He","Jianfeng Xu"],"pdf_url":"https://arxiv.org/pdf/2411.14755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14752v1","updated":"2024-11-22T06:16:56Z","published":"2024-11-22T06:16:56Z","title":"Comparative Analysis of nnUNet and MedNeXt for Head and Neck Tumor\n  Segmentation in MRI-guided Radiotherapy","summary":"  Radiation therapy (RT) is essential in treating head and neck cancer (HNC),\nwith magnetic resonance imaging(MRI)-guided RT offering superior soft tissue\ncontrast and functional imaging. However, manual tumor segmentation is\ntime-consuming and complex, and therfore remains a challenge. In this study, we\npresent our solution as team TUMOR to the HNTS-MRG24 MICCAI Challenge which is\nfocused on automated segmentation of primary gross tumor volumes (GTVp) and\nmetastatic lymph node gross tumor volume (GTVn) in pre-RT and mid-RT MRI\nimages. We utilized the HNTS-MRG2024 dataset, which consists of 150 MRI scans\nfrom patients diagnosed with HNC, including original and registered pre-RT and\nmid-RT T2-weighted images with corresponding segmentation masks for GTVp and\nGTVn. We employed two state-of-the-art models in deep learning, nnUNet and\nMedNeXt. For Task 1, we pretrained models on pre-RT registered and mid-RT\nimages, followed by fine-tuning on original pre-RT images. For Task 2, we\ncombined registered pre-RT images, registered pre-RT segmentation masks, and\nmid-RT data as a multi-channel input for training. Our solution for Task 1\nachieved 1st place in the final test phase with an aggregated Dice Similarity\nCoefficient of 0.8254, and our solution for Task 2 ranked 8th with a score of\n0.7005. The proposed solution is publicly available at Github Repository.\n","authors":["Nikoo Moradi","André Ferreira","Behrus Puladi","Jens Kleesiek","Emad Fatemizadeh","Gijs Luijten","Victor Alves","Jan Egger"],"pdf_url":"https://arxiv.org/pdf/2411.14752v1.pdf","comment":"15 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.14751v1","updated":"2024-11-22T06:13:42Z","published":"2024-11-22T06:13:42Z","title":"TopoSD: Topology-Enhanced Lane Segment Perception with SDMap Prior","summary":"  Recent advances in autonomous driving systems have shifted towards reducing\nreliance on high-definition maps (HDMaps) due to the huge costs of annotation\nand maintenance. Instead, researchers are focusing on online vectorized HDMap\nconstruction using on-board sensors. However, sensor-only approaches still face\nchallenges in long-range perception due to the restricted views imposed by the\nmounting angles of onboard cameras, just as human drivers also rely on\nbird's-eye-view navigation maps for a comprehensive understanding of road\nstructures. To address these issues, we propose to train the perception model\nto \"see\" standard definition maps (SDMaps). We encode SDMap elements into\nneural spatial map representations and instance tokens, and then incorporate\nsuch complementary features as prior information to improve the bird's eye view\n(BEV) feature for lane geometry and topology decoding. Based on the lane\nsegment representation framework, the model simultaneously predicts lanes,\ncentrelines and their topology. To further enhance the ability of geometry\nprediction and topology reasoning, we also use a topology-guided decoder to\nrefine the predictions by exploiting the mutual relationships between\ntopological and geometric features. We perform extensive experiments on\nOpenLane-V2 datasets to validate the proposed method. The results show that our\nmodel outperforms state-of-the-art methods by a large margin, with gains of\n+6.7 and +9.1 on the mAP and topology metrics. Our analysis also reveals that\nmodels trained with SDMap noise augmentation exhibit enhanced robustness.\n","authors":["Sen Yang","Minyue Jiang","Ziwei Fan","Xiaolu Xie","Xiao Tan","Yingying Li","Errui Ding","Liang Wang","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2411.14751v1.pdf","comment":"17 pages, 7 figures, and 7 tables"},{"id":"http://arxiv.org/abs/2411.14750v1","updated":"2024-11-22T06:11:35Z","published":"2024-11-22T06:11:35Z","title":"Ordinal Multiple-instance Learning for Ulcerative Colitis Severity\n  Estimation with Selective Aggregated Transformer","summary":"  Patient-level diagnosis of severity in ulcerative colitis (UC) is common in\nreal clinical settings, where the most severe score in a patient is recorded.\nHowever, previous UC classification methods (i.e., image-level estimation)\nmainly assumed the input was a single image. Thus, these methods can not\nutilize severity labels recorded in real clinical settings. In this paper, we\npropose a patient-level severity estimation method by a transformer with\nselective aggregator tokens, where a severity label is estimated from multiple\nimages taken from a patient, similar to a clinical setting. Our method can\neffectively aggregate features of severe parts from a set of images captured in\neach patient, and it facilitates improving the discriminative ability between\nadjacent severity classes. Experiments demonstrate the effectiveness of the\nproposed method on two datasets compared with the state-of-the-art MIL methods.\nMoreover, we evaluated our method in real clinical settings and confirmed that\nour method outperformed the previous image-level methods. The code is publicly\navailable at\nhttps://github.com/Shiku-Kaito/Ordinal-Multiple-instance-Learning-for-Ulcerative-Colitis-Severity-Estimation.\n","authors":["Kaito Shiku","Kazuya Nishimura","Daiki Suehiro","Kiyohito Tanaka","Ryoma Bise"],"pdf_url":"https://arxiv.org/pdf/2411.14750v1.pdf","comment":"10 pages, 9 figures, Accepted in WACV 2025"},{"id":"http://arxiv.org/abs/2407.01782v4","updated":"2024-11-22T05:56:30Z","published":"2024-07-01T20:21:09Z","title":"Addressing a fundamental limitation in deep vision models: lack of\n  spatial attention","summary":"  The primary aim of this manuscript is to underscore a significant limitation\nin current deep learning models, particularly vision models. Unlike human\nvision, which efficiently selects only the essential visual areas for further\nprocessing, leading to high speed and low energy consumption, deep vision\nmodels process the entire image. In this work, we examine this issue from a\nbroader perspective and propose two solutions that could pave the way for the\nnext generation of more efficient vision models. In the first solution,\nconvolution and pooling operations are selectively applied to altered regions,\nwith a change map sent to subsequent layers. This map indicates which\ncomputations need to be repeated. In the second solution, only the modified\nregions are processed by a semantic segmentation model, and the resulting\nsegments are inserted into the corresponding areas of the previous output map.\nThe code is available at https://github.com/aliborji/spatial_attention.\n","authors":["Ali Borji"],"pdf_url":"https://arxiv.org/pdf/2407.01782v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15094v2","updated":"2024-11-22T05:41:58Z","published":"2024-08-27T14:25:42Z","title":"Constrained Diffusion Models via Dual Training","summary":"  Diffusion models have attained prominence for their ability to synthesize a\nprobability distribution for a given dataset via a diffusion process, enabling\nthe generation of new data points with high fidelity. However, diffusion\nprocesses are prone to generating samples that reflect biases in a training\ndataset. To address this issue, we develop constrained diffusion models by\nimposing diffusion constraints based on desired distributions that are informed\nby requirements. Specifically, we cast the training of diffusion models under\nrequirements as a constrained distribution optimization problem that aims to\nreduce the distribution difference between original and generated data while\nobeying constraints on the distribution of generated data. We show that our\nconstrained diffusion models generate new data from a mixture data distribution\nthat achieves the optimal trade-off among objective and constraints. To train\nconstrained diffusion models, we develop a dual training algorithm and\ncharacterize the optimality of the trained constrained diffusion model. We\nempirically demonstrate the effectiveness of our constrained models in two\nconstrained generation tasks: (i) we consider a dataset with one or more\nunderrepresented classes where we train the model with constraints to ensure\nfairly sampling from all classes during inference; (ii) we fine-tune a\npre-trained diffusion model to sample from a new dataset while avoiding\noverfitting.\n","authors":["Shervin Khalafi","Dongsheng Ding","Alejandro Ribeiro"],"pdf_url":"https://arxiv.org/pdf/2408.15094v2.pdf","comment":"31 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2411.14744v1","updated":"2024-11-22T05:41:00Z","published":"2024-11-22T05:41:00Z","title":"Point Cloud Understanding via Attention-Driven Contrastive Learning","summary":"  Recently Transformer-based models have advanced point cloud understanding by\nleveraging self-attention mechanisms, however, these methods often overlook\nlatent information in less prominent regions, leading to increased sensitivity\nto perturbations and limited global comprehension. To solve this issue, we\nintroduce PointACL, an attention-driven contrastive learning framework designed\nto address these limitations. Our method employs an attention-driven dynamic\nmasking strategy that guides the model to focus on under-attended regions,\nenhancing the understanding of global structures within the point cloud. Then\nwe combine the original pre-training loss with a contrastive learning loss,\nimproving feature discrimination and generalization. Extensive experiments\nvalidate the effectiveness of PointACL, as it achieves state-of-the-art\nperformance across a variety of 3D understanding tasks, including object\nclassification, part segmentation, and few-shot learning. Specifically, when\nintegrated with different Transformer backbones like Point-MAE and PointGPT,\nPointACL demonstrates improved performance on datasets such as ScanObjectNN,\nModelNet40, and ShapeNetPart. This highlights its superior capability in\ncapturing both global and local features, as well as its enhanced robustness\nagainst perturbations and incomplete data.\n","authors":["Yi Wang","Jiaze Wang","Ziyu Guo","Renrui Zhang","Donghao Zhou","Guangyong Chen","Anfeng Liu","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2411.14744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14743v1","updated":"2024-11-22T05:36:38Z","published":"2024-11-22T05:36:38Z","title":"FOCUS: Knowledge-enhanced Adaptive Visual Compression for Few-shot Whole\n  Slide Image Classification","summary":"  Few-shot learning presents a critical solution for cancer diagnosis in\ncomputational pathology (CPath), addressing fundamental limitations in data\navailability, particularly the scarcity of expert annotations and patient\nprivacy constraints. A key challenge in this paradigm stems from the inherent\ndisparity between the limited training set of whole slide images (WSIs) and the\nenormous number of contained patches, where a significant portion of these\npatches lacks diagnostically relevant information, potentially diluting the\nmodel's ability to learn and focus on critical diagnostic features. While\nrecent works attempt to address this by incorporating additional knowledge,\nseveral crucial gaps hinder further progress: (1) despite the emergence of\npowerful pathology foundation models (FMs), their potential remains largely\nuntapped, with most approaches limiting their use to basic feature extraction;\n(2) current language guidance mechanisms attempt to align text prompts with\nvast numbers of WSI patches all at once, struggling to leverage rich\npathological semantic information. To this end, we introduce the\nknowledge-enhanced adaptive visual compression framework, dubbed FOCUS, which\nuniquely combines pathology FMs with language prior knowledge to enable a\nfocused analysis of diagnostically relevant regions by prioritizing\ndiscriminative WSI patches. Our approach implements a progressive three-stage\ncompression strategy: we first leverage FMs for global visual redundancy\nelimination, and integrate compressed features with language prompts for\nsemantic relevance assessment, then perform neighbor-aware visual token\nfiltering while preserving spatial coherence. Extensive experiments on\npathological datasets spanning breast, lung, and ovarian cancers demonstrate\nits superior performance in few-shot pathology diagnosis. Code will be made\navailable at https://github.com/dddavid4real/FOCUS.\n","authors":["Zhengrui Guo","Conghao Xiong","Jiabo Ma","Qichen Sun","Lishuang Feng","Jinzhuo Wang","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2411.14743v1.pdf","comment":"15 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.14740v1","updated":"2024-11-22T05:22:11Z","published":"2024-11-22T05:22:11Z","title":"TEXGen: a Generative Diffusion Model for Mesh Textures","summary":"  While high-quality texture maps are essential for realistic 3D asset\nrendering, few studies have explored learning directly in the texture space,\nespecially on large-scale datasets. In this work, we depart from the\nconventional approach of relying on pre-trained 2D diffusion models for\ntest-time optimization of 3D textures. Instead, we focus on the fundamental\nproblem of learning in the UV texture space itself. For the first time, we\ntrain a large diffusion model capable of directly generating high-resolution\ntexture maps in a feed-forward manner. To facilitate efficient learning in\nhigh-resolution UV spaces, we propose a scalable network architecture that\ninterleaves convolutions on UV maps with attention layers on point clouds.\nLeveraging this architectural design, we train a 700 million parameter\ndiffusion model that can generate UV texture maps guided by text prompts and\nsingle-view images. Once trained, our model naturally supports various extended\napplications, including text-guided texture inpainting, sparse-view texture\ncompletion, and text-driven texture synthesis. Project page is at\nhttp://cvmi-lab.github.io/TEXGen/.\n","authors":["Xin Yu","Ze Yuan","Yuan-Chen Guo","Ying-Tian Liu","JianHui Liu","Yangguang Li","Yan-Pei Cao","Ding Liang","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2411.14740v1.pdf","comment":"Accepted to SIGGRAPH Asia Journal Article (TOG 2024)"},{"id":"http://arxiv.org/abs/2406.09413v3","updated":"2024-11-22T05:12:30Z","published":"2024-06-13T17:59:56Z","title":"Interpreting the Weight Space of Customized Diffusion Models","summary":"  We investigate the space of weights spanned by a large collection of\ncustomized diffusion models. We populate this space by creating a dataset of\nover 60,000 models, each of which is a base model fine-tuned to insert a\ndifferent person's visual identity. We model the underlying manifold of these\nweights as a subspace, which we term weights2weights. We demonstrate three\nimmediate applications of this space that result in new diffusion models --\nsampling, editing, and inversion. First, sampling a set of weights from this\nspace results in a new model encoding a novel identity. Next, we find linear\ndirections in this space corresponding to semantic edits of the identity (e.g.,\nadding a beard), resulting in a new model with the original identity edited.\nFinally, we show that inverting a single image into this space encodes a\nrealistic identity into a model, even if the input image is out of distribution\n(e.g., a painting). We further find that these linear properties of the\ndiffusion model weight space extend to other visual concepts. Our results\nindicate that the weight space of fine-tuned diffusion models can behave as an\ninterpretable meta-latent space producing new models.\n","authors":["Amil Dravid","Yossi Gandelsman","Kuan-Chieh Wang","Rameen Abdal","Gordon Wetzstein","Alexei A. Efros","Kfir Aberman"],"pdf_url":"https://arxiv.org/pdf/2406.09413v3.pdf","comment":"Project Page: https://snap-research.github.io/weights2weights"},{"id":"http://arxiv.org/abs/2411.14737v1","updated":"2024-11-22T05:11:51Z","published":"2024-11-22T05:11:51Z","title":"AI Tailoring: Evaluating Influence of Image Features on Fashion Product\n  Popularity","summary":"  Identifying key product features that influence consumer preferences is\nessential in the fashion industry. In this study, we introduce a robust\nmethodology to ascertain the most impactful features in fashion product images,\nutilizing past market sales data. First, we propose the metric called\n\"influence score\" to quantitatively assess the importance of product features.\nThen we develop a forecasting model, the Fashion Demand Predictor (FDP), which\nintegrates Transformer-based models and Random Forest to predict market\npopularity based on product images. We employ image-editing diffusion models to\nmodify these images and perform an ablation study, which validates the impact\nof the highest and lowest-scoring features on the model's popularity\npredictions. Additionally, we further validate these results through surveys\nthat gather human rankings of preferences, confirming the accuracy of the FDP\nmodel's predictions and the efficacy of our method in identifying influential\nfeatures. Notably, products enhanced with \"good\" features show marked\nimprovements in predicted popularity over their modified counterparts. Our\napproach develops a fully automated and systematic framework for fashion image\nanalysis that provides valuable guidance for downstream tasks such as fashion\nproduct design and marketing strategy development.\n","authors":["Xiaomin Li","Junyi Sha"],"pdf_url":"https://arxiv.org/pdf/2411.14737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.04370v4","updated":"2024-11-22T05:09:29Z","published":"2024-05-07T14:51:05Z","title":"Diff-IP2D: Diffusion-Based Hand-Object Interaction Prediction on\n  Egocentric Videos","summary":"  Understanding how humans would behave during hand-object interaction is vital\nfor applications in service robot manipulation and extended reality. To achieve\nthis, some recent works have been proposed to simultaneously forecast hand\ntrajectories and object affordances on human egocentric videos. The joint\nprediction serves as a comprehensive representation of future hand-object\ninteractions in 2D space, indicating potential human motion and motivation.\nHowever, the existing approaches mostly adopt the autoregressive paradigm for\nunidirectional prediction, which lacks mutual constraints within the holistic\nfuture sequence, and accumulates errors along the time axis. Meanwhile, these\nworks basically overlook the effect of camera egomotion on first-person view\npredictions. To address these limitations, we propose a novel diffusion-based\ninteraction prediction method, namely Diff-IP2D, to forecast future hand\ntrajectories and object affordances concurrently in an iterative\nnon-autoregressive manner. We transform the sequential 2D images into latent\nfeature space and design a denoising diffusion model to predict future latent\ninteraction features conditioned on past ones. Motion features are further\nintegrated into the conditional denoising process to enable Diff-IP2D aware of\nthe camera wearer's dynamics for more accurate interaction prediction.\nExtensive experiments demonstrate that our method significantly outperforms the\nstate-of-the-art baselines on both the off-the-shelf metrics and our newly\nproposed evaluation protocol. This highlights the efficacy of leveraging a\ngenerative paradigm for 2D hand-object interaction prediction. The code of\nDiff-IP2D is released as open source at https://github.com/IRMVLab/Diff-IP2D.\n","authors":["Junyi Ma","Jingyi Xu","Xieyuanli Chen","Hesheng Wang"],"pdf_url":"https://arxiv.org/pdf/2405.04370v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14725v1","updated":"2024-11-22T04:41:20Z","published":"2024-11-22T04:41:20Z","title":"Evaluating and Advancing Multimodal Large Language Models in Ability\n  Lens","summary":"  As multimodal large language models (MLLMs) advance rapidly, rigorous\nevaluation has become essential, providing further guidance for their\ndevelopment. In this work, we focus on a unified and robust evaluation of\n\\textbf{vision perception} abilities, the foundational skill of MLLMs. We find\nthat existing perception benchmarks, each focusing on different question types,\ndomains, and evaluation metrics, introduce significant evaluation variance,\ncomplicating comprehensive assessments of perception abilities when relying on\nany single benchmark. To address this, we introduce \\textbf{AbilityLens}, a\nunified benchmark designed to evaluate MLLMs across six key perception\nabilities, focusing on both accuracy and stability, with each ability\nencompassing diverse question types, domains, and metrics. With the assistance\nof AbilityLens, we: (1) identify the strengths and weaknesses of current\nmodels, highlighting stability patterns and revealing a notable performance gap\nbetween open-source and closed-source models; (2) introduce an online\nevaluation mode, which uncovers interesting ability conflict and early\nconvergence phenomena during MLLM training; and (3) design a simple\nability-specific model merging method that combines the best ability checkpoint\nfrom early training stages, effectively mitigating performance decline due to\nability conflict. The benchmark and online leaderboard will be released soon.\n","authors":["Feng Chen","Chenhui Gou","Jing Liu","Yang Yang","Zhaoyang Li","Jiyuan Zhang","Zhenbang Sun","Bohan Zhuang","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2411.14725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14723v1","updated":"2024-11-22T04:36:12Z","published":"2024-11-22T04:36:12Z","title":"Effective SAM Combination for Open-Vocabulary Semantic Segmentation","summary":"  Open-vocabulary semantic segmentation aims to assign pixel-level labels to\nimages across an unlimited range of classes. Traditional methods address this\nby sequentially connecting a powerful mask proposal generator, such as the\nSegment Anything Model (SAM), with a pre-trained vision-language model like\nCLIP. But these two-stage approaches often suffer from high computational\ncosts, memory inefficiencies. In this paper, we propose ESC-Net, a novel\none-stage open-vocabulary segmentation model that leverages the SAM decoder\nblocks for class-agnostic segmentation within an efficient inference framework.\nBy embedding pseudo prompts generated from image-text correlations into SAM's\npromptable segmentation framework, ESC-Net achieves refined spatial aggregation\nfor accurate mask predictions. ESC-Net achieves superior performance on\nstandard benchmarks, including ADE20K, PASCAL-VOC, and PASCAL-Context,\noutperforming prior methods in both efficiency and accuracy. Comprehensive\nablation studies further demonstrate its robustness across challenging\nconditions.\n","authors":["Minhyeok Lee","Suhwan Cho","Jungho Lee","Sunghun Yang","Heeseung Choi","Ig-Jae Kim","Sangyoun Lee"],"pdf_url":"https://arxiv.org/pdf/2411.14723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01544v2","updated":"2024-11-22T04:32:55Z","published":"2024-10-02T13:30:32Z","title":"Boosting Weakly-Supervised Referring Image Segmentation via Progressive\n  Comprehension","summary":"  This paper explores the weakly-supervised referring image segmentation (WRIS)\nproblem, and focuses on a challenging setup where target localization is\nlearned directly from image-text pairs. We note that the input text description\ntypically already contains detailed information on how to localize the target\nobject, and we also observe that humans often follow a step-by-step\ncomprehension process (\\ie, progressively utilizing target-related attributes\nand relations as cues) to identify the target object. Hence, we propose a novel\nProgressive Comprehension Network (PCNet) to leverage target-related textual\ncues from the input description for progressively localizing the target object.\nSpecifically, we first use a Large Language Model (LLM) to decompose the input\ntext description into short phrases. These short phrases are taken as\ntarget-related cues and fed into a Conditional Referring Module (CRM) in\nmultiple stages, to allow updating the referring text embedding and enhance the\nresponse map for target localization in a multi-stage manner. Based on the CRM,\nwe then propose a Region-aware Shrinking (RaS) loss to constrain the visual\nlocalization to be conducted progressively in a coarse-to-fine manner across\ndifferent stages. Finally, we introduce an Instance-aware Disambiguation (IaD)\nloss to suppress instance localization ambiguity by differentiating overlapping\nresponse maps generated by different referring texts on the same image.\nExtensive experiments show that our method outperforms SOTA methods on three\ncommon benchmarks.\n","authors":["Zaiquan Yang","Yuhao Liu","Jiaying Lin","Gerhard Hancke","Rynson W. H. Lau"],"pdf_url":"https://arxiv.org/pdf/2410.01544v2.pdf","comment":"Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2411.14717v1","updated":"2024-11-22T04:09:23Z","published":"2024-11-22T04:09:23Z","title":"FedMLLM: Federated Fine-tuning MLLM on Multimodal Heterogeneity Data","summary":"  Multimodal Large Language Models (MLLMs) have made significant advancements,\ndemonstrating powerful capabilities in processing and understanding multimodal\ndata. Fine-tuning MLLMs with Federated Learning (FL) allows for expanding the\ntraining data scope by including private data sources, thereby enhancing their\npractical applicability in privacy-sensitive domains. However, current research\nremains in the early stage, particularly in addressing the \\textbf{multimodal\nheterogeneities} in real-world applications. In this paper, we introduce a\nbenchmark for evaluating various downstream tasks in the federated fine-tuning\nof MLLMs within multimodal heterogeneous scenarios, laying the groundwork for\nthe research in the field. Our benchmark encompasses two datasets, five\ncomparison baselines, and four multimodal scenarios, incorporating over ten\ntypes of modal heterogeneities. To address the challenges posed by modal\nheterogeneity, we develop a general FedMLLM framework that integrates four\nrepresentative FL methods alongside two modality-agnostic strategies. Extensive\nexperimental results show that our proposed FL paradigm improves the\nperformance of MLLMs by broadening the range of training data and mitigating\nmultimodal heterogeneity. Code is available at https://github.com/1xbq1/FedMLLM\n","authors":["Binqian Xu","Xiangbo Shu","Haiyang Mei","Guosen Xie","Basura Fernando","Mike Zheng Shou","Jinhui Tang"],"pdf_url":"https://arxiv.org/pdf/2411.14717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14716v1","updated":"2024-11-22T03:59:41Z","published":"2024-11-22T03:59:41Z","title":"VisionPAD: A Vision-Centric Pre-training Paradigm for Autonomous Driving","summary":"  This paper introduces VisionPAD, a novel self-supervised pre-training\nparadigm designed for vision-centric algorithms in autonomous driving. In\ncontrast to previous approaches that employ neural rendering with explicit\ndepth supervision, VisionPAD utilizes more efficient 3D Gaussian Splatting to\nreconstruct multi-view representations using only images as supervision.\nSpecifically, we introduce a self-supervised method for voxel velocity\nestimation. By warping voxels to adjacent frames and supervising the rendered\noutputs, the model effectively learns motion cues in the sequential data.\nFurthermore, we adopt a multi-frame photometric consistency approach to enhance\ngeometric perception. It projects adjacent frames to the current frame based on\nrendered depths and relative poses, boosting the 3D geometric representation\nthrough pure image supervision. Extensive experiments on autonomous driving\ndatasets demonstrate that VisionPAD significantly improves performance in 3D\nobject detection, occupancy prediction and map segmentation, surpassing\nstate-of-the-art pre-training strategies by a considerable margin.\n","authors":["Haiming Zhang","Wending Zhou","Yiyao Zhu","Xu Yan","Jiantao Gao","Dongfeng Bai","Yingjie Cai","Bingbing Liu","Shuguang Cui","Zhen Li"],"pdf_url":"https://arxiv.org/pdf/2411.14716v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21735v2","updated":"2024-11-22T03:58:23Z","published":"2024-07-31T16:43:20Z","title":"EMatch: A Unified Framework for Event-based Optical Flow and Stereo\n  Matching","summary":"  Event cameras have shown promise in vision applications like optical flow\nestimation and stereo matching, with many specialized architectures leveraging\nthe asynchronous and sparse nature of event data. However, existing works only\nfocus event data within the confines of task-specific domains, overlooking how\ntasks across the temporal and spatial domains can reinforce each other. In this\npaper, we reformulate event-based flow estimation and stereo matching as a\nunified dense correspondence matching problem, enabling us to solve both tasks\nwithin a single model by directly matching features in a shared representation\nspace. Specifically, our method utilizes a Temporal Recurrent Network to\naggregate event features across temporal or spatial domains, and a Spatial\nContextual Attention to enhance knowledge transfer across event flows via\ntemporal or spatial interactions. By utilizing a shared feature similarities\nmodule that integrates knowledge from event streams via temporal or spatial\ninteractions, our network performs optical flow estimation from temporal event\nsegment inputs and stereo matching from spatial event segment inputs\nsimultaneously. We demonstrate that our unified model inherently supports\nmulti-task fusion and cross-task transfer. Without the need for retraining for\nspecific task, our model can effectively handle both optical flow and stereo\nestimation, achieving state-of-the-art performance on both tasks.\n","authors":["Pengjie Zhang","Lin Zhu","Xiao Wang","Lizhi Wang","Wanxuan Lu","Hua Huang"],"pdf_url":"https://arxiv.org/pdf/2407.21735v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14715v1","updated":"2024-11-22T03:52:37Z","published":"2024-11-22T03:52:37Z","title":"Any-to-3D Generation via Hybrid Diffusion Supervision","summary":"  Recent progress in 3D object generation has been fueled by the strong priors\noffered by diffusion models. However, existing models are tailored to specific\ntasks, accommodating only one modality at a time and necessitating retraining\nto change modalities. Given an image-to-3D model and a text prompt, a naive\napproach is to convert text prompts to images and then use the image-to-3D\nmodel for generation. This approach is both time-consuming and labor-intensive,\nresulting in unavoidable information loss during modality conversion. To\naddress this, we introduce XBind, a unified framework for any-to-3D generation\nusing cross-modal pre-alignment techniques. XBind integrates an\nmultimodal-aligned encoder with pre-trained diffusion models to generate 3D\nobjects from any modalities, including text, images, and audio. We subsequently\npresent a novel loss function, termed Modality Similarity (MS) Loss, which\naligns the embeddings of the modality prompts and the rendered images,\nfacilitating improved alignment of the 3D objects with multiple modalities.\nAdditionally, Hybrid Diffusion Supervision combined with a Three-Phase\nOptimization process improves the quality of the generated 3D objects.\nExtensive experiments showcase XBind's broad generation capabilities in\nany-to-3D scenarios. To our knowledge, this is the first method to generate 3D\nobjects from any modality prompts. Project page:\nhttps://zeroooooooow1440.github.io/.\n","authors":["Yijun Fan","Yiwei Ma","Jiayi Ji","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2411.14715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14704v1","updated":"2024-11-22T03:28:55Z","published":"2024-11-22T03:28:55Z","title":"Cross-Modal Pre-Aligned Method with Global and Local Information for\n  Remote-Sensing Image and Text Retrieval","summary":"  Remote sensing cross-modal text-image retrieval (RSCTIR) has gained attention\nfor its utility in information mining. However, challenges remain in\neffectively integrating global and local information due to variations in\nremote sensing imagery and ensuring proper feature pre-alignment before modal\nfusion, which affects retrieval accuracy and efficiency. To address these\nissues, we propose CMPAGL, a cross-modal pre-aligned method leveraging global\nand local information. Our Gswin transformer block combines local window\nself-attention and global-local window cross-attention to capture multi-scale\nfeatures. A pre-alignment mechanism simplifies modal fusion training, improving\nretrieval performance. Additionally, we introduce a similarity matrix\nreweighting (SMR) algorithm for reranking, and enhance the triplet loss\nfunction with an intra-class distance term to optimize feature learning.\nExperiments on four datasets, including RSICD and RSITMD, validate CMPAGL's\neffectiveness, achieving up to 4.65% improvement in R@1 and 2.28% in mean\nRecall (mR) over state-of-the-art methods.\n","authors":["Zengbao Sun","Ming Zhao","Gaorui Liu","André Kaup"],"pdf_url":"https://arxiv.org/pdf/2411.14704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.01229v2","updated":"2024-11-22T03:08:34Z","published":"2023-06-02T01:40:08Z","title":"Exploring the Boundaries of Semi-Supervised Facial Expression\n  Recognition using In-Distribution, Out-of-Distribution, and Unconstrained\n  Data","summary":"  Deep learning-based methods have been the key driving force behind much of\nthe recent success of facial expression recognition (FER) systems. However, the\nneed for large amounts of labelled data remains a challenge. Semi-supervised\nlearning offers a way to overcome this limitation, allowing models to learn\nfrom a small amount of labelled data along with a large unlabelled dataset.\nWhile semi-supervised learning has shown promise in FER, most current methods\nfrom general computer vision literature have not been explored in the context\nof FER. In this work, we present a comprehensive study on 11 of the most recent\nsemi-supervised methods, in the context of FER, namely Pi-model, Pseudo-label,\nMean Teacher, VAT, UDA, MixMatch, ReMixMatch, FlexMatch, CoMatch, and CCSSL.\nOur investigation covers semi-supervised learning from in-distribution,\nout-of-distribution, unconstrained, and very small unlabelled data. Our\nevaluation includes five FER datasets plus one large face dataset for\nunconstrained learning. Our results demonstrate that FixMatch consistently\nachieves better performance on in-distribution unlabelled data, while\nReMixMatch stands out among all methods for out-of-distribution, unconstrained,\nand scarce unlabelled data scenarios. Another significant observation is that\nwith an equal number of labelled samples, semi-supervised learning delivers a\nconsiderable improvement over supervised learning, regardless of whether the\nunlabelled data is in-distribution, out-of-distribution, or unconstrained. We\nalso conduct sensitivity analyses on critical hyper-parameters for the two best\nmethods of each setting. To facilitate reproducibility and further development,\nwe make our code publicly available at: github.com/ShuvenduRoy/SSL_FER_OOD.\n","authors":["Shuvendu Roy","Ali Etemad"],"pdf_url":"https://arxiv.org/pdf/2306.01229v2.pdf","comment":"Accepted in IEEE Transactions on Affective Computing (TAFFC), 2024"},{"id":"http://arxiv.org/abs/2411.14695v1","updated":"2024-11-22T03:05:06Z","published":"2024-11-22T03:05:06Z","title":"Anti-Forgetting Adaptation for Unsupervised Person Re-identification","summary":"  Regular unsupervised domain adaptive person re-identification (ReID) focuses\non adapting a model from a source domain to a fixed target domain. However, an\nadapted ReID model can hardly retain previously-acquired knowledge and\ngeneralize to unseen data. In this paper, we propose a Dual-level Joint\nAdaptation and Anti-forgetting (DJAA) framework, which incrementally adapts a\nmodel to new domains without forgetting source domain and each adapted target\ndomain. We explore the possibility of using prototype and instance-level\nconsistency to mitigate the forgetting during the adaptation. Specifically, we\nstore a small number of representative image samples and corresponding cluster\nprototypes in a memory buffer, which is updated at each adaptation step. With\nthe buffered images and prototypes, we regularize the image-to-image similarity\nand image-to-prototype similarity to rehearse old knowledge. After the\nmulti-step adaptation, the model is tested on all seen domains and several\nunseen domains to validate the generalization ability of our method. Extensive\nexperiments demonstrate that our proposed method significantly improves the\nanti-forgetting, generalization and backward-compatible ability of an\nunsupervised person ReID model.\n","authors":["Hao Chen","Francois Bremond","Nicu Sebe","Shiliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14695v1.pdf","comment":"Accepted to TPAMI"},{"id":"http://arxiv.org/abs/2410.24060v4","updated":"2024-11-22T02:48:41Z","published":"2024-10-31T15:57:04Z","title":"Understanding Generalizability of Diffusion Models Requires Rethinking\n  the Hidden Gaussian Structure","summary":"  In this work, we study the generalizability of diffusion models by looking\ninto the hidden properties of the learned score functions, which are\nessentially a series of deep denoisers trained on various noise levels. We\nobserve that as diffusion models transition from memorization to\ngeneralization, their corresponding nonlinear diffusion denoisers exhibit\nincreasing linearity. This discovery leads us to investigate the linear\ncounterparts of the nonlinear diffusion models, which are a series of linear\nmodels trained to match the function mappings of the nonlinear diffusion\ndenoisers. Surprisingly, these linear denoisers are approximately the optimal\ndenoisers for a multivariate Gaussian distribution characterized by the\nempirical mean and covariance of the training dataset. This finding implies\nthat diffusion models have the inductive bias towards capturing and utilizing\nthe Gaussian structure (covariance information) of the training dataset for\ndata generation. We empirically demonstrate that this inductive bias is a\nunique property of diffusion models in the generalization regime, which becomes\nincreasingly evident when the model's capacity is relatively small compared to\nthe training dataset size. In the case that the model is highly\noverparameterized, this inductive bias emerges during the initial training\nphases before the model fully memorizes its training data. Our study provides\ncrucial insights into understanding the notable strong generalization\nphenomenon recently observed in real-world diffusion models.\n","authors":["Xiang Li","Yixiang Dai","Qing Qu"],"pdf_url":"https://arxiv.org/pdf/2410.24060v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14688v1","updated":"2024-11-22T02:46:44Z","published":"2024-11-22T02:46:44Z","title":"Whats in a Video: Factorized Autoregressive Decoding for Online Dense\n  Video Captioning","summary":"  Generating automatic dense captions for videos that accurately describe their\ncontents remains a challenging area of research. Most current models require\nprocessing the entire video at once. Instead, we propose an efficient, online\napproach which outputs frequent, detailed and temporally aligned captions,\nwithout access to future frames. Our model uses a novel autoregressive\nfactorized decoding architecture, which models the sequence of visual features\nfor each time segment, outputting localized descriptions and efficiently\nleverages the context from the previous video segments. This allows the model\nto output frequent, detailed captions to more comprehensively describe the\nvideo, according to its actual local content, rather than mimic the training\ndata. Second, we propose an optimization for efficient training and inference,\nwhich enables scaling to longer videos. Our approach shows excellent\nperformance compared to both offline and online methods, and uses 20\\% less\ncompute. The annotations produced are much more comprehensive and frequent, and\ncan further be utilized in automatic video tagging and in large-scale video\ndata harvesting.\n","authors":["AJ Piergiovanni","Dahun Kim","Michael S. Ryoo","Isaac Noble","Anelia Angelova"],"pdf_url":"https://arxiv.org/pdf/2411.14688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14412v2","updated":"2024-11-22T02:41:02Z","published":"2024-11-21T18:46:45Z","title":"Adversarial Poisoning Attack on Quantum Machine Learning Models","summary":"  With the growing interest in Quantum Machine Learning (QML) and the\nincreasing availability of quantum computers through cloud providers,\naddressing the potential security risks associated with QML has become an\nurgent priority. One key concern in the QML domain is the threat of data\npoisoning attacks in the current quantum cloud setting. Adversarial access to\ntraining data could severely compromise the integrity and availability of QML\nmodels. Classical data poisoning techniques require significant knowledge and\ntraining to generate poisoned data, and lack noise resilience, making them\nineffective for QML models in the Noisy Intermediate Scale Quantum (NISQ) era.\nIn this work, we first propose a simple yet effective technique to measure\nintra-class encoder state similarity (ESS) by analyzing the outputs of encoding\ncircuits. Leveraging this approach, we introduce a quantum indiscriminate data\npoisoning attack, QUID. Through extensive experiments conducted in both\nnoiseless and noisy environments (e.g., IBM\\_Brisbane's noise), across various\narchitectures and datasets, QUID achieves up to $92\\%$ accuracy degradation in\nmodel performance compared to baseline models and up to $75\\%$ accuracy\ndegradation compared to random label-flipping. We also tested QUID against\nstate-of-the-art classical defenses, with accuracy degradation still exceeding\n$50\\%$, demonstrating its effectiveness. This work represents the first attempt\nto reevaluate data poisoning attacks in the context of QML.\n","authors":["Satwik Kundu","Swaroop Ghosh"],"pdf_url":"https://arxiv.org/pdf/2411.14412v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12448v2","updated":"2024-11-22T02:31:13Z","published":"2024-11-19T12:15:40Z","title":"Large Language Models for Lossless Image Compression: Next-Pixel\n  Prediction in Language Space is All You Need","summary":"  We have recently witnessed that ``Intelligence\" and `` Compression\" are the\ntwo sides of the same coin, where the language large model (LLM) with\nunprecedented intelligence is a general-purpose lossless compressor for various\ndata modalities. This attribute particularly appeals to the lossless image\ncompression community, given the increasing need to compress high-resolution\nimages in the current streaming media era. Consequently, a spontaneous envision\nemerges: Can the compression performance of the LLM elevate lossless image\ncompression to new heights? However, our findings indicate that the naive\napplication of LLM-based lossless image compressors suffers from a considerable\nperformance gap compared with existing state-of-the-art (SOTA) codecs on common\nbenchmark datasets. In light of this, we are dedicated to fulfilling the\nunprecedented intelligence (compression) capacity of the LLM for lossless image\ncompression tasks, thereby bridging the gap between theoretical and practical\ncompression performance. Specifically, we propose P$^{2}$-LLM, a next-pixel\nprediction-based LLM, which integrates various elaborated insights and\nmethodologies, \\textit{e.g.,} pixel-level priors, the in-context ability of\nLLM, and a pixel-level semantic preservation strategy, to enhance the\nunderstanding capacity of pixel sequences for better next-pixel predictions.\nExtensive experiments on benchmark datasets demonstrate that P$^{2}$-LLM can\nbeat SOTA classical and learned codecs.\n","authors":["Kecheng Chen","Pingping Zhang","Hui Liu","Jie Liu","Yibing Liu","Jiaxin Huang","Shiqi Wang","Hong Yan","Haoliang Li"],"pdf_url":"https://arxiv.org/pdf/2411.12448v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14684v1","updated":"2024-11-22T02:29:37Z","published":"2024-11-22T02:29:37Z","title":"Cross Group Attention and Group-wise Rolling for Multimodal Medical\n  Image Synthesis","summary":"  Multimodal MR image synthesis aims to generate missing modality image by\nfusing and mapping a few available MRI data. Most existing approaches typically\nadopt an image-to-image translation scheme. However, these methods often suffer\nfrom sub-optimal performance due to the spatial misalignment between different\nmodalities while they are typically treated as input channels. Therefore, in\nthis paper, we propose an Adaptive Group-wise Interaction Network (AGI-Net)\nthat explores both inter-modality and intra-modality relationships for\nmultimodal MR image synthesis. Specifically, groups are first pre-defined along\nthe channel dimension and then we perform an adaptive rolling for the standard\nconvolutional kernel to capture inter-modality spatial correspondences. At the\nsame time, a cross-group attention module is introduced to fuse information\nacross different channel groups, leading to better feature representation. We\nevaluated the effectiveness of our model on the publicly available IXI and\nBraTS2023 datasets, where the AGI-Net achieved state-of-the-art performance for\nmultimodal MR image synthesis. Code will be released.\n","authors":["Tao Song","Yicheng Wu","Minhao Hu","Xiangde Luo","Linda Wei","Guotai Wang","Yi Guo","Feng Xu","Shaoting Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07966v4","updated":"2024-11-22T02:23:26Z","published":"2024-06-12T07:44:22Z","title":"Real-world Image Dehazing with Coherence-based Label Generator and\n  Cooperative Unfolding Network","summary":"  Real-world Image Dehazing (RID) aims to alleviate haze-induced degradation in\nreal-world settings. This task remains challenging due to the complexities in\naccurately modeling real haze distributions and the scarcity of paired\nreal-world data. To address these challenges, we first introduce a cooperative\nunfolding network that jointly models atmospheric scattering and image scenes,\neffectively integrating physical knowledge into deep networks to restore\nhaze-contaminated details. Additionally, we propose the first RID-oriented\niterative mean-teacher framework, termed the Coherence-based Label Generator,\nto generate high-quality pseudo labels for network training. Specifically, we\nprovide an optimal label pool to store the best pseudo-labels during network\ntraining, leveraging both global and local coherence to select high-quality\ncandidates and assign weights to prioritize haze-free regions. We verify the\neffectiveness of our method, with experiments demonstrating that it achieves\nstate-of-the-art performance on RID tasks. Code will be available at\n\\url{https://github.com/cnyvfang/CORUN-Colabator}.\n","authors":["Chengyu Fang","Chunming He","Fengyang Xiao","Yulun Zhang","Longxiang Tang","Yuelin Zhang","Kai Li","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2406.07966v4.pdf","comment":"Accepted at NeurIPS 2024 as a Spotlight Paper"},{"id":"http://arxiv.org/abs/2411.13918v2","updated":"2024-11-22T02:17:55Z","published":"2024-11-21T08:13:24Z","title":"Quantization without Tears","summary":"  Deep neural networks, while achieving remarkable success across diverse\ntasks, demand significant resources, including computation, GPU memory,\nbandwidth, storage, and energy. Network quantization, as a standard compression\nand acceleration technique, reduces storage costs and enables potential\ninference acceleration by discretizing network weights and activations into a\nfinite set of integer values. However, current quantization methods are often\ncomplex and sensitive, requiring extensive task-specific hyperparameters, where\neven a single misconfiguration can impair model performance, limiting\ngenerality across different models and tasks. In this paper, we propose\nQuantization without Tears (QwT), a method that simultaneously achieves\nquantization speed, accuracy, simplicity, and generality. The key insight of\nQwT is to incorporate a lightweight additional structure into the quantized\nnetwork to mitigate information loss during quantization. This structure\nconsists solely of a small set of linear layers, keeping the method simple and\nefficient. More importantly, it provides a closed-form solution, allowing us to\nimprove accuracy effortlessly under 2 minutes. Extensive experiments across\nvarious vision, language, and multimodal tasks demonstrate that QwT is both\nhighly effective and versatile. In fact, our approach offers a robust solution\nfor network quantization that combines simplicity, accuracy, and adaptability,\nwhich provides new insights for the design of novel quantization paradigms.\n","authors":["Minghao Fu","Hao Yu","Jie Shao","Junjie Zhou","Ke Zhu","Jianxin Wu"],"pdf_url":"https://arxiv.org/pdf/2411.13918v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14663v1","updated":"2024-11-22T01:41:27Z","published":"2024-11-22T01:41:27Z","title":"BrightVAE: Luminosity Enhancement in Underexposed Endoscopic Images","summary":"  The enhancement of image luminosity is especially critical in endoscopic\nimages. Underexposed endoscopic images often suffer from reduced contrast and\nuneven brightness, significantly impacting diagnostic accuracy and treatment\nplanning. Internal body imaging is challenging due to uneven lighting and\nshadowy regions. Enhancing such images is essential since precise image\ninterpretation is crucial for patient outcomes. In this paper, we introduce\nBrightVAE, an architecture based on the hierarchical Vector Quantized\nVariational Autoencoder (hierarchical VQ-VAE) tailored explicitly for enhancing\nluminosity in low-light endoscopic images. Our architecture is meticulously\ndesigned to tackle the unique challenges inherent in endoscopic imaging, such\nas significant variations in illumination and obscured details due to poor\nlighting conditions. The proposed model emphasizes advanced feature extraction\nfrom three distinct viewpoints-incorporating various receptive fields, skip\nconnections, and feature attentions to robustly enhance image quality and\nsupport more accurate medical diagnoses. Through rigorous experimental\nanalysis, we demonstrate the effectiveness of these techniques in enhancing\nlow-light endoscopic images. To evaluate the performance of our architecture,\nwe employ three widely recognized metrics-SSIM, PSNR, and LPIPS-specifically on\nEndo4IE dataset, which consists of endoscopic images. We evaluated our method\nusing the Endo4IE dataset, which consists exclusively of endoscopic images, and\nshowed significant advancements over the state-of-the-art methods for enhancing\nluminosity in endoscopic imaging.\n","authors":["Farzaneh Koohestani","Zahra Nabizadeh","Nader Karimi","Shahram Shirani","Shadrokh Samavi"],"pdf_url":"https://arxiv.org/pdf/2411.14663v1.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.23245v2","updated":"2024-11-22T01:35:33Z","published":"2024-10-30T17:29:25Z","title":"PointRecon: Online Point-based 3D Reconstruction via Ray-based 2D-3D\n  Matching","summary":"  We propose a novel online, point-based 3D reconstruction method from posed\nmonocular RGB videos. Our model maintains a global point cloud representation\nof the scene, continuously updating the features and 3D locations of points as\nnew images are observed. It expands the point cloud with newly detected points\nwhile carefully removing redundancies. The point cloud updates and the depth\npredictions for new points are achieved through a novel ray-based 2D-3D feature\nmatching technique, which is robust against errors in previous point position\npredictions. In contrast to offline methods, our approach processes\ninfinite-length sequences and provides real-time updates. Additionally, the\npoint cloud imposes no pre-defined resolution or scene size constraints, and\nits unified global representation ensures view consistency across perspectives.\nExperiments on the ScanNet dataset show that our method achieves comparable\nquality among online MVS approaches. Project page:\nhttps://arthurhero.github.io/projects/pointrecon\n","authors":["Chen Ziwen","Zexiang Xu","Li Fuxin"],"pdf_url":"https://arxiv.org/pdf/2410.23245v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16845v3","updated":"2024-11-22T00:24:06Z","published":"2024-09-25T11:53:58Z","title":"IRASNet: Improved Feature-Level Clutter Reduction for Domain Generalized\n  SAR-ATR","summary":"  Recently, computer-aided design models and electromagnetic simulations have\nbeen used to augment synthetic aperture radar (SAR) data for deep learning.\nHowever, an automatic target recognition (ATR) model struggles with domain\nshift when using synthetic data because the model learns specific clutter\npatterns present in such data, which disturbs performance when applied to\nmeasured data with different clutter distributions. This study proposes a\nframework particularly designed for domain-generalized SAR-ATR called IRASNet,\nenabling effective feature-level clutter reduction and domain-invariant feature\nlearning. First, we propose a clutter reduction module (CRM) that maximizes the\nsignal-to-clutter ratio on feature maps. The module reduces the impact of\nclutter at the feature level while preserving target and shadow information,\nthereby improving ATR performance. Second, we integrate adversarial learning\nwith CRM to extract clutter-reduced domain-invariant features. The integration\nbridges the gap between synthetic and measured datasets without requiring\nmeasured data during training. Third, we improve feature extraction from target\nand shadow regions by implementing a positional supervision task using mask\nground truth encoding. The improvement enhances the ability of the model to\ndiscriminate between classes. Our proposed IRASNet presents new\nstate-of-the-art public SAR datasets utilizing target and shadow information to\nachieve superior performance across various test conditions. IRASNet not only\nenhances generalization performance but also significantly improves\nfeature-level clutter reduction, making it a valuable advancement in the field\nof radar image pattern recognition.\n","authors":["Oh-Tae Jang","Min-Jun Kim","Sung-Ho Kim","Hee-Sub Shin","Kyung-Tae Kim"],"pdf_url":"https://arxiv.org/pdf/2409.16845v3.pdf","comment":"16 pages, 11 figures"},{"id":"http://arxiv.org/abs/2411.14639v1","updated":"2024-11-22T00:09:49Z","published":"2024-11-22T00:09:49Z","title":"Differentially Private Adaptation of Diffusion Models via Noisy\n  Aggregated Embeddings","summary":"  We introduce novel methods for adapting diffusion models under differential\nprivacy (DP) constraints, enabling privacy-preserving style and content\ntransfer without fine-tuning. Traditional approaches to private adaptation,\nsuch as DP-SGD, incur significant computational overhead and degrade model\nperformance when applied to large, complex models. Our approach instead\nleverages embedding-based techniques: Universal Guidance and Textual Inversion\n(TI), adapted with differentially private mechanisms. We apply these methods to\nStable Diffusion for style adaptation using two private datasets: a collection\nof artworks by a single artist and pictograms from the Paris 2024 Olympics.\nExperimental results show that the TI-based adaptation achieves superior\nfidelity in style transfer, even under strong privacy guarantees, while both\nmethods maintain high privacy resilience by employing calibrated noise and\nsubsampling strategies. Our findings demonstrate a feasible and efficient\npathway for privacy-preserving diffusion model adaptation, balancing data\nprotection with the fidelity of generated images, and offer insights into\nembedding-driven methods for DP in generative AI applications.\n","authors":["Pura Peetathawatchai","Wei-Ning Chen","Berivan Isik","Sanmi Koyejo","Albert No"],"pdf_url":"https://arxiv.org/pdf/2411.14639v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2411.15005v1","updated":"2024-11-22T15:29:05Z","published":"2024-11-22T15:29:05Z","title":"Multi-granularity Interest Retrieval and Refinement Network for\n  Long-Term User Behavior Modeling in CTR Prediction","summary":"  Click-through Rate (CTR) prediction is crucial for online personalization\nplatforms. Recent advancements have shown that modeling rich user behaviors can\nsignificantly improve the performance of CTR prediction. Current long-term user\nbehavior modeling algorithms predominantly follow two cascading stages. The\nfirst stage retrieves subsequence related to the target item from the long-term\nbehavior sequence, while the second stage models the relationship between the\nsubsequence and the target item. Despite significant progress, these methods\nhave two critical flaws. First, the retrieval query typically includes only\ntarget item information, limiting the ability to capture the user's diverse\ninterests. Second, relational information, such as sequential and interactive\ninformation within the subsequence, is frequently overlooked. Therefore, it\nrequires to be further mined to more accurately model user interests.\n  To this end, we propose Multi-granularity Interest Retrieval and Refinement\nNetwork (MIRRN). Specifically, we first construct queries based on behaviors\nobserved at different time scales to obtain subsequences, each capturing users'\ninterest at various granularities. We then introduce an noval multi-head\nFourier transformer to efficiently learn sequential and interactive information\nwithin the subsequences, leading to more accurate modeling of user interests.\nFinally, we employ multi-head target attention to adaptively assess the impact\nof these multi-granularity interests on the target item. Extensive experiments\nhave demonstrated that MIRRN significantly outperforms state-of-the-art\nbaselines. Furthermore, an A/B test shows that MIRRN increases the average\nnumber of listening songs by 1.32% and the average time of listening songs by\n0.55% on a popular music streaming app. The implementation code is publicly\navailable at https://github.com/psycho-demon/MIRRN.\n","authors":["Xiang Xu","Hao Wang","Wei Guo","Luankang Zhang","Wanshan Yang","Runlong Yu","Yong Liu","Defu Lian","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2411.15005v1.pdf","comment":"KDD2025"},{"id":"http://arxiv.org/abs/2411.02451v2","updated":"2024-11-22T14:11:13Z","published":"2024-11-03T10:06:14Z","title":"High-performance automated abstract screening with large language model\n  ensembles","summary":"  Large language models (LLMs) excel in tasks requiring processing and\ninterpretation of input text. Abstract screening is a labour-intensive\ncomponent of systematic review involving repetitive application of inclusion\nand exclusion criteria on a large volume of studies identified by a literature\nsearch. Here, LLMs (GPT-3.5 Turbo, GPT-4 Turbo, GPT-4o, Llama 3 70B, Gemini 1.5\nPro, and Claude Sonnet 3.5) were trialled on systematic reviews in a full issue\nof the Cochrane Library to evaluate their accuracy in zero-shot binary\nclassification for abstract screening. Trials over a subset of 800 records\nidentified optimal prompting strategies and demonstrated superior performance\nof LLMs to human researchers in terms of sensitivity (LLM-max = 1.000,\nhuman-max = 0.775), precision (LLM-max = 0.927, human-max = 0.911), and\nbalanced accuracy (LLM-max = 0.904, human-max = 0.865). The best performing\nLLM-prompt combinations were trialled across every replicated search result (n\n= 119,691), and exhibited consistent sensitivity (range 0.756-1.000) but\ndiminished precision (range 0.004-0.096). 66 LLM-human and LLM-LLM ensembles\nexhibited perfect sensitivity with a maximal precision of 0.458, with less\nobserved performance drop in larger trials. Significant variation in\nperformance was observed between reviews, highlighting the importance of\ndomain-specific validation before deployment. LLMs may reduce the human labour\ncost of systematic review with maintained or improved accuracy and sensitivity.\nSystematic review is the foundation of evidence synthesis across academic\ndisciplines, including evidence-based medicine, and LLMs may increase the\nefficiency and quality of this mode of research.\n","authors":["Rohan Sanghera","Arun James Thirunavukarasu","Marc El Khoury","Jessica O'Logbon","Yuqing Chen","Archie Watt","Mustafa Mahmood","Hamid Butt","George Nishimura","Andrew Soltan"],"pdf_url":"https://arxiv.org/pdf/2411.02451v2.pdf","comment":"RS and AJT are joint-first authors"},{"id":"http://arxiv.org/abs/2411.14922v1","updated":"2024-11-22T13:24:01Z","published":"2024-11-22T13:24:01Z","title":"GOT4Rec: Graph of Thoughts for Sequential Recommendation","summary":"  With the advancement of large language models (LLMs), researchers have\nexplored various methods to optimally leverage their comprehension and\ngeneration capabilities in sequential recommendation scenarios. However,\nseveral challenges persist in this endeavor. Firstly, most existing approaches\nrely on the input-output prompting paradigm, which can result in irrelevant or\ninaccurate responses. Secondly, while there have been attempts to enhance LLMs\nusing prompting strategies such as chain-of-thought (CoT), these efforts have\nnot fully harnessed the reasoning abilities of LLMs or effectively captured the\nmultifaceted information contained within user sequences. To address these\nlimitations, we propose GOT4Rec, a sequential recommendation method that\nutilizes the graph of thoughts (GoT) prompting strategy. Specifically, we\nidentify and utilize three key types of information within user history\nsequences: short-term interests, long-term interests and collaborative\ninformation from other users. Our approach enables LLMs to independently reason\nand generate recommendations based on these distinct types of information,\nsubsequently aggregating the results within the GoT framework to derive the\nfinal recommended items. This method allows LLMs, with enhanced reasoning\ncapabilities, to more effectively consider the diverse information within user\nsequences, resulting in more accurate recommendations and more comprehensive\nexplanations. Extensive experiments on real-world datasets demonstrate the\neffectiveness of GOT4Rec, indicating that it outperforms existing\nstate-of-the-art baselines. Our code is available at\nhttps://anonymous.4open.science/r/GOT4Rec-ED99.\n","authors":["Zewen Long","Liang Wang","Shu Wu","Qiang Liu","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2411.14922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14914v1","updated":"2024-11-22T13:15:03Z","published":"2024-11-22T13:15:03Z","title":"A Reproducibility and Generalizability Study of Large Language Models\n  for Query Generation","summary":"  Systematic literature reviews (SLRs) are a cornerstone of academic research,\nyet they are often labour-intensive and time-consuming due to the detailed\nliterature curation process. The advent of generative AI and large language\nmodels (LLMs) promises to revolutionize this process by assisting researchers\nin several tedious tasks, one of them being the generation of effective Boolean\nqueries that will select the publications to consider including in a review.\nThis paper presents an extensive study of Boolean query generation using LLMs\nfor systematic reviews, reproducing and extending the work of Wang et al. and\nAlaniz et al. Our study investigates the replicability and reliability of\nresults achieved using ChatGPT and compares its performance with open-source\nalternatives like Mistral and Zephyr to provide a more comprehensive analysis\nof LLMs for query generation.\n  Therefore, we implemented a pipeline, which automatically creates a Boolean\nquery for a given review topic by using a previously defined LLM, retrieves all\ndocuments for this query from the PubMed database and then evaluates the\nresults. With this pipeline we first assess whether the results obtained using\nChatGPT for query generation are reproducible and consistent. We then\ngeneralize our results by analyzing and evaluating open-source models and\nevaluating their efficacy in generating Boolean queries.\n  Finally, we conduct a failure analysis to identify and discuss the\nlimitations and shortcomings of using LLMs for Boolean query generation. This\nexamination helps to understand the gaps and potential areas for improvement in\nthe application of LLMs to information retrieval tasks. Our findings highlight\nthe strengths, limitations, and potential of LLMs in the domain of information\nretrieval and literature review automation.\n","authors":["Moritz Staudinger","Wojciech Kusa","Florina Piroi","Aldo Lipani","Allan Hanbury"],"pdf_url":"https://arxiv.org/pdf/2411.14914v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14760v1","updated":"2024-11-22T06:46:41Z","published":"2024-11-22T06:46:41Z","title":"The 1st Workshop on Human-Centered Recommender Systems","summary":"  Recommender systems are quintessential applications of human-computer\ninteraction. Widely utilized in daily life, they offer significant convenience\nbut also present numerous challenges, such as the information cocoon effect,\nprivacy concerns, fairness issues, and more. Consequently, this workshop aims\nto provide a platform for researchers to explore the development of\nHuman-Centered Recommender Systems~(HCRS). HCRS refers to the creation of\nrecommender systems that prioritize human needs, values, and capabilities at\nthe core of their design and operation. In this workshop, topics will include,\nbut are not limited to, robustness, privacy, transparency, fairness, diversity,\naccountability, ethical considerations, and user-friendly design. We hope to\nengage in discussions on how to implement and enhance these properties in\nrecommender systems. Additionally, participants will explore diverse evaluation\nmethods, including innovative metrics that capture user satisfaction and trust.\nThis workshop seeks to foster a collaborative environment for researchers to\nshare insights and advance the field toward more ethical, user-centric, and\nsocially responsible recommender systems.\n","authors":["Kaike Zhang","Yunfan Wu","Yougang lyu","Du Su","Yingqiang Ge","Shuchang Liu","Qi Cao","Zhaochun Ren","Fei Sun"],"pdf_url":"https://arxiv.org/pdf/2411.14760v1.pdf","comment":"Workshop at TheWebConf 2025"},{"id":"http://arxiv.org/abs/2411.14739v1","updated":"2024-11-22T05:18:35Z","published":"2024-11-22T05:18:35Z","title":"IRLab@iKAT24: Learned Sparse Retrieval with Multi-aspect LLM Query\n  Generation for Conversational Search","summary":"  The Interactive Knowledge Assistant Track (iKAT) 2024 focuses on advancing\nconversational assistants, able to adapt their interaction and responses from\npersonalized user knowledge. The track incorporates a Personal Textual\nKnowledge Base (PTKB) alongside Conversational AI tasks, such as passage\nranking and response generation. Query Rewrite being an effective approach for\nresolving conversational context, we explore Large Language Models (LLMs), as\nquery rewriters. Specifically, our submitted runs explore multi-aspect query\ngeneration using the MQ4CS framework, which we further enhance with Learned\nSparse Retrieval via the SPLADE architecture, coupled with robust cross-encoder\nmodels. We also propose an alternative to the previous interleaving strategy,\naggregating multiple aspects during the reranking phase. Our findings indicate\nthat multi-aspect query generation is effective in enhancing performance when\nintegrated with advanced retrieval and reranking models. Our results also lead\nthe way for better personalization in Conversational Search, relying on LLMs to\nintegrate personalization within query rewrite, and outperforming human rewrite\nperformance.\n","authors":["Simon Lupart","Zahra Abbasiantaeb","Mohammad Aliannejadi"],"pdf_url":"https://arxiv.org/pdf/2411.14739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14713v1","updated":"2024-11-22T03:43:41Z","published":"2024-11-22T03:43:41Z","title":"LIBER: Lifelong User Behavior Modeling Based on Large Language Models","summary":"  CTR prediction plays a vital role in recommender systems. Recently, large\nlanguage models (LLMs) have been applied in recommender systems due to their\nemergence abilities. While leveraging semantic information from LLMs has shown\nsome improvements in the performance of recommender systems, two notable\nlimitations persist in these studies. First, LLM-enhanced recommender systems\nencounter challenges in extracting valuable information from lifelong user\nbehavior sequences within textual contexts for recommendation tasks. Second,\nthe inherent variability in human behaviors leads to a constant stream of new\nbehaviors and irregularly fluctuating user interests. This characteristic\nimposes two significant challenges on existing models. On the one hand, it\npresents difficulties for LLMs in effectively capturing the dynamic shifts in\nuser interests within these sequences, and on the other hand, there exists the\nissue of substantial computational overhead if the LLMs necessitate recurrent\ncalls upon each update to the user sequences. In this work, we propose Lifelong\nUser Behavior Modeling (LIBER) based on large language models, which includes\nthree modules: (1) User Behavior Streaming Partition (UBSP), (2) User Interest\nLearning (UIL), and (3) User Interest Fusion (UIF). Initially, UBSP is employed\nto condense lengthy user behavior sequences into shorter partitions in an\nincremental paradigm, facilitating more efficient processing. Subsequently, UIL\nleverages LLMs in a cascading way to infer insights from these partitions.\nFinally, UIF integrates the textual outputs generated by the aforementioned\nprocesses to construct a comprehensive representation, which can be\nincorporated by any recommendation model to enhance performance. LIBER has been\ndeployed on Huawei's music recommendation service and achieved substantial\nimprovements in users' play count and play time by 3.01% and 7.69%.\n","authors":["Chenxu Zhu","Shigang Quan","Bo Chen","Jianghao Lin","Xiaoling Cai","Hong Zhu","Xiangyang Li","Yunjia Xi","Weinan Zhang","Ruiming Tang"],"pdf_url":"https://arxiv.org/pdf/2411.14713v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14704v1","updated":"2024-11-22T03:28:55Z","published":"2024-11-22T03:28:55Z","title":"Cross-Modal Pre-Aligned Method with Global and Local Information for\n  Remote-Sensing Image and Text Retrieval","summary":"  Remote sensing cross-modal text-image retrieval (RSCTIR) has gained attention\nfor its utility in information mining. However, challenges remain in\neffectively integrating global and local information due to variations in\nremote sensing imagery and ensuring proper feature pre-alignment before modal\nfusion, which affects retrieval accuracy and efficiency. To address these\nissues, we propose CMPAGL, a cross-modal pre-aligned method leveraging global\nand local information. Our Gswin transformer block combines local window\nself-attention and global-local window cross-attention to capture multi-scale\nfeatures. A pre-alignment mechanism simplifies modal fusion training, improving\nretrieval performance. Additionally, we introduce a similarity matrix\nreweighting (SMR) algorithm for reranking, and enhance the triplet loss\nfunction with an intra-class distance term to optimize feature learning.\nExperiments on four datasets, including RSICD and RSITMD, validate CMPAGL's\neffectiveness, achieving up to 4.65% improvement in R@1 and 2.28% in mean\nRecall (mR) over state-of-the-art methods.\n","authors":["Zengbao Sun","Ming Zhao","Gaorui Liu","André Kaup"],"pdf_url":"https://arxiv.org/pdf/2411.14704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07482v2","updated":"2024-11-22T00:48:57Z","published":"2024-11-12T02:08:19Z","title":"Enhancing Link Prediction with Fuzzy Graph Attention Networks and\n  Dynamic Negative Sampling","summary":"  Link prediction is crucial for understanding complex networks but traditional\nGraph Neural Networks (GNNs) often rely on random negative sampling, leading to\nsuboptimal performance. This paper introduces Fuzzy Graph Attention Networks\n(FGAT), a novel approach integrating fuzzy rough sets for dynamic negative\nsampling and enhanced node feature aggregation. Fuzzy Negative Sampling (FNS)\nsystematically selects high-quality negative edges based on fuzzy similarities,\nimproving training efficiency. FGAT layer incorporates fuzzy rough set\nprinciples, enabling robust and discriminative node representations.\nExperiments on two research collaboration networks demonstrate FGAT's superior\nlink prediction accuracy, outperforming state-of-the-art baselines by\nleveraging the power of fuzzy rough sets for effective negative sampling and\nnode feature learning.\n","authors":["Jinming Xing","Ruilin Xing"],"pdf_url":"https://arxiv.org/pdf/2411.07482v2.pdf","comment":"5 pages"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2411.15131v1","updated":"2024-11-22T18:56:56Z","published":"2024-11-22T18:56:56Z","title":"WildLMa: Long Horizon Loco-Manipulation in the Wild","summary":"  `In-the-wild' mobile manipulation aims to deploy robots in diverse real-world\nenvironments, which requires the robot to (1) have skills that generalize\nacross object configurations; (2) be capable of long-horizon task execution in\ndiverse environments; and (3) perform complex manipulation beyond\npick-and-place. Quadruped robots with manipulators hold promise for extending\nthe workspace and enabling robust locomotion, but existing results do not\ninvestigate such a capability. This paper proposes WildLMa with three\ncomponents to address these issues: (1) adaptation of learned low-level\ncontroller for VR-enabled whole-body teleoperation and traversability; (2)\nWildLMa-Skill -- a library of generalizable visuomotor skills acquired via\nimitation learning or heuristics and (3) WildLMa-Planner -- an interface of\nlearned skills that allow LLM planners to coordinate skills for long-horizon\ntasks. We demonstrate the importance of high-quality training data by achieving\nhigher grasping success rate over existing RL baselines using only tens of\ndemonstrations. WildLMa exploits CLIP for language-conditioned imitation\nlearning that empirically generalizes to objects unseen in training\ndemonstrations. Besides extensive quantitative evaluation, we qualitatively\ndemonstrate practical robot applications, such as cleaning up trash in\nuniversity hallways or outdoor terrains, operating articulated objects, and\nrearranging items on a bookshelf.\n","authors":["Ri-Zhao Qiu","Yuchen Song","Xuanbin Peng","Sai Aneesh Suryadevara","Ge Yang","Minghuan Liu","Mazeyu Ji","Chengzhe Jia","Ruihan Yang","Xueyan Zou","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2411.15131v1.pdf","comment":"Website: https://wildlma.github.io/"},{"id":"http://arxiv.org/abs/2411.15128v1","updated":"2024-11-22T18:51:51Z","published":"2024-11-22T18:51:51Z","title":"Health AI Developer Foundations","summary":"  Robust medical Machine Learning (ML) models have the potential to\nrevolutionize healthcare by accelerating clinical research, improving workflows\nand outcomes, and producing novel insights or capabilities. Developing such ML\nmodels from scratch is cost prohibitive and requires substantial compute, data,\nand time (e.g., expert labeling). To address these challenges, we introduce\nHealth AI Developer Foundations (HAI-DEF), a suite of pre-trained,\ndomain-specific foundation models, tools, and recipes to accelerate building ML\nfor health applications. The models cover various modalities and domains,\nincluding radiology (X-rays and computed tomography), histopathology,\ndermatological imaging, and audio. These models provide domain specific\nembeddings that facilitate AI development with less labeled data, shorter\ntraining times, and reduced computational costs compared to traditional\napproaches. In addition, we utilize a common interface and style across these\nmodels, and prioritize usability to enable developers to integrate HAI-DEF\nefficiently. We present model evaluations across various tasks and conclude\nwith a discussion of their application and evaluation, covering the importance\nof ensuring efficacy, fairness, and equity. Finally, while HAI-DEF and\nspecifically the foundation models lower the barrier to entry for ML in\nhealthcare, we emphasize the importance of validation with problem- and\npopulation-specific data for each desired usage setting. This technical report\nwill be updated over time as more modalities and features are added.\n","authors":["Atilla P. Kiraly","Sebastien Baur","Kenneth Philbrick","Fereshteh Mahvar","Liron Yatziv","Tiffany Chen","Bram Sterling","Nick George","Fayaz Jamil","Jing Tang","Kai Bailey","Faruk Ahmed","Akshay Goel","Abbi Ward","Lin Yang","Andrew Sellergren","Yossi Matias","Avinatan Hassidim","Shravya Shetty","Daniel Golden","Shekoofeh Azizi","David F. Steiner","Yun Liu","Tim Thelin","Rory Pilgrim","Can Kirmizibayrak"],"pdf_url":"https://arxiv.org/pdf/2411.15128v1.pdf","comment":"16 pages, 8 figures"},{"id":"http://arxiv.org/abs/2411.15127v1","updated":"2024-11-22T18:46:30Z","published":"2024-11-22T18:46:30Z","title":"PRIMUS: Pretraining IMU Encoders with Multimodal Self-Supervision","summary":"  Sensing human motions through Inertial Measurement Units (IMUs) embedded in\npersonal devices has enabled significant applications in health and wellness.\nWhile labeled IMU data is scarce, we can collect unlabeled or weakly labeled\nIMU data to model human motions. For video or text modalities, the \"pretrain\nand adapt\" approach utilizes large volumes of unlabeled or weakly labeled data\nfor pretraining, building a strong feature extractor, followed by adaptation to\nspecific tasks using limited labeled data. This approach has not been widely\nadopted in the IMU domain for two reasons: (1) pretraining methods are poorly\nunderstood in the context of IMU, and (2) open-source pretrained models that\ngeneralize across datasets are rarely publicly available. In this paper, we aim\nto address the first issue by proposing PRIMUS, a method for PRetraining IMU\nencoderS. We conduct a systematic and unified evaluation of various\nself-supervised and multimodal learning pretraining objectives. Our findings\nindicate that using PRIMUS, which combines self-supervision, multimodal\nsupervision, and nearest-neighbor supervision, can significantly enhance\ndownstream performance. With fewer than 500 labeled samples per class, PRIMUS\neffectively enhances downstream performance by up to 15% in held-out test data,\ncompared to the state-of-the-art multimodal training method. To benefit the\nbroader community, our code and pre-trained IMU encoders will be made publicly\navailable at github.com/nokia-bell-labs upon publication.\n","authors":["Arnav M. Das","Chi Ian Tang","Fahim Kawsar","Mohammad Malekzadeh"],"pdf_url":"https://arxiv.org/pdf/2411.15127v1.pdf","comment":"Also presented under the title \"PRIMUS: Pretraining IMU Encoders with\n  Multimodal and Self-Supervised Learning\" at NeurIPS 2024 TSALM Workshop (Time\n  Series in the Age of Large Models)"},{"id":"http://arxiv.org/abs/2411.05857v2","updated":"2024-11-22T18:34:58Z","published":"2024-11-07T05:12:51Z","title":"Financial Fraud Detection using Jump-Attentive Graph Neural Networks","summary":"  As the availability of financial services online continues to grow, the\nincidence of fraud has surged correspondingly. Fraudsters continually seek new\nand innovative ways to circumvent the detection algorithms in place.\nTraditionally, fraud detection relied on rule-based methods, where rules were\nmanually created based on transaction data features. However, these techniques\nsoon became ineffective due to their reliance on manual rule creation and their\ninability to detect complex data patterns. Today, a significant portion of the\nfinancial services sector employs various machine learning algorithms, such as\nXGBoost, Random Forest, and neural networks, to model transaction data. While\nthese techniques have proven more efficient than rule-based methods, they still\nfail to capture interactions between different transactions and their\ninterrelationships. Recently, graph-based techniques have been adopted for\nfinancial fraud detection, leveraging graph topology to aggregate neighborhood\ninformation of transaction data using Graph Neural Networks (GNNs). Despite\nshowing improvements over previous methods, these techniques still struggle to\nkeep pace with the evolving camouflaging tactics of fraudsters and suffer from\ninformation loss due to over-smoothing. In this paper, we propose a novel\nalgorithm that employs an efficient neighborhood sampling method, effective for\ncamouflage detection and preserving crucial feature information from\nnon-similar nodes. Additionally, we introduce a novel GNN architecture that\nutilizes attention mechanisms and preserves holistic neighborhood information\nto prevent information loss. We test our algorithm on financial data to show\nthat our method outperforms other state-of-the-art graph algorithms.\n","authors":["Prashank Kadam"],"pdf_url":"https://arxiv.org/pdf/2411.05857v2.pdf","comment":"International Conference on Machine Learning and Applications 2024"},{"id":"http://arxiv.org/abs/2411.15114v1","updated":"2024-11-22T18:30:46Z","published":"2024-11-22T18:30:46Z","title":"RE-Bench: Evaluating frontier AI R&D capabilities of language model\n  agents against human experts","summary":"  Frontier AI safety policies highlight automation of AI research and\ndevelopment (R&D) by AI agents as an important capability to anticipate.\nHowever, there exist few evaluations for AI R&D capabilities, and none that are\nhighly realistic and have a direct comparison to human performance. We\nintroduce RE-Bench (Research Engineering Benchmark, v1), which consists of 7\nchallenging, open-ended ML research engineering environments and data from 71\n8-hour attempts by 61 distinct human experts. We confirm that our experts make\nprogress in the environments given 8 hours, with 82% of expert attempts\nachieving a non-zero score and 24% matching or exceeding our strong reference\nsolutions. We compare humans to several public frontier models through\nbest-of-k with varying time budgets and agent designs, and find that the best\nAI agents achieve a score 4x higher than human experts when both are given a\ntotal time budget of 2 hours per environment. However, humans currently display\nbetter returns to increasing time budgets, narrowly exceeding the top AI agent\nscores given an 8-hour budget, and achieving 2x the score of the top AI agent\nwhen both are given 32 total hours (across different attempts). Qualitatively,\nwe find that modern AI agents possess significant expertise in many ML topics\n-- e.g. an agent wrote a faster custom Triton kernel than any of our human\nexperts' -- and can generate and test solutions over ten times faster than\nhumans, at much lower cost. We open-source the evaluation environments, human\nexpert data, analysis code and agent trajectories to facilitate future\nresearch.\n","authors":["Hjalmar Wijk","Tao Lin","Joel Becker","Sami Jawhar","Neev Parikh","Thomas Broadley","Lawrence Chan","Michael Chen","Josh Clymer","Jai Dhyani","Elena Ericheva","Katharyn Garcia","Brian Goodrich","Nikola Jurkovic","Megan Kinniment","Aron Lajko","Seraphina Nix","Lucas Sato","William Saunders","Maksym Taran","Ben West","Elizabeth Barnes"],"pdf_url":"https://arxiv.org/pdf/2411.15114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.05771v2","updated":"2024-11-22T18:29:47Z","published":"2024-11-08T18:33:03Z","title":"Sketched Equivariant Imaging Regularization and Deep Internal Learning\n  for Inverse Problems","summary":"  Equivariant Imaging (EI) regularization has become the de-facto technique for\nunsupervised training of deep imaging networks, without any need of\nground-truth data. Observing that the EI-based unsupervised training paradigm\ncurrently has significant computational redundancy leading to inefficiency in\nhigh-dimensional applications, we propose a sketched EI regularization which\nleverages the randomized sketching techniques for acceleration. We then extend\nour sketched EI regularization to develop an accelerated deep internal learning\nframework -- Sketched Equivariant Deep Image Prior (Sk-EI-DIP), which can be\nefficiently applied for single-image and task-adapted reconstruction.\nAdditionally, for network adaptation tasks, we propose a parameter-efficient\napproach for accelerating both EI-DIP and Sk-EI-DIP via optimizing only the\nnormalization layers. Our numerical study on X-ray CT image reconstruction\ntasks demonstrate that our approach can achieve order-of-magnitude\ncomputational acceleration over standard EI-based counterpart in single-input\nsetting, and network adaptation at test time.\n","authors":["Guixian Xu","Jinglai Li","Junqi Tang"],"pdf_url":"https://arxiv.org/pdf/2411.05771v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15113v1","updated":"2024-11-22T18:29:37Z","published":"2024-11-22T18:29:37Z","title":"Efficient Pruning of Text-to-Image Models: Insights from Pruning Stable\n  Diffusion","summary":"  As text-to-image models grow increasingly powerful and complex, their\nburgeoning size presents a significant obstacle to widespread adoption,\nespecially on resource-constrained devices. This paper presents a pioneering\nstudy on post-training pruning of Stable Diffusion 2, addressing the critical\nneed for model compression in text-to-image domain. Our study tackles the\npruning techniques for the previously unexplored multi-modal generation models,\nand particularly examines the pruning impact on the textual component and the\nimage generation component separately. We conduct a comprehensive comparison on\npruning the model or the single component of the model in various sparsities.\nOur results yield previously undocumented findings. For example, contrary to\nestablished trends in language model pruning, we discover that simple magnitude\npruning outperforms more advanced techniques in text-to-image context.\nFurthermore, our results show that Stable Diffusion 2 can be pruned to 38.5%\nsparsity with minimal quality loss, achieving a significant reduction in model\nsize. We propose an optimal pruning configuration that prunes the text encoder\nto 47.5% and the diffusion generator to 35%. This configuration maintains image\ngeneration quality while substantially reducing computational requirements. In\naddition, our work uncovers intriguing questions about information encoding in\ntext-to-image models: we observe that pruning beyond certain thresholds leads\nto sudden performance drops (unreadable images), suggesting that specific\nweights encode critical semantics information. This finding opens new avenues\nfor future research in model compression, interoperability, and bias\nidentification in text-to-image models. By providing crucial insights into the\npruning behavior of text-to-image models, our study lays the groundwork for\ndeveloping more efficient and accessible AI-driven image generation systems\n","authors":["Samarth N Ramesh","Zhixue Zhao"],"pdf_url":"https://arxiv.org/pdf/2411.15113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15111v1","updated":"2024-11-22T18:25:13Z","published":"2024-11-22T18:25:13Z","title":"Learnable Activation Functions in Physics-Informed Neural Networks for\n  Solving Partial Differential Equations","summary":"  We investigate the use of learnable activation functions in Physics-Informed\nNeural Networks (PINNs) for solving Partial Differential Equations (PDEs).\nSpecifically, we compare the efficacy of traditional Multilayer Perceptrons\n(MLPs) with fixed and learnable activations against Kolmogorov-Arnold Networks\n(KANs), which employ learnable basis functions. Physics-informed neural\nnetworks (PINNs) have emerged as an effective method for directly incorporating\nphysical laws into the learning process, offering a data-efficient solution for\nboth the forward and inverse problems associated with PDEs. However, challenges\nsuch as effective training and spectral bias, where low-frequency components\nare learned more effectively, often limit their applicability to problems\ncharacterized by rapid oscillations or sharp transitions. By employing\ndifferent activation or basis functions on MLP and KAN, we assess their impact\non convergence behavior and spectral bias mitigation, and the accurate\napproximation of PDEs. The findings offer insights into the design of neural\nnetwork architectures that balance training efficiency, convergence speed, and\ntest accuracy for PDE solvers. By evaluating the influence of activation or\nbasis function choices, this work provides guidelines for developing more\nrobust and accurate PINN models. The source code and pre-trained models used in\nthis study are made publicly available to facilitate reproducibility and future\nexploration.\n","authors":["Afrah Fareaa","Mustafa Serdar Celebi"],"pdf_url":"https://arxiv.org/pdf/2411.15111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04292v2","updated":"2024-11-22T18:11:23Z","published":"2024-02-06T10:15:38Z","title":"AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies","summary":"  Diffusion-based imitation learning improves Behavioral Cloning (BC) on\nmulti-modal decision-making, but comes at the cost of significantly slower\ninference due to the recursion in the diffusion process. It urges us to design\nefficient policy generators while keeping the ability to generate diverse\nactions. To address this challenge, we propose AdaFlow, an imitation learning\nframework based on flow-based generative modeling. AdaFlow represents the\npolicy with state-conditioned ordinary differential equations (ODEs), which are\nknown as probability flows. We reveal an intriguing connection between the\nconditional variance of their training loss and the discretization error of the\nODEs. With this insight, we propose a variance-adaptive ODE solver that can\nadjust its step size in the inference stage, making AdaFlow an adaptive\ndecision-maker, offering rapid inference without sacrificing diversity.\nInterestingly, it automatically reduces to a one-step generator when the action\ndistribution is uni-modal. Our comprehensive empirical evaluation shows that\nAdaFlow achieves high performance with fast inference speed.\n","authors":["Xixi Hu","Bo Liu","Xingchao Liu","Qiang Liu"],"pdf_url":"https://arxiv.org/pdf/2402.04292v2.pdf","comment":"NeuRIPS 2024"},{"id":"http://arxiv.org/abs/2411.15109v1","updated":"2024-11-22T18:11:16Z","published":"2024-11-22T18:11:16Z","title":"Effective Littlestone Dimension","summary":"  Delle Rose et al.~(COLT'23) introduced an effective version of the\nVapnik-Chervonenkis dimension, and showed that it characterizes improper PAC\nlearning with total computable learners. In this paper, we introduce and study\na similar effectivization of the notion of Littlestone dimension. Finite\neffective Littlestone dimension is a necessary condition for computable online\nlearning but is not a sufficient one -- which we already establish for classes\nof the effective Littlestone dimension 2. However, the effective Littlestone\ndimension equals the optimal mistake bound for computable learners in two\nspecial cases: a) for classes of Littlestone dimension 1 and b) when the\nlearner receives as additional information an upper bound on the numbers to be\nguessed. Interestingly, finite effective Littlestone dimension also guarantees\nthat the class consists only of computable functions.\n","authors":["Valentino Delle Rose","Alexander Kozachinskiy","Tomasz Steifer"],"pdf_url":"https://arxiv.org/pdf/2411.15109v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2411.15106v1","updated":"2024-11-22T18:09:27Z","published":"2024-11-22T18:09:27Z","title":"About Time: Advances, Challenges, and Outlooks of Action Understanding","summary":"  We have witnessed impressive advances in video action understanding.\nIncreased dataset sizes, variability, and computation availability have enabled\nleaps in performance and task diversification. Current systems can provide\ncoarse- and fine-grained descriptions of video scenes, extract segments\ncorresponding to queries, synthesize unobserved parts of videos, and predict\ncontext. This survey comprehensively reviews advances in uni- and multi-modal\naction understanding across a range of tasks. We focus on prevalent challenges,\noverview widely adopted datasets, and survey seminal works with an emphasis on\nrecent advances. We broadly distinguish between three temporal scopes: (1)\nrecognition tasks of actions observed in full, (2) prediction tasks for ongoing\npartially observed actions, and (3) forecasting tasks for subsequent unobserved\naction. This division allows us to identify specific action modeling and video\nrepresentation challenges. Finally, we outline future directions to address\ncurrent shortcomings.\n","authors":["Alexandros Stergiou","Ronald Poppe"],"pdf_url":"https://arxiv.org/pdf/2411.15106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15102v1","updated":"2024-11-22T18:06:14Z","published":"2024-11-22T18:06:14Z","title":"AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution","summary":"  The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.\n","authors":["Fengyuan Liu","Nikhil Kandpal","Colin Raffel"],"pdf_url":"https://arxiv.org/pdf/2411.15102v1.pdf","comment":"29 pages, 11 figures"},{"id":"http://arxiv.org/abs/2411.15101v1","updated":"2024-11-22T18:04:46Z","published":"2024-11-22T18:04:46Z","title":"What You See is Not What You Get: Neural Partial Differential Equations\n  and The Illusion of Learning","summary":"  Differentiable Programming for scientific machine learning (SciML) has\nrecently seen considerable interest and success, as it directly embeds neural\nnetworks inside PDEs, often called as NeuralPDEs, derived from first principle\nphysics. Therefore, there is a widespread assumption in the community that\nNeuralPDEs are more trustworthy and generalizable than black box models.\nHowever, like any SciML model, differentiable programming relies predominantly\non high-quality PDE simulations as \"ground truth\" for training. However,\nmathematics dictates that these are only discrete numerical approximations of\nthe true physics. Therefore, we ask: Are NeuralPDEs and differentiable\nprogramming models trained on PDE simulations as physically interpretable as we\nthink? In this work, we rigorously attempt to answer these questions, using\nestablished ideas from numerical analysis, experiments, and analysis of model\nJacobians. Our study shows that NeuralPDEs learn the artifacts in the\nsimulation training data arising from the discretized Taylor Series truncation\nerror of the spatial derivatives. Additionally, NeuralPDE models are\nsystematically biased, and their generalization capability is likely enabled by\na fortuitous interplay of numerical dissipation and truncation error in the\ntraining dataset and NeuralPDE, which seldom happens in practical applications.\nThis bias manifests aggressively even in relatively accessible 1-D equations,\nraising concerns about the veracity of differentiable programming on complex,\nhigh-dimensional, real-world PDEs, and in dataset integrity of foundation\nmodels. Further, we observe that the initial condition constrains the\ntruncation error in initial-value problems in PDEs, thereby exerting\nlimitations to extrapolation. Finally, we demonstrate that an eigenanalysis of\nmodel weights can indicate a priori if the model will be inaccurate for\nout-of-distribution testing.\n","authors":["Arvind Mohan","Ashesh Chattopadhyay","Jonah Miller"],"pdf_url":"https://arxiv.org/pdf/2411.15101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14390v2","updated":"2024-11-22T18:04:33Z","published":"2024-11-21T18:24:06Z","title":"Persistent Homology for Structural Characterization in Disordered\n  Systems","summary":"  We propose a unified framework based on persistent homology (PH) to\ncharacterize both local and global structures in disordered systems. It can\nsimultaneously generate local and global descriptors using the same algorithm\nand data structure, and has shown to be highly effective and interpretable in\npredicting particle rearrangements and classifying global phases. Based on this\nframework, we define a non-parametric metric, the Separation Index (SI), which\nnot only outperforms traditional bond-orientational order parameters in phase\nclassification tasks but also establishes a connection between particle\nenvironments and the global phase structure. Our methods provide an effective\nframework for understanding and analyzing the properties of disordered\nmaterials, with broad potential applications in materials science and even\nwider studies of complex systems.\n","authors":["An Wang","Li Zou"],"pdf_url":"https://arxiv.org/pdf/2411.14390v2.pdf","comment":"19 pages, 17 figures"},{"id":"http://arxiv.org/abs/2405.08363v2","updated":"2024-11-22T17:56:26Z","published":"2024-05-14T07:05:18Z","title":"UnMarker: A Universal Attack on Defensive Image Watermarking","summary":"  Reports regarding the misuse of Generative AI (GenAI) to create deepfakes are\nfrequent. Defensive watermarking enables GenAI providers to hide fingerprints\nin their images and use them later for deepfake detection. Yet, its potential\nhas not been fully explored. We present UnMarker -- the first practical\nuniversal attack on defensive watermarking. Unlike existing attacks, UnMarker\nrequires no detector feedback, no unrealistic knowledge of the watermarking\nscheme or similar models, and no advanced denoising pipelines that may not be\navailable. Instead, being the product of an in-depth analysis of the\nwatermarking paradigm revealing that robust schemes must construct their\nwatermarks in the spectral amplitudes, UnMarker employs two novel adversarial\noptimizations to disrupt the spectra of watermarked images, erasing the\nwatermarks. Evaluations against SOTA schemes prove UnMarker's effectiveness. It\nnot only defeats traditional schemes while retaining superior quality compared\nto existing attacks but also breaks semantic watermarks that alter an image's\nstructure, reducing the best detection rate to $43\\%$ and rendering them\nuseless. To our knowledge, UnMarker is the first practical attack on semantic\nwatermarks, which have been deemed the future of defensive watermarking. Our\nfindings show that defensive watermarking is not a viable defense against\ndeepfakes, and we urge the community to explore alternatives.\n","authors":["Andre Kassis","Urs Hengartner"],"pdf_url":"https://arxiv.org/pdf/2405.08363v2.pdf","comment":"To appear at IEEE S&P 2025"},{"id":"http://arxiv.org/abs/2411.15099v1","updated":"2024-11-22T17:55:39Z","published":"2024-11-22T17:55:39Z","title":"Context-Aware Multimodal Pretraining","summary":"  Large-scale multimodal representation learning successfully optimizes for\nzero-shot transfer at test time. Yet the standard pretraining paradigm\n(contrastive learning on large amounts of image-text data) does not explicitly\nencourage representations to support few-shot adaptation. In this work, we\npropose a simple, but carefully designed extension to multimodal pretraining\nwhich enables representations to accommodate additional context. Using this\nobjective, we show that vision-language models can be trained to exhibit\nsignificantly increased few-shot adaptation: across 21 downstream tasks, we\nfind up to four-fold improvements in test-time sample efficiency, and average\nfew-shot adaptation gains of over 5%, while retaining zero-shot generalization\nperformance across model scales and training durations. In particular, equipped\nwith simple, training-free, metric-based adaptation mechanisms, our\nrepresentations easily surpass more complex and expensive optimization-based\nschemes, vastly simplifying generalization to new domains.\n","authors":["Karsten Roth","Zeynep Akata","Dima Damen","Ivana Balažević","Olivier J. Hénaff"],"pdf_url":"https://arxiv.org/pdf/2411.15099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15098v1","updated":"2024-11-22T17:55:15Z","published":"2024-11-22T17:55:15Z","title":"OminiControl: Minimal and Universal Control for Diffusion Transformer","summary":"  In this paper, we introduce OminiControl, a highly versatile and\nparameter-efficient framework that integrates image conditions into pre-trained\nDiffusion Transformer (DiT) models. At its core, OminiControl leverages a\nparameter reuse mechanism, enabling the DiT to encode image conditions using\nitself as a powerful backbone and process them with its flexible multi-modal\nattention processors. Unlike existing methods, which rely heavily on additional\nencoder modules with complex architectures, OminiControl (1) effectively and\nefficiently incorporates injected image conditions with only ~0.1% additional\nparameters, and (2) addresses a wide range of image conditioning tasks in a\nunified manner, including subject-driven generation and spatially-aligned\nconditions such as edges, depth, and more. Remarkably, these capabilities are\nachieved by training on images generated by the DiT itself, which is\nparticularly beneficial for subject-driven generation. Extensive evaluations\ndemonstrate that OminiControl outperforms existing UNet-based and DiT-adapted\nmodels in both subject-driven and spatially-aligned conditional generation.\nAdditionally, we release our training dataset, Subjects200K, a diverse\ncollection of over 200,000 identity-consistent images, along with an efficient\ndata synthesis pipeline to advance research in subject-consistent generation.\n","authors":["Zhenxiong Tan","Songhua Liu","Xingyi Yang","Qiaochu Xue","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2411.15098v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15096v1","updated":"2024-11-22T17:51:21Z","published":"2024-11-22T17:51:21Z","title":"RED: Effective Trajectory Representation Learning with Comprehensive\n  Information","summary":"  Trajectory representation learning (TRL) maps trajectories to vectors that\ncan then be used for various downstream tasks, including trajectory similarity\ncomputation, trajectory classification, and travel-time estimation. However,\nexisting TRL methods often produce vectors that, when used in downstream tasks,\nyield insufficiently accurate results. A key reason is that they fail to\nutilize the comprehensive information encompassed by trajectories. We propose a\nself-supervised TRL framework, called RED, which effectively exploits multiple\ntypes of trajectory information. Overall, RED adopts the Transformer as the\nbackbone model and masks the constituting paths in trajectories to train a\nmasked autoencoder (MAE). In particular, RED considers the moving patterns of\ntrajectories by employing a Road-aware masking strategy} that retains key paths\nof trajectories during masking, thereby preserving crucial information of the\ntrajectories. RED also adopts a spatial-temporal-user joint Embedding scheme to\nencode comprehensive information when preparing the trajectories as model\ninputs. To conduct training, RED adopts Dual-objective task learning}: the\nTransformer encoder predicts the next segment in a trajectory, and the\nTransformer decoder reconstructs the entire trajectory. RED also considers the\nspatial-temporal correlations of trajectories by modifying the attention\nmechanism of the Transformer. We compare RED with 9 state-of-the-art TRL\nmethods for 4 downstream tasks on 3 real-world datasets, finding that RED can\nusually improve the accuracy of the best-performing baseline by over 5%.\n","authors":["Silin Zhou","Shuo Shang","Lisi Chen","Christian S. Jensen","Panos Kalnis"],"pdf_url":"https://arxiv.org/pdf/2411.15096v1.pdf","comment":"This paper is accepted by VLDB2025"},{"id":"http://arxiv.org/abs/2411.15095v1","updated":"2024-11-22T17:50:27Z","published":"2024-11-22T17:50:27Z","title":"Dimension-independent rates for structured neural density estimation","summary":"  We show that deep neural networks achieve dimension-independent rates of\nconvergence for learning structured densities such as those arising in image,\naudio, video, and text applications. More precisely, we demonstrate that neural\nnetworks with a simple $L^2$-minimizing loss achieve a rate of $n^{-1/(4+r)}$\nin nonparametric density estimation when the underlying density is Markov to a\ngraph whose maximum clique size is at most $r$, and we provide evidence that in\nthe aforementioned applications, this size is typically constant, i.e.,\n$r=O(1)$. We then establish that the optimal rate in $L^1$ is $n^{-1/(2+r)}$\nwhich, compared to the standard nonparametric rate of $n^{-1/(2+d)}$, reveals\nthat the effective dimension of such problems is the size of the largest clique\nin the Markov random field. These rates are independent of the data's ambient\ndimension, making them applicable to realistic models of image, sound, video,\nand text data. Our results provide a novel justification for deep learning's\nability to circumvent the curse of dimensionality, demonstrating\ndimension-independent convergence rates in these contexts.\n","authors":["Robert A. Vandermeulen","Wai Ming Tai","Bryon Aragam"],"pdf_url":"https://arxiv.org/pdf/2411.15095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16881v2","updated":"2024-11-22T17:47:37Z","published":"2024-10-22T10:33:00Z","title":"Just In Time Transformers","summary":"  Precise energy load forecasting in residential households is crucial for\nmitigating carbon emissions and enhancing energy efficiency; indeed, accurate\nforecasting enables utility companies and policymakers, who advocate\nsustainable energy practices, to optimize resource utilization. Moreover, smart\nmeters provide valuable information by allowing for granular insights into\nconsumption patterns. Building upon available smart meter data, our study aims\nto cluster consumers into distinct groups according to their energy usage\nbehaviours, effectively capturing a diverse spectrum of consumption patterns.\nNext, we design JITtrans (Just In Time transformer), a novel transformer deep\nlearning model that significantly improves energy consumption forecasting\naccuracy, with respect to traditional forecasting methods. Extensive\nexperimental results validate our claims using proprietary smart meter data.\nOur findings highlight the potential of advanced predictive technologies to\nrevolutionize energy management and advance sustainable power systems: the\ndevelopment of efficient and eco-friendly energy solutions critically depends\non such technologies.\n","authors":["Ahmed Ala Eddine Benali","Massimo Cafaro","Italo Epicoco","Marco Pulimeno","Enrico Junior Schioppa"],"pdf_url":"https://arxiv.org/pdf/2410.16881v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.06202v4","updated":"2024-11-22T17:39:16Z","published":"2023-06-09T19:10:16Z","title":"NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics","summary":"  Machine learning provides a valuable tool for analyzing high-dimensional\nfunctional neuroimaging data, and is proving effective in predicting various\nneurological conditions, psychiatric disorders, and cognitive patterns. In\nfunctional magnetic resonance imaging (MRI) research, interactions between\nbrain regions are commonly modeled using graph-based representations. The\npotency of graph machine learning methods has been established across myriad\ndomains, marking a transformative step in data interpretation and predictive\nmodeling. Yet, despite their promise, the transposition of these techniques to\nthe neuroimaging domain has been challenging due to the expansive number of\npotential preprocessing pipelines and the large parameter search space for\ngraph-based dataset construction. In this paper, we introduce NeuroGraph, a\ncollection of graph-based neuroimaging datasets, and demonstrated its utility\nfor predicting multiple categories of behavioral and cognitive traits. We delve\ndeeply into the dataset generation search space by crafting 35 datasets that\nencompass static and dynamic brain connectivity, running in excess of 15\nbaseline methods for benchmarking. Additionally, we provide generic frameworks\nfor learning on both static and dynamic graphs. Our extensive experiments lead\nto several key observations. Notably, using correlation vectors as node\nfeatures, incorporating larger number of regions of interest, and employing\nsparser graphs lead to improved performance. To foster further advancements in\ngraph-based data driven neuroimaging analysis, we offer a comprehensive\nopen-source Python package that includes the benchmark datasets, baseline\nimplementations, model training, and standard evaluation.\n","authors":["Anwar Said","Roza G. Bayrak","Tyler Derr","Mudassir Shabbir","Daniel Moyer","Catie Chang","Xenofon Koutsoukos"],"pdf_url":"https://arxiv.org/pdf/2306.06202v4.pdf","comment":"NeurIPS23"},{"id":"http://arxiv.org/abs/2411.15087v1","updated":"2024-11-22T17:28:43Z","published":"2024-11-22T17:28:43Z","title":"Instance-Aware Generalized Referring Expression Segmentation","summary":"  Recent works on Generalized Referring Expression Segmentation (GRES) struggle\nwith handling complex expressions referring to multiple distinct objects. This\nis because these methods typically employ an end-to-end foreground-background\nsegmentation and lack a mechanism to explicitly differentiate and associate\ndifferent object instances to the text query. To this end, we propose\nInstAlign, a method that incorporates object-level reasoning into the\nsegmentation process. Our model leverages both text and image inputs to extract\na set of object-level tokens that capture both the semantic information in the\ninput prompt and the objects within the image. By modeling the text-object\nalignment via instance-level supervision, each token uniquely represents an\nobject segment in the image, while also aligning with relevant semantic\ninformation from the text. Extensive experiments on the gRefCOCO and Ref-ZOM\nbenchmarks demonstrate that our method significantly advances state-of-the-art\nperformance, setting a new standard for precise and flexible GRES.\n","authors":["E-Ro Nguyen","Hieu Le","Dimitris Samaras","Michael Ryoo"],"pdf_url":"https://arxiv.org/pdf/2411.15087v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2411.15084v1","updated":"2024-11-22T17:19:58Z","published":"2024-11-22T17:19:58Z","title":"Leapfrog Latent Consistency Model (LLCM) for Medical Images Generation","summary":"  The scarcity of accessible medical image data poses a significant obstacle in\neffectively training deep learning models for medical diagnosis, as hospitals\nrefrain from sharing their data due to privacy concerns. In response, we\ngathered a diverse dataset named MedImgs, which comprises over 250,127 images\nspanning 61 disease types and 159 classes of both humans and animals from\nopen-source repositories. We propose a Leapfrog Latent Consistency Model (LLCM)\nthat is distilled from a retrained diffusion model based on the collected\nMedImgs dataset, which enables our model to generate real-time high-resolution\nimages. We formulate the reverse diffusion process as a probability flow\nordinary differential equation (PF-ODE) and solve it in latent space using the\nLeapfrog algorithm. This formulation enables rapid sampling without\nnecessitating additional iterations. Our model demonstrates state-of-the-art\nperformance in generating medical images. Furthermore, our model can be\nfine-tuned with any custom medical image datasets, facilitating the generation\nof a vast array of images. Our experimental results outperform those of\nexisting models on unseen dog cardiac X-ray images. Source code is available at\nhttps://github.com/lskdsjy/LeapfrogLCM.\n","authors":["Lakshmikar R. Polamreddy","Kalyan Roy","Sheng-Han Yueh","Deepshikha Mahato","Shilpa Kuppili","Jialu Li","Youshan Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.15084v1.pdf","comment":"Total 16 pages including 5 figures and 36 references"},{"id":"http://arxiv.org/abs/2411.15082v1","updated":"2024-11-22T17:18:08Z","published":"2024-11-22T17:18:08Z","title":"Towards Speaker Identification with Minimal Dataset and Constrained\n  Resources using 1D-Convolution Neural Network","summary":"  Voice recognition and speaker identification are vital for applications in\nsecurity and personal assistants. This paper presents a lightweight\n1D-Convolutional Neural Network (1D-CNN) designed to perform speaker\nidentification on minimal datasets. Our approach achieves a validation accuracy\nof 97.87%, leveraging data augmentation techniques to handle background noise\nand limited training samples. Future improvements include testing on larger\ndatasets and integrating transfer learning methods to enhance generalizability.\nWe provide all code, the custom dataset, and the trained models to facilitate\nreproducibility. These resources are available on our GitHub repository:\nhttps://github.com/IrfanNafiz/RecMe.\n","authors":["Irfan Nafiz Shahan","Pulok Ahmed Auvi"],"pdf_url":"https://arxiv.org/pdf/2411.15082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15074v1","updated":"2024-11-22T17:03:25Z","published":"2024-11-22T17:03:25Z","title":"Learning to Stabilize Faces","summary":"  Nowadays, it is possible to scan faces and automatically register them with\nhigh quality. However, the resulting face meshes often need further processing:\nwe need to stabilize them to remove unwanted head movement. Stabilization is\nimportant for tasks like game development or movie making which require facial\nexpressions to be cleanly separated from rigid head motion. Since manual\nstabilization is labor-intensive, there have been attempts to automate it.\nHowever, previous methods remain impractical: they either still require some\nmanual input, produce imprecise alignments, rely on dubious heuristics and slow\noptimization, or assume a temporally ordered input. Instead, we present a new\nlearning-based approach that is simple and fully automatic. We treat\nstabilization as a regression problem: given two face meshes, our network\ndirectly predicts the rigid transform between them that brings their skulls\ninto alignment. We generate synthetic training data using a 3D Morphable Model\n(3DMM), exploiting the fact that 3DMM parameters separate skull motion from\nfacial skin motion. Through extensive experiments we show that our approach\noutperforms the state-of-the-art both quantitatively and qualitatively on the\ntasks of stabilizing discrete sets of facial expressions as well as dynamic\nfacial performances. Furthermore, we provide an ablation study detailing the\ndesign choices and best practices to help others adopt our approach for their\nown uses. Supplementary videos can be found on the project webpage\nsyntec-research.github.io/FaceStab.\n","authors":["Jan Bednarik","Erroll Wood","Vasileios Choutas","Timo Bolkart","Daoye Wang","Chenglei Wu","Thabo Beeler"],"pdf_url":"https://arxiv.org/pdf/2411.15074v1.pdf","comment":"Eurographics 2024"},{"id":"http://arxiv.org/abs/2411.15067v1","updated":"2024-11-22T16:56:12Z","published":"2024-11-22T16:56:12Z","title":"Linear convergence of proximal descent schemes on the Wasserstein space","summary":"  We investigate proximal descent methods, inspired by the minimizing movement\nscheme introduced by Jordan, Kinderlehrer and Otto, for optimizing\nentropy-regularized functionals on the Wasserstein space. We establish linear\nconvergence under flat convexity assumptions, thereby relaxing the common\nreliance on geodesic convexity. Our analysis circumvents the need for\ndiscrete-time adaptations of the Evolution Variational Inequality (EVI).\nInstead, we leverage a uniform logarithmic Sobolev inequality (LSI) and the\nentropy \"sandwich\" lemma, extending the analysis from arXiv:2201.10469 and\narXiv:2202.01009. The major challenge in the proof via LSI is to show that the\nrelative Fisher information $I(\\cdot|\\pi)$ is well-defined at every step of the\nscheme. Since the relative entropy is not Wasserstein differentiable, we prove\nthat along the scheme the iterates belong to a certain class of Sobolev\nregularity, and hence the relative entropy $\\operatorname{KL}(\\cdot|\\pi)$ has a\nunique Wasserstein sub-gradient, and that the relative Fisher information is\nindeed finite.\n","authors":["Razvan-Andrei Lascu","Mateusz B. Majka","David Šiška","Łukasz Szpruch"],"pdf_url":"https://arxiv.org/pdf/2411.15067v1.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2411.15066v1","updated":"2024-11-22T16:54:17Z","published":"2024-11-22T16:54:17Z","title":"SPAC-Net: Rethinking Point Cloud Completion with Structural Prior","summary":"  Point cloud completion aims to infer a complete shape from its partial\nobservation. Many approaches utilize a pure encoderdecoder paradigm in which\ncomplete shape can be directly predicted by shape priors learned from partial\nscans, however, these methods suffer from the loss of details inevitably due to\nthe feature abstraction issues. In this paper, we propose a novel\nframework,termed SPAC-Net, that aims to rethink the completion task under the\nguidance of a new structural prior, we call it interface. Specifically, our\nmethod first investigates Marginal Detector (MAD) module to localize the\ninterface, defined as the intersection between the known observation and the\nmissing parts. Based on the interface, our method predicts the coarse shape by\nlearning the displacement from the points in interface move to their\ncorresponding position in missing parts. Furthermore, we devise an additional\nStructure Supplement(SSP) module before the upsampling stage to enhance the\nstructural details of the coarse shape, enabling the upsampling module to focus\nmore on the upsampling task. Extensive experiments have been conducted on\nseveral challenging benchmarks, and the results demonstrate that our method\noutperforms existing state-of-the-art approaches.\n","authors":["Zizhao Wu","Jian Shi","Xuan Deng","Cheng Zhang","Genfu Yang","Ming Zeng","Yunhai Wang"],"pdf_url":"https://arxiv.org/pdf/2411.15066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15060v1","updated":"2024-11-22T16:46:00Z","published":"2024-11-22T16:46:00Z","title":"Detecting Hallucinations in Virtual Histology with Neural Precursors","summary":"  Significant biomedical research and clinical care rely on the histopathologic\nexamination of tissue structure using microscopy of stained tissue. Virtual\nstaining (VS) offers a promising alternative with the potential to reduce cost\nand eliminate the use of toxic reagents. However, the critical challenge of\nhallucinations limits confidence in its use, necessitating a VS co-pilot to\ndetect these hallucinations. Here, we first formally establish the problem of\nhallucination detection in VS. Next, we introduce a scalable, post-hoc\nhallucination detection method that identifies a Neural Hallucination Precursor\n(NHP) from VS model embeddings for test-time detection. We report extensive\nvalidation across diverse and challenging VS settings to demonstrate NHP's\neffectiveness and robustness. Furthermore, we show that VS models with fewer\nhallucinations do not necessarily disclose them better, risking a false sense\nof security when reporting just the former metric. This highlights the need for\na reassessment of current VS evaluation practices.\n","authors":["Ji-Hun Oh","Kianoush Falahkheirkhah","Rohit Bhargava"],"pdf_url":"https://arxiv.org/pdf/2411.15060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04037v2","updated":"2024-11-22T16:42:26Z","published":"2024-03-06T20:34:08Z","title":"OCD-FL: A Novel Communication-Efficient Peer Selection-based\n  Decentralized Federated Learning","summary":"  The conjunction of edge intelligence and the ever-growing Internet-of-Things\n(IoT) network heralds a new era of collaborative machine learning, with\nfederated learning (FL) emerging as the most prominent paradigm. With the\ngrowing interest in these learning schemes, researchers started addressing some\nof their most fundamental limitations. Indeed, conventional FL with a central\naggregator presents a single point of failure and a network bottleneck. To\nbypass this issue, decentralized FL where nodes collaborate in a peer-to-peer\nnetwork has been proposed. Despite the latter's efficiency, communication costs\nand data heterogeneity remain key challenges in decentralized FL. In this\ncontext, we propose a novel scheme, called opportunistic\ncommunication-efficient decentralized federated learning, a.k.a., OCD-FL,\nconsisting of a systematic FL peer selection for collaboration, aiming to\nachieve maximum FL knowledge gain while reducing energy consumption.\nExperimental results demonstrate the capability of OCD-FL to achieve similar or\nbetter performances than the fully collaborative FL, while significantly\nreducing consumed energy by at least 30% and up to 80%.\n","authors":["Nizar Masmoudi","Wael Jaafar"],"pdf_url":"https://arxiv.org/pdf/2403.04037v2.pdf","comment":"6 pages, under review in IEEE Transactions on Vehicular Technology as\n  a Correspondance (rev. 1)"},{"id":"http://arxiv.org/abs/2411.15051v1","updated":"2024-11-22T16:38:03Z","published":"2024-11-22T16:38:03Z","title":"Fantastic Biases (What are They) and Where to Find Them","summary":"  Deep Learning models tend to learn correlations of patterns on huge datasets.\nThe bigger these systems are, the more complex are the phenomena they can\ndetect, and the more data they need for this. The use of Artificial\nIntelligence (AI) is becoming increasingly ubiquitous in our society, and its\nimpact is growing everyday. The promises it holds strongly depend on their fair\nand universal use, such as access to information or education for all. In a\nworld of inequalities, they can help to reach the most disadvantaged areas.\nHowever, such a universal systems must be able to represent society, without\nbenefiting some at the expense of others. We must not reproduce the\ninequalities observed throughout the world, but educate these IAs to go beyond\nthem. We have seen cases where these systems use gender, race, or even class\ninformation in ways that are not appropriate for resolving their tasks. Instead\nof real causal reasoning, they rely on spurious correlations, which is what we\nusually call a bias. In this paper, we first attempt to define what is a bias\nin general terms. It helps us to demystify the concept of bias, to understand\nwhy we can find them everywhere and why they are sometimes useful. Second, we\nfocus over the notion of what is generally seen as negative bias, the one we\nwant to avoid in machine learning, before presenting a general zoology\ncontaining the most common of these biases. We finally conclude by looking at\nclassical methods to detect them, by means of specially crafted datasets of\ntemplates and specific algorithms, and also classical methods to mitigate them.\n","authors":["Valentin Barriere"],"pdf_url":"https://arxiv.org/pdf/2411.15051v1.pdf","comment":"Publication in Spanish in the Journal Bits de Ciencias:\n  https://www.dcc.uchile.cl/media/bits/pdfs/bits26.2-sesgos-fantasticos.pdf"},{"id":"http://arxiv.org/abs/2411.15046v1","updated":"2024-11-22T16:31:36Z","published":"2024-11-22T16:31:36Z","title":"On Multi-Agent Inverse Reinforcement Learning","summary":"  In multi-agent systems, the agent behavior is highly influenced by its\nutility function, as these utilities shape both individual goals as well as\ninteractions with the other agents. Inverse Reinforcement Learning (IRL) is a\nwell-established approach to inferring the utility function by observing an\nexpert behavior within a given environment. In this paper, we extend the IRL\nframework to the multi-agent setting, assuming to observe agents who are\nfollowing Nash Equilibrium (NE) policies. We theoretically investigate the set\nof utilities that explain the behavior of NE experts. Specifically, we provide\nan explicit characterization of the feasible reward set and analyze how errors\nin estimating the transition dynamics and expert behavior impact the recovered\nrewards. Building on these findings, we provide the first sample complexity\nanalysis for the multi-agent IRL problem. Finally, we provide a numerical\nevaluation of our theoretical results.\n","authors":["Till Freihaut","Giorgia Ramponi"],"pdf_url":"https://arxiv.org/pdf/2411.15046v1.pdf","comment":"Currently under review"},{"id":"http://arxiv.org/abs/2410.00903v2","updated":"2024-11-22T16:31:25Z","published":"2024-10-01T17:46:21Z","title":"Causal Representation Learning with Generative Artificial Intelligence:\n  Application to Texts as Treatments","summary":"  In this paper, we demonstrate how to enhance the validity of causal inference\nwith unstructured high-dimensional treatments like texts, by leveraging the\npower of generative Artificial Intelligence. Specifically, we propose to use a\ndeep generative model such as large language models (LLMs) to efficiently\ngenerate treatments and use their internal representation for subsequent causal\neffect estimation. We show that the knowledge of this true internal\nrepresentation helps disentangle the treatment features of interest, such as\nspecific sentiments and certain topics, from other possibly unknown confounding\nfeatures. Unlike the existing methods, our proposed approach eliminates the\nneed to learn causal representation from the data and hence produces more\naccurate and efficient estimates. We formally establish the conditions required\nfor the nonparametric identification of the average treatment effect, propose\nan estimation strategy that avoids the violation of the overlap assumption, and\nderive the asymptotic properties of the proposed estimator through the\napplication of double machine learning. Finally, using an instrumental\nvariables approach, we extend the proposed methodology to the settings, in\nwhich the treatment feature is based on human perception rather than is assumed\nto be fixed given the treatment object. The proposed methodology is also\napplicable to text reuse where an LLM is used to regenerate the existing texts.\nWe conduct simulation and empirical studies, using the generated text data from\nan open-source LLM, Llama 3, to illustrate the advantages of our estimator over\nthe state-of-the-art causal representation learning algorithms.\n","authors":["Kosuke Imai","Kentaro Nakamura"],"pdf_url":"https://arxiv.org/pdf/2410.00903v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23148v3","updated":"2024-11-22T16:18:55Z","published":"2024-10-30T16:04:16Z","title":"HiBO: Hierarchical Bayesian Optimization via Adaptive Search Space\n  Partitioning","summary":"  Optimizing black-box functions in high-dimensional search spaces has been\nknown to be challenging for traditional Bayesian Optimization (BO). In this\npaper, we introduce HiBO, a novel hierarchical algorithm integrating\nglobal-level search space partitioning information into the acquisition\nstrategy of a local BO-based optimizer. HiBO employs a search-tree-based\nglobal-level navigator to adaptively split the search space into partitions\nwith different sampling potential. The local optimizer then utilizes this\nglobal-level information to guide its acquisition strategy towards most\npromising regions within the search space. A comprehensive set of evaluations\ndemonstrates that HiBO outperforms state-of-the-art methods in high-dimensional\nsynthetic benchmarks and presents significant practical effectiveness in the\nreal-world task of tuning configurations of database management systems\n(DBMSs).\n","authors":["Wenxuan Li","Taiyi Wang","Eiko Yoneki"],"pdf_url":"https://arxiv.org/pdf/2410.23148v3.pdf","comment":"There are some ethically sensitive words to be further modified in\n  this paper. Hope that we can withdraw it first and re-post it back after a\n  further investigation into the related guidelines"},{"id":"http://arxiv.org/abs/2411.09813v2","updated":"2024-11-22T16:14:02Z","published":"2024-11-14T21:07:52Z","title":"Can Features for Phishing URL Detection Be Trusted Across Diverse\n  Datasets? A Case Study with Explainable AI","summary":"  Phishing has been a prevalent cyber threat that manipulates users into\nrevealing sensitive private information through deceptive tactics, designed to\nmasquerade as trustworthy entities. Over the years, proactively detection of\nphishing URLs (or websites) has been established as an widely-accepted defense\napproach. In literature, we often find supervised Machine Learning (ML) models\nwith highly competitive performance for detecting phishing websites based on\nthe extracted features from both phishing and benign (i.e., legitimate)\nwebsites. However, it is still unclear if these features or indicators are\ndependent on a particular dataset or they are generalized for overall phishing\ndetection. In this paper, we delve deeper into this issue by analyzing two\npublicly available phishing URL datasets, where each dataset has its own set of\nunique and overlapping features related to URL string and website contents. We\nwant to investigate if overlapping features are similar in nature across\ndatasets and how does the model perform when trained on one dataset and tested\non the other. We conduct practical experiments and leverage explainable AI\n(XAI) methods such as SHAP plots to provide insights into different features'\ncontributions in case of phishing detection to answer our primary question,\n\"Can features for phishing URL detection be trusted across diverse dataset?\".\nOur case study experiment results show that features for phishing URL detection\ncan often be dataset-dependent and thus may not be trusted across different\ndatasets even though they share same set of feature behaviors.\n","authors":["Maraz Mia","Darius Derakhshan","Mir Mehedi A. Pritom"],"pdf_url":"https://arxiv.org/pdf/2411.09813v2.pdf","comment":"9 pages, 9 figures, 11th International Conference on Networking,\n  Systems, and Security (NSysS 2024), 2024, Khulna, Bangladesh"},{"id":"http://arxiv.org/abs/2205.14627v3","updated":"2024-11-22T16:13:22Z","published":"2022-05-29T11:06:29Z","title":"Continuous Generative Neural Networks: A Wavelet-Based Architecture in\n  Function Spaces","summary":"  In this work, we present and study Continuous Generative Neural Networks\n(CGNNs), namely, generative models in the continuous setting: the output of a\nCGNN belongs to an infinite-dimensional function space. The architecture is\ninspired by DCGAN, with one fully connected layer, several convolutional layers\nand nonlinear activation functions. In the continuous $L^2$ setting, the\ndimensions of the spaces of each layer are replaced by the scales of a\nmultiresolution analysis of a compactly supported wavelet. We present\nconditions on the convolutional filters and on the nonlinearity that guarantee\nthat a CGNN is injective. This theory finds applications to inverse problems,\nand allows for deriving Lipschitz stability estimates for (possibly nonlinear)\ninfinite-dimensional inverse problems with unknowns belonging to the manifold\ngenerated by a CGNN. Several numerical simulations, including signal\ndeblurring, illustrate and validate this approach.\n","authors":["Giovanni S. Alberti","Matteo Santacesaria","Silvia Sciutto"],"pdf_url":"https://arxiv.org/pdf/2205.14627v3.pdf","comment":"40 pages, 8 figures"},{"id":"http://arxiv.org/abs/2411.15036v1","updated":"2024-11-22T16:08:42Z","published":"2024-11-22T16:08:42Z","title":"Safe Multi-Agent Reinforcement Learning with Convergence to Generalized\n  Nash Equilibrium","summary":"  Multi-agent reinforcement learning (MARL) has achieved notable success in\ncooperative tasks, demonstrating impressive performance and scalability.\nHowever, deploying MARL agents in real-world applications presents critical\nsafety challenges. Current safe MARL algorithms are largely based on the\nconstrained Markov decision process (CMDP) framework, which enforces\nconstraints only on discounted cumulative costs and lacks an all-time safety\nassurance. Moreover, these methods often overlook the feasibility issue (the\nsystem will inevitably violate state constraints within certain regions of the\nconstraint set), resulting in either suboptimal performance or increased\nconstraint violations. To address these challenges, we propose a novel\ntheoretical framework for safe MARL with $\\textit{state-wise}$ constraints,\nwhere safety requirements are enforced at every state the agents visit. To\nresolve the feasibility issue, we leverage a control-theoretic notion of the\nfeasible region, the controlled invariant set (CIS), characterized by the\nsafety value function. We develop a multi-agent method for identifying CISs,\nensuring convergence to a Nash equilibrium on the safety value function. By\nincorporating CIS identification into the learning process, we introduce a\nmulti-agent dual policy iteration algorithm that guarantees convergence to a\ngeneralized Nash equilibrium in state-wise constrained cooperative Markov\ngames, achieving an optimal balance between feasibility and performance.\nFurthermore, for practical deployment in complex high-dimensional systems, we\npropose $\\textit{Multi-Agent Dual Actor-Critic}$ (MADAC), a safe MARL algorithm\nthat approximates the proposed iteration scheme within the deep RL paradigm.\nEmpirical evaluations on safe MARL benchmarks demonstrate that MADAC\nconsistently outperforms existing methods, delivering much higher rewards while\nreducing constraint violations.\n","authors":["Zeyang Li","Navid Azizan"],"pdf_url":"https://arxiv.org/pdf/2411.15036v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15034v1","updated":"2024-11-22T16:08:03Z","published":"2024-11-22T16:08:03Z","title":"HeadRouter: A Training-free Image Editing Framework for MM-DiTs by\n  Adaptively Routing Attention Heads","summary":"  Diffusion Transformers (DiTs) have exhibited robust capabilities in image\ngeneration tasks. However, accurate text-guided image editing for multimodal\nDiTs (MM-DiTs) still poses a significant challenge. Unlike UNet-based\nstructures that could utilize self/cross-attention maps for semantic editing,\nMM-DiTs inherently lack support for explicit and consistent incorporated text\nguidance, resulting in semantic misalignment between the edited results and\ntexts. In this study, we disclose the sensitivity of different attention heads\nto different image semantics within MM-DiTs and introduce HeadRouter, a\ntraining-free image editing framework that edits the source image by adaptively\nrouting the text guidance to different attention heads in MM-DiTs. Furthermore,\nwe present a dual-token refinement module to refine text/image token\nrepresentations for precise semantic guidance and accurate region expression.\nExperimental results on multiple benchmarks demonstrate HeadRouter's\nperformance in terms of editing fidelity and image quality.\n","authors":["Yu Xu","Fan Tang","Juan Cao","Yuxin Zhang","Xiaoyu Kong","Jintao Li","Oliver Deussen","Tong-Yee Lee"],"pdf_url":"https://arxiv.org/pdf/2411.15034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23054v2","updated":"2024-11-22T16:04:44Z","published":"2024-10-30T14:21:33Z","title":"Controlling Language and Diffusion Models by Transporting Activations","summary":"  The increasing capabilities of large generative models and their ever more\nwidespread deployment have raised concerns about their reliability, safety, and\npotential misuse. To address these issues, recent works have proposed to\ncontrol model generation by steering model activations in order to effectively\ninduce or prevent the emergence of concepts or behaviors in the generated\noutput. In this paper we introduce Activation Transport (AcT), a general\nframework to steer activations guided by optimal transport theory that\ngeneralizes many previous activation-steering works. AcT is modality-agnostic\nand provides fine-grained control over the model behavior with negligible\ncomputational overhead, while minimally impacting model abilities. We\nexperimentally show the effectiveness and versatility of our approach by\naddressing key challenges in large language models (LLMs) and text-to-image\ndiffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate\ntoxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is,\nwe show how AcT enables fine-grained style control and concept negation.\n","authors":["Pau Rodriguez","Arno Blaas","Michal Klein","Luca Zappella","Nicholas Apostoloff","Marco Cuturi","Xavier Suau"],"pdf_url":"https://arxiv.org/pdf/2410.23054v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15024v1","updated":"2024-11-22T15:55:19Z","published":"2024-11-22T15:55:19Z","title":"DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models","summary":"  Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.\n","authors":["Keda Tao","Can Qin","Haoxuan You","Yang Sui","Huan Wang"],"pdf_url":"https://arxiv.org/pdf/2411.15024v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2305.03223v3","updated":"2024-11-22T15:46:06Z","published":"2023-05-05T00:57:55Z","title":"Structural Group Unfairness: Measurement and Mitigation by means of the\n  Effective Resistance","summary":"  Social networks contribute to the distribution of social capital, defined as\nthe relationships, norms of trust and reciprocity within a community or society\nthat facilitate cooperation and collective action. Therefore, better positioned\nmembers in a social network benefit from faster access to diverse information\nand higher influence on information dissemination. A variety of methods have\nbeen proposed in the literature to measure social capital at an individual\nlevel. However, there is a lack of methods to quantify social capital at a\ngroup level, which is particularly important when the groups are defined on the\ngrounds of protected attributes. To fill this gap, we propose to measure the\nsocial capital of a group of nodes by means of the effective resistance and\nemphasize the importance of considering the entire network topology. Grounded\nin spectral graph theory, we introduce three effective resistance-based\nmeasures of group social capital, namely group isolation, group diameter and\ngroup control, where the groups are defined according to the value of a\nprotected attribute. We denote the social capital disparity among different\ngroups in a network as structural group unfairness, and propose to mitigate it\nby means of a budgeted edge augmentation heuristic that systematically\nincreases the social capital of the most disadvantaged group. In experiments on\nreal-world networks, we uncover significant levels of structural group\nunfairness when using gender as the protected attribute, with females being the\nmost disadvantaged group in comparison to males. We also illustrate how our\nproposed edge augmentation approach is able to not only effectively mitigate\nthe structural group unfairness but also increase the social capital of all\ngroups in the network.\n","authors":["Adrian Arnaiz-Rodriguez","Georgina Curto","Nuria Oliver"],"pdf_url":"https://arxiv.org/pdf/2305.03223v3.pdf","comment":"Accepted at International AAAI Conference on Web and Social Media\n  (ICWSM) 2025. Please cite accordingly"},{"id":"http://arxiv.org/abs/2411.15014v1","updated":"2024-11-22T15:42:43Z","published":"2024-11-22T15:42:43Z","title":"On the Linear Speedup of Personalized Federated Reinforcement Learning\n  with Shared Representations","summary":"  Federated reinforcement learning (FedRL) enables multiple agents to\ncollaboratively learn a policy without sharing their local trajectories\ncollected during agent-environment interactions. However, in practice, the\nenvironments faced by different agents are often heterogeneous, leading to poor\nperformance by the single policy learned by existing FedRL algorithms on\nindividual agents. In this paper, we take a further step and introduce a\n\\emph{personalized} FedRL framework (PFedRL) by taking advantage of possibly\nshared common structure among agents in heterogeneous environments.\nSpecifically, we develop a class of PFedRL algorithms named PFedRL-Rep that\nlearns (1) a shared feature representation collaboratively among all agents,\nand (2) an agent-specific weight vector personalized to its local environment.\nWe analyze the convergence of PFedTD-Rep, a particular instance of the\nframework with temporal difference (TD) learning and linear representations. To\nthe best of our knowledge, we are the first to prove a linear convergence\nspeedup with respect to the number of agents in the PFedRL setting. To achieve\nthis, we show that PFedTD-Rep is an example of the federated two-timescale\nstochastic approximation with Markovian noise. Experimental results demonstrate\nthat PFedTD-Rep, along with an extension to the control setting based on deep\nQ-networks (DQN), not only improve learning in heterogeneous settings, but also\nprovide better generalization to new environments.\n","authors":["Guojun Xiong","Shufan Wang","Daniel Jiang","Jian Li"],"pdf_url":"https://arxiv.org/pdf/2411.15014v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.10851v2","updated":"2024-11-22T15:41:13Z","published":"2023-04-21T09:52:19Z","title":"What Do GNNs Actually Learn? Towards Understanding their Representations","summary":"  In recent years, graph neural networks (GNNs) have achieved great success in\nthe field of graph representation learning. Although prior work has shed light\non the expressiveness of those models (\\ie whether they can distinguish pairs\nof non-isomorphic graphs), it is still not clear what structural information is\nencoded into the node representations that are learned by those models. In this\npaper, we address this gap by studying the node representations learned by four\nstandard GNN models. We find that some models produce identical representations\nfor all nodes, while the representations learned by other models are linked to\nsome notion of walks of specific length that start from the nodes. We establish\nLipschitz bounds for these models with respect to the number of (normalized)\nwalks. Additionally, we investigate the influence of node features on the\nlearned representations. We find that if the initial representations of all\nnodes point in the same direction, the representations learned at the $k$-th\nlayer of the models are also related to the initial features of nodes that can\nbe reached in exactly $k$ steps. We also apply our findings to understand the\nphenomenon of oversquashing that occurs in GNNs. Our theoretical analysis is\nvalidated through experiments on synthetic and real-world datasets.\n","authors":["Giannis Nikolentzos","Michail Chatzianastasis","Michalis Vazirgiannis"],"pdf_url":"https://arxiv.org/pdf/2304.10851v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15007v1","updated":"2024-11-22T15:31:20Z","published":"2024-11-22T15:31:20Z","title":"FTA generation using GenAI with an Autonomy sensor Usecase","summary":"  Functional safety forms an important aspect in the design of systems. Its\nemphasis on the automotive industry has evolved significantly over the years.\nTill date many methods have been developed to get appropriate FTA(Fault Tree\nanalysis) for various scenarios and features pertaining to Autonomous Driving.\nThis paper is an attempt to explore the scope of using Generative Artificial\nIntelligence(GenAI) in order to develop Fault Tree Analysis(FTA) with the use\ncase of malfunction for the Lidar sensor in mind. We explore various available\nopen source Large Language Models(LLM) models and then dive deep into one of\nthem to study its responses and provide our analysis. This paper successfully\nshows the possibility to train existing Large Language models through Prompt\nEngineering for fault tree analysis for any Autonomy usecase aided with\nPlantUML tool.\n","authors":["Sneha Sudhir Shetiya","Divya Garikapati","Veeraja Sohoni"],"pdf_url":"https://arxiv.org/pdf/2411.15007v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13485v2","updated":"2024-11-22T15:24:07Z","published":"2024-11-20T17:35:21Z","title":"Utilizing Large Language Models to Synthesize Product Desirability\n  Datasets","summary":"  This research explores the application of large language models (LLMs) to\ngenerate synthetic datasets for Product Desirability Toolkit (PDT) testing, a\nkey component in evaluating user sentiment and product experience. Utilizing\ngpt-4o-mini, a cost-effective alternative to larger commercial LLMs, three\nmethods, Word+Review, Review+Word, and Supply-Word, were each used to\nsynthesize 1000 product reviews. The generated datasets were assessed for\nsentiment alignment, textual diversity, and data generation cost. Results\ndemonstrated high sentiment alignment across all methods, with Pearson\ncorrelations ranging from 0.93 to 0.97. Supply-Word exhibited the highest\ndiversity and coverage of PDT terms, although with increased generation costs.\nDespite minor biases toward positive sentiments, in situations with limited\ntest data, LLM-generated synthetic data offers significant advantages,\nincluding scalability, cost savings, and flexibility in dataset production.\n","authors":["John D. Hastings","Sherri Weitl-Harms","Joseph Doty","Zachary J. Myers","Warren Thompson"],"pdf_url":"https://arxiv.org/pdf/2411.13485v2.pdf","comment":"9 pages, 2 figures, 6 tables, updated author list"},{"id":"http://arxiv.org/abs/2411.15002v1","updated":"2024-11-22T15:19:40Z","published":"2024-11-22T15:19:40Z","title":"A New Way: Kronecker-Factored Approximate Curvature Deep Hedging and its\n  Benefits","summary":"  This paper advances the computational efficiency of Deep Hedging frameworks\nthrough the novel integration of Kronecker-Factored Approximate Curvature\n(K-FAC) optimization. While recent literature has established Deep Hedging as a\ndata-driven alternative to traditional risk management strategies, the\ncomputational burden of training neural networks with first-order methods\nremains a significant impediment to practical implementation. The proposed\narchitecture couples Long Short-Term Memory (LSTM) networks with K-FAC\nsecond-order optimization, specifically addressing the challenges of sequential\nfinancial data and curvature estimation in recurrent networks. Empirical\nvalidation using simulated paths from a calibrated Heston stochastic volatility\nmodel demonstrates that the K-FAC implementation achieves marked improvements\nin convergence dynamics and hedging efficacy. The methodology yields a 78.3%\nreduction in transaction costs ($t = 56.88$, $p < 0.001$) and a 34.4% decrease\nin profit and loss (P&L) variance compared to Adam optimization. Moreover, the\nK-FAC-enhanced model exhibits superior risk-adjusted performance with a Sharpe\nratio of 0.0401, contrasting with $-0.0025$ for the baseline model. These\nresults provide compelling evidence that second-order optimization methods can\nmaterially enhance the tractability of Deep Hedging implementations. The\nfindings contribute to the growing literature on computational methods in\nquantitative finance while highlighting the potential for advanced optimization\ntechniques to bridge the gap between theoretical frameworks and practical\napplications in financial markets.\n","authors":["Tsogt-Ochir Enkhbayar"],"pdf_url":"https://arxiv.org/pdf/2411.15002v1.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2411.14991v1","updated":"2024-11-22T15:01:44Z","published":"2024-11-22T15:01:44Z","title":"Free Energy Projective Simulation (FEPS): Active inference with\n  interpretability","summary":"  In the last decade, the free energy principle (FEP) and active inference\n(AIF) have achieved many successes connecting conceptual models of learning and\ncognition to mathematical models of perception and action. This effort is\ndriven by a multidisciplinary interest in understanding aspects of\nself-organizing complex adaptive systems, including elements of agency. Various\nreinforcement learning (RL) models performing active inference have been\nproposed and trained on standard RL tasks using deep neural networks. Recent\nwork has focused on improving such agents' performance in complex environments\nby incorporating the latest machine learning techniques. In this paper, we take\nan alternative approach. Within the constraints imposed by the FEP and AIF, we\nattempt to model agents in an interpretable way without deep neural networks by\nintroducing Free Energy Projective Simulation (FEPS). Using internal rewards\nonly, FEPS agents build a representation of their partially observable\nenvironments with which they interact. Following AIF, the policy to achieve a\ngiven task is derived from this world model by minimizing the expected free\nenergy. Leveraging the interpretability of the model, techniques are introduced\nto deal with long-term goals and reduce prediction errors caused by erroneous\nhidden state estimation. We test the FEPS model on two RL environments inspired\nfrom behavioral biology: a timed response task and a navigation task in a\npartially observable grid. Our results show that FEPS agents fully resolve the\nambiguity of both environments by appropriately contextualizing their\nobservations based on prediction accuracy only. In addition, they infer optimal\npolicies flexibly for any target observation in the environment.\n","authors":["Joséphine Pazem","Marius Krumm","Alexander Q. Vining","Lukas J. Fiderer","Hans J. Briegel"],"pdf_url":"https://arxiv.org/pdf/2411.14991v1.pdf","comment":"26 pages (including 5 pages appendix), 6 figures"},{"id":"http://arxiv.org/abs/2404.13736v2","updated":"2024-11-22T15:01:12Z","published":"2024-04-21T18:24:34Z","title":"Interval Abstractions for Robust Counterfactual Explanations","summary":"  Counterfactual Explanations (CEs) have emerged as a major paradigm in\nexplainable AI research, providing recourse recommendations for users affected\nby the decisions of machine learning models. However, CEs found by existing\nmethods often become invalid when slight changes occur in the parameters of the\nmodel they were generated for. The literature lacks a way to provide exhaustive\nrobustness guarantees for CEs under model changes, in that existing methods to\nimprove CEs' robustness are mostly heuristic, and the robustness performances\nare evaluated empirically using only a limited number of retrained models. To\nbridge this gap, we propose a novel interval abstraction technique for\nparametric machine learning models, which allows us to obtain provable\nrobustness guarantees for CEs under a possibly infinite set of plausible model\nchanges $\\Delta$. Based on this idea, we formalise a robustness notion for CEs,\nwhich we call $\\Delta$-robustness, in both binary and multi-class\nclassification settings. We present procedures to verify $\\Delta$-robustness\nbased on Mixed Integer Linear Programming, using which we further propose\nalgorithms to generate CEs that are $\\Delta$-robust. In an extensive empirical\nstudy involving neural networks and logistic regression models, we demonstrate\nthe practical applicability of our approach. We discuss two strategies for\ndetermining the appropriate hyperparameters in our method, and we\nquantitatively benchmark CEs generated by eleven methods, highlighting the\neffectiveness of our algorithms in finding robust CEs.\n","authors":["Junqi Jiang","Francesco Leofante","Antonio Rago","Francesca Toni"],"pdf_url":"https://arxiv.org/pdf/2404.13736v2.pdf","comment":"Published in Artificial Intelligence Journal"},{"id":"http://arxiv.org/abs/2411.14984v1","updated":"2024-11-22T14:44:51Z","published":"2024-11-22T14:44:51Z","title":"Adaptive Group Robust Ensemble Knowledge Distillation","summary":"  Neural networks can learn spurious correlations in the data, often leading to\nperformance disparity for underrepresented subgroups. Studies have demonstrated\nthat the disparity is amplified when knowledge is distilled from a complex\nteacher model to a relatively \"simple\" student model. Prior work has shown that\nensemble deep learning methods can improve the performance of the worst-case\nsubgroups; however, it is unclear if this advantage carries over when\ndistilling knowledge from an ensemble of teachers, especially when the teacher\nmodels are debiased. This study demonstrates that traditional ensemble\nknowledge distillation can significantly drop the performance of the worst-case\nsubgroups in the distilled student model even when the teacher models are\ndebiased. To overcome this, we propose Adaptive Group Robust Ensemble Knowledge\nDistillation (AGRE-KD), a simple ensembling strategy to ensure that the student\nmodel receives knowledge beneficial for unknown underrepresented subgroups.\nLeveraging an additional biased model, our method selectively chooses teachers\nwhose knowledge would better improve the worst-performing subgroups by\nupweighting the teachers with gradient directions deviating from the biased\nmodel. Our experiments on several datasets demonstrate the superiority of the\nproposed ensemble distillation technique and show that it can even outperform\nclassic model ensembles based on majority voting.\n","authors":["Patrik Kenfack","Ulrich Aïvodji","Samira Ebrahimi Kahou"],"pdf_url":"https://arxiv.org/pdf/2411.14984v1.pdf","comment":"Workshop Algorithmic Fairness through the Lens of Metrics and\n  Evaluation at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2407.16877v2","updated":"2024-11-22T14:35:53Z","published":"2024-07-23T22:57:23Z","title":"Neural Network-Based Bandit: A Medium Access Control for the IIoT Alarm\n  Scenario","summary":"  Efficient Random Access (RA) is critical for enabling reliable communication\nin Industrial Internet of Things (IIoT) networks. Herein, we propose a deep\nreinforcement learning based distributed RA scheme, entitled Neural\nNetwork-Based Bandit (NNBB), for the IIoT alarm scenario. In such a scenario,\nthe devices may detect a common critical event, and the goal is to ensure the\nalarm information is delivered successfully from at least one device. The\nproposed NNBB scheme is implemented at each device, where it trains itself\nonline and establishes implicit inter-device coordination to achieve the common\ngoal. Devices can transmit simultaneously on multiple orthogonal channels and\neach possible transmission pattern constitutes a possible action for the NNBB,\nwhich uses a deep neural network to determine the action. Our simulation\nresults show that as the number of devices in the network increases, so does\nthe performance gain of the NNBB compared to the Multi-Armed Bandit (MAB) RA\nbenchmark. For instance, NNBB experiences a 7% success rate drop when there are\nfour channels and the number of devices increases from 10 to 60, while MAB\nfaces a 25% drop.\n","authors":["Prasoon Raghuwanshi","Onel Luis Alcaraz López","Neelesh B. Mehta","Hirley Alves","Matti Latva-aho"],"pdf_url":"https://arxiv.org/pdf/2407.16877v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14972v1","updated":"2024-11-22T14:27:59Z","published":"2024-11-22T14:27:59Z","title":"Open-Amp: Synthetic Data Framework for Audio Effect Foundation Models","summary":"  This paper introduces Open-Amp, a synthetic data framework for generating\nlarge-scale and diverse audio effects data. Audio effects are relevant to many\nmusical audio processing and Music Information Retrieval (MIR) tasks, such as\nmodelling of analog audio effects, automatic mixing, tone matching and\ntranscription. Existing audio effects datasets are limited in scope, usually\nincluding relatively few audio effects processors and a limited amount of input\naudio signals. Our proposed framework overcomes these issues, by crowdsourcing\nneural network emulations of guitar amplifiers and effects, created by users of\nopen-source audio effects emulation software. This allows users of Open-Amp\ncomplete control over the input signals to be processed by the effects models,\nas well as providing high-quality emulations of hundreds of devices. Open-Amp\ncan render audio online during training, allowing great flexibility in data\naugmentation. Our experiments show that using Open-Amp to train a guitar\neffects encoder achieves new state-of-the-art results on multiple guitar\neffects classification tasks. Furthermore, we train a one-to-many guitar\neffects model using Open-Amp, and use it to emulate unseen analog effects via\nmanipulation of its learned latent space, indicating transferability to analog\nguitar effects data.\n","authors":["Alec Wright","Alistair Carson","Lauri Juvela"],"pdf_url":"https://arxiv.org/pdf/2411.14972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14971v1","updated":"2024-11-22T14:27:27Z","published":"2024-11-22T14:27:27Z","title":"Leveraging LLMs for Legacy Code Modernization: Challenges and\n  Opportunities for LLM-Generated Documentation","summary":"  Legacy software systems, written in outdated languages like MUMPS and\nmainframe assembly, pose challenges in efficiency, maintenance, staffing, and\nsecurity. While LLMs offer promise for modernizing these systems, their ability\nto understand legacy languages is largely unknown. This paper investigates the\nutilization of LLMs to generate documentation for legacy code using two\ndatasets: an electronic health records (EHR) system in MUMPS and open-source\napplications in IBM mainframe Assembly Language Code (ALC). We propose a\nprompting strategy for generating line-wise code comments and a rubric to\nevaluate their completeness, readability, usefulness, and hallucination. Our\nstudy assesses the correlation between human evaluations and automated metrics,\nsuch as code complexity and reference-based metrics. We find that LLM-generated\ncomments for MUMPS and ALC are generally hallucination-free, complete,\nreadable, and useful compared to ground-truth comments, though ALC poses\nchallenges. However, no automated metrics strongly correlate with comment\nquality to predict or measure LLM performance. Our findings highlight the\nlimitations of current automated measures and the need for better evaluation\nmetrics for LLM-generated documentation in legacy systems.\n","authors":["Colin Diggs","Michael Doyle","Amit Madan","Siggy Scott","Emily Escamilla","Jacob Zimmer","Naveed Nekoo","Paul Ursino","Michael Bartholf","Zachary Robin","Anand Patel","Chris Glasz","William Macke","Paul Kirk","Jasper Phillips","Arun Sridharan","Doug Wendt","Scott Rosen","Nitin Naik","Justin F. Brunelle","Samruddhi Thaker"],"pdf_url":"https://arxiv.org/pdf/2411.14971v1.pdf","comment":"Abbreviated version submitted to LLM4Code 2025 (a workshop co-located\n  with ICSE 2025), 13 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.14961v1","updated":"2024-11-22T14:19:01Z","published":"2024-11-22T14:19:01Z","title":"LoRA-FAIR: Federated LoRA Fine-Tuning with Aggregation and\n  Initialization Refinement","summary":"  Foundation models (FMs) achieve strong performance across diverse tasks with\ntask-specific fine-tuning, yet full parameter fine-tuning is often\ncomputationally prohibitive for large models. Parameter-efficient fine-tuning\n(PEFT) methods like Low-Rank Adaptation (LoRA) reduce this cost by introducing\nlow-rank matrices for tuning fewer parameters. While LoRA allows for efficient\nfine-tuning, it requires significant data for adaptation, making Federated\nLearning (FL) an appealing solution due to its privacy-preserving collaborative\nframework. However, combining LoRA with FL introduces two key challenges: the\n\\textbf{Server-Side LoRA Aggregation Bias}, where server-side averaging of LoRA\nmatrices diverges from the ideal global update, and the \\textbf{Client-Side\nLoRA Initialization Drift}, emphasizing the need for consistent initialization\nacross rounds. Existing approaches address these challenges individually,\nlimiting their effectiveness. We propose LoRA-FAIR, a novel method that tackles\nboth issues by introducing a correction term on the server while keeping the\noriginal LoRA modules, enhancing aggregation efficiency and accuracy. LoRA-FAIR\nmaintains computational and communication efficiency, yielding superior\nperformance over state-of-the-art methods. Experimental results on ViT and\nMLP-Mixer models across large-scale datasets demonstrate that LoRA-FAIR\nconsistently achieves performance improvements in FL settings.\n","authors":["Jieming Bian","Lei Wang","Letian Zhang","Jie Xu"],"pdf_url":"https://arxiv.org/pdf/2411.14961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14953v1","updated":"2024-11-22T14:12:35Z","published":"2024-11-22T14:12:35Z","title":"Evaluating Vision Transformer Models for Visual Quality Control in\n  Industrial Manufacturing","summary":"  One of the most promising use-cases for machine learning in industrial\nmanufacturing is the early detection of defective products using a quality\ncontrol system. Such a system can save costs and reduces human errors due to\nthe monotonous nature of visual inspections. Today, a rich body of research\nexists which employs machine learning methods to identify rare defective\nproducts in unbalanced visual quality control datasets. These methods typically\nrely on two components: A visual backbone to capture the features of the input\nimage and an anomaly detection algorithm that decides if these features are\nwithin an expected distribution. With the rise of transformer architecture as\nvisual backbones of choice, there exists now a great variety of different\ncombinations of these two components, ranging all along the trade-off between\ndetection quality and inference time. Facing this variety, practitioners in the\nfield often have to spend a considerable amount of time on researching the\nright combination for their use-case at hand. Our contribution is to help\npractitioners with this choice by reviewing and evaluating current vision\ntransformer models together with anomaly detection methods. For this, we chose\nSotA models of both disciplines, combined them and evaluated them towards the\ngoal of having small, fast and efficient anomaly detection models suitable for\nindustrial manufacturing. We evaluated the results of our experiments on the\nwell-known MVTecAD and BTAD datasets. Moreover, we give guidelines for choosing\na suitable model architecture for a quality control system in practice,\nconsidering given use-case and hardware constraints.\n","authors":["Miriam Alber","Christoph Hönes","Patrick Baier"],"pdf_url":"https://arxiv.org/pdf/2411.14953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14946v1","updated":"2024-11-22T13:57:56Z","published":"2024-11-22T13:57:56Z","title":"Reliable Evaluation of Attribution Maps in CNNs: A Perturbation-Based\n  Approach","summary":"  In this paper, we present an approach for evaluating attribution maps, which\nplay a central role in interpreting the predictions of convolutional neural\nnetworks (CNNs). We show that the widely used insertion/deletion metrics are\nsusceptible to distribution shifts that affect the reliability of the ranking.\nOur method proposes to replace pixel modifications with adversarial\nperturbations, which provides a more robust evaluation framework. By using\nsmoothness and monotonicity measures, we illustrate the effectiveness of our\napproach in correcting distribution shifts. In addition, we conduct the most\ncomprehensive quantitative and qualitative assessment of attribution maps to\ndate. Introducing baseline attribution maps as sanity checks, we find that our\nmetric is the only contender to pass all checks. Using Kendall's $\\tau$ rank\ncorrelation coefficient, we show the increased consistency of our metric across\n15 dataset-architecture combinations. Of the 16 attribution maps tested, our\nresults clearly show SmoothGrad to be the best map currently available. This\nresearch makes an important contribution to the development of attribution maps\nby providing a reliable and consistent evaluation framework. To ensure\nreproducibility, we will provide the code along with our results.\n","authors":["Lars Nieradzik","Henrike Stephani","Janis Keuper"],"pdf_url":"https://arxiv.org/pdf/2411.14946v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14942v1","updated":"2024-11-22T13:54:52Z","published":"2024-11-22T13:54:52Z","title":"Comparative Study of Neural Network Methods for Solving Topological\n  Solitons","summary":"  Topological solitons, which are stable, localized solutions of nonlinear\ndifferential equations, are crucial in various fields of physics and\nmathematics, including particle physics and cosmology. However, solving these\nsolitons presents significant challenges due to the complexity of the\nunderlying equations and the computational resources required for accurate\nsolutions. To address this, we have developed a novel method using neural\nnetwork (NN) to efficiently solve solitons. A similar NN approach is\nPhysics-Informed Neural Networks (PINN). In a comparative analysis between our\nmethod and PINN, we find that our method achieves shorter computation times\nwhile maintaining the same level of accuracy. This advancement in computational\nefficiency not only overcomes current limitations but also opens new avenues\nfor studying topological solitons and their dynamical behavior.\n","authors":["Koji Hashimoto","Koshiro Matsuo","Masaki Murata","Gakuto Ogiwara"],"pdf_url":"https://arxiv.org/pdf/2411.14942v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2411.14939v1","updated":"2024-11-22T13:50:56Z","published":"2024-11-22T13:50:56Z","title":"Many happy returns: machine learning to support platelet issuing and\n  waste reduction in hospital blood banks","summary":"  Efforts to reduce platelet wastage in hospital blood banks have focused on\nordering policies, but the predominant practice of issuing the oldest unit\nfirst may not be optimal when some units are returned unused. We propose a\nnovel, machine learning (ML)-guided issuing policy to increase the likelihood\nof returned units being reissued before expiration. Our ML model trained to\npredict returns on 17,297 requests for platelets gave AUROC 0.74 on 9,353\nheld-out requests. Prior to ML model development we built a simulation of the\nblood bank operation that incorporated returns to understand the scale of\nbenefits of such a model. Using our trained model in the simulation gave an\nestimated reduction in wastage of 14%. Our partner hospital is considering\nadopting our approach, which would be particularly beneficial for hospitals\nwith higher return rates and where units have a shorter remaining useful life\non arrival.\n","authors":["Joseph Farrington","Samah Alimam","Martin Utley","Kezhi Li","Wai Keong Wong"],"pdf_url":"https://arxiv.org/pdf/2411.14939v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14937v1","updated":"2024-11-22T13:49:56Z","published":"2024-11-22T13:49:56Z","title":"Geminio: Language-Guided Gradient Inversion Attacks in Federated\n  Learning","summary":"  Foundation models that bridge vision and language have made significant\nprogress, inspiring numerous life-enriching applications. However, their\npotential for misuse to introduce new threats remains largely unexplored. This\npaper reveals that vision-language models (VLMs) can be exploited to overcome\nlongstanding limitations in gradient inversion attacks (GIAs) within federated\nlearning (FL), where an FL server reconstructs private data samples from\ngradients shared by victim clients. Current GIAs face challenges in\nreconstructing high-resolution images, especially when the victim has a large\nlocal data batch. While focusing reconstruction on valuable samples rather than\nthe entire batch is promising, existing methods lack the flexibility to allow\nattackers to specify their target data. In this paper, we introduce Geminio,\nthe first approach to transform GIAs into semantically meaningful, targeted\nattacks. Geminio enables a brand new privacy attack experience: attackers can\ndescribe, in natural language, the types of data they consider valuable, and\nGeminio will prioritize reconstruction to focus on those high-value samples.\nThis is achieved by leveraging a pretrained VLM to guide the optimization of a\nmalicious global model that, when shared with and optimized by a victim,\nretains only gradients of samples that match the attacker-specified query.\nExtensive experiments demonstrate Geminio's effectiveness in pinpointing and\nreconstructing targeted samples, with high success rates across complex\ndatasets under FL and large batch sizes and showing resilience against existing\ndefenses.\n","authors":["Junjie Shan","Ziqi Zhao","Jialin Lu","Rui Zhang","Siu Ming Yiu","Ka-Ho Chow"],"pdf_url":"https://arxiv.org/pdf/2411.14937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14927v1","updated":"2024-11-22T13:34:29Z","published":"2024-11-22T13:34:29Z","title":"LiDAR-based End-to-end Temporal Perception for Vehicle-Infrastructure\n  Cooperation","summary":"  Temporal perception, the ability to detect and track objects over time, is\ncritical in autonomous driving for maintaining a comprehensive understanding of\ndynamic environments. However, this task is hindered by significant challenges,\nincluding incomplete perception caused by occluded objects and observational\nblind spots, which are common in single-vehicle perception systems. To address\nthese issues, we introduce LET-VIC, a LiDAR-based End-to-End Tracking framework\nfor Vehicle-Infrastructure Cooperation (VIC). LET-VIC leverages\nVehicle-to-Everything (V2X) communication to enhance temporal perception by\nfusing spatial and temporal data from both vehicle and infrastructure sensors.\nFirst, it spatially integrates Bird's Eye View (BEV) features from vehicle-side\nand infrastructure-side LiDAR data, creating a comprehensive view that\nmitigates occlusions and compensates for blind spots. Second, LET-VIC\nincorporates temporal context across frames, allowing the model to leverage\nhistorical data for enhanced tracking stability and accuracy. To further\nimprove robustness, LET-VIC includes a Calibration Error Compensation (CEC)\nmodule to address sensor misalignments and ensure precise feature alignment.\nExperiments on the V2X-Seq-SPD dataset demonstrate that LET-VIC significantly\noutperforms baseline models, achieving at least a 13.7% improvement in mAP and\na 13.1% improvement in AMOTA without considering communication delays. This\nwork offers a practical solution and a new research direction for advancing\ntemporal perception in autonomous driving through vehicle-infrastructure\ncooperation.\n","authors":["Zhenwei Yang","Jilei Mao","Wenxian Yang","Yibo Ai","Yu Kong","Haibao Yu","Weidong Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14927v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2411.14923v1","updated":"2024-11-22T13:27:07Z","published":"2024-11-22T13:27:07Z","title":"Predictive Modeling For Real-Time Personalized Health Monitoring in\n  Muscular Dystrophy Management","summary":"  Muscular Dystrophy is a group of genetic disorders that progressively affect\nthe strength and functioning of muscles, thereby affecting millions of people\nworldwide. The lifetime nature of MD requires continuous follow-up care due to\nits progressive nature. This conceptual paper proposes an Internet of\nThings-based system to support the management of MD through remote,\nmulti-dimensional monitoring of patients in order to provide real-time health\nstatus updates. Traditional methods have failed to give actionable data in real\ntime, hence denying healthcare providers the opportunity to make evidence-based\ndecisions. Technology-driven approaches are urgently needed to provide deep\ninsights into disease progression and patient health. It aims to enhance\ntreatment strategies, enabling patients to better manage their condition and\ngiving healthcare professionals more confidence in their management decisions.\n","authors":["Mohammed Akkaoui"],"pdf_url":"https://arxiv.org/pdf/2411.14923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13187v3","updated":"2024-11-22T13:05:40Z","published":"2024-11-20T10:40:08Z","title":"Engagement-Driven Content Generation with Large Language Models","summary":"  Large Language Models (LLMs) exhibit significant persuasion capabilities in\none-on-one interactions, but their influence within social networks remains\nunderexplored. This study investigates the potential social impact of LLMs in\nthese environments, where interconnected users and complex opinion dynamics\npose unique challenges. In particular, we address the following research\nquestion: can LLMs learn to generate meaningful content that maximizes user\nengagement on social networks?\n  To answer this question, we define a pipeline to guide the LLM-based content\ngeneration which employs reinforcement learning with simulated feedback. In our\nframework, the reward is based on an engagement model borrowed from the\nliterature on opinion dynamics and information propagation. Moreover, we force\nthe text generated by the LLM to be aligned with a given topic and to satisfy a\nminimum fluency requirement.\n  Using our framework, we analyze the capabilities and limitations of LLMs in\ntackling the given task, specifically considering the relative positions of the\nLLM as an agent within the social network and the distribution of opinions in\nthe network on the given topic. Our findings show the full potential of LLMs in\ncreating social engagement. Notable properties of our approach are that the\nlearning procedure is adaptive to the opinion distribution of the underlying\nnetwork and agnostic to the specifics of the engagement model, which is\nembedded as a plug-and-play component. In this regard, our approach can be\neasily refined for more complex engagement tasks and interventions in\ncomputational social science.\n  The code used for the experiments is publicly available at\nhttps://anonymous.4open.science/r/EDCG/.\n","authors":["Erica Coppolillo","Federico Cinus","Marco Minici","Francesco Bonchi","Giuseppe Manco"],"pdf_url":"https://arxiv.org/pdf/2411.13187v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14904v1","updated":"2024-11-22T13:01:36Z","published":"2024-11-22T13:01:36Z","title":"Exploring Kolmogorov-Arnold Networks for Interpretable Time Series\n  Classification","summary":"  Time series classification is a relevant step supporting decision-making\nprocesses in various domains, and deep neural models have shown promising\nperformance.\n  Despite significant advancements in deep learning, the theoretical\nunderstanding of how and why complex architectures function remains limited,\nprompting the need for more interpretable models. Recently, the\nKolmogorov-Arnold Networks (KANs) have been proposed as a more interpretable\nalternative. While KAN-related research is significantly rising, to date, the\nstudy of KAN architectures for time series classification has been limited.\n  In this paper, we aim to conduct a comprehensive and robust exploration of\nthe KAN architecture for time series classification on the UCR benchmark. More\nspecifically, we look at a) how reference architectures for forecasting\ntransfer to classification, at the b) hyperparameter and implementation\ninfluence on the classification performance in view of finding the one that\nperforms best on the selected benchmark, the c) complexity trade-offs and d)\ninterpretability advantages. Our results show that (1) Efficient KAN\noutperforms MLP in performance and computational efficiency, showcasing its\nsuitability for tasks classification tasks. (2) Efficient KAN is more stable\nthan KAN across grid sizes, depths, and layer configurations, particularly with\nlower learning rates. (3) KAN maintains competitive accuracy compared to\nstate-of-the-art models like HIVE-COTE2, with smaller architectures and faster\ntraining times, supporting its balance of performance and transparency. (4) The\ninterpretability of the KAN model aligns with findings from SHAP analysis,\nreinforcing its capacity for transparent decision-making.\n","authors":["Irina Barašin","Blaž Bertalanič","Miha Mohorčič","Carolina Fortuna"],"pdf_url":"https://arxiv.org/pdf/2411.14904v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16485v2","updated":"2024-11-22T12:58:21Z","published":"2024-07-23T14:00:18Z","title":"Learning General Continuous Constraint from Demonstrations via\n  Positive-Unlabeled Learning","summary":"  Planning for a wide range of real-world tasks necessitates to know and write\nall constraints. However, instances exist where these constraints are either\nunknown or challenging to specify accurately. A possible solution is to infer\nthe unknown constraints from expert demonstration. The majority of prior works\nlimit themselves to learning simple linear constraints, or require strong\nknowledge of the true constraint parameterization or environmental model. To\nmitigate these problems, this paper presents a positive-unlabeled (PU) learning\napproach to infer a continuous, arbitrary and possibly nonlinear, constraint\nfrom demonstration. From a PU learning view, We treat all data in\ndemonstrations as positive (feasible) data, and learn a (sub)-optimal policy to\ngenerate high-reward-winning but potentially infeasible trajectories, which\nserve as unlabeled data containing both feasible and infeasible states. Under\nan assumption on data distribution, a feasible-infeasible classifier (i.e.,\nconstraint model) is learned from the two datasets through a postprocessing PU\nlearning technique. The entire method employs an iterative framework\nalternating between updating the policy, which generates and selects\nhigher-reward policies, and updating the constraint model. Additionally, a\nmemory buffer is introduced to record and reuse samples from previous\niterations to prevent forgetting. The effectiveness of the proposed method is\nvalidated in two Mujoco environments, successfully inferring continuous\nnonlinear constraints and outperforming a baseline method in terms of\nconstraint accuracy and policy safety.\n","authors":["Baiyu Peng","Aude Billard"],"pdf_url":"https://arxiv.org/pdf/2407.16485v2.pdf","comment":"The paper is hastily uploaded. We prefer to improve it and upload it\n  later, and possibily after it is published"},{"id":"http://arxiv.org/abs/2404.17916v2","updated":"2024-11-22T12:51:47Z","published":"2024-04-27T14:05:18Z","title":"FedCRL: Personalized Federated Learning with Contrastive Shared\n  Representations for Label Heterogeneity in Non-IID Data","summary":"  Heterogeneity resulting from label distribution skew and data scarcity can\nlead to inaccuracy and unfairness in intelligent communication applications\nthat mainly rely on distributed computing. To deal with it, this paper proposes\na novel personalized federated learning algorithm, named Federated Contrastive\nShareable Representations (FedCoSR), to facilitate knowledge sharing among\nclients while maintaining data privacy. Specifically, parameters of local\nmodels' shallow layers and typical local representations are both considered\nshareable information for the server and aggregated globally. To address poor\nperformance caused by label distribution skew among clients, contrastive\nlearning is adopted between local and global representations to enrich local\nknowledge. Additionally, to ensure fairness for clients with scarce data,\nFedCoSR introduces adaptive local aggregation to coordinate the global model\ninvolvement in each client. Our simulations demonstrate FedCoSR's effectiveness\nin mitigating label heterogeneity by achieving accuracy and fairness\nimprovements over existing methods on datasets with varying degrees of label\nheterogeneity.\n","authors":["Chenghao Huang","Xiaolu Chen","Yanru Zhang","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2404.17916v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.05857v3","updated":"2024-11-22T12:39:18Z","published":"2023-06-09T12:39:41Z","title":"How Sparse Can We Prune A Deep Network: A Fundamental Limit Viewpoint","summary":"  Network pruning is a commonly used measure to alleviate the storage and\ncomputational burden of deep neural networks. However, the fundamental limit of\nnetwork pruning is still lacking. To close the gap, in this work we'll take a\nfirst-principles approach, i.e. we'll directly impose the sparsity constraint\non the loss function and leverage the framework of statistical dimension in\nconvex geometry, thus we're able to characterize the sharp phase transition\npoint, i.e. the fundamental limit of the pruning ratio. Through this limit,\nwe're able to identify two key factors that determine the pruning ratio limit,\nnamely, weight magnitude and network sharpness. Generally speaking, the flatter\nthe loss landscape or the smaller the weight magnitude, the smaller pruning\nratio. Moreover, we provide efficient countermeasures to address the challenges\nin the computation of the pruning limit, which involves accurate spectrum\nestimation of a large-scale and non-positive Hessian matrix. Moreover, through\nthe lens of the pruning ratio threshold, we can provide rigorous\ninterpretations on several heuristics in existing pruning algorithms. Extensive\nexperiments are performed that demonstrate that our theoretical pruning ratio\nthreshold coincides very well with the experiments. All codes are available at:\nhttps://github.com/QiaozheZhang/Global-One-shot-Pruning\n","authors":["Qiaozhe Zhang","Ruijie Zhang","Jun Sun","Yingzhuang Liu"],"pdf_url":"https://arxiv.org/pdf/2306.05857v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12214v2","updated":"2024-11-22T12:30:38Z","published":"2023-11-20T22:08:17Z","title":"Random Fourier Signature Features","summary":"  Tensor algebras give rise to one of the most powerful measures of similarity\nfor sequences of arbitrary length called the signature kernel accompanied with\nattractive theoretical guarantees from stochastic analysis. Previous algorithms\nto compute the signature kernel scale quadratically in terms of the length and\nthe number of the sequences. To mitigate this severe computational bottleneck,\nwe develop a random Fourier feature-based acceleration of the signature kernel\nacting on the inherently non-Euclidean domain of sequences. We show uniform\napproximation guarantees for the proposed unbiased estimator of the signature\nkernel, while keeping its computation linear in the sequence length and number.\nIn addition, combined with recent advances on tensor projections, we derive two\neven more scalable time series features with favourable concentration\nproperties and computational complexity both in time and memory. Our empirical\nresults show that the reduction in computational cost comes at a negligible\nprice in terms of accuracy on moderate-sized datasets, and it enables one to\nscale to large datasets up to a million time series.\n","authors":["Csaba Toth","Harald Oberhauser","Zoltan Szabo"],"pdf_url":"https://arxiv.org/pdf/2311.12214v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.07473v2","updated":"2024-11-22T12:22:39Z","published":"2023-01-18T12:30:44Z","title":"Discrete Latent Structure in Neural Networks","summary":"  Many types of data from fields including natural language processing,\ncomputer vision, and bioinformatics, are well represented by discrete,\ncompositional structures such as trees, sequences, or matchings. Latent\nstructure models are a powerful tool for learning to extract such\nrepresentations, offering a way to incorporate structural bias, discover\ninsight about the data, and interpret decisions. However, effective training is\nchallenging, as neural networks are typically designed for continuous\ncomputation.\n  This text explores three broad strategies for learning with discrete latent\nstructure: continuous relaxation, surrogate gradients, and probabilistic\nestimation. Our presentation relies on consistent notations for a wide range of\nmodels. As such, we reveal many new connections between latent structure\nlearning strategies, showing how most consist of the same small set of\nfundamental building blocks, but use them differently, leading to substantially\ndifferent applicability and properties.\n","authors":["Vlad Niculae","Caio F. Corro","Nikita Nangia","Tsvetomila Mihaylova","André F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2301.07473v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14886v1","updated":"2024-11-22T12:10:03Z","published":"2024-11-22T12:10:03Z","title":"CardioLab: Laboratory Values Estimation and Monitoring from\n  Electrocardiogram Signals -- A Multimodal Deep Learning Approach","summary":"  Background: Laboratory values are fundamental to medical diagnosis and\nmanagement, but acquiring these values can be costly, invasive, and\ntime-consuming. While electrocardiogram (ECG) patterns have been linked to\ncertain laboratory abnormalities, the comprehensive modeling of these\nrelationships remains underexplored.\n  Methods: We utilize MIMIC-IV dataset to develop multimodal deep-learning\nmodels to demonstrate the feasibility of estimating (real-time) and monitoring\n(predict at future intervals) laboratory value abnormalities from ECG\nwaveforms, demographics, biometrics, and vital signs.\n  Results: The models exhibit a strong predictive performance with AUROC scores\nabove 0.70 in a statistically significant manner for 23 laboratory values in\nthe estimation setting and up to 26 values in the monitoring setting. Most\nnotably, the accurately predictable values encompassing abnormalities across\ndiverse physiological categories such as cardiac, renal, hematological,\nmetabolic, immunological and coagulation. To name examples, for estimation\nNTproBNP (>353 pg/mL) with 0.882, whereas for monitoring at 30 minutes Urea\nnitrogen (<6 mg/dL) with 0.851, at 60 minutes creatinine (<0.5 mg/dL) with\n0.85, and at 120 minutes hemoglobin (>17.5 g/dL) with 0.821.\n  Conclusions: This study provides first evidence for the feasibility of using\nECG data alongside clinical routine data for the real-time estimation and\nmonitoring of laboratory value abnormalities, which could provide a\nnon-invasive, cost-effective supplement to traditional laboratory testing, with\nstrong implications for enhanced patient monitoring and early intervention.\nFurther validation could facilitate their integration into routine clinical\npractice.\n","authors":["Juan Miguel Lopez Alcaraz","Nils Strodthoff"],"pdf_url":"https://arxiv.org/pdf/2411.14886v1.pdf","comment":"7 pages, 1 figure, code under\n  https://github.com/AI4HealthUOL/CardioLab"},{"id":"http://arxiv.org/abs/2411.14883v1","updated":"2024-11-22T12:06:24Z","published":"2024-11-22T12:06:24Z","title":"Boundless Across Domains: A New Paradigm of Adaptive Feature and\n  Cross-Attention for Domain Generalization in Medical Image Segmentation","summary":"  Domain-invariant representation learning is a powerful method for domain\ngeneralization. Previous approaches face challenges such as high computational\ndemands, training instability, and limited effectiveness with high-dimensional\ndata, potentially leading to the loss of valuable features. To address these\nissues, we hypothesize that an ideal generalized representation should exhibit\nsimilar pattern responses within the same channel across cross-domain images.\nBased on this hypothesis, we use deep features from the source domain as\nqueries, and deep features from the generated domain as keys and values.\nThrough a cross-channel attention mechanism, the original deep features are\nreconstructed into robust regularization representations, forming an explicit\nconstraint that guides the model to learn domain-invariant representations.\nAdditionally, style augmentation is another common method. However, existing\nmethods typically generate new styles through convex combinations of source\ndomains, which limits the diversity of training samples by confining the\ngenerated styles to the original distribution. To overcome this limitation, we\npropose an Adaptive Feature Blending (AFB) method that generates\nout-of-distribution samples while exploring the in-distribution space,\nsignificantly expanding the domain range. Extensive experimental results\ndemonstrate that our proposed methods achieve superior performance on two\nstandard domain generalization benchmarks for medical image segmentation.\n","authors":["Yuheng Xu","Taiping Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14883v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.14875v1","updated":"2024-11-22T11:55:37Z","published":"2024-11-22T11:55:37Z","title":"Iterative Reweighted Framework Based Algorithms for Sparse Linear\n  Regression with Generalized Elastic Net Penalty","summary":"  The elastic net penalty is frequently employed in high-dimensional statistics\nfor parameter regression and variable selection. It is particularly beneficial\ncompared to lasso when the number of predictors greatly surpasses the number of\nobservations. However, empirical evidence has shown that the $\\ell_q$-norm\npenalty (where $0 < q < 1$) often provides better regression compared to the\n$\\ell_1$-norm penalty, demonstrating enhanced robustness in various scenarios.\nIn this paper, we explore a generalized elastic net model that employs a\n$\\ell_r$-norm (where $r \\geq 1$) in loss function to accommodate various types\nof noise, and employs a $\\ell_q$-norm (where $0 < q < 1$) to replace the\n$\\ell_1$-norm in elastic net penalty. Theoretically, we establish the\ncomputable lower bounds for the nonzero entries of the generalized first-order\nstationary points of the proposed generalized elastic net model. For\nimplementation, we develop two efficient algorithms based on the locally\nLipschitz continuous $\\epsilon$-approximation to $\\ell_q$-norm. The first\nalgorithm employs an alternating direction method of multipliers (ADMM), while\nthe second utilizes a proximal majorization-minimization method (PMM), where\nthe subproblems are addressed using the semismooth Newton method (SNN). We also\nperform extensive numerical experiments with both simulated and real data,\nshowing that both algorithms demonstrate superior performance. Notably, the\nPMM-SSN is efficient than ADMM, even though the latter provides a simpler\nimplementation.\n","authors":["Yanyun Ding","Zhenghua Yao","Peili Li","Yunhai Xiao"],"pdf_url":"https://arxiv.org/pdf/2411.14875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14871v1","updated":"2024-11-22T11:45:33Z","published":"2024-11-22T11:45:33Z","title":"Prioritize Denoising Steps on Diffusion Model Preference Alignment via\n  Explicit Denoised Distribution Estimation","summary":"  Diffusion models have shown remarkable success in text-to-image generation,\nmaking alignment methods for these models increasingly important. A key\nchallenge is the sparsity of preference labels, which are typically available\nonly at the terminal of denoising trajectories. This raises the issue of how to\nassign credit across denoising steps based on these sparse labels. In this\npaper, we propose Denoised Distribution Estimation (DDE), a novel method for\ncredit assignment. Unlike previous approaches that rely on auxiliary models or\nhand-crafted schemes, DDE derives its strategy more explicitly. The proposed\nDDE directly estimates the terminal denoised distribution from the perspective\nof each step. It is equipped with two estimation strategies and capable of\nrepresenting the entire denoising trajectory with a single model inference.\nTheoretically and empirically, we show that DDE prioritizes optimizing the\nmiddle part of the denoising trajectory, resulting in a novel and effective\ncredit assignment scheme. Extensive experiments demonstrate that our approach\nachieves superior performance, both quantitatively and qualitatively.\n","authors":["Dingyuan Shi","Yong Wang","Hangyu Li","Xiangxiang Chu"],"pdf_url":"https://arxiv.org/pdf/2411.14871v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14870v1","updated":"2024-11-22T11:43:39Z","published":"2024-11-22T11:43:39Z","title":"Application of AI to formal methods -- an analysis of current trends","summary":"  With artificial intelligence (AI) being well established within the daily\nlives of research communities, we turn our gaze toward an application area that\nappears intuitively unsuited for probabilistic decision-making: the area of\nformal methods (FM). FM aim to provide sound and understandable reasoning about\nproblems in computer science, which seemingly collides with the black-box\nnature that inhibits many AI approaches. However, many researchers have crossed\nthis gap and applied AI techniques to enhance FM approaches. As this dichotomy\nof FM and AI sparked our interest, we conducted a systematic mapping study to\nmap the current landscape of research publications. In this study, we\ninvestigate the previous five years of applied AI to FM (2019-2023), as these\ncorrespond to periods of high activity. This investigation results in 189\nentries, which we explore in more detail to find current trends, highlight\nresearch gaps, and give suggestions for future research.\n","authors":["Sebastian Stock","Jannik Dunkelau","Atif Mashkoor"],"pdf_url":"https://arxiv.org/pdf/2411.14870v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14869v1","updated":"2024-11-22T11:35:42Z","published":"2024-11-22T11:35:42Z","title":"BIP3D: Bridging 2D Images and 3D Perception for Embodied Intelligence","summary":"  In embodied intelligence systems, a key component is 3D perception algorithm,\nwhich enables agents to understand their surrounding environments. Previous\nalgorithms primarily rely on point cloud, which, despite offering precise\ngeometric information, still constrain perception performance due to inherent\nsparsity, noise, and data scarcity. In this work, we introduce a novel\nimage-centric 3D perception model, BIP3D, which leverages expressive image\nfeatures with explicit 3D position encoding to overcome the limitations of\npoint-centric methods. Specifically, we leverage pre-trained 2D vision\nfoundation models to enhance semantic understanding, and introduce a spatial\nenhancer module to improve spatial understanding. Together, these modules\nenable BIP3D to achieve multi-view, multi-modal feature fusion and end-to-end\n3D perception. In our experiments, BIP3D outperforms current state-of-the-art\nresults on the EmbodiedScan benchmark, achieving improvements of 5.69% in the\n3D detection task and 15.25% in the 3D visual grounding task.\n","authors":["Xuewu Lin","Tianwei Lin","Lichao Huang","Hongyu Xie","Zhizhong Su"],"pdf_url":"https://arxiv.org/pdf/2411.14869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19345v2","updated":"2024-11-22T11:24:39Z","published":"2024-09-28T13:24:11Z","title":"Unveil Benign Overfitting for Transformer in Vision: Training Dynamics,\n  Convergence, and Generalization","summary":"  Transformers have demonstrated great power in the recent development of large\nfoundational models. In particular, the Vision Transformer (ViT) has brought\nrevolutionary changes to the field of vision, achieving significant\naccomplishments on the experimental side. However, their theoretical\ncapabilities, particularly in terms of generalization when trained to overfit\ntraining data, are still not fully understood. To address this gap, this work\ndelves deeply into the benign overfitting perspective of transformers in\nvision. To this end, we study the optimization of a Transformer composed of a\nself-attention layer with softmax followed by a fully connected layer under\ngradient descent on a certain data distribution model. By developing techniques\nthat address the challenges posed by softmax and the interdependent nature of\nmultiple weights in transformer optimization, we successfully characterized the\ntraining dynamics and achieved generalization in post-training. Our results\nestablish a sharp condition that can distinguish between the small test error\nphase and the large test error regime, based on the signal-to-noise ratio in\nthe data model. The theoretical results are further verified by experimental\nsimulation. To the best of our knowledge, this is the first work to\ncharacterize benign overfitting for Transformers.\n","authors":["Jiarui Jiang","Wei Huang","Miao Zhang","Taiji Suzuki","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2409.19345v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14863v1","updated":"2024-11-22T11:24:14Z","published":"2024-11-22T11:24:14Z","title":"Latent Schrodinger Bridge: Prompting Latent Diffusion for Fast Unpaired\n  Image-to-Image Translation","summary":"  Diffusion models (DMs), which enable both image generation from noise and\ninversion from data, have inspired powerful unpaired image-to-image (I2I)\ntranslation algorithms. However, they often require a larger number of neural\nfunction evaluations (NFEs), limiting their practical applicability. In this\npaper, we tackle this problem with Schrodinger Bridges (SBs), which are\nstochastic differential equations (SDEs) between distributions with minimal\ntransport cost. We analyze the probability flow ordinary differential equation\n(ODE) formulation of SBs, and observe that we can decompose its vector field\ninto a linear combination of source predictor, target predictor, and noise\npredictor. Inspired by this observation, we propose Latent Schrodinger Bridges\n(LSBs) that approximate the SB ODE via pre-trained Stable Diffusion, and\ndevelop appropriate prompt optimization and change of variables formula to\nmatch the training and inference between distributions. We demonstrate that our\nalgorithm successfully conduct competitive I2I translation in unsupervised\nsetting with only a fraction of computation cost required by previous DM-based\nI2I methods.\n","authors":["Jeongsol Kim","Beomsu Kim","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2411.14863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12603v2","updated":"2024-11-22T11:21:50Z","published":"2024-11-19T16:06:32Z","title":"STREAM: A Universal State-Space Model for Sparse Geometric Data","summary":"  Handling sparse and unstructured geometric data, such as point clouds or\nevent-based vision, is a pressing challenge in the field of machine vision.\nRecently, sequence models such as Transformers and state-space models entered\nthe domain of geometric data. These methods require specialized preprocessing\nto create a sequential view of a set of points. Furthermore, prior works\ninvolving sequence models iterate geometric data with either uniform or learned\nstep sizes, implicitly relying on the model to infer the underlying geometric\nstructure. In this work, we propose to encode geometric structure explicitly\ninto the parameterization of a state-space model. State-space models are based\non linear dynamics governed by a one-dimensional variable such as time or a\nspatial coordinate. We exploit this dynamic variable to inject relative\ndifferences of coordinates into the step size of the state-space model. The\nresulting geometric operation computes interactions between all pairs of N\npoints in O(N) steps. Our model deploys the Mamba selective state-space model\nwith a modified CUDA kernel to efficiently map sparse geometric data to modern\nhardware. The resulting sequence model, which we call STREAM, achieves\ncompetitive results on a range of benchmarks from point-cloud classification to\nevent-based vision and audio classification. STREAM demonstrates a powerful\ninductive bias for sparse geometric data by improving the PointMamba baseline\nwhen trained from scratch on the ModelNet40 and ScanObjectNN point cloud\nanalysis datasets. It further achieves, for the first time, 100% test accuracy\non all 11 classes of the DVS128 Gestures dataset.\n","authors":["Mark Schöne","Yash Bhisikar","Karan Bania","Khaleelulla Khan Nazeer","Christian Mayr","Anand Subramoney","David Kappel"],"pdf_url":"https://arxiv.org/pdf/2411.12603v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14860v1","updated":"2024-11-22T11:18:20Z","published":"2024-11-22T11:18:20Z","title":"Ex Uno Pluria: Insights on Ensembling in Low Precision Number Systems","summary":"  While ensembling deep neural networks has shown promise in improving\ngeneralization performance, scaling current ensemble methods for large models\nremains challenging. Given that recent progress in deep learning is largely\ndriven by the scale, exemplified by the widespread adoption of large-scale\nneural network architectures, scalability emerges an increasingly critical\nissue for machine learning algorithms in the era of large-scale models. In this\nwork, we first showcase the potential of low precision ensembling, where\nensemble members are derived from a single model within low precision number\nsystems in a training-free manner. Our empirical analysis demonstrates the\neffectiveness of our proposed low precision ensembling method compared to\nexisting ensemble approaches.\n","authors":["Giung Nam","Juho Lee"],"pdf_url":"https://arxiv.org/pdf/2411.14860v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.14855v1","updated":"2024-11-22T11:13:33Z","published":"2024-11-22T11:13:33Z","title":"Applications of fractional calculus in learned optimization","summary":"  Fractional gradient descent has been studied extensively, with a focus on its\nability to extend traditional gradient descent methods by incorporating\nfractional-order derivatives. This approach allows for more flexibility in\nnavigating complex optimization landscapes and offers advantages in certain\ntypes of problems, particularly those involving non-linearities and chaotic\ndynamics. Yet, the challenge of fine-tuning the fractional order parameters\nremains unsolved. In this work, we demonstrate that it is possible to train a\nneural network to predict the order of the gradient effectively.\n","authors":["Teodor Alexandru Szente","James Harrison","Mihai Zanfir","Cristian Sminchisescu"],"pdf_url":"https://arxiv.org/pdf/2411.14855v1.pdf","comment":"NeurIPS Workshop on Optimization for Machine Learning"},{"id":"http://arxiv.org/abs/2303.00515v7","updated":"2024-11-22T10:28:44Z","published":"2023-02-28T04:37:26Z","title":"Interpretable Water Level Forecaster with Spatiotemporal Causal\n  Attention Mechanisms","summary":"  Accurate forecasting of river water levels is vital for effectively managing\ntraffic flow and mitigating the risks associated with natural disasters. This\ntask presents challenges due to the intricate factors influencing the flow of a\nriver. Recent advances in machine learning have introduced numerous effective\nforecasting methods. However, these methods lack interpretability due to their\ncomplex structure, resulting in limited reliability. Addressing this issue,\nthis study proposes a deep learning model that quantifies interpretability,\nwith an emphasis on water level forecasting. This model focuses on generating\nquantitative interpretability measurements, which align with the common\nknowledge embedded in the input data. This is facilitated by the utilization of\na transformer architecture that is purposefully designed with masking,\nincorporating a multi-layer network that captures spatiotemporal causation. We\nperform a comparative analysis on the Han River dataset obtained from Seoul,\nSouth Korea, from 2016 to 2021. The results illustrate that our approach offers\nenhanced interpretability consistent with common knowledge, outperforming\ncompeting methods and also enhances robustness against distribution shift.\n","authors":["Sunghcul Hong","Yunjin Choi","Jong-June Jeon"],"pdf_url":"https://arxiv.org/pdf/2303.00515v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12872v2","updated":"2024-11-22T10:26:27Z","published":"2024-11-19T21:34:50Z","title":"From Text to Pose to Image: Improving Diffusion Model Control and\n  Quality","summary":"  In the last two years, text-to-image diffusion models have become extremely\npopular. As their quality and usage increase, a major concern has been the need\nfor better output control. In addition to prompt engineering, one effective\nmethod to improve the controllability of diffusion models has been to condition\nthem on additional modalities such as image style, depth map, or keypoints.\nThis forms the basis of ControlNets or Adapters. When attempting to apply these\nmethods to control human poses in outputs of text-to-image diffusion models,\ntwo main challenges have arisen. The first challenge is generating poses\nfollowing a wide range of semantic text descriptions, for which previous\nmethods involved searching for a pose within a dataset of (caption, pose)\npairs. The second challenge is conditioning image generation on a specified\npose while keeping both high aesthetic and high pose fidelity. In this article,\nwe fix these two main issues by introducing a text-to-pose (T2P) generative\nmodel alongside a new sampling algorithm, and a new pose adapter that\nincorporates more pose keypoints for higher pose fidelity. Together, these two\nnew state-of-the-art models enable, for the first time, a generative\ntext-to-pose-to-image framework for higher pose control in diffusion models. We\nrelease all models and the code used for the experiments at\nhttps://github.com/clement-bonnet/text-to-pose.\n","authors":["Clément Bonnet","Ariel N. Lee","Franck Wertel","Antoine Tamano","Tanguy Cizain","Pablo Ducru"],"pdf_url":"https://arxiv.org/pdf/2411.12872v2.pdf","comment":"Published at the NeurIPS 2024 Workshop on Compositional Learning:\n  Perspectives, Methods, and Paths Forward"},{"id":"http://arxiv.org/abs/2411.14839v1","updated":"2024-11-22T10:25:44Z","published":"2024-11-22T10:25:44Z","title":"Bayesian dynamic mode decomposition for real-time ship motion digital\n  twinning","summary":"  Digital twins are widely considered enablers of groundbreaking changes in the\ndevelopment, operation, and maintenance of novel generations of products. They\nare meant to provide reliable and timely predictions to inform decisions along\nthe entire product life cycle. One of their most interesting applications in\nthe naval field is the digital twinning of ship performances in waves, a\ncrucial aspect in design and operation safety. In this paper, a Bayesian\nextension of the Hankel dynamic mode decomposition method is proposed for ship\nmotion's nowcasting as a prediction tool for naval digital twins. The proposed\nalgorithm meets all the requirements for formulations devoted to digital\ntwinning, being able to adapt the resulting models with the data incoming from\nthe physical system, using a limited amount of data, producing real-time\npredictions, and estimating their reliability. Results are presented and\ndiscussed for the course-keeping of the 5415M model in beam-quartering sea\nstate 7 irregular waves at Fr = 0.33, using data from three different CFD\nsolvers. The results show predictions keeping good accuracy levels up to five\nwave encounter periods, with the Bayesian formulation improving the\ndeterministic forecasts. In addition, a connection between the predicted\nuncertainty and prediction accuracy is found.\n","authors":["Giorgio Palma","Andrea Serani","Kevin McTaggart","Shawn Aram","David W. Wundrow","David Drazen","Matteo Diez"],"pdf_url":"https://arxiv.org/pdf/2411.14839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.17368v2","updated":"2024-11-22T10:25:01Z","published":"2023-09-29T16:17:12Z","title":"Machine Learning for Practical Quantum Error Mitigation","summary":"  Quantum computers progress toward outperforming classical supercomputers, but\nquantum errors remain their primary obstacle. The key to overcoming errors on\nnear-term devices has emerged through the field of quantum error mitigation,\nenabling improved accuracy at the cost of additional run time. Here, through\nexperiments on state-of-the-art quantum computers using up to 100 qubits, we\ndemonstrate that without sacrificing accuracy machine learning for quantum\nerror mitigation (ML-QEM) drastically reduces the cost of mitigation. We\nbenchmark ML-QEM using a variety of machine learning models -- linear\nregression, random forests, multi-layer perceptrons, and graph neural networks\n-- on diverse classes of quantum circuits, over increasingly complex\ndevice-noise profiles, under interpolation and extrapolation, and in both\nnumerics and experiments. These tests employ the popular digital zero-noise\nextrapolation method as an added reference. Finally, we propose a path toward\nscalable mitigation by using ML-QEM to mimic traditional mitigation methods\nwith superior runtime efficiency. Our results show that classical machine\nlearning can extend the reach and practicality of quantum error mitigation by\nreducing its overheads and highlight its broader potential for practical\nquantum computations.\n","authors":["Haoran Liao","Derek S. Wang","Iskandar Sitdikov","Ciro Salcedo","Alireza Seif","Zlatko K. Minev"],"pdf_url":"https://arxiv.org/pdf/2309.17368v2.pdf","comment":"11 pages, 7 figures (main text) + 9 pages, 4 figures (supplementary\n  information)"},{"id":"http://arxiv.org/abs/2411.14834v1","updated":"2024-11-22T10:17:32Z","published":"2024-11-22T10:17:32Z","title":"Gradient Masking All-at-Once: Ensemble Everything Everywhere Is Not\n  Robust","summary":"  Ensemble everything everywhere is a defense to adversarial examples that was\nrecently proposed to make image classifiers robust. This defense works by\nensembling a model's intermediate representations at multiple noisy image\nresolutions, producing a single robust classification. This defense was shown\nto be effective against multiple state-of-the-art attacks. Perhaps even more\nconvincingly, it was shown that the model's gradients are perceptually aligned:\nattacks against the model produce noise that perceptually resembles the\ntargeted class.\n  In this short note, we show that this defense is not robust to adversarial\nattack. We first show that the defense's randomness and ensembling method cause\nsevere gradient masking. We then use standard adaptive attack techniques to\nreduce the defense's robust accuracy from 48% to 1% on CIFAR-100 and from 62%\nto 4% on CIFAR-10, under the $\\ell_\\infty$-norm threat model with\n$\\varepsilon=8/255$.\n","authors":["Jie Zhang","Kristina Nikolić","Nicholas Carlini","Florian Tramèr"],"pdf_url":"https://arxiv.org/pdf/2411.14834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14832v1","updated":"2024-11-22T10:10:53Z","published":"2024-11-22T10:10:53Z","title":"VisGraphVar: A Benchmark Generator for Assessing Variability in Graph\n  Analysis Using Large Vision-Language Models","summary":"  The fast advancement of Large Vision-Language Models (LVLMs) has shown\nimmense potential. These models are increasingly capable of tackling abstract\nvisual tasks. Geometric structures, particularly graphs with their inherent\nflexibility and complexity, serve as an excellent benchmark for evaluating\nthese models' predictive capabilities. While human observers can readily\nidentify subtle visual details and perform accurate analyses, our investigation\nreveals that state-of-the-art LVLMs exhibit consistent limitations in specific\nvisual graph scenarios, especially when confronted with stylistic variations.\nIn response to these challenges, we introduce VisGraphVar (Visual Graph\nVariability), a customizable benchmark generator able to produce graph images\nfor seven distinct task categories (detection, classification, segmentation,\npattern recognition, link prediction, reasoning, matching), designed to\nsystematically evaluate the strengths and limitations of individual LVLMs. We\nuse VisGraphVar to produce 990 graph images and evaluate six LVLMs, employing\ntwo distinct prompting strategies, namely zero-shot and chain-of-thought. The\nfindings demonstrate that variations in visual attributes of images (e.g., node\nlabeling and layout) and the deliberate inclusion of visual imperfections, such\nas overlapping nodes, significantly affect model performance. This research\nemphasizes the importance of a comprehensive evaluation across graph-related\ntasks, extending beyond reasoning alone. VisGraphVar offers valuable insights\nto guide the development of more reliable and robust systems capable of\nperforming advanced visual graph analysis.\n","authors":["Camilo Chacón Sartori","Christian Blum","Filippo Bistaffa"],"pdf_url":"https://arxiv.org/pdf/2411.14832v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14827v1","updated":"2024-11-22T10:07:02Z","published":"2024-11-22T10:07:02Z","title":"Physically Interpretable Probabilistic Domain Characterization","summary":"  Characterizing domains is essential for models analyzing dynamic\nenvironments, as it allows them to adapt to evolving conditions or to hand the\ntask over to backup systems when facing conditions outside their operational\ndomain. Existing solutions typically characterize a domain by solving a\nregression or classification problem, which limits their applicability as they\nonly provide a limited summarized description of the domain. In this paper, we\npresent a novel approach to domain characterization by characterizing domains\nas probability distributions. Particularly, we develop a method to predict the\nlikelihood of different weather conditions from images captured by\nvehicle-mounted cameras by estimating distributions of physical parameters\nusing normalizing flows. To validate our proposed approach, we conduct\nexperiments within the context of autonomous vehicles, focusing on predicting\nthe distribution of weather parameters to characterize the operational domain.\nThis domain is characterized by physical parameters (absolute characterization)\nand arbitrarily predefined domains (relative characterization). Finally, we\nevaluate whether a system can safely operate in a target domain by comparing it\nto multiple source domains where safety has already been established. This\napproach holds significant potential, as accurate weather prediction and\neffective domain adaptation are crucial for autonomous systems to adjust to\ndynamic environmental conditions.\n","authors":["Anaïs Halin","Sébastien Piérard","Renaud Vandeghen","Benoît Gérin","Maxime Zanella","Martin Colot","Jan Held","Anthony Cioppa","Emmanuel Jean","Gianluca Bontempi","Saïd Mahmoudi","Benoît Macq","Marc Van Droogenbroeck"],"pdf_url":"https://arxiv.org/pdf/2411.14827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08162v3","updated":"2024-11-22T09:47:27Z","published":"2022-12-15T21:50:54Z","title":"Huber-energy measure quantization","summary":"  We describe a measure quantization procedure i.e., an algorithm which finds\nthe best approximation of a target probability law (and more generally signed\nfinite variation measure) by a sum of $Q$ Dirac masses ($Q$ being the\nquantization parameter). The procedure is implemented by minimizing the\nstatistical distance between the original measure and its quantized version;\nthe distance is built from a negative definite kernel and, if necessary, can be\ncomputed on the fly and feed to a stochastic optimization algorithm (such as\nSGD, Adam, ...). We investigate theoretically the fundamental questions of\nexistence of the optimal measure quantizer and identify what are the required\nkernel properties that guarantee suitable behavior. We propose two best linear\nunbiased (BLUE) estimators for the squared statistical distance and use them in\nan unbiased procedure, called HEMQ, to find the optimal quantization. We test\nHEMQ on several databases: multi-dimensional Gaussian mixtures, Wiener space\ncubature, Italian wine cultivars and the MNIST image database. The results\nindicate that the HEMQ algorithm is robust and versatile and, for the class of\nHuber-energy kernels, matches the expected intuitive behavior.\n","authors":["Gabriel Turinici"],"pdf_url":"https://arxiv.org/pdf/2212.08162v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14823v1","updated":"2024-11-22T09:44:13Z","published":"2024-11-22T09:44:13Z","title":"Omni-IML: Towards Unified Image Manipulation Localization","summary":"  Image manipulation can lead to misinterpretation of visual content, posing\nsignificant risks to information security. Image Manipulation Localization\n(IML) has thus received increasing attention. However, existing IML methods\nrely heavily on task-specific designs, making them perform well only on one\ntarget image type but are mostly random guessing on other image types, and even\njoint training on multiple image types causes significant performance\ndegradation. This hinders the deployment for real applications as it notably\nincreases maintenance costs and the misclassification of image types leads to\nserious error accumulation. To this end, we propose Omni-IML, the first\ngeneralist model to unify diverse IML tasks. Specifically, Omni-IML achieves\ngeneralism by adopting the Modal Gate Encoder and the Dynamic Weight Decoder to\nadaptively determine the optimal encoding modality and the optimal decoder\nfilters for each sample. We additionally propose an Anomaly Enhancement module\nthat enhances the features of tampered regions with box supervision and helps\nthe generalist model to extract common features across different IML tasks. We\nvalidate our approach on IML tasks across three major scenarios: natural\nimages, document images, and face images. Without bells and whistles, our\nOmni-IML achieves state-of-the-art performance on all three tasks with a single\nunified model, providing valuable strategies and insights for real-world\napplication and future research in generalist image forensics. Our code will be\npublicly available.\n","authors":["Chenfan Qu","Yiwu Zhong","Fengjun Guo","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2411.14823v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07247v2","updated":"2024-11-22T09:17:02Z","published":"2024-03-12T02:09:39Z","title":"GuideGen: A Text-Guided Framework for Full-torso Anatomy and CT Volume\n  Generation","summary":"  The recently emerging conditional diffusion models seem promising for\nmitigating the labor and expenses in building large 3D medical imaging\ndatasets. However, previous studies on 3D CT generation have yet to fully\ncapitalize on semantic and textual conditions, and they have primarily focused\non specific organs characterized by a local structure and fixed contrast. In\nthis work, we present GuideGen, a controllable framework that generates\nanatomical masks and corresponding CT volumes for the entire torso-from chest\nto pelvis-based on free-form text prompts. Our approach includes three core\ncomponents: a text-conditional semantic synthesizer for creating realistic\nfull-torso anatomies; a contrast-aware autoencoder for detailed, high-fidelity\nfeature extraction across varying contrast levels; and a latent feature\ngenerator that ensures alignment between CT images, anatomical semantics and\ninput prompts. To train and evaluate GuideGen, we compile a multi-modality\ncancer imaging dataset with paired CT and clinical descriptions from 12 public\nTCIA datasets and one private real-world dataset. Comprehensive evaluations\nacross generation quality, cross-modality alignment, and data usability on\nmulti-organ and tumor segmentation tasks demonstrate GuideGen's superiority\nover existing CT generation methods.\n","authors":["Linrui Dai","Rongzhao Zhang","Yongrui Yu","Xiaofan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.07247v2.pdf","comment":"submitted to CVPR2025"},{"id":"http://arxiv.org/abs/2411.14811v1","updated":"2024-11-22T09:12:02Z","published":"2024-11-22T09:12:02Z","title":"Fine-Grained Alignment in Vision-and-Language Navigation through\n  Bayesian Optimization","summary":"  This paper addresses the challenge of fine-grained alignment in\nVision-and-Language Navigation (VLN) tasks, where robots navigate realistic 3D\nenvironments based on natural language instructions. Current approaches use\ncontrastive learning to align language with visual trajectory sequences.\nNevertheless, they encounter difficulties with fine-grained vision negatives.\nTo enhance cross-modal embeddings, we introduce a novel Bayesian\nOptimization-based adversarial optimization framework for creating fine-grained\ncontrastive vision samples. To validate the proposed methodology, we conduct a\nseries of experiments to assess the effectiveness of the enriched embeddings on\nfine-grained vision negatives. We conduct experiments on two common VLN\nbenchmarks R2R and REVERIE, experiments on the them demonstrate that these\nembeddings benefit navigation, and can lead to a promising performance\nenhancement. Our source code and trained models are available at:\nhttps://anonymous.4open.science/r/FGVLN.\n","authors":["Yuhang Song","Mario Gianni","Chenguang Yang","Kunyang Lin","Te-Chuan Chiu","Anh Nguyen","Chun-Yi Lee"],"pdf_url":"https://arxiv.org/pdf/2411.14811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14808v1","updated":"2024-11-22T09:08:58Z","published":"2024-11-22T09:08:58Z","title":"High-Resolution Image Synthesis via Next-Token Prediction","summary":"  Denoising with a Joint-Embedding Predictive Architecture (D-JEPA), an\nautoregressive model, has demonstrated outstanding performance in\nclass-conditional image generation. However, the application of next-token\nprediction in high-resolution text-to-image generation remains underexplored.\nIn this paper, we introduce D-JEPA$\\cdot$T2I, an extension of D-JEPA\nincorporating flow matching loss, designed to enable data-efficient continuous\nresolution learning. D-JEPA$\\cdot$T2I leverages a multimodal visual transformer\nto effectively integrate textual and visual features and adopts Visual Rotary\nPositional Embedding (VoPE) to facilitate continuous resolution learning.\nFurthermore, we devise a data feedback mechanism that significantly enhances\ndata utilization efficiency. For the first time, we achieve state-of-the-art\n\\textbf{high-resolution} image synthesis via next-token prediction.\n  The experimental code and pretrained models will be open-sourced at\n\\url{https://d-jepa.github.io/t2i}.\n","authors":["Dengsheng Chen","Jie Hu","Tiezhu Yue","Xiaoming Wei"],"pdf_url":"https://arxiv.org/pdf/2411.14808v1.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2411.14807v1","updated":"2024-11-22T09:08:36Z","published":"2024-11-22T09:08:36Z","title":"Harlequin: Color-driven Generation of Synthetic Data for Referring\n  Expression Comprehension","summary":"  Referring Expression Comprehension (REC) aims to identify a particular object\nin a scene by a natural language expression, and is an important topic in\nvisual language understanding. State-of-the-art methods for this task are based\non deep learning, which generally requires expensive and manually labeled\nannotations. Some works tackle the problem with limited-supervision learning or\nrelying on Large Vision and Language Models. However, the development of\ntechniques to synthesize labeled data is overlooked. In this paper, we propose\na novel framework that generates artificial data for the REC task, taking into\naccount both textual and visual modalities. At first, our pipeline processes\nexisting data to create variations in the annotations. Then, it generates an\nimage using altered annotations as guidance. The result of this pipeline is a\nnew dataset, called Harlequin, made by more than 1M queries. This approach\neliminates manual data collection and annotation, enabling scalability and\nfacilitating arbitrary complexity. We pre-train three REC models on Harlequin,\nthen fine-tuned and evaluated on human-annotated datasets. Our experiments show\nthat the pre-training on artificial data is beneficial for performance.\n","authors":["Luca Parolari","Elena Izzo","Lamberto Ballan"],"pdf_url":"https://arxiv.org/pdf/2411.14807v1.pdf","comment":"Accepted to ICPR 2024"},{"id":"http://arxiv.org/abs/2411.06387v2","updated":"2024-11-22T08:54:17Z","published":"2024-11-10T08:11:05Z","title":"Self-Training Meets Consistency: Improving LLMs' Reasoning With\n  Consistency-Driven Rationale Evaluation","summary":"  Self-training approach for large language models (LLMs) improves reasoning\nabilities by training the models on their self-generated rationales. Previous\napproaches have labeled rationales that produce correct answers for a given\nquestion as appropriate for training. However, a single measure risks\nmisjudging rationale quality, leading the models to learn flawed reasoning\npatterns. To address this issue, we propose CREST (Consistency-driven Rationale\nEvaluation for Self-Training), a self-training framework that further evaluates\neach rationale through follow-up questions and leverages this evaluation to\nguide its training. Specifically, we introduce two methods: (1) filtering out\nrationales that frequently result in incorrect answers on follow-up questions\nand (2) preference learning based on mixed preferences from rationale\nevaluation results of both original and follow-up questions. Experiments on\nthree question-answering datasets using open LLMs show that CREST not only\nimproves the logical robustness and correctness of rationales but also improves\nreasoning abilities compared to previous self-training approaches.\n","authors":["Jaehyeok Lee","Keisuke Sakaguchi","JinYeong Bak"],"pdf_url":"https://arxiv.org/pdf/2411.06387v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2411.14798v1","updated":"2024-11-22T08:49:08Z","published":"2024-11-22T08:49:08Z","title":"Facial Features Matter: a Dynamic Watermark based Proactive Deepfake\n  Detection Approach","summary":"  Current passive deepfake face-swapping detection methods encounter\nsignificance bottlenecks in model generalization capabilities. Meanwhile,\nproactive detection methods often use fixed watermarks which lack a close\nrelationship with the content they protect and are vulnerable to security\nrisks. Dynamic watermarks based on facial features offer a promising solution,\nas these features provide unique identifiers. Therefore, this paper proposes a\nFacial Feature-based Proactive deepfake detection method (FaceProtect), which\nutilizes changes in facial characteristics during deepfake manipulation as a\nnovel detection mechanism. We introduce a GAN-based One-way Dynamic Watermark\nGenerating Mechanism (GODWGM) that uses 128-dimensional facial feature vectors\nas inputs. This method creates irreversible mappings from facial features to\nwatermarks, enhancing protection against various reverse inference attacks.\nAdditionally, we propose a Watermark-based Verification Strategy (WVS) that\ncombines steganography with GODWGM, allowing simultaneous transmission of the\nbenchmark watermark representing facial features within the image. Experimental\nresults demonstrate that our proposed method maintains exceptional detection\nperformance and exhibits high practicality on images altered by various\ndeepfake techniques.\n","authors":["Shulin Lan","Kanlin Liu","Yazhou Zhao","Chen Yang","Yingchao Wang","Xingshan Yao","Liehuang Zhu"],"pdf_url":"https://arxiv.org/pdf/2411.14798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14797v1","updated":"2024-11-22T08:48:30Z","published":"2024-11-22T08:48:30Z","title":"Continual SFT Matches Multimodal RLHF with Negative Supervision","summary":"  Multimodal RLHF usually happens after supervised finetuning (SFT) stage to\ncontinually improve vision-language models' (VLMs) comprehension. Conventional\nwisdom holds its superiority over continual SFT during this preference\nalignment stage. In this paper, we observe that the inherent value of\nmultimodal RLHF lies in its negative supervision, the logit of the rejected\nresponses. We thus propose a novel negative supervised finetuning (nSFT)\napproach that fully excavates these information resided. Our nSFT disentangles\nthis negative supervision in RLHF paradigm, and continually aligns VLMs with a\nsimple SFT loss. This is more memory efficient than multimodal RLHF where 2\n(e.g., DPO) or 4 (e.g., PPO) large VLMs are strictly required. The\neffectiveness of nSFT is rigorously proved by comparing it with various\nmultimodal RLHF approaches, across different dataset sources, base VLMs and\nevaluation metrics. Besides, fruitful of ablations are provided to support our\nhypothesis. We hope this paper will stimulate further research to properly\nalign large vision language models.\n","authors":["Ke Zhu","Yu Wang","Yanpeng Sun","Qiang Chen","Jiangjiang Liu","Gang Zhang","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2411.14797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18279v2","updated":"2024-11-22T08:45:55Z","published":"2024-06-26T12:05:49Z","title":"Improving EO Foundation Models with Confidence Assessment for enhanced\n  Semantic segmentation","summary":"  Confidence assessments of semantic segmentation algorithms are important.\nIdeally, deep learning models should have the ability to predict in advance\nwhether their output is likely to be incorrect. Assessing the confidence levels\nof model predictions in Earth Observation (EO) classification is essential, as\nit can enhance semantic segmentation performance and help prevent further\nexploitation of the results in case of erroneous prediction. The model we\ndeveloped, Confidence Assessment for enhanced Semantic segmentation (CAS),\nevaluates confidence at both the segment and pixel levels, providing both\nlabels and confidence scores as output. Our model, CAS, identifies segments\nwith incorrect predicted labels using the proposed combined confidence metric,\nrefines the model, and enhances its performance. This work has significant\napplications, particularly in evaluating EO Foundation Models on semantic\nsegmentation downstream tasks, such as land cover classification using\nSentinel-2 satellite data. The evaluation results show that this strategy is\neffective and that the proposed model CAS outperforms other baseline models.\n","authors":["Nikolaos Dionelis","Nicolas Longepe"],"pdf_url":"https://arxiv.org/pdf/2406.18279v2.pdf","comment":"5 pages, 7 figures, 4 tables, Accepted"},{"id":"http://arxiv.org/abs/2411.14796v1","updated":"2024-11-22T08:41:33Z","published":"2024-11-22T08:41:33Z","title":"Adaptive Hyper-Graph Convolution Network for Skeleton-based Human Action\n  Recognition with Virtual Connections","summary":"  The shared topology of human skeletons motivated the recent investigation of\ngraph convolutional network (GCN) solutions for action recognition. However,\nthe existing GCNs rely on the binary connection of two neighbouring vertices\n(joints) formed by an edge (bone), overlooking the potential of constructing\nmulti-vertex convolution structures. In this paper we address this oversight\nand explore the merits of a hyper-graph convolutional network (Hyper-GCN) to\nachieve the aggregation of rich semantic information conveyed by skeleton\nvertices. In particular, our Hyper-GCN adaptively optimises multi-scale\nhyper-graphs during training, revealing the action-driven multi-vertex\nrelations. Besides, virtual connections are often designed to support efficient\nfeature aggregation, implicitly extending the spectrum of dependencies within\nthe skeleton. By injecting virtual connections into hyper-graphs, the semantic\nclues of diverse action categories can be highlighted. The results of\nexperiments conducted on the NTU-60, NTU-120, and NW-UCLA datasets, demonstrate\nthe merits of our Hyper-GCN, compared to the state-of-the-art methods.\nSpecifically, we outperform the existing solutions on NTU-120, achieving 90.2\\%\nand 91.4\\% in terms of the top-1 recognition accuracy on X-Sub and X-Set.\n","authors":["Youwei Zhou","Tianyang Xu","Cong Wu","Xiaojun Wu","Josef Kittler"],"pdf_url":"https://arxiv.org/pdf/2411.14796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14789v1","updated":"2024-11-22T08:17:46Z","published":"2024-11-22T08:17:46Z","title":"Simplifying CLIP: Unleashing the Power of Large-Scale Models on\n  Consumer-level Computers","summary":"  Contrastive Language-Image Pre-training (CLIP) has attracted a surge of\nattention for its superior zero-shot performance and excellent transferability\nto downstream tasks. However, training such large-scale models usually requires\nsubstantial computation and storage, which poses barriers for general users\nwith consumer-level computers. Motivated by this observation, in this paper we\ninvestigate how to achieve competitive performance on only one Nvidia RTX3090\nGPU and with one terabyte for storing dataset. On one hand, we simplify the\ntransformer block structure and combine Weight Inheritance with multi-stage\nKnowledge Distillation (WIKD), thereby reducing the parameters and improving\nthe inference speed during training along with deployment. On the other hand,\nconfronted with the convergence challenge posed by small dataset, we generate\nsynthetic captions for each sample as data augmentation, and devise a novel\nPair Matching (PM) loss to fully exploit the distinguishment among positive and\nnegative image-text pairs. Extensive experiments demonstrate that our model can\nachieve a new state-of-the-art datascale-parameter-accuracy tradeoff, which\ncould further popularize the CLIP model in the related research community.\n","authors":["Hongbo Liu"],"pdf_url":"https://arxiv.org/pdf/2411.14789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14989v5","updated":"2024-11-22T07:57:14Z","published":"2024-02-22T22:00:03Z","title":"Stable Neural Stochastic Differential Equations in Analyzing Irregular\n  Time Series Data","summary":"  Irregular sampling intervals and missing values in real-world time series\ndata present challenges for conventional methods that assume consistent\nintervals and complete data. Neural Ordinary Differential Equations (Neural\nODEs) offer an alternative approach, utilizing neural networks combined with\nODE solvers to learn continuous latent representations through parameterized\nvector fields. Neural Stochastic Differential Equations (Neural SDEs) extend\nNeural ODEs by incorporating a diffusion term, although this addition is not\ntrivial, particularly when addressing irregular intervals and missing values.\nConsequently, careful design of drift and diffusion functions is crucial for\nmaintaining stability and enhancing performance, while incautious choices can\nresult in adverse properties such as the absence of strong solutions,\nstochastic destabilization, or unstable Euler discretizations, significantly\naffecting Neural SDEs' performance. In this study, we propose three stable\nclasses of Neural SDEs: Langevin-type SDE, Linear Noise SDE, and Geometric SDE.\nThen, we rigorously demonstrate their robustness in maintaining excellent\nperformance under distribution shift, while effectively preventing overfitting.\nTo assess the effectiveness of our approach, we conduct extensive experiments\non four benchmark datasets for interpolation, forecasting, and classification\ntasks, and analyze the robustness of our methods with 30 public datasets under\ndifferent missing rates. Our results demonstrate the efficacy of the proposed\nmethod in handling real-world irregular time series data.\n","authors":["YongKyung Oh","Dongyoung Lim","Sungil Kim"],"pdf_url":"https://arxiv.org/pdf/2402.14989v5.pdf","comment":"Published at the Twelfth International Conference on Learning\n  Representations (ICLR 2024), Spotlight presentation (Notable Top 5%).\n  https://openreview.net/forum?id=4VIgNuQ1pY"},{"id":"http://arxiv.org/abs/2411.14783v1","updated":"2024-11-22T07:52:28Z","published":"2024-11-22T07:52:28Z","title":"Segmenting Action-Value Functions Over Time-Scales in SARSA using\n  TD($Δ$)","summary":"  In numerous episodic reinforcement learning (RL) settings, SARSA-based\nmethodologies are employed to enhance policies aimed at maximizing returns over\nlong horizons. Conventional SARSA algorithms, however, have difficulties in\nbalancing bias and variation due to the reliance on a singular, fixed discount\nfactor. This study expands the temporal difference decomposition approach,\nTD($\\triangle$), to the SARSA algorithm. SARSA, a widely utilised on-policy RL\nmethod, enhances action-value functions via temporal difference updates.\nTD($\\triangle$) facilitates learning over several time-scales by breaking the\naction-value function into components associated with distinct discount\nfactors. This decomposition improves learning efficiency and stability,\nparticularly in problems necessitating long-horizon optimization. We illustrate\nthat our methodology mitigates bias in SARSA's updates while facilitating\naccelerated convergence in contexts characterized by dense rewards.\nExperimental findings across many benchmark tasks indicate that the proposed\nSARSA($\\triangle$) surpasses conventional TD learning methods in both tabular\nand deep RL contexts.\n","authors":["Mahammad Humayoo"],"pdf_url":"https://arxiv.org/pdf/2411.14783v1.pdf","comment":"17 pages. arXiv admin note: text overlap with arXiv:2411.14019"},{"id":"http://arxiv.org/abs/2406.14596v4","updated":"2024-11-22T07:43:21Z","published":"2024-06-20T17:45:02Z","title":"VLM Agents Generate Their Own Memories: Distilling Experience into\n  Embodied Programs of Thought","summary":"  Large-scale generative language and vision-language models (LLMs and VLMs)\nexcel in few-shot in-context learning for decision making and instruction\nfollowing. However, they require high-quality exemplar demonstrations in their\ncontext window. In this work, we ask: Can LLMs and VLMs generate their own\nexamples from generic, sub-optimal demonstrations? We propose In-Context\nAbstraction Learning (ICAL), a method that builds a memory of multimodal\nexperience from sub-optimal demonstrations and human feedback. Given a task\ndemonstration that may contain inefficiencies or mistakes, a VLM abstracts the\ntrajectory into a generalized program of thoughts by correcting inefficient\nactions and annotating cognitive abstractions: causal relationships, object\nstate changes, temporal subgoals, and task-relevant visual elements. These\nprograms of thought are iteratively improved through human feedback while the\nagent executes the trajectory in a similar environment. The resulting examples\nsignificantly improve decision-making in retrieval-augmented LLM and VLM\nagents. Moreover, as the agent's library of examples grows, it becomes more\nefficient, relying less on human feedback and requiring fewer environment\ninteractions per demonstration. Our ICAL agent surpasses the SOTA in\ndialogue-based instruction following in TEACh, multimodal web agents in\nVisualWebArena, and action anticipation in Ego4D. In TEACh, we achieve a 12.6%\nimprovement in goal-condition success. In VisualWebArena, our task success rate\nimproves over few-shot GPT4V. In Ego4D action forecasting, we improve over\nfew-shot GPT-4V and remain competitive with supervised models. We show\nfinetuning our retrieval-augmented in-context agent yields additional\nimprovements. Our approach significantly reduces reliance on manual prompt\nengineering and consistently outperforms in-context learning from action plans\nthat lack such programs of thought.\n","authors":["Gabriel Sarch","Lawrence Jang","Michael J. Tarr","William W. Cohen","Kenneth Marino","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2406.14596v4.pdf","comment":"Project website: http://ical-learning.github.io/"},{"id":"http://arxiv.org/abs/2305.09145v2","updated":"2024-11-22T07:23:23Z","published":"2023-05-16T03:51:34Z","title":"Deep ReLU Networks Have Surprisingly Simple Polytopes","summary":"  A ReLU network is a piecewise linear function over polytopes. Figuring out\nthe properties of such polytopes is of fundamental importance for the research\nand development of neural networks. So far, either theoretical or empirical\nstudies on polytopes only stay at the level of counting their number, which is\nfar from a complete characterization. Here, we propose to study the shapes of\npolytopes via the number of faces of the polytope. Then, by computing and\nanalyzing the histogram of faces across polytopes, we find that a ReLU network\nhas relatively simple polytopes under both initialization and gradient descent,\nalthough these polytopes can be rather diverse and complicated by a specific\ndesign. This finding can be appreciated as a kind of generalized implicit bias,\nsubjected to the intrinsic geometric constraint in space partition of a ReLU\nnetwork. Next, we perform a combinatorial analysis to explain why adding depth\ndoes not generate a more complicated polytope by bounding the average number of\nfaces of polytopes with the dimensionality. Our results concretely reveal what\nkind of simple functions a network learns and what will happen when a network\ngoes deep. Also, by characterizing the shape of polytopes, the number of faces\ncan be a novel leverage for other problems, \\textit{e.g.}, serving as a generic\ntool to explain the power of popular shortcut networks such as ResNet and\nanalyzing the impact of different regularization strategies on a network's\nspace partition.\n","authors":["Feng-Lei Fan","Wei Huang","Xiangru Zhong","Lecheng Ruan","Tieyong Zeng","Huan Xiong","Fei Wang"],"pdf_url":"https://arxiv.org/pdf/2305.09145v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14768v1","updated":"2024-11-22T07:15:46Z","published":"2024-11-22T07:15:46Z","title":"Grid and Road Expressions Are Complementary for Trajectory\n  Representation Learning","summary":"  Trajectory representation learning (TRL) maps trajectories to vectors that\ncan be used for many downstream tasks. Existing TRL methods use either grid\ntrajectories, capturing movement in free space, or road trajectories, capturing\nmovement in a road network, as input. We observe that the two types of\ntrajectories are complementary, providing either region and location\ninformation or providing road structure and movement regularity. Therefore, we\npropose a novel multimodal TRL method, dubbed GREEN, to jointly utilize Grid\nand Road trajectory Expressions for Effective representatioN learning. In\nparticular, we transform raw GPS trajectories into both grid and road\ntrajectories and tailor two encoders to capture their respective information.\nTo align the two encoders such that they complement each other, we adopt a\ncontrastive loss to encourage them to produce similar embeddings for the same\nraw trajectory and design a mask language model (MLM) loss to use grid\ntrajectories to help reconstruct masked road trajectories. To learn the final\ntrajectory representation, a dual-modal interactor is used to fuse the outputs\nof the two encoders via cross-attention. We compare GREEN with 7\nstate-of-the-art TRL methods for 3 downstream tasks, finding that GREEN\nconsistently outperforms all baselines and improves the accuracy of the\nbest-performing baseline by an average of 15.99\\%.\n","authors":["Silin Zhou","Shuo Shang","Lisi Chen","Peng Han","Christian S. Jensen"],"pdf_url":"https://arxiv.org/pdf/2411.14768v1.pdf","comment":"This paper is accepted by KDD2025(August Cycle)"},{"id":"http://arxiv.org/abs/2411.14765v1","updated":"2024-11-22T07:11:35Z","published":"2024-11-22T07:11:35Z","title":"An Attention-based Framework for Fair Contrastive Learning","summary":"  Contrastive learning has proven instrumental in learning unbiased\nrepresentations of data, especially in complex environments characterized by\nhigh-cardinality and high-dimensional sensitive information. However, existing\napproaches within this setting require predefined modelling assumptions of\nbias-causing interactions that limit the model's ability to learn debiased\nrepresentations. In this work, we propose a new method for fair contrastive\nlearning that employs an attention mechanism to model bias-causing\ninteractions, enabling the learning of a fairer and semantically richer\nembedding space. In particular, our attention mechanism avoids bias-causing\nsamples that confound the model and focuses on bias-reducing samples that help\nlearn semantically meaningful representations. We verify the advantages of our\nmethod against existing baselines in fair contrastive learning and show that\nour approach can significantly boost bias removal from learned representations\nwithout compromising downstream accuracy.\n","authors":["Stefan K. Nielsen","Tan M. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2411.14765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.09064v2","updated":"2024-11-22T07:00:06Z","published":"2024-11-13T22:44:25Z","title":"Minimax Optimal Two-Sample Testing under Local Differential Privacy","summary":"  We explore the trade-off between privacy and statistical utility in private\ntwo-sample testing under local differential privacy (LDP) for both multinomial\nand continuous data. We begin by addressing the multinomial case, where we\nintroduce private permutation tests using practical privacy mechanisms such as\nLaplace, discrete Laplace, and Google's RAPPOR. We then extend our multinomial\napproach to continuous data via binning and study its uniform separation rates\nunder LDP over H\\\"older and Besov smoothness classes. The proposed tests for\nboth discrete and continuous cases rigorously control the type I error for any\nfinite sample size, strictly adhere to LDP constraints, and achieve minimax\nseparation rates under LDP. The attained minimax rates reveal inherent\nprivacy-utility trade-offs that are unavoidable in private testing. To address\nscenarios with unknown smoothness parameters in density testing, we propose an\nadaptive test based on a Bonferroni-type approach that ensures robust\nperformance without prior knowledge of the smoothness parameters. We validate\nour theoretical findings with extensive numerical experiments and demonstrate\nthe practical relevance and effectiveness of our proposed methods.\n","authors":["Jongmin Mun","Seungwoo Kwak","Ilmun Kim"],"pdf_url":"https://arxiv.org/pdf/2411.09064v2.pdf","comment":"66 pages, 6 figures, 1 table; added a graphical illustration of\n  central and local differential privacy in Section 1, referenced the Python\n  package, fixed typos, and changed the citation style"},{"id":"http://arxiv.org/abs/2410.14742v2","updated":"2024-11-22T06:59:27Z","published":"2024-10-17T10:17:23Z","title":"ArrivalNet: Predicting City-wide Bus/Tram Arrival Time with\n  Two-dimensional Temporal Variation Modeling","summary":"  Accurate arrival time prediction (ATP) of buses and trams plays a crucial\nrole in public transport operations. Current methods focused on modeling\none-dimensional temporal information but overlooked the latent periodic\ninformation within time series. Moreover, most studies developed algorithms for\nATP based on a single or a few routes of public transport, which reduces the\ntransferability of the prediction models and their applicability in public\ntransport management systems. To this end, this paper proposes\n\\textit{ArrivalNet}, a two-dimensional temporal variation-based multi-step ATP\nfor buses and trams. It decomposes the one-dimensional temporal sequence into\nintra-periodic and inter-periodic variations, which can be recast into\ntwo-dimensional tensors (2D blocks). Each row of a tensor contains the time\npoints within a period, and each column involves the time points at the same\nintra-periodic index across various periods. The transformed 2D blocks in\ndifferent frequencies have an image-like feature representation that enables\neffective learning with computer vision backbones (e.g., convolutional neural\nnetwork). Drawing on the concept of residual neural network, the 2D block\nmodule is designed as a basic module for flexible aggregation. Meanwhile,\ncontextual factors like workdays, peak hours, and intersections, are also\nutilized in the augmented feature representation to improve the performance of\nprediction. 125 days of public transport data from Dresden were collected for\nmodel training and validation. Experimental results show that the root mean\nsquare error, mean absolute error, and mean absolute percentage error of the\nproposed predictor decrease by at least 6.1\\%, 14.7\\%, and 34.2\\% compared with\nstate-of-the-art baseline methods.\n","authors":["Zirui Li","Patrick Wolf","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2410.14742v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14762v1","updated":"2024-11-22T06:50:44Z","published":"2024-11-22T06:50:44Z","title":"Efficient Long Video Tokenization via Coordinated-based Patch\n  Reconstruction","summary":"  Efficient tokenization of videos remains a challenge in training vision\nmodels that can process long videos. One promising direction is to develop a\ntokenizer that can encode long video clips, as it would enable the tokenizer to\nleverage the temporal coherence of videos better for tokenization. However,\ntraining existing tokenizers on long videos often incurs a huge training cost\nas they are trained to reconstruct all the frames at once. In this paper, we\nintroduce CoordTok, a video tokenizer that learns a mapping from\ncoordinate-based representations to the corresponding patches of input videos,\ninspired by recent advances in 3D generative models. In particular, CoordTok\nencodes a video into factorized triplane representations and reconstructs\npatches that correspond to randomly sampled $(x,y,t)$ coordinates. This allows\nfor training large tokenizer models directly on long videos without requiring\nexcessive training resources. Our experiments show that CoordTok can\ndrastically reduce the number of tokens for encoding long video clips. For\ninstance, CoordTok can encode a 128-frame video with 128$\\times$128 resolution\ninto 1280 tokens, while baselines need 6144 or 8192 tokens to achieve similar\nreconstruction quality. We further show that this efficient video tokenization\nenables memory-efficient training of a diffusion transformer that can generate\n128 frames at once.\n","authors":["Huiwon Jang","Sihyun Yu","Jinwoo Shin","Pieter Abbeel","Younggyo Seo"],"pdf_url":"https://arxiv.org/pdf/2411.14762v1.pdf","comment":"Code is available on the project webpage:\n  https://huiwon-jang.github.io/coordtok/"},{"id":"http://arxiv.org/abs/2411.14759v1","updated":"2024-11-22T06:46:16Z","published":"2024-11-22T06:46:16Z","title":"Hammer: Towards Efficient Hot-Cold Data Identification via Online\n  Learning","summary":"  Efficient management of storage resources in big data and cloud computing\nenvironments requires accurate identification of data's \"cold\" and \"hot\"\nstates. Traditional methods, such as rule-based algorithms and early AI\ntechniques, often struggle with dynamic workloads, leading to low accuracy,\npoor adaptability, and high operational overhead. To address these issues, we\npropose a novel solution based on online learning strategies. Our approach\ndynamically adapts to changing data access patterns, achieving higher accuracy\nand lower operational costs. Rigorous testing with both synthetic and\nreal-world datasets demonstrates a significant improvement, achieving a 90%\naccuracy rate in hot-cold classification. Additionally, the computational and\nstorage overheads are considerably reduced.\n","authors":["Kai Lu","Siqi Zhao","Jiguang Wan"],"pdf_url":"https://arxiv.org/pdf/2411.14759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14752v1","updated":"2024-11-22T06:16:56Z","published":"2024-11-22T06:16:56Z","title":"Comparative Analysis of nnUNet and MedNeXt for Head and Neck Tumor\n  Segmentation in MRI-guided Radiotherapy","summary":"  Radiation therapy (RT) is essential in treating head and neck cancer (HNC),\nwith magnetic resonance imaging(MRI)-guided RT offering superior soft tissue\ncontrast and functional imaging. However, manual tumor segmentation is\ntime-consuming and complex, and therfore remains a challenge. In this study, we\npresent our solution as team TUMOR to the HNTS-MRG24 MICCAI Challenge which is\nfocused on automated segmentation of primary gross tumor volumes (GTVp) and\nmetastatic lymph node gross tumor volume (GTVn) in pre-RT and mid-RT MRI\nimages. We utilized the HNTS-MRG2024 dataset, which consists of 150 MRI scans\nfrom patients diagnosed with HNC, including original and registered pre-RT and\nmid-RT T2-weighted images with corresponding segmentation masks for GTVp and\nGTVn. We employed two state-of-the-art models in deep learning, nnUNet and\nMedNeXt. For Task 1, we pretrained models on pre-RT registered and mid-RT\nimages, followed by fine-tuning on original pre-RT images. For Task 2, we\ncombined registered pre-RT images, registered pre-RT segmentation masks, and\nmid-RT data as a multi-channel input for training. Our solution for Task 1\nachieved 1st place in the final test phase with an aggregated Dice Similarity\nCoefficient of 0.8254, and our solution for Task 2 ranked 8th with a score of\n0.7005. The proposed solution is publicly available at Github Repository.\n","authors":["Nikoo Moradi","André Ferreira","Behrus Puladi","Jens Kleesiek","Emad Fatemizadeh","Gijs Luijten","Victor Alves","Jan Egger"],"pdf_url":"https://arxiv.org/pdf/2411.14752v1.pdf","comment":"15 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.14751v1","updated":"2024-11-22T06:13:42Z","published":"2024-11-22T06:13:42Z","title":"TopoSD: Topology-Enhanced Lane Segment Perception with SDMap Prior","summary":"  Recent advances in autonomous driving systems have shifted towards reducing\nreliance on high-definition maps (HDMaps) due to the huge costs of annotation\nand maintenance. Instead, researchers are focusing on online vectorized HDMap\nconstruction using on-board sensors. However, sensor-only approaches still face\nchallenges in long-range perception due to the restricted views imposed by the\nmounting angles of onboard cameras, just as human drivers also rely on\nbird's-eye-view navigation maps for a comprehensive understanding of road\nstructures. To address these issues, we propose to train the perception model\nto \"see\" standard definition maps (SDMaps). We encode SDMap elements into\nneural spatial map representations and instance tokens, and then incorporate\nsuch complementary features as prior information to improve the bird's eye view\n(BEV) feature for lane geometry and topology decoding. Based on the lane\nsegment representation framework, the model simultaneously predicts lanes,\ncentrelines and their topology. To further enhance the ability of geometry\nprediction and topology reasoning, we also use a topology-guided decoder to\nrefine the predictions by exploiting the mutual relationships between\ntopological and geometric features. We perform extensive experiments on\nOpenLane-V2 datasets to validate the proposed method. The results show that our\nmodel outperforms state-of-the-art methods by a large margin, with gains of\n+6.7 and +9.1 on the mAP and topology metrics. Our analysis also reveals that\nmodels trained with SDMap noise augmentation exhibit enhanced robustness.\n","authors":["Sen Yang","Minyue Jiang","Ziwei Fan","Xiaolu Xie","Xiao Tan","Yingying Li","Errui Ding","Liang Wang","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2411.14751v1.pdf","comment":"17 pages, 7 figures, and 7 tables"},{"id":"http://arxiv.org/abs/2411.14750v1","updated":"2024-11-22T06:11:35Z","published":"2024-11-22T06:11:35Z","title":"Ordinal Multiple-instance Learning for Ulcerative Colitis Severity\n  Estimation with Selective Aggregated Transformer","summary":"  Patient-level diagnosis of severity in ulcerative colitis (UC) is common in\nreal clinical settings, where the most severe score in a patient is recorded.\nHowever, previous UC classification methods (i.e., image-level estimation)\nmainly assumed the input was a single image. Thus, these methods can not\nutilize severity labels recorded in real clinical settings. In this paper, we\npropose a patient-level severity estimation method by a transformer with\nselective aggregator tokens, where a severity label is estimated from multiple\nimages taken from a patient, similar to a clinical setting. Our method can\neffectively aggregate features of severe parts from a set of images captured in\neach patient, and it facilitates improving the discriminative ability between\nadjacent severity classes. Experiments demonstrate the effectiveness of the\nproposed method on two datasets compared with the state-of-the-art MIL methods.\nMoreover, we evaluated our method in real clinical settings and confirmed that\nour method outperformed the previous image-level methods. The code is publicly\navailable at\nhttps://github.com/Shiku-Kaito/Ordinal-Multiple-instance-Learning-for-Ulcerative-Colitis-Severity-Estimation.\n","authors":["Kaito Shiku","Kazuya Nishimura","Daiki Suehiro","Kiyohito Tanaka","Ryoma Bise"],"pdf_url":"https://arxiv.org/pdf/2411.14750v1.pdf","comment":"10 pages, 9 figures, Accepted in WACV 2025"},{"id":"http://arxiv.org/abs/2405.19544v3","updated":"2024-11-22T05:55:58Z","published":"2024-05-29T22:12:52Z","title":"One-Shot Safety Alignment for Large Language Models via Optimal\n  Dualization","summary":"  The growing safety concerns surrounding large language models raise an urgent\nneed to align them with diverse human preferences to simultaneously enhance\ntheir helpfulness and safety. A promising approach is to enforce safety\nconstraints through Reinforcement Learning from Human Feedback (RLHF). For such\nconstrained RLHF, typical Lagrangian-based primal-dual policy optimization\nmethods are computationally expensive and often unstable. This paper presents a\nperspective of dualization that reduces constrained alignment to an equivalent\nunconstrained alignment problem. We do so by pre-optimizing a smooth and convex\ndual function that has a closed form. This shortcut eliminates the need for\ncumbersome primal-dual policy iterations, greatly reducing the computational\nburden and improving training stability. Our strategy leads to two practical\nalgorithms in model-based and preference-based settings (MoCAN and PeCAN,\nrespectively). A broad range of experiments demonstrate the effectiveness and\nmerits of our algorithms.\n","authors":["Xinmeng Huang","Shuo Li","Edgar Dobriban","Osbert Bastani","Hamed Hassani","Dongsheng Ding"],"pdf_url":"https://arxiv.org/pdf/2405.19544v3.pdf","comment":"32 pages, 6 figures, 8 tables"},{"id":"http://arxiv.org/abs/2411.14748v1","updated":"2024-11-22T05:53:46Z","published":"2024-11-22T05:53:46Z","title":"Cosmological Analysis with Calibrated Neural Quantile Estimation and\n  Approximate Simulators","summary":"  A major challenge in extracting information from current and upcoming surveys\nof cosmological Large-Scale Structure (LSS) is the limited availability of\ncomputationally expensive high-fidelity simulations. We introduce Neural\nQuantile Estimation (NQE), a new Simulation-Based Inference (SBI) method that\nleverages a large number of approximate simulations for training and a small\nnumber of high-fidelity simulations for calibration. This approach guarantees\nan unbiased posterior and achieves near-optimal constraining power when the\napproximate simulations are reasonably accurate. As a proof of concept, we\ndemonstrate that cosmological parameters can be inferred at field level from\nprojected 2-dim dark matter density maps up to $k_{\\rm max}\\sim1.5\\,h$/Mpc at\n$z=0$ by training on $\\sim10^4$ Particle-Mesh (PM) simulations with transfer\nfunction correction and calibrating with $\\sim10^2$ Particle-Particle (PP)\nsimulations. The calibrated posteriors closely match those obtained by directly\ntraining on $\\sim10^4$ expensive PP simulations, but at a fraction of the\ncomputational cost. Our method offers a practical and scalable framework for\nSBI of cosmological LSS, enabling precise inference across vast volumes and\ndown to small scales.\n","authors":["He Jia"],"pdf_url":"https://arxiv.org/pdf/2411.14748v1.pdf","comment":"5+4 pages, 5+3 figures, to be submitted, comments are welcome"},{"id":"http://arxiv.org/abs/2408.15094v2","updated":"2024-11-22T05:41:58Z","published":"2024-08-27T14:25:42Z","title":"Constrained Diffusion Models via Dual Training","summary":"  Diffusion models have attained prominence for their ability to synthesize a\nprobability distribution for a given dataset via a diffusion process, enabling\nthe generation of new data points with high fidelity. However, diffusion\nprocesses are prone to generating samples that reflect biases in a training\ndataset. To address this issue, we develop constrained diffusion models by\nimposing diffusion constraints based on desired distributions that are informed\nby requirements. Specifically, we cast the training of diffusion models under\nrequirements as a constrained distribution optimization problem that aims to\nreduce the distribution difference between original and generated data while\nobeying constraints on the distribution of generated data. We show that our\nconstrained diffusion models generate new data from a mixture data distribution\nthat achieves the optimal trade-off among objective and constraints. To train\nconstrained diffusion models, we develop a dual training algorithm and\ncharacterize the optimality of the trained constrained diffusion model. We\nempirically demonstrate the effectiveness of our constrained models in two\nconstrained generation tasks: (i) we consider a dataset with one or more\nunderrepresented classes where we train the model with constraints to ensure\nfairly sampling from all classes during inference; (ii) we fine-tune a\npre-trained diffusion model to sample from a new dataset while avoiding\noverfitting.\n","authors":["Shervin Khalafi","Dongsheng Ding","Alejandro Ribeiro"],"pdf_url":"https://arxiv.org/pdf/2408.15094v2.pdf","comment":"31 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2411.14744v1","updated":"2024-11-22T05:41:00Z","published":"2024-11-22T05:41:00Z","title":"Point Cloud Understanding via Attention-Driven Contrastive Learning","summary":"  Recently Transformer-based models have advanced point cloud understanding by\nleveraging self-attention mechanisms, however, these methods often overlook\nlatent information in less prominent regions, leading to increased sensitivity\nto perturbations and limited global comprehension. To solve this issue, we\nintroduce PointACL, an attention-driven contrastive learning framework designed\nto address these limitations. Our method employs an attention-driven dynamic\nmasking strategy that guides the model to focus on under-attended regions,\nenhancing the understanding of global structures within the point cloud. Then\nwe combine the original pre-training loss with a contrastive learning loss,\nimproving feature discrimination and generalization. Extensive experiments\nvalidate the effectiveness of PointACL, as it achieves state-of-the-art\nperformance across a variety of 3D understanding tasks, including object\nclassification, part segmentation, and few-shot learning. Specifically, when\nintegrated with different Transformer backbones like Point-MAE and PointGPT,\nPointACL demonstrates improved performance on datasets such as ScanObjectNN,\nModelNet40, and ShapeNetPart. This highlights its superior capability in\ncapturing both global and local features, as well as its enhanced robustness\nagainst perturbations and incomplete data.\n","authors":["Yi Wang","Jiaze Wang","Ziyu Guo","Renrui Zhang","Donghao Zhou","Guangyong Chen","Anfeng Liu","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2411.14744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14349v2","updated":"2024-11-22T05:18:23Z","published":"2024-11-21T17:43:51Z","title":"Agnostic Learning of Arbitrary ReLU Activation under Gaussian Marginals","summary":"  We consider the problem of learning an arbitrarily-biased ReLU activation (or\nneuron) over Gaussian marginals with the squared loss objective. Despite the\nReLU neuron being the basic building block of modern neural networks, we still\ndo not understand the basic algorithmic question of whether one arbitrary ReLU\nneuron is learnable in the non-realizable setting. In particular, all existing\npolynomial time algorithms only provide approximation guarantees for the\nbetter-behaved unbiased setting or restricted bias setting.\n  Our main result is a polynomial time statistical query (SQ) algorithm that\ngives the first constant factor approximation for arbitrary bias. It outputs a\nReLU activation that achieves a loss of $O(\\mathrm{OPT}) + \\varepsilon$ in time\n$\\mathrm{poly}(d,1/\\varepsilon)$, where $\\mathrm{OPT}$ is the loss obtained by\nthe optimal ReLU activation. Our algorithm presents an interesting departure\nfrom existing algorithms, which are all based on gradient descent and thus fall\nwithin the class of correlational statistical query (CSQ) algorithms. We\ncomplement our algorithmic result by showing that no polynomial time CSQ\nalgorithm can achieve a constant factor approximation. Together, these results\nshed light on the intrinsic limitation of gradient descent, while identifying\narguably the simplest setting (a single neuron) where there is a separation\nbetween SQ and CSQ algorithms.\n","authors":["Anxin Guo","Aravindan Vijayaraghavan"],"pdf_url":"https://arxiv.org/pdf/2411.14349v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09413v3","updated":"2024-11-22T05:12:30Z","published":"2024-06-13T17:59:56Z","title":"Interpreting the Weight Space of Customized Diffusion Models","summary":"  We investigate the space of weights spanned by a large collection of\ncustomized diffusion models. We populate this space by creating a dataset of\nover 60,000 models, each of which is a base model fine-tuned to insert a\ndifferent person's visual identity. We model the underlying manifold of these\nweights as a subspace, which we term weights2weights. We demonstrate three\nimmediate applications of this space that result in new diffusion models --\nsampling, editing, and inversion. First, sampling a set of weights from this\nspace results in a new model encoding a novel identity. Next, we find linear\ndirections in this space corresponding to semantic edits of the identity (e.g.,\nadding a beard), resulting in a new model with the original identity edited.\nFinally, we show that inverting a single image into this space encodes a\nrealistic identity into a model, even if the input image is out of distribution\n(e.g., a painting). We further find that these linear properties of the\ndiffusion model weight space extend to other visual concepts. Our results\nindicate that the weight space of fine-tuned diffusion models can behave as an\ninterpretable meta-latent space producing new models.\n","authors":["Amil Dravid","Yossi Gandelsman","Kuan-Chieh Wang","Rameen Abdal","Gordon Wetzstein","Alexei A. Efros","Kfir Aberman"],"pdf_url":"https://arxiv.org/pdf/2406.09413v3.pdf","comment":"Project Page: https://snap-research.github.io/weights2weights"},{"id":"http://arxiv.org/abs/2411.14737v1","updated":"2024-11-22T05:11:51Z","published":"2024-11-22T05:11:51Z","title":"AI Tailoring: Evaluating Influence of Image Features on Fashion Product\n  Popularity","summary":"  Identifying key product features that influence consumer preferences is\nessential in the fashion industry. In this study, we introduce a robust\nmethodology to ascertain the most impactful features in fashion product images,\nutilizing past market sales data. First, we propose the metric called\n\"influence score\" to quantitatively assess the importance of product features.\nThen we develop a forecasting model, the Fashion Demand Predictor (FDP), which\nintegrates Transformer-based models and Random Forest to predict market\npopularity based on product images. We employ image-editing diffusion models to\nmodify these images and perform an ablation study, which validates the impact\nof the highest and lowest-scoring features on the model's popularity\npredictions. Additionally, we further validate these results through surveys\nthat gather human rankings of preferences, confirming the accuracy of the FDP\nmodel's predictions and the efficacy of our method in identifying influential\nfeatures. Notably, products enhanced with \"good\" features show marked\nimprovements in predicted popularity over their modified counterparts. Our\napproach develops a fully automated and systematic framework for fashion image\nanalysis that provides valuable guidance for downstream tasks such as fashion\nproduct design and marketing strategy development.\n","authors":["Xiaomin Li","Junyi Sha"],"pdf_url":"https://arxiv.org/pdf/2411.14737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14733v1","updated":"2024-11-22T05:01:35Z","published":"2024-11-22T05:01:35Z","title":"FLARE: FP-Less PTQ and Low-ENOB ADC Based AMS-PiM for Error-Resilient,\n  Fast, and Efficient Transformer Acceleration","summary":"  Encoder-based transformers, powered by self-attention layers, have\nrevolutionized machine learning with their context-aware representations.\nHowever, their quadratic growth in computational and memory demands presents\nsignificant bottlenecks. Analog-Mixed-Signal Process-in-Memory (AMS-PiM)\narchitectures address these challenges by enabling efficient on-chip\nprocessing. Traditionally, AMS-PiM relies on Quantization-Aware Training (QAT),\nwhich is hardware-efficient but requires extensive retraining to adapt models\nto AMS-PiMs, making it increasingly impractical for transformer models.\nPost-Training Quantization (PTQ) mitigates this training overhead but\nintroduces significant hardware inefficiencies. PTQ relies on\ndequantization-quantization (DQ-Q) processes, floating-point units (FPUs), and\nhigh-ENOB (Effective Number of Bits) analog-to-digital converters (ADCs).\nParticularly, High-ENOB ADCs scale exponentially in area and energy\n($2^{ENOB}$), reduce sensing margins, and increase susceptibility to process,\nvoltage, and temperature (PVT) variations, further compounding PTQ's challenges\nin AMS-PiM systems. To overcome these limitations, we propose RAP, an AMS-PiM\narchitecture that eliminates DQ-Q processes, introduces FPU- and division-free\nnonlinear processing, and employs a low-ENOB-ADC-based sparse Matrix Vector\nmultiplication technique. Using the proposed techniques, RAP improves error\nresiliency, area/energy efficiency, and computational speed while preserving\nnumerical stability. Experimental results demonstrate that RAP outperforms\nstate-of-the-art GPUs and conventional PiM architectures in energy efficiency,\nlatency, and accuracy, making it a scalable solution for the efficient\ndeployment of transformers.\n","authors":["Donghyeon Yi","Seoyoung Lee","Jongho Kim","Junyoung Kim","Sohmyung Ha","Ik Joon Chang","Minkyu Je"],"pdf_url":"https://arxiv.org/pdf/2411.14733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14728v1","updated":"2024-11-22T04:48:58Z","published":"2024-11-22T04:48:58Z","title":"K-GBS3FCM -- KNN Graph-Based Safe Semi-Supervised Fuzzy C-Means","summary":"  Clustering data using prior domain knowledge, starting from a partially\nlabeled set, has recently been widely investigated. Often referred to as\nsemi-supervised clustering, this approach leverages labeled data to enhance\nclustering accuracy. To maximize algorithm performance, it is crucial to ensure\nthe safety of this prior knowledge. Methods addressing this concern are termed\nsafe semi-supervised clustering (S3C) algorithms. This paper introduces the KNN\ngraph-based safety-aware semi-supervised fuzzy c-means algorithm (K-GBS3FCM),\nwhich dynamically assesses neighborhood relationships between labeled and\nunlabeled data using the K-Nearest Neighbors (KNN) algorithm. This approach\naims to optimize the use of labeled data while minimizing the adverse effects\nof incorrect labels. Additionally, it is proposed a mechanism that adjusts the\ninfluence of labeled data on unlabeled ones through regularization parameters\nand the average safety degree. Experimental results on multiple benchmark\ndatasets demonstrate that the graph-based approach effectively leverages prior\nknowledge to enhance clustering accuracy. The proposed method was significantly\nsuperior in 64% of the 56 test configurations, obtaining higher levels of\nclustering accuracy when compared to other semi-supervised and traditional\nunsupervised methods. This research highlights the potential of integrating\ngraph-based approaches, such as KNN, with established techniques to develop\nadvanced clustering algorithms, offering significant applications in fields\nthat rely on both labeled and unlabeled data for more effective clustering.\n","authors":["Gabriel Santos","Rita Julia","Marcelo Nascimento"],"pdf_url":"https://arxiv.org/pdf/2411.14728v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2411.14727v1","updated":"2024-11-22T04:46:31Z","published":"2024-11-22T04:46:31Z","title":"Attributed Graph Clustering via Generalized Quaternion Representation\n  Learning","summary":"  Clustering complex data in the form of attributed graphs has attracted\nincreasing attention, where appropriate graph representation is a critical\nprerequisite for accurate cluster analysis. However, the Graph Convolutional\nNetwork will homogenize the representation of graph nodes due to the well-known\nover-smoothing effect. This limits the network architecture to a shallow one,\nlosing the ability to capture the critical global distribution information for\nclustering. Therefore, we propose a generalized graph auto-encoder network,\nwhich introduces quaternion operations to the encoders to achieve efficient\nstructured feature representation learning without incurring deeper network and\nlarger-scale parameters. The generalization of our method lies in the following\ntwo aspects: 1) connecting the quaternion operation naturally suitable for four\nfeature components with graph data of arbitrary attribute dimensions, and 2)\nintroducing a generalized graph clustering objective as a loss term to obtain\nclustering-friendly representations without requiring a pre-specified number of\nclusters $k$. It turns out that the representations of nodes learned by the\nproposed Graph Clustering based on Generalized Quaternion representation\nlearning (GCGQ) are more discriminative, containing global distribution\ninformation, and are more general, suiting downstream clustering under\ndifferent $k$s. Extensive experiments including significance tests, ablation\nstudies, and qualitative results, illustrate the superiority of GCGQ. The\nsource code is temporarily opened at\n\\url{https://anonymous.4open.science/r/ICLR-25-No7181-codes}.\n","authors":["Junyang Chen","Yiqun Zhang","Mengke Li","Yang Lu","Yiu-ming Cheung"],"pdf_url":"https://arxiv.org/pdf/2411.14727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14726v1","updated":"2024-11-22T04:45:55Z","published":"2024-11-22T04:45:55Z","title":"Enhancing Molecular Design through Graph-based Topological Reinforcement\n  Learning","summary":"  The generation of drug-like molecules is crucial for drug design. Existing\nreinforcement learning (RL) methods often overlook structural information.\nHowever, feature engineering-based methods usually merely focus on binding\naffinity prediction without substantial molecular modification. To address\nthis, we present Graph-based Topological Reinforcement Learning (GraphTRL),\nwhich integrates both chemical and structural data for improved molecular\ngeneration. GraphTRL leverages multiscale weighted colored graphs (MWCG) and\npersistent homology, combined with molecular fingerprints, as the state space\nfor RL. Evaluations show that GraphTRL outperforms existing methods in binding\naffinity prediction, offering a promising approach to accelerate drug\ndiscovery.\n","authors":["Xiangyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14725v1","updated":"2024-11-22T04:41:20Z","published":"2024-11-22T04:41:20Z","title":"Evaluating and Advancing Multimodal Large Language Models in Ability\n  Lens","summary":"  As multimodal large language models (MLLMs) advance rapidly, rigorous\nevaluation has become essential, providing further guidance for their\ndevelopment. In this work, we focus on a unified and robust evaluation of\n\\textbf{vision perception} abilities, the foundational skill of MLLMs. We find\nthat existing perception benchmarks, each focusing on different question types,\ndomains, and evaluation metrics, introduce significant evaluation variance,\ncomplicating comprehensive assessments of perception abilities when relying on\nany single benchmark. To address this, we introduce \\textbf{AbilityLens}, a\nunified benchmark designed to evaluate MLLMs across six key perception\nabilities, focusing on both accuracy and stability, with each ability\nencompassing diverse question types, domains, and metrics. With the assistance\nof AbilityLens, we: (1) identify the strengths and weaknesses of current\nmodels, highlighting stability patterns and revealing a notable performance gap\nbetween open-source and closed-source models; (2) introduce an online\nevaluation mode, which uncovers interesting ability conflict and early\nconvergence phenomena during MLLM training; and (3) design a simple\nability-specific model merging method that combines the best ability checkpoint\nfrom early training stages, effectively mitigating performance decline due to\nability conflict. The benchmark and online leaderboard will be released soon.\n","authors":["Feng Chen","Chenhui Gou","Jing Liu","Yang Yang","Zhaoyang Li","Jiyuan Zhang","Zhenbang Sun","Bohan Zhuang","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2411.14725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14721v1","updated":"2024-11-22T04:28:56Z","published":"2024-11-22T04:28:56Z","title":"MolReFlect: Towards In-Context Fine-grained Alignments between Molecules\n  and Texts","summary":"  Molecule discovery is a pivotal research field, impacting everything from the\nmedicines we take to the materials we use. Recently, Large Language Models\n(LLMs) have been widely adopted in molecule understanding and generation, yet\nthe alignments between molecules and their corresponding captions remain a\nsignificant challenge. Previous endeavours often treat the molecule as a\ngeneral SMILES string or molecular graph, neglecting the fine-grained\nalignments between the molecular sub-structures and the descriptive textual\nphrases, which are crucial for accurate and explainable predictions. In this\ncase, we introduce MolReFlect, a novel teacher-student framework designed to\ncontextually perform the molecule-caption alignments in a fine-grained way. Our\napproach initially leverages a larger teacher LLM to label the detailed\nalignments by directly extracting critical phrases from molecule captions or\nSMILES strings and implying them to corresponding sub-structures or\ncharacteristics. To refine these alignments, we propose In-Context Selective\nReflection, which retrieves previous extraction results as context examples for\nteacher LLM to reflect and lets a smaller student LLM select from in-context\nreflection and previous extraction results. Finally, we enhance the learning\nprocess of the student LLM through Chain-of-Thought In-Context Molecule Tuning,\nintegrating the fine-grained alignments and the reasoning processes within the\nChain-of-Thought format. Our experimental results demonstrate that MolReFlect\nenables LLMs like Mistral-7B to significantly outperform the previous\nbaselines, achieving SOTA performance on the ChEBI-20 dataset. This advancement\nnot only enhances the generative capabilities of LLMs in the molecule-caption\ntranslation task, but also contributes to a more explainable framework.\n","authors":["Jiatong Li","Yunqing Liu","Wei Liu","Jingdi Le","Di Zhang","Wenqi Fan","Dongzhan Zhou","Yuqiang Li","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2411.14721v1.pdf","comment":"22 pages, 12 figures"},{"id":"http://arxiv.org/abs/2305.05933v2","updated":"2024-11-22T04:27:10Z","published":"2023-05-10T07:05:43Z","title":"Spectrum Breathing: Protecting Over-the-Air Federated Learning Against\n  Interference","summary":"  Federated Learning (FL) is a widely embraced paradigm for distilling\nartificial intelligence from distributed mobile data. However, the deployment\nof FL in mobile networks can be compromised by exposure to interference from\nneighboring cells or jammers. Existing interference mitigation techniques\nrequire multi-cell cooperation or at least interference channel state\ninformation, which is expensive in practice. On the other hand, power control\nthat treats interference as noise may not be effective due to limited power\nbudgets, and also that this mechanism can trigger countermeasures by\ninterference sources. As a practical approach for protecting FL against\ninterference, we propose Spectrum Breathing, which cascades stochastic-gradient\npruning and spread spectrum to suppress interference without bandwidth\nexpansion. The cost is higher learning latency by exploiting the graceful\ndegradation of learning speed due to pruning. We synchronize the two operations\nsuch that their levels are controlled by the same parameter, Breathing Depth.\nTo optimally control the parameter, we develop a martingale-based approach to\nconvergence analysis of Over-the-Air FL with spectrum breathing, termed\nAirBreathing FL. We show a performance tradeoff between gradient-pruning and\ninterference-induced error as regulated by the breathing depth. Given receive\nSIR and model size, the optimization of the tradeoff yields two schemes for\ncontrolling the breathing depth that can be either fixed or adaptive to\nchannels and the learning process. As shown by experiments, in scenarios where\ntraditional Over-the-Air FL fails to converge in the presence of strong\ninterference, AirBreahing FL with either fixed or adaptive breathing depth can\nensure convergence where the adaptive scheme achieves close-to-ideal\nperformance.\n","authors":["Zhanwei Wang","Kaibin Huang","Yonina C. Eldar"],"pdf_url":"https://arxiv.org/pdf/2305.05933v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14717v1","updated":"2024-11-22T04:09:23Z","published":"2024-11-22T04:09:23Z","title":"FedMLLM: Federated Fine-tuning MLLM on Multimodal Heterogeneity Data","summary":"  Multimodal Large Language Models (MLLMs) have made significant advancements,\ndemonstrating powerful capabilities in processing and understanding multimodal\ndata. Fine-tuning MLLMs with Federated Learning (FL) allows for expanding the\ntraining data scope by including private data sources, thereby enhancing their\npractical applicability in privacy-sensitive domains. However, current research\nremains in the early stage, particularly in addressing the \\textbf{multimodal\nheterogeneities} in real-world applications. In this paper, we introduce a\nbenchmark for evaluating various downstream tasks in the federated fine-tuning\nof MLLMs within multimodal heterogeneous scenarios, laying the groundwork for\nthe research in the field. Our benchmark encompasses two datasets, five\ncomparison baselines, and four multimodal scenarios, incorporating over ten\ntypes of modal heterogeneities. To address the challenges posed by modal\nheterogeneity, we develop a general FedMLLM framework that integrates four\nrepresentative FL methods alongside two modality-agnostic strategies. Extensive\nexperimental results show that our proposed FL paradigm improves the\nperformance of MLLMs by broadening the range of training data and mitigating\nmultimodal heterogeneity. Code is available at https://github.com/1xbq1/FedMLLM\n","authors":["Binqian Xu","Xiangbo Shu","Haiyang Mei","Guosen Xie","Basura Fernando","Mike Zheng Shou","Jinhui Tang"],"pdf_url":"https://arxiv.org/pdf/2411.14717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14716v1","updated":"2024-11-22T03:59:41Z","published":"2024-11-22T03:59:41Z","title":"VisionPAD: A Vision-Centric Pre-training Paradigm for Autonomous Driving","summary":"  This paper introduces VisionPAD, a novel self-supervised pre-training\nparadigm designed for vision-centric algorithms in autonomous driving. In\ncontrast to previous approaches that employ neural rendering with explicit\ndepth supervision, VisionPAD utilizes more efficient 3D Gaussian Splatting to\nreconstruct multi-view representations using only images as supervision.\nSpecifically, we introduce a self-supervised method for voxel velocity\nestimation. By warping voxels to adjacent frames and supervising the rendered\noutputs, the model effectively learns motion cues in the sequential data.\nFurthermore, we adopt a multi-frame photometric consistency approach to enhance\ngeometric perception. It projects adjacent frames to the current frame based on\nrendered depths and relative poses, boosting the 3D geometric representation\nthrough pure image supervision. Extensive experiments on autonomous driving\ndatasets demonstrate that VisionPAD significantly improves performance in 3D\nobject detection, occupancy prediction and map segmentation, surpassing\nstate-of-the-art pre-training strategies by a considerable margin.\n","authors":["Haiming Zhang","Wending Zhou","Yiyao Zhu","Xu Yan","Jiantao Gao","Dongfeng Bai","Yingjie Cai","Bingbing Liu","Shuguang Cui","Zhen Li"],"pdf_url":"https://arxiv.org/pdf/2411.14716v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06255v2","updated":"2024-11-22T03:43:55Z","published":"2024-09-10T06:55:17Z","title":"Market Reaction to News Flows in Supply Chain Networks","summary":"  This study examines how positive and negative news about firms affects their\nstock prices and, moreover, how it affects stock prices of the firms' suppliers\nand clients, using a large sample of publicly listed firms around the world and\nanother of Japanese listed firms. The level of positiveness and negativeness of\neach news article is determined by FinBERT, a natural language processing model\nfine-tuned specifically for financial information. Supply chains of firms\nacross the world are identified mostly by financial statements, while those of\nJapanese firms are taken from large-scale firm-level surveys. We find that\npositive news increases the change rate of stock prices of firms mentioned in\nthe news before its disclosure, most likely because of diffusion of information\nthrough private channels. Positive news also raises stock prices of the firms'\nsuppliers and clients before its disclosure, confirming propagation of market\nvalues through supply chains. In addition, we generally find a larger post-news\neffect on stock prices of the mentioned firms and their suppliers and clients\nthan the pre-news effect. The positive difference between the post- and\npre-news effects can be considered as the net effect of the disclosure of\npositive news, controlling for information diffusion through private channels.\nHowever, the post-news effect on suppliers and clients in Japan is smaller than\nthe pre-news effect, which is the opposite result to non-domestic firms from\naround the world.\n","authors":["Hiroyasu Inoue","Yasuyuki Todo"],"pdf_url":"https://arxiv.org/pdf/2409.06255v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2009.03238v2","updated":"2024-11-22T03:39:33Z","published":"2020-08-27T23:43:25Z","title":"A Joint Network Optimization Framework to Predict Clinical Severity from\n  Resting State Functional MRI Data","summary":"  We propose a novel optimization framework to predict clinical severity from\nresting state fMRI (rs-fMRI) data. Our model consists of two coupled terms. The\nfirst term decomposes the correlation matrices into a sparse set of\nrepresentative subnetworks that define a network manifold. These subnetworks\nare modeled as rank-one outer-products which correspond to the elemental\npatterns of co-activation across the brain; the subnetworks are combined via\npatient-specific non-negative coefficients. The second term is a linear\nregression model that uses the patient-specific coefficients to predict a\nmeasure of clinical severity. We validate our framework on two separate\ndatasets in a ten fold cross validation setting. The first is a cohort of\nfifty-eight patients diagnosed with Autism Spectrum Disorder (ASD). The second\ndataset consists of sixty three patients from a publicly available ASD\ndatabase. Our method outperforms standard semi-supervised frameworks, which\nemploy conventional graph theoretic and statistical representation learning\ntechniques to relate the rs-fMRI correlations to behavior. In contrast, our\njoint network optimization framework exploits the structure of the rs-fMRI\ncorrelation matrices to simultaneously capture group level effects and patient\nheterogeneity. Finally, we demonstrate that our proposed framework robustly\nidentifies clinically relevant networks characteristic of ASD.\n","authors":["Niharika Shimona D'Souza","Mary Beth Nebel","Nicholas Wymbs","Stewart H. Mostofsky","Archana Venkataraman"],"pdf_url":"https://arxiv.org/pdf/2009.03238v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14711v1","updated":"2024-11-22T03:38:20Z","published":"2024-11-22T03:38:20Z","title":"Can GNNs Learn Link Heuristics? A Concise Review and Evaluation of Link\n  Prediction Methods","summary":"  This paper explores the ability of Graph Neural Networks (GNNs) in learning\nvarious forms of information for link prediction, alongside a brief review of\nexisting link prediction methods. Our analysis reveals that GNNs cannot\neffectively learn structural information related to the number of common\nneighbors between two nodes, primarily due to the nature of set-based pooling\nof the neighborhood aggregation scheme. Also, our extensive experiments\nindicate that trainable node embeddings can improve the performance of\nGNN-based link prediction models. Importantly, we observe that the denser the\ngraph, the greater such the improvement. We attribute this to the\ncharacteristics of node embeddings, where the link state of each link sample\ncould be encoded into the embeddings of nodes that are involved in the\nneighborhood aggregation of the two nodes in that link sample. In denser\ngraphs, every node could have more opportunities to attend the neighborhood\naggregation of other nodes and encode states of more link samples to its\nembedding, thus learning better node embeddings for link prediction. Lastly, we\ndemonstrate that the insights gained from our research carry important\nimplications in identifying the limitations of existing link prediction\nmethods, which could guide the future development of more robust algorithms.\n","authors":["Shuming Liang","Yu Ding","Zhidong Li","Bin Liang","Siqi Zhang","Yang Wang","Fang Chen"],"pdf_url":"https://arxiv.org/pdf/2411.14711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14708v1","updated":"2024-11-22T03:33:51Z","published":"2024-11-22T03:33:51Z","title":"Understanding LLM Embeddings for Regression","summary":"  With the rise of large language models (LLMs) for flexibly processing\ninformation as strings, a natural application is regression, specifically by\npreprocessing string representations into LLM embeddings as downstream features\nfor metric prediction. In this paper, we provide one of the first comprehensive\ninvestigations into embedding-based regression and demonstrate that LLM\nembeddings as features can be better for high-dimensional regression tasks than\nusing traditional feature engineering. This regression performance can be\nexplained in part due to LLM embeddings over numeric data inherently preserving\nLipschitz continuity over the feature space. Furthermore, we quantify the\ncontribution of different model effects, most notably model size and language\nunderstanding, which we find surprisingly do not always improve regression\nperformance.\n","authors":["Eric Tang","Bangding Yang","Xingyou Song"],"pdf_url":"https://arxiv.org/pdf/2411.14708v1.pdf","comment":"15 pages, 13 figures"},{"id":"http://arxiv.org/abs/2411.14696v1","updated":"2024-11-22T03:08:24Z","published":"2024-11-22T03:08:24Z","title":"Quantum Hamiltonian Descent for Graph Partition","summary":"  We introduce Quantum Hamiltonian Descent as a novel approach to solve the\ngraph partition problem. By reformulating graph partition as a Quadratic\nUnconstrained Binary Optimization (QUBO) problem, we leverage QHD's\nquantum-inspired dynamics to identify optimal community structures. Our method\nimplements a multi-level refinement strategy that alternates between QUBO\nformulation and QHD optimization to iteratively improve partition quality.\nExperimental results demonstrate that our QHD-based approach achieves superior\nmodularity scores (up to 5.49\\%) improvement with reduced computational\noverhead compared to traditional optimization methods. This work establishes\nQHD as an effective quantum-inspired framework for tackling graph partition\nchallenges in large-scale networks.\n","authors":["Jinglei Cheng","Ruilin Zhou","Yuhang Gan","Chen Qian","Junyu Liu"],"pdf_url":"https://arxiv.org/pdf/2411.14696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14695v1","updated":"2024-11-22T03:05:06Z","published":"2024-11-22T03:05:06Z","title":"Anti-Forgetting Adaptation for Unsupervised Person Re-identification","summary":"  Regular unsupervised domain adaptive person re-identification (ReID) focuses\non adapting a model from a source domain to a fixed target domain. However, an\nadapted ReID model can hardly retain previously-acquired knowledge and\ngeneralize to unseen data. In this paper, we propose a Dual-level Joint\nAdaptation and Anti-forgetting (DJAA) framework, which incrementally adapts a\nmodel to new domains without forgetting source domain and each adapted target\ndomain. We explore the possibility of using prototype and instance-level\nconsistency to mitigate the forgetting during the adaptation. Specifically, we\nstore a small number of representative image samples and corresponding cluster\nprototypes in a memory buffer, which is updated at each adaptation step. With\nthe buffered images and prototypes, we regularize the image-to-image similarity\nand image-to-prototype similarity to rehearse old knowledge. After the\nmulti-step adaptation, the model is tested on all seen domains and several\nunseen domains to validate the generalization ability of our method. Extensive\nexperiments demonstrate that our proposed method significantly improves the\nanti-forgetting, generalization and backward-compatible ability of an\nunsupervised person ReID model.\n","authors":["Hao Chen","Francois Bremond","Nicu Sebe","Shiliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14695v1.pdf","comment":"Accepted to TPAMI"},{"id":"http://arxiv.org/abs/2411.14694v1","updated":"2024-11-22T02:58:45Z","published":"2024-11-22T02:58:45Z","title":"A Data-Driven Pool Strategy for Price-Makers Under Imperfect Information","summary":"  This paper studies the pool strategy for price-makers under imperfect\ninformation. In this occasion, market participants cannot obtain essential\ntransmission parameters of the power system. Thus, price-makers should estimate\nthe market results with respect to their offer curves using available\nhistorical information. The linear programming model of economic dispatch is\nanalyzed with the theory of rim multi-parametric linear programming (rim-MPLP).\nThe characteristics of system patterns (combinations of status flags for\ngenerating units and transmission lines) are revealed. A multi-class\nclassification model based on support vector machine (SVM) is trained to map\nthe offer curves to system patterns, which is then integrated into the decision\nframework of the price-maker. The performance of the proposed method is\nvalidated on the IEEE 30-bus system, Illinois synthetic 200-bus system, and\nSouth Carolina synthetic 500-bus system.\n","authors":["Kedi Zheng","Hongye Guo","Qixin Chen"],"pdf_url":"https://arxiv.org/pdf/2411.14694v1.pdf","comment":"Paper accepted for IEEE Transactions on Power Systems. Personal use\n  of this material is permitted. Permission from IEEE must be obtained for all\n  other uses"},{"id":"http://arxiv.org/abs/2411.14691v1","updated":"2024-11-22T02:56:47Z","published":"2024-11-22T02:56:47Z","title":"EV-PINN: A Physics-Informed Neural Network for Predicting Electric\n  Vehicle Dynamics","summary":"  An onboard prediction of dynamic parameters (e.g. Aerodynamic drag, rolling\nresistance) enables accurate path planning for EVs. This paper presents\nEV-PINN, a Physics-Informed Neural Network approach in predicting instantaneous\nbattery power and cumulative energy consumption during cruising while\ngeneralizing to the nonlinear dynamics of an EV. Our method learns real-world\nparameters such as motor efficiency, regenerative braking efficiency, vehicle\nmass, coefficient of aerodynamic drag, and coefficient of rolling resistance\nusing automatic differentiation based on dynamics and ensures consistency with\nground truth vehicle data. EV-PINN was validated using 15 and 35 minutes of\nin-situ battery log data from the Tesla Model 3 Long Range and Tesla Model S,\nrespectively. With only vehicle speed and time as inputs, our model achieves\nhigh accuracy and generalization to dynamics, with validation losses of\n0.002195 and 0.002292, respectively. This demonstrates EV-PINN's effectiveness\nin estimating parameters and predicting battery usage under actual driving\nconditions without the need for additional sensors.\n","authors":["Hansol Lim","Jee Won Lee","Jonathan Boyack","Jongseong Brad Choi"],"pdf_url":"https://arxiv.org/pdf/2411.14691v1.pdf","comment":"This work has been submitted to the 2025 IEEE International\n  Conference on Robotics and Automation (ICRA) for possible publication"},{"id":"http://arxiv.org/abs/2410.24060v4","updated":"2024-11-22T02:48:41Z","published":"2024-10-31T15:57:04Z","title":"Understanding Generalizability of Diffusion Models Requires Rethinking\n  the Hidden Gaussian Structure","summary":"  In this work, we study the generalizability of diffusion models by looking\ninto the hidden properties of the learned score functions, which are\nessentially a series of deep denoisers trained on various noise levels. We\nobserve that as diffusion models transition from memorization to\ngeneralization, their corresponding nonlinear diffusion denoisers exhibit\nincreasing linearity. This discovery leads us to investigate the linear\ncounterparts of the nonlinear diffusion models, which are a series of linear\nmodels trained to match the function mappings of the nonlinear diffusion\ndenoisers. Surprisingly, these linear denoisers are approximately the optimal\ndenoisers for a multivariate Gaussian distribution characterized by the\nempirical mean and covariance of the training dataset. This finding implies\nthat diffusion models have the inductive bias towards capturing and utilizing\nthe Gaussian structure (covariance information) of the training dataset for\ndata generation. We empirically demonstrate that this inductive bias is a\nunique property of diffusion models in the generalization regime, which becomes\nincreasingly evident when the model's capacity is relatively small compared to\nthe training dataset size. In the case that the model is highly\noverparameterized, this inductive bias emerges during the initial training\nphases before the model fully memorizes its training data. Our study provides\ncrucial insights into understanding the notable strong generalization\nphenomenon recently observed in real-world diffusion models.\n","authors":["Xiang Li","Yixiang Dai","Qing Qu"],"pdf_url":"https://arxiv.org/pdf/2410.24060v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14688v1","updated":"2024-11-22T02:46:44Z","published":"2024-11-22T02:46:44Z","title":"Whats in a Video: Factorized Autoregressive Decoding for Online Dense\n  Video Captioning","summary":"  Generating automatic dense captions for videos that accurately describe their\ncontents remains a challenging area of research. Most current models require\nprocessing the entire video at once. Instead, we propose an efficient, online\napproach which outputs frequent, detailed and temporally aligned captions,\nwithout access to future frames. Our model uses a novel autoregressive\nfactorized decoding architecture, which models the sequence of visual features\nfor each time segment, outputting localized descriptions and efficiently\nleverages the context from the previous video segments. This allows the model\nto output frequent, detailed captions to more comprehensively describe the\nvideo, according to its actual local content, rather than mimic the training\ndata. Second, we propose an optimization for efficient training and inference,\nwhich enables scaling to longer videos. Our approach shows excellent\nperformance compared to both offline and online methods, and uses 20\\% less\ncompute. The annotations produced are much more comprehensive and frequent, and\ncan further be utilized in automatic video tagging and in large-scale video\ndata harvesting.\n","authors":["AJ Piergiovanni","Dahun Kim","Michael S. Ryoo","Isaac Noble","Anelia Angelova"],"pdf_url":"https://arxiv.org/pdf/2411.14688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10556v2","updated":"2024-11-22T02:46:30Z","published":"2024-08-20T05:38:50Z","title":"Hokoff: Real Game Dataset from Honor of Kings and its Offline\n  Reinforcement Learning Benchmarks","summary":"  The advancement of Offline Reinforcement Learning (RL) and Offline\nMulti-Agent Reinforcement Learning (MARL) critically depends on the\navailability of high-quality, pre-collected offline datasets that represent\nreal-world complexities and practical applications. However, existing datasets\noften fall short in their simplicity and lack of realism. To address this gap,\nwe propose Hokoff, a comprehensive set of pre-collected datasets that covers\nboth offline RL and offline MARL, accompanied by a robust framework, to\nfacilitate further research. This data is derived from Honor of Kings, a\nrecognized Multiplayer Online Battle Arena (MOBA) game known for its intricate\nnature, closely resembling real-life situations. Utilizing this framework, we\nbenchmark a variety of offline RL and offline MARL algorithms. We also\nintroduce a novel baseline algorithm tailored for the inherent hierarchical\naction space of the game. We reveal the incompetency of current offline RL\napproaches in handling task complexity, generalization and multi-task learning.\n","authors":["Yun Qu","Boyuan Wang","Jianzhun Shao","Yuhang Jiang","Chen Chen","Zhenbin Ye","Lin Liu","Junfeng Yang","Lin Lai","Hongyang Qin","Minwen Deng","Juchao Zhuo","Deheng Ye","Qiang Fu","Wei Yang","Guang Yang","Lanxiao Huang","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2408.10556v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14680v1","updated":"2024-11-22T02:24:15Z","published":"2024-11-22T02:24:15Z","title":"Self-Supervised Learning for Ordered Three-Dimensional Structures","summary":"  Recent work has proven that training large language models with\nself-supervised tasks and fine-tuning these models to complete new tasks in a\ntransfer learning setting is a powerful idea, enabling the creation of models\nwith many parameters, even with little labeled data; however, the number of\ndomains that have harnessed these advancements has been limited. In this work,\nwe formulate a set of geometric tasks suitable for the large-scale study of\nordered three-dimensional structures, without requiring any human intervention\nin data labeling. We build deep rotation- and permutation-equivariant neural\nnetworks based on geometric algebra and use them to solve these tasks on both\nidealized and simulated three-dimensional structures. Quantifying order in\ncomplex-structured assemblies remains a long-standing challenge in materials\nphysics; these models can elucidate the behavior of real self-assembling\nsystems in a variety of ways, from distilling insights from learned tasks\nwithout further modification to solving new tasks with smaller amounts of\nlabeled data via transfer learning.\n","authors":["Matthew Spellings","Maya Martirossyan","Julia Dshemuchadse"],"pdf_url":"https://arxiv.org/pdf/2411.14680v1.pdf","comment":"Version as submitted to the Learning on Graphs Conference 2022, with\n  small clarifying edits"},{"id":"http://arxiv.org/abs/2411.14679v1","updated":"2024-11-22T02:22:59Z","published":"2024-11-22T02:22:59Z","title":"Recursive Gaussian Process State Space Model","summary":"  Learning dynamical models from data is not only fundamental but also holds\ngreat promise for advancing principle discovery, time-series prediction, and\ncontroller design. Among various approaches, Gaussian Process State-Space\nModels (GPSSMs) have recently gained significant attention due to their\ncombination of flexibility and interpretability. However, for online learning,\nthe field lacks an efficient method suitable for scenarios where prior\ninformation regarding data distribution and model function is limited. To\naddress this issue, this paper proposes a recursive GPSSM method with adaptive\ncapabilities for both operating domains and Gaussian process (GP)\nhyperparameters. Specifically, we first utilize first-order linearization to\nderive a Bayesian update equation for the joint distribution between the system\nstate and the GP model, enabling closed-form and domain-independent learning.\nSecond, an online selection algorithm for inducing points is developed based on\ninformative criteria to achieve lightweight learning. Third, to support online\nhyperparameter optimization, we recover historical measurement information from\nthe current filtering distribution. Comprehensive evaluations on both synthetic\nand real-world datasets demonstrate the superior accuracy, computational\nefficiency, and adaptability of our method compared to state-of-the-art online\nGPSSM techniques.\n","authors":["Tengjie Zheng","Lin Cheng","Shengping Gong","Xu Huang"],"pdf_url":"https://arxiv.org/pdf/2411.14679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14677v1","updated":"2024-11-22T02:18:28Z","published":"2024-11-22T02:18:28Z","title":"Exploring the Use of Machine Learning Weather Models in Data\n  Assimilation","summary":"  The use of machine learning (ML) models in meteorology has attracted\nsignificant attention for their potential to improve weather forecasting\nefficiency and accuracy. GraphCast and NeuralGCM, two promising ML-based\nweather models, are at the forefront of this innovation. However, their\nsuitability for data assimilation (DA) systems, particularly for\nfour-dimensional variational (4DVar) DA, remains under-explored. This study\nevaluates the tangent linear (TL) and adjoint (AD) models of both GraphCast and\nNeuralGCM to assess their viability for integration into a DA framework.\n  We compare the TL/AD results of GraphCast and NeuralGCM with those of the\nModel for Prediction Across Scales - Atmosphere (MPAS-A), a well-established\nnumerical weather prediction (NWP) model. The comparison focuses on the\nphysical consistency and reliability of TL/AD responses to perturbations. While\nthe adjoint results of both GraphCast and NeuralGCM show some similarity to\nthose of MPAS-A, they also exhibit unphysical noise at various vertical levels,\nraising concerns about their robustness for operational DA systems.\n  The implications of this study extend beyond 4DVar applications. Unphysical\nbehavior and noise in ML-derived TL/AD models could lead to inaccurate error\ncovariances and unreliable ensemble forecasts, potentially degrading the\noverall performance of ensemble-based DA systems, as well. Addressing these\nchallenges is critical to ensuring that ML models, such as GraphCast and\nNeuralGCM, can be effectively integrated into operational DA systems, paving\nthe way for more accurate and efficient weather predictions.\n","authors":["Xiaoxu Tian","Daniel Holdaway","Daryl Kleist"],"pdf_url":"https://arxiv.org/pdf/2411.14677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02853v3","updated":"2024-11-22T02:15:10Z","published":"2024-11-05T06:57:47Z","title":"ADOPT: Modified Adam Can Converge with Any $β_2$ with the Optimal\n  Rate","summary":"  Adam is one of the most popular optimization algorithms in deep learning.\nHowever, it is known that Adam does not converge in theory unless choosing a\nhyperparameter, i.e., $\\beta_2$, in a problem-dependent manner. There have been\nmany attempts to fix the non-convergence (e.g., AMSGrad), but they require an\nimpractical assumption that the gradient noise is uniformly bounded. In this\npaper, we propose a new adaptive gradient method named ADOPT, which achieves\nthe optimal convergence rate of $\\mathcal{O} ( 1 / \\sqrt{T} )$ with any choice\nof $\\beta_2$ without depending on the bounded noise assumption. ADOPT addresses\nthe non-convergence issue of Adam by removing the current gradient from the\nsecond moment estimate and changing the order of the momentum update and the\nnormalization by the second moment estimate. We also conduct intensive\nnumerical experiments, and verify that our ADOPT achieves superior results\ncompared to Adam and its variants across a wide range of tasks, including image\nclassification, generative modeling, natural language processing, and deep\nreinforcement learning. The implementation is available at\nhttps://github.com/iShohei220/adopt.\n","authors":["Shohei Taniguchi","Keno Harada","Gouki Minegishi","Yuta Oshima","Seong Cheol Jeong","Go Nagahara","Tomoshi Iiyama","Masahiro Suzuki","Yusuke Iwasawa","Yutaka Matsuo"],"pdf_url":"https://arxiv.org/pdf/2411.02853v3.pdf","comment":"Accepted at Neural Information Processing Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2411.14666v1","updated":"2024-11-22T01:57:14Z","published":"2024-11-22T01:57:14Z","title":"Brain-Computer Interfaces for Emotional Regulation in Patients with\n  Various Disorders","summary":"  Neurological and Physiological Disorders that impact emotional regulation\neach have their own unique characteristics which are important to understand in\norder to create a generalized solution to all of them. The purpose of this\nexperiment is to explore the potential applications of EEG-based Brain-Computer\nInterfaces (BCIs) in enhancing emotional regulation for individuals with\nneurological and physiological disorders. The research focuses on the\ndevelopment of a novel neural network algorithm for understanding EEG data,\nwith a particular emphasis on recognizing and regulating emotional states. The\nprocedure involves the collection of EEG-based emotion data from open-Neuro.\nUsing novel data modification techniques, information from the dataset can be\naltered to create a dataset that has neural patterns of patients with disorders\nwhilst showing emotional change. The data analysis reveals promising results,\nas the algorithm is able to successfully classify emotional states with a high\ndegree of accuracy. This suggests that EEG-based BCIs have the potential to be\na valuable tool in aiding individuals with a range of neurological and\nphysiological disorders in recognizing and regulating their emotions. To\nimprove upon this work, data collection on patients with neurological disorders\nshould be done to improve overall sample diversity.\n","authors":["Vedant Mehta"],"pdf_url":"https://arxiv.org/pdf/2411.14666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14665v1","updated":"2024-11-22T01:54:53Z","published":"2024-11-22T01:54:53Z","title":"Double Machine Learning for Adaptive Causal Representation in\n  High-Dimensional Data","summary":"  Adaptive causal representation learning from observational data is presented,\nintegrated with an efficient sample splitting technique within the\nsemiparametric estimating equation framework. The support points sample\nsplitting (SPSS), a subsampling method based on energy distance, is employed\nfor efficient double machine learning (DML) in causal inference. The support\npoints are selected and split as optimal representative points of the full raw\ndata in a random sample, in contrast to the traditional random splitting, and\nproviding an optimal sub-representation of the underlying data generating\ndistribution. They offer the best representation of a full big dataset, whereas\nthe unit structural information of the underlying distribution via the\ntraditional random data splitting is most likely not preserved. Three machine\nlearning estimators were adopted for causal inference, support vector machine\n(SVM), deep learning (DL), and a hybrid super learner (SL) with deep learning\n(SDL), using SPSS. A comparative study is conducted between the proposed SVM,\nDL, and SDL representations using SPSS, and the benchmark results from\nChernozhukov et al. (2018), which employed random forest, neural network, and\nregression trees with a random k-fold cross-fitting technique on the\n401(k)-pension plan real data. The simulations show that DL with SPSS and the\nhybrid methods of DL and SL with SPSS outperform SVM with SPSS in terms of\ncomputational efficiency and the estimation quality, respectively.\n","authors":["Lynda Aouar","Han Yu"],"pdf_url":"https://arxiv.org/pdf/2411.14665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11788v3","updated":"2024-11-22T01:54:26Z","published":"2024-04-17T22:44:22Z","title":"NonGEMM Bench: Understanding the Performance Horizon of the Latest ML\n  Workloads with NonGEMM Workloads","summary":"  Machine Learning (ML) operators are the building blocks to design ML models\nwith various target applications. GEneral Matrix Multiplication (GEMM)\noperators are the backbone of ML models. They are notorious for being\ncomputationally expensive requiring billions of multiply-and-accumulate.\nTherefore, significant effort has been put to study and optimize the GEMM\noperators in order to speed up the execution of ML models. GPUs and\naccelerators are widely deployed to accelerate ML workloads by optimizing the\nexecution of GEMM operators. Nonetheless, the performance of NonGEMM operators\nhave not been studied as thoroughly as GEMMs. Therefore, this paper describes\n\\bench, a benchmark to study NonGEMM operators. We first construct \\bench using\npopular ML workloads from different domains, then perform case studies on\nvarious grade GPU platforms to analyze the behavior of NonGEMM operators in GPU\naccelerated systems. Finally, we present some key takeaways to bridge the gap\nbetween GEMM and NonGEMM operators and to offer the community with potential\nnew optimization directions.\n","authors":["Rachid Karami","Chakshu Moar","Sheng-Chun Kao","Hyoukjun Kwon"],"pdf_url":"https://arxiv.org/pdf/2404.11788v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14664v1","updated":"2024-11-22T01:43:58Z","published":"2024-11-22T01:43:58Z","title":"Sparsifying Suprema of Gaussian Processes","summary":"  We give a dimension-independent sparsification result for suprema of centered\nGaussian processes: Let $T$ be any (possibly infinite) bounded set of vectors\nin $\\mathbb{R}^n$, and let $\\{{\\boldsymbol{X}}_t\\}_{t\\in T}$ be the canonical\nGaussian process on $T$. We show that there is an $O_\\varepsilon(1)$-size\nsubset $S \\subseteq T$ and a set of real values $\\{c_s\\}_{s \\in S}$ such that\n$\\sup_{s \\in S} \\{{\\boldsymbol{X}}_s + c_s\\}$ is an $\\varepsilon$-approximator\nof $\\sup_{t \\in T} {\\boldsymbol{X}}_t$. Notably, the size of $S$ is completely\nindependent of both the size of $T$ and of the ambient dimension $n$.\n  We use this to show that every norm is essentially a junta when viewed as a\nfunction over Gaussian space: Given any norm $\\nu(x)$ on $\\mathbb{R}^n$, there\nis another norm $\\psi(x)$ which depends only on the projection of $x$ along\n$O_\\varepsilon(1)$ directions, for which $\\psi({\\boldsymbol{g}})$ is a\nmultiplicative $(1 \\pm \\varepsilon)$-approximation of $\\nu({\\boldsymbol{g}})$\nwith probability $1-\\varepsilon$ for ${\\boldsymbol{g}} \\sim N(0,I_n)$.\n  We also use our sparsification result for suprema of centered Gaussian\nprocesses to give a sparsification lemma for convex sets of bounded geometric\nwidth: Any intersection of (possibly infinitely many) halfspaces in\n$\\mathbb{R}^n$ that are at distance $O(1)$ from the origin is\n$\\varepsilon$-close, under $N(0,I_n)$, to an intersection of only\n$O_\\varepsilon(1)$ many halfspaces.\n  We describe applications to agnostic learning and tolerant property testing.\n","authors":["Anindya De","Shivam Nadimpalli","Ryan O'Donnell","Rocco A. Servedio"],"pdf_url":"https://arxiv.org/pdf/2411.14664v1.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2408.10517v2","updated":"2024-11-22T01:42:14Z","published":"2024-08-20T03:35:28Z","title":"Integrating Multi-Modal Input Token Mixer Into Mamba-Based Decision\n  Models: Decision MetaMamba","summary":"  Sequence modeling with State Space models (SSMs) has demonstrated performance\nsurpassing that of Transformers in various tasks, raising expectations for\ntheir potential to outperform the Decision Transformer and its enhanced\nvariants in offline reinforcement learning (RL). However, decision models based\non Mamba, a state-of-the-art SSM, failed to achieve superior performance\ncompared to these enhanced Decision Transformers. We hypothesize that this\nlimitation arises from information loss during the selective scanning phase. To\naddress this, we propose the Decision MetaMamba (DMM), which augments Mamba\nwith a token mixer in its input layer. This mixer explicitly accounts for the\nmultimodal nature of offline RL inputs, comprising state, action, and\nreturn-to-go. The DMM demonstrates improved performance while significantly\nreducing parameter count compared to prior models. Notably, similar performance\ngains were achieved using a simple linear token mixer, emphasizing the\nimportance of preserving information from proximate time steps rather than the\nspecific design of the token mixer itself. This novel modification to Mamba's\ninput layer represents a departure from conventional timestamp-based encoding\napproaches used in Transformers. By enhancing performance of Mamba in offline\nRL, characterized by memory efficiency and fast inference, this work opens new\navenues for its broader application in future RL research.\n","authors":["Wall Kim"],"pdf_url":"https://arxiv.org/pdf/2408.10517v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.04391v9","updated":"2024-11-22T01:41:55Z","published":"2023-02-09T01:09:57Z","title":"The Re-Label Method For Data-Centric Machine Learning","summary":"  In industry deep learning application, our manually labeled data has a\ncertain number of noisy data. To solve this problem and achieve more than 90\nscore in dev dataset, we present a simple method to find the noisy data and\nre-label the noisy data by human, given the model predictions as references in\nhuman labeling. In this paper, we illustrate our idea for a broad set of deep\nlearning tasks, includes classification, sequence tagging, object detection,\nsequence generation, click-through rate prediction. The dev dataset evaluation\nresults and human evaluation results verify our idea.\n","authors":["Tong Guo"],"pdf_url":"https://arxiv.org/pdf/2302.04391v9.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20772v3","updated":"2024-11-22T01:41:37Z","published":"2024-10-28T06:17:20Z","title":"Introducing Spectral Attention for Long-Range Dependency in Time Series\n  Forecasting","summary":"  Sequence modeling faces challenges in capturing long-range dependencies\nacross diverse tasks. Recent linear and transformer-based forecasters have\nshown superior performance in time series forecasting. However, they are\nconstrained by their inherent inability to effectively address long-range\ndependencies in time series data, primarily due to using fixed-size inputs for\nprediction. Furthermore, they typically sacrifice essential temporal\ncorrelation among consecutive training samples by shuffling them into\nmini-batches. To overcome these limitations, we introduce a fast and effective\nSpectral Attention mechanism, which preserves temporal correlations among\nsamples and facilitates the handling of long-range information while\nmaintaining the base model structure. Spectral Attention preserves long-period\ntrends through a low-pass filter and facilitates gradient to flow between\nsamples. Spectral Attention can be seamlessly integrated into most sequence\nmodels, allowing models with fixed-sized look-back windows to capture\nlong-range dependencies over thousands of steps. Through extensive experiments\non 11 real-world time series datasets using 7 recent forecasting models, we\nconsistently demonstrate the efficacy of our Spectral Attention mechanism,\nachieving state-of-the-art results.\n","authors":["Bong Gyun Kang","Dongjun Lee","HyunGi Kim","DoHyun Chung","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2410.20772v3.pdf","comment":"Co-first Author: Bong Gyun Kang, Dongjun Lee. NeurIPS 2024\n  (Conference on Neural Information Processing Systems)"},{"id":"http://arxiv.org/abs/2411.14662v1","updated":"2024-11-22T01:38:47Z","published":"2024-11-22T01:38:47Z","title":"Multiset Transformer: Advancing Representation Learning in Persistence\n  Diagrams","summary":"  To improve persistence diagram representation learning, we propose Multiset\nTransformer. This is the first neural network that utilizes attention\nmechanisms specifically designed for multisets as inputs and offers rigorous\ntheoretical guarantees of permutation invariance. The architecture integrates\nmultiset-enhanced attentions with a pool-decomposition scheme, allowing\nmultiplicities to be preserved across equivariant layers. This capability\nenables full leverage of multiplicities while significantly reducing both\ncomputational and spatial complexity compared to the Set Transformer.\nAdditionally, our method can greatly benefit from clustering as a preprocessing\nstep to further minimize complexity, an advantage not possessed by the Set\nTransformer. Experimental results demonstrate that the Multiset Transformer\noutperforms existing neural network methods in the realm of persistence diagram\nrepresentation learning.\n","authors":["Minghua Wang","Ziyun Huang","Jinhui Xu"],"pdf_url":"https://arxiv.org/pdf/2411.14662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04683v2","updated":"2024-11-22T00:52:08Z","published":"2024-10-07T01:34:42Z","title":"Towards Measuring Goal-Directedness in AI Systems","summary":"  Recent advances in deep learning have brought attention to the possibility of\ncreating advanced, general AI systems that outperform humans across many tasks.\nHowever, if these systems pursue unintended goals, there could be catastrophic\nconsequences. A key prerequisite for AI systems pursuing unintended goals is\nwhether they will behave in a coherent and goal-directed manner in the first\nplace, optimizing for some unknown goal; there exists significant research\ntrying to evaluate systems for said behaviors. However, the most rigorous\ndefinitions of goal-directedness we currently have are difficult to compute in\nreal-world settings. Drawing upon this previous literature, we explore policy\ngoal-directedness within reinforcement learning (RL) environments. In our\nfindings, we propose a different family of definitions of the goal-directedness\nof a policy that analyze whether it is well-modeled as near-optimal for many\n(sparse) reward functions. We operationalize this preliminary definition of\ngoal-directedness and test it in toy Markov decision process (MDP)\nenvironments. Furthermore, we explore how goal-directedness could be measured\nin frontier large-language models (LLMs). Our contribution is a definition of\ngoal-directedness that is simpler and more easily computable in order to\napproach the question of whether AI systems could pursue dangerous goals. We\nrecommend further exploration of measuring coherence and goal-directedness,\nbased on our findings.\n","authors":["Dylan Xu","Juan-Pablo Rivera"],"pdf_url":"https://arxiv.org/pdf/2410.04683v2.pdf","comment":"Updated acknowledgements"},{"id":"http://arxiv.org/abs/2411.07482v2","updated":"2024-11-22T00:48:57Z","published":"2024-11-12T02:08:19Z","title":"Enhancing Link Prediction with Fuzzy Graph Attention Networks and\n  Dynamic Negative Sampling","summary":"  Link prediction is crucial for understanding complex networks but traditional\nGraph Neural Networks (GNNs) often rely on random negative sampling, leading to\nsuboptimal performance. This paper introduces Fuzzy Graph Attention Networks\n(FGAT), a novel approach integrating fuzzy rough sets for dynamic negative\nsampling and enhanced node feature aggregation. Fuzzy Negative Sampling (FNS)\nsystematically selects high-quality negative edges based on fuzzy similarities,\nimproving training efficiency. FGAT layer incorporates fuzzy rough set\nprinciples, enabling robust and discriminative node representations.\nExperiments on two research collaboration networks demonstrate FGAT's superior\nlink prediction accuracy, outperforming state-of-the-art baselines by\nleveraging the power of fuzzy rough sets for effective negative sampling and\nnode feature learning.\n","authors":["Jinming Xing","Ruilin Xing"],"pdf_url":"https://arxiv.org/pdf/2411.07482v2.pdf","comment":"5 pages"}],"Multimedia":[{"id":"http://arxiv.org/abs/2411.15128v1","updated":"2024-11-22T18:51:51Z","published":"2024-11-22T18:51:51Z","title":"Health AI Developer Foundations","summary":"  Robust medical Machine Learning (ML) models have the potential to\nrevolutionize healthcare by accelerating clinical research, improving workflows\nand outcomes, and producing novel insights or capabilities. Developing such ML\nmodels from scratch is cost prohibitive and requires substantial compute, data,\nand time (e.g., expert labeling). To address these challenges, we introduce\nHealth AI Developer Foundations (HAI-DEF), a suite of pre-trained,\ndomain-specific foundation models, tools, and recipes to accelerate building ML\nfor health applications. The models cover various modalities and domains,\nincluding radiology (X-rays and computed tomography), histopathology,\ndermatological imaging, and audio. These models provide domain specific\nembeddings that facilitate AI development with less labeled data, shorter\ntraining times, and reduced computational costs compared to traditional\napproaches. In addition, we utilize a common interface and style across these\nmodels, and prioritize usability to enable developers to integrate HAI-DEF\nefficiently. We present model evaluations across various tasks and conclude\nwith a discussion of their application and evaluation, covering the importance\nof ensuring efficacy, fairness, and equity. Finally, while HAI-DEF and\nspecifically the foundation models lower the barrier to entry for ML in\nhealthcare, we emphasize the importance of validation with problem- and\npopulation-specific data for each desired usage setting. This technical report\nwill be updated over time as more modalities and features are added.\n","authors":["Atilla P. Kiraly","Sebastien Baur","Kenneth Philbrick","Fereshteh Mahvar","Liron Yatziv","Tiffany Chen","Bram Sterling","Nick George","Fayaz Jamil","Jing Tang","Kai Bailey","Faruk Ahmed","Akshay Goel","Abbi Ward","Lin Yang","Andrew Sellergren","Yossi Matias","Avinatan Hassidim","Shravya Shetty","Daniel Golden","Shekoofeh Azizi","David F. Steiner","Yun Liu","Tim Thelin","Rory Pilgrim","Can Kirmizibayrak"],"pdf_url":"https://arxiv.org/pdf/2411.15128v1.pdf","comment":"16 pages, 8 figures"},{"id":"http://arxiv.org/abs/2305.09145v2","updated":"2024-11-22T07:23:23Z","published":"2023-05-16T03:51:34Z","title":"Deep ReLU Networks Have Surprisingly Simple Polytopes","summary":"  A ReLU network is a piecewise linear function over polytopes. Figuring out\nthe properties of such polytopes is of fundamental importance for the research\nand development of neural networks. So far, either theoretical or empirical\nstudies on polytopes only stay at the level of counting their number, which is\nfar from a complete characterization. Here, we propose to study the shapes of\npolytopes via the number of faces of the polytope. Then, by computing and\nanalyzing the histogram of faces across polytopes, we find that a ReLU network\nhas relatively simple polytopes under both initialization and gradient descent,\nalthough these polytopes can be rather diverse and complicated by a specific\ndesign. This finding can be appreciated as a kind of generalized implicit bias,\nsubjected to the intrinsic geometric constraint in space partition of a ReLU\nnetwork. Next, we perform a combinatorial analysis to explain why adding depth\ndoes not generate a more complicated polytope by bounding the average number of\nfaces of polytopes with the dimensionality. Our results concretely reveal what\nkind of simple functions a network learns and what will happen when a network\ngoes deep. Also, by characterizing the shape of polytopes, the number of faces\ncan be a novel leverage for other problems, \\textit{e.g.}, serving as a generic\ntool to explain the power of popular shortcut networks such as ResNet and\nanalyzing the impact of different regularization strategies on a network's\nspace partition.\n","authors":["Feng-Lei Fan","Wei Huang","Xiangru Zhong","Lecheng Ruan","Tieyong Zeng","Huan Xiong","Fei Wang"],"pdf_url":"https://arxiv.org/pdf/2305.09145v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.00262v3","updated":"2024-11-22T22:56:10Z","published":"2021-07-31T15:13:39Z","title":"Speech2AffectiveGestures: Synthesizing Co-Speech Gestures with\n  Generative Adversarial Affective Expression Learning","summary":"  We present a generative adversarial network to synthesize 3D pose sequences\nof co-speech upper-body gestures with appropriate affective expressions. Our\nnetwork consists of two components: a generator to synthesize gestures from a\njoint embedding space of features encoded from the input speech and the seed\nposes, and a discriminator to distinguish between the synthesized pose\nsequences and real 3D pose sequences. We leverage the Mel-frequency cepstral\ncoefficients and the text transcript computed from the input speech in separate\nencoders in our generator to learn the desired sentiments and the associated\naffective cues. We design an affective encoder using multi-scale\nspatial-temporal graph convolutions to transform 3D pose sequences into latent,\npose-based affective features. We use our affective encoder in both our\ngenerator, where it learns affective features from the seed poses to guide the\ngesture synthesis, and our discriminator, where it enforces the synthesized\ngestures to contain the appropriate affective expressions. We perform extensive\nevaluations on two benchmark datasets for gesture synthesis from the speech,\nthe TED Gesture Dataset and the GENEA Challenge 2020 Dataset. Compared to the\nbest baselines, we improve the mean absolute joint error by 10--33%, the mean\nacceleration difference by 8--58%, and the Fr\\'echet Gesture Distance by\n21--34%. We also conduct a user study and observe that compared to the best\ncurrent baselines, around 15.28% of participants indicated our synthesized\ngestures appear more plausible, and around 16.32% of participants felt the\ngestures had more appropriate affective expressions aligned with the speech.\n","authors":["Uttaran Bhattacharya","Elizabeth Childs","Nicholas Rewkowski","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2108.00262v3.pdf","comment":"11 pages, 4 figures, 2 tables. Proceedings of the 29th ACM\n  International Conference on Multimedia, October 20-24, 2021, Virtual Event,\n  China"},{"id":"http://arxiv.org/abs/2303.03870v3","updated":"2024-11-22T22:44:49Z","published":"2023-01-30T22:20:24Z","title":"DanceAnyWay: Synthesizing Beat-Guided 3D Dances with Randomized Temporal\n  Contrastive Learning","summary":"  We present DanceAnyWay, a generative learning method to synthesize\nbeat-guided dances of 3D human characters synchronized with music. Our method\nlearns to disentangle the dance movements at the beat frames from the dance\nmovements at all the remaining frames by operating at two hierarchical levels.\nAt the coarser \"beat\" level, it encodes the rhythm, pitch, and melody\ninformation of the input music via dedicated feature representations only at\nthe beat frames. It leverages them to synthesize the beat poses of the target\ndances using a sequence-to-sequence learning framework. At the finer\n\"repletion\" level, our method encodes similar rhythm, pitch, and melody\ninformation from all the frames of the input music via dedicated feature\nrepresentations. It generates the full dance sequences by combining the\nsynthesized beat and repletion poses and enforcing plausibility through an\nadversarial learning framework. Our training paradigm also enforces\nfine-grained diversity in the synthesized dances through a randomized temporal\ncontrastive loss, which ensures different segments of the dance sequences have\ndifferent movements and avoids motion freezing or collapsing to repetitive\nmovements. We evaluate the performance of our approach through extensive\nexperiments on the benchmark AIST++ dataset and observe improvements of about\n7%-12% in motion quality metrics and 1.5%-4% in motion diversity metrics over\nthe current baselines, respectively. We also conducted a user study to evaluate\nthe visual quality of our synthesized dances. We note that, on average, the\nsamples generated by our method were about 9-48% more preferred by the\nparticipants and had a 4-27% better five-point Likert-scale score over the best\navailable current baseline in terms of motion quality and synchronization. Our\nsource code and project page are available at\nhttps://github.com/aneeshbhattacharya/DanceAnyWay.\n","authors":["Aneesh Bhattacharya","Manas Paranjape","Uttaran Bhattacharya","Aniket Bera"],"pdf_url":"https://arxiv.org/pdf/2303.03870v3.pdf","comment":"11 pages, 7 figures, 3 tables. To appear as part of the proceedings\n  of the 38th Annual AAAI Conference on Artificial Intelligence, 2024"}]},"2024-11-25T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2411.16679v1","updated":"2024-11-25T18:59:30Z","published":"2024-11-25T18:59:30Z","title":"Do Large Language Models Perform Latent Multi-Hop Reasoning without\n  Exploiting Shortcuts?","summary":"  We evaluate how well Large Language Models (LLMs) latently recall and compose\nfacts to answer multi-hop queries like \"In the year Scarlett Johansson was\nborn, the Summer Olympics were hosted in the country of\". One major challenge\nin evaluating this ability is that LLMs may have developed shortcuts by\nencounters of the head entity \"Scarlett Johansson\" and the answer entity\n\"United States\" in the same training sequences or merely guess the answer based\non frequency-based priors. To prevent shortcuts, we exclude test queries where\nthe head and answer entities co-appear in pretraining corpora. Through careful\nselection of relations and facts and systematic removal of cases where models\nmight guess answers or exploit partial matches, we construct an evaluation\ndataset SOCRATES (ShOrtCut-fRee lATent rEaSoning). We observe that LLMs\ndemonstrate promising latent multi-hop reasoning abilities without exploiting\nshortcuts, but only for certain types of queries. For queries requiring latent\nrecall of countries as the intermediate answer, the best models achieve 80%\nlatent composability, but this drops to just 5% for the recall of years.\nComparisons with Chain-of-Thought composability highlight a significant gap\nbetween the ability of models to reason latently versus explicitly. Analysis\nreveals that latent representations of the intermediate answer are constructed\nmore often in queries with higher latent composability, and shows the emergence\nof latent multi-hop reasoning during pretraining.\n","authors":["Sohee Yang","Nora Kassner","Elena Gribovskaya","Sebastian Riedel","Mor Geva"],"pdf_url":"https://arxiv.org/pdf/2411.16679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16657v1","updated":"2024-11-25T18:41:56Z","published":"2024-11-25T18:41:56Z","title":"DreamRunner: Fine-Grained Storytelling Video Generation with\n  Retrieval-Augmented Motion Adaptation","summary":"  Storytelling video generation (SVG) has recently emerged as a task to create\nlong, multi-motion, multi-scene videos that consistently represent the story\ndescribed in the input text script. SVG holds great potential for diverse\ncontent creation in media and entertainment; however, it also presents\nsignificant challenges: (1) objects must exhibit a range of fine-grained,\ncomplex motions, (2) multiple objects need to appear consistently across\nscenes, and (3) subjects may require multiple motions with seamless transitions\nwithin a single scene. To address these challenges, we propose DreamRunner, a\nnovel story-to-video generation method: First, we structure the input script\nusing a large language model (LLM) to facilitate both coarse-grained scene\nplanning as well as fine-grained object-level layout and motion planning. Next,\nDreamRunner presents retrieval-augmented test-time adaptation to capture target\nmotion priors for objects in each scene, supporting diverse motion\ncustomization based on retrieved videos, thus facilitating the generation of\nnew videos with complex, scripted motions. Lastly, we propose a novel\nspatial-temporal region-based 3D attention and prior injection module SR3AI for\nfine-grained object-motion binding and frame-by-frame semantic control. We\ncompare DreamRunner with various SVG baselines, demonstrating state-of-the-art\nperformance in character consistency, text alignment, and smooth transitions.\nAdditionally, DreamRunner exhibits strong fine-grained condition-following\nability in compositional text-to-video generation, significantly outperforming\nbaselines on T2V-ComBench. Finally, we validate DreamRunner's robust ability to\ngenerate multi-object interactions with qualitative examples.\n","authors":["Zun Wang","Jialu Li","Han Lin","Jaehong Yoon","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2411.16657v1.pdf","comment":"Project website: https://dreamrunner-story2video.github.io/"},{"id":"http://arxiv.org/abs/2411.16646v1","updated":"2024-11-25T18:28:26Z","published":"2024-11-25T18:28:26Z","title":"Self-Generated Critiques Boost Reward Modeling for Language Models","summary":"  Reward modeling is crucial for aligning large language models (LLMs) with\nhuman preferences, especially in reinforcement learning from human feedback\n(RLHF). However, current reward models mainly produce scalar scores and\nstruggle to incorporate critiques in a natural language format. We hypothesize\nthat predicting both critiques and the scalar reward would improve reward\nmodeling ability. Motivated by this, we propose Critic-RM, a framework that\nimproves reward models using self-generated critiques without extra\nsupervision. Critic-RM employs a two-stage process: generating and filtering\nhigh-quality critiques, followed by joint fine-tuning on reward prediction and\ncritique generation. Experiments across benchmarks show that Critic-RM improves\nreward modeling accuracy by 3.7%-7.3% compared to standard reward models and\nLLM judges, demonstrating strong performance and data efficiency. Additional\nstudies further validate the effectiveness of generated critiques in rectifying\nflawed reasoning steps with 2.5%-3.2% gains in improving reasoning accuracy.\n","authors":["Yue Yu","Zhengxing Chen","Aston Zhang","Liang Tan","Chenguang Zhu","Richard Yuanzhe Pang","Yundi Qian","Xuewei Wang","Suchin Gururangan","Chao Zhang","Melanie Kambadur","Dhruv Mahajan","Rui Hou"],"pdf_url":"https://arxiv.org/pdf/2411.16646v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2411.16642v1","updated":"2024-11-25T18:23:58Z","published":"2024-11-25T18:23:58Z","title":"Preventing Jailbreak Prompts as Malicious Tools for Cybercriminals: A\n  Cyber Defense Perspective","summary":"  Jailbreak prompts pose a significant threat in AI and cybersecurity, as they\nare crafted to bypass ethical safeguards in large language models, potentially\nenabling misuse by cybercriminals. This paper analyzes jailbreak prompts from a\ncyber defense perspective, exploring techniques like prompt injection and\ncontext manipulation that allow harmful content generation, content filter\nevasion, and sensitive information extraction. We assess the impact of\nsuccessful jailbreaks, from misinformation and automated social engineering to\nhazardous content creation, including bioweapons and explosives. To address\nthese threats, we propose strategies involving advanced prompt analysis,\ndynamic safety protocols, and continuous model fine-tuning to strengthen AI\nresilience. Additionally, we highlight the need for collaboration among AI\nresearchers, cybersecurity experts, and policymakers to set standards for\nprotecting AI systems. Through case studies, we illustrate these cyber defense\napproaches, promoting responsible AI practices to maintain system integrity and\npublic trust. \\textbf{\\color{red}Warning: This paper contains content which the\nreader may find offensive.}\n","authors":["Jean Marie Tshimula","Xavier Ndona","D'Jeff K. Nkashama","Pierre-Martin Tardif","Froduald Kabanza","Marc Frappier","Shengrui Wang"],"pdf_url":"https://arxiv.org/pdf/2411.16642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16638v1","updated":"2024-11-25T18:15:15Z","published":"2024-11-25T18:15:15Z","title":"Do Automatic Factuality Metrics Measure Factuality? A Critical\n  Evaluation","summary":"  Modern LLMs can now produce highly readable abstractive summaries, to the\npoint where traditional automated metrics for evaluating summary quality, such\nas ROUGE, have become saturated. However, LLMs still sometimes introduce\nunwanted content into summaries, i.e., information inconsistent with or\nunsupported by their source. Measuring the occurrence of these often subtle\n``hallucinations'' automatically has proved to be challenging. This in turn has\nmotivated development of a variety of metrics intended to measure the factual\nconsistency of generated summaries against their source. But are these\napproaches measuring what they purport to do? In this work, we stress-test\nautomatic factuality metrics. Specifically, we investigate whether and to what\ndegree superficial attributes of summary texts suffice to predict\n``factuality'', finding that a (supervised) model using only such shallow\nfeatures is reasonably competitive with SOTA factuality scoring methods. We\nthen evaluate how factuality metrics respond to factual corrections in\ninconsistent summaries and find that only a few show meaningful improvements.\nIn contrast, some metrics are more sensitive to benign, non-factual edits.\nMotivated by these insights, we show that one can ``game'' (most) automatic\nfactuality metrics, i.e., reliably inflate ``factuality'' scores by appending\ninnocuous sentences to generated summaries.Taken together, our results raise\nquestions about the degree to which we should rely on existing automated\nfactuality metrics and what exactly we want ``factuality metrics'' to measure.\n","authors":["Sanjana Ramprasad","Byron C. Wallace"],"pdf_url":"https://arxiv.org/pdf/2411.16638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.09059v2","updated":"2024-11-25T18:01:56Z","published":"2023-07-18T08:23:46Z","title":"Text-guided Image Restoration and Semantic Enhancement for Text-to-Image\n  Person Retrieval","summary":"  The goal of Text-to-Image Person Retrieval (TIPR) is to retrieve specific\nperson images according to the given textual descriptions. A primary challenge\nin this task is bridging the substantial representational gap between visual\nand textual modalities. The prevailing methods map texts and images into\nunified embedding space for matching, while the intricate semantic\ncorrespondences between texts and images are still not effectively constructed.\nTo address this issue, we propose a novel TIPR framework to build fine-grained\ninteractions and alignment between person images and the corresponding texts.\nSpecifically, via fine-tuning the Contrastive Language-Image Pre-training\n(CLIP) model, a visual-textual dual encoder is firstly constructed, to\npreliminarily align the image and text features. Secondly, a Text-guided Image\nRestoration (TIR) auxiliary task is proposed to map abstract textual entities\nto specific image regions, improving the alignment between local textual and\nvisual embeddings. Additionally, a cross-modal triplet loss is presented to\nhandle hard samples, and further enhance the model's discriminability for minor\ndifferences. Moreover, a pruning-based text data augmentation approach is\nproposed to enhance focus on essential elements in descriptions, thereby\navoiding excessive model attention to less significant information. The\nexperimental results show our proposed method outperforms state-of-the-art\nmethods on three popular benchmark datasets, and the code will be made publicly\navailable at https://github.com/Delong-liu-bupt/SEN.\n","authors":["Delong Liu","Haiwen Li","Zhicheng Zhao","Yuan Dong","Nikolaos V. Boulgouris"],"pdf_url":"https://arxiv.org/pdf/2307.09059v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14405v2","updated":"2024-11-25T17:57:55Z","published":"2024-11-21T18:37:33Z","title":"Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions","summary":"  Currently OpenAI o1 sparks a surge of interest in the study of large\nreasoning models (LRM). Building on this momentum, Marco-o1 not only focuses on\ndisciplines with standard answers, such as mathematics, physics, and coding --\nwhich are well-suited for reinforcement learning (RL) -- but also places\ngreater emphasis on open-ended resolutions. We aim to address the question:\n''Can the o1 model effectively generalize to broader domains where clear\nstandards are absent and rewards are challenging to quantify?'' Marco-o1 is\npowered by Chain-of-Thought (CoT) fine-tuning, Monte Carlo Tree Search (MCTS),\nreflection mechanisms, and innovative reasoning strategies -- optimized for\ncomplex real-world problem-solving tasks.\n","authors":["Yu Zhao","Huifeng Yin","Bo Zeng","Hao Wang","Tianqi Shi","Chenyang Lyu","Longyue Wang","Weihua Luo","Kaifu Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.14405v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16618v1","updated":"2024-11-25T17:57:52Z","published":"2024-11-25T17:57:52Z","title":"StructFormer: Document Structure-based Masked Attention and its Impact\n  on Language Model Pre-Training","summary":"  Most state-of-the-art techniques for Language Models (LMs) today rely on\ntransformer-based architectures and their ubiquitous attention mechanism.\nHowever, the exponential growth in computational requirements with longer input\nsequences confines Transformers to handling short passages. Recent efforts have\naimed to address this limitation by introducing selective attention mechanisms,\nnotably local and global attention. While sparse attention mechanisms, akin to\nfull attention in being Turing-complete, have been theoretically established,\ntheir practical impact on pre-training remains unexplored. This study focuses\non empirically assessing the influence of global attention on BERT\npre-training. The primary steps involve creating an extensive corpus of\nstructure-aware text through arXiv data, alongside a text-only counterpart. We\ncarry out pre-training on these two datasets, investigate shifts in attention\npatterns, and assess their implications for downstream tasks. Our analysis\nunderscores the significance of incorporating document structure into LM\nmodels, demonstrating their capacity to excel in more abstract tasks, such as\ndocument understanding.\n","authors":["Kaustubh Ponkshe","Venkatapathy Subramanian","Natwar Modani","Ganesh Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2411.16618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16613v1","updated":"2024-11-25T17:48:59Z","published":"2024-11-25T17:48:59Z","title":"Recent Trends in Linear Text Segmentation: a Survey","summary":"  Linear Text Segmentation is the task of automatically tagging text documents\nwith topic shifts, i.e. the places in the text where the topics change. A\nwell-established area of research in Natural Language Processing, drawing from\nwell-understood concepts in linguistic and computational linguistic research,\nthe field has recently seen a lot of interest as a result of the surge of text,\nvideo, and audio available on the web, which in turn require ways of\nsummarising and categorizing the mole of content for which linear text\nsegmentation is a fundamental step. In this survey, we provide an extensive\noverview of current advances in linear text segmentation, describing the state\nof the art in terms of resources and approaches for the task. Finally, we\nhighlight the limitations of available resources and of the task itself, while\nindicating ways forward based on the most recent literature and under-explored\nresearch directions.\n","authors":["Iacopo Ghinassi","Lin Wang","Chris Newell","Matthew Purver"],"pdf_url":"https://arxiv.org/pdf/2411.16613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08509v2","updated":"2024-11-25T17:35:07Z","published":"2024-04-12T14:46:15Z","title":"Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\n  Prediction","summary":"  Large language models (LLMs) have been driving a new wave of interactive AI\napplications across numerous domains. However, efficiently serving LLM\ninference requests is challenging due to their unpredictable execution times\noriginating from the autoregressive nature of generative models. Existing LLM\nserving systems exploit first-come-first-serve (FCFS) scheduling, suffering\nfrom head-of-line blocking issues. To address the non-deterministic nature of\nLLMs and enable efficient interactive LLM serving, we present a speculative\nshortest-job-first (SSJF) scheduler that uses a light proxy model to predict\nLLM output sequence lengths. Our open-source SSJF implementation does not\nrequire changes to memory management or batching strategies. Evaluations on\nreal-world datasets and production workload traces show that SSJF reduces\naverage job completion times by 30.5-39.6% and increases throughput by 2.2-3.6x\ncompared to FCFS schedulers, across no batching, dynamic batching, and\ncontinuous batching settings.\n","authors":["Haoran Qiu","Weichao Mao","Archit Patke","Shengkun Cui","Saurabh Jha","Chen Wang","Hubertus Franke","Zbigniew T. Kalbarczyk","Tamer Başar","Ravishankar K. Iyer"],"pdf_url":"https://arxiv.org/pdf/2404.08509v2.pdf","comment":"Accepted at AIOps'24"},{"id":"http://arxiv.org/abs/2411.16594v1","updated":"2024-11-25T17:28:44Z","published":"2024-11-25T17:28:44Z","title":"From Generation to Judgment: Opportunities and Challenges of\n  LLM-as-a-judge","summary":"  Assessment and evaluation have long been critical challenges in artificial\nintelligence (AI) and natural language processing (NLP). However, traditional\nmethods, whether matching-based or embedding-based, often fall short of judging\nsubtle attributes and delivering satisfactory results. Recent advancements in\nLarge Language Models (LLMs) inspire the \"LLM-as-a-judge\" paradigm, where LLMs\nare leveraged to perform scoring, ranking, or selection across various tasks\nand applications. This paper provides a comprehensive survey of LLM-based\njudgment and assessment, offering an in-depth overview to advance this emerging\nfield. We begin by giving detailed definitions from both input and output\nperspectives. Then we introduce a comprehensive taxonomy to explore\nLLM-as-a-judge from three dimensions: what to judge, how to judge and where to\njudge. Finally, we compile benchmarks for evaluating LLM-as-a-judge and\nhighlight key challenges and promising directions, aiming to provide valuable\ninsights and inspire future research in this promising research area. Paper\nlist and more resources about LLM-as-a-judge can be found at\n\\url{https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge} and\n\\url{https://llm-as-a-judge.github.io}.\n","authors":["Dawei Li","Bohan Jiang","Liangjie Huang","Alimohammad Beigi","Chengshuai Zhao","Zhen Tan","Amrita Bhattacharjee","Yuxuan Jiang","Canyu Chen","Tianhao Wu","Kai Shu","Lu Cheng","Huan Liu"],"pdf_url":"https://arxiv.org/pdf/2411.16594v1.pdf","comment":"32 pages, 5 figures"},{"id":"http://arxiv.org/abs/2411.16579v1","updated":"2024-11-25T17:11:54Z","published":"2024-11-25T17:11:54Z","title":"Enhancing LLM Reasoning via Critique Models with Test-Time and\n  Training-Time Supervision","summary":"  Training large language models (LLMs) to spend more time thinking and\nreflection before responding is crucial for effectively solving complex\nreasoning tasks in fields such as science, coding, and mathematics. However,\nthe effectiveness of mechanisms like self-reflection and self-correction\ndepends on the model's capacity to accurately assess its own performance, which\ncan be limited by factors such as initial accuracy, question difficulty, and\nthe lack of external feedback. In this paper, we delve into a two-player\nparadigm that separates the roles of reasoning and critique models, where the\ncritique model provides step-level feedback to supervise the reasoning (actor)\nmodel during both test-time and train-time. We first propose AutoMathCritique,\nan automated and scalable framework for collecting critique data, resulting in\na dataset of $76,321$ responses paired with step-level feedback. Fine-tuning\nlanguage models with this dataset enables them to generate natural language\nfeedback for mathematical reasoning. We demonstrate that the critique models\nconsistently improve the actor's performance on difficult queries at test-time,\nespecially when scaling up inference-time computation. Motivated by these\nfindings, we introduce the critique-based supervision to the actor's\nself-training process, and propose a critique-in-the-loop self-improvement\nmethod. Experiments show that the method improves the actor's exploration\nefficiency and solution diversity, especially on challenging queries, leading\nto a stronger reasoning model. Lastly, we take the preliminary step to explore\ntraining self-talk reasoning models via critique supervision and showcase its\npotential. Our code and datasets are at\n\\href{https://mathcritique.github.io/}{https://mathcritique.github.io/}.\n","authors":["Zhiheng Xi","Dingwen Yang","Jixuan Huang","Jiafu Tang","Guanyu Li","Yiwen Ding","Wei He","Boyang Hong","Shihan Do","Wenyu Zhan","Xiao Wang","Rui Zheng","Tao Ji","Xiaowei Shi","Yitao Zhai","Rongxiang Weng","Jingang Wang","Xunliang Cai","Tao Gui","Zuxuan Wu","Qi Zhang","Xipeng Qiu","Xuanjing Huang","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2411.16579v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2409.18992v2","updated":"2024-11-25T16:55:33Z","published":"2024-09-17T22:10:05Z","title":"A Review of Mechanistic Models of Event Comprehension","summary":"  This review examines theoretical assumptions and computational models of\nevent comprehension, tracing the evolution from discourse comprehension\ntheories to contemporary event cognition frameworks. The review covers key\ndiscourse comprehension accounts, including Construction-Integration, Event\nIndexing, Causal Network, and Resonance models, highlighting their\ncontributions to understanding cognitive processes in comprehension. I then\ndiscuss contemporary theoretical frameworks of event comprehension, including\nEvent Segmentation Theory (Zacks et al., 2007), the Event Horizon Model\n(Radvansky & Zacks, 2014), and Hierarchical Generative Framework (Kuperberg,\n2021), which emphasize prediction, causality, and multilevel representations in\nevent understanding. Building on these theories, I evaluate five computational\nmodels of event comprehension: REPRISE (Butz et al., 2019), Structured Event\nMemory (SEM; Franklin et al., 2020), the Lu model (Lu et al., 2022), the\nGumbsch model (Gumbsch et al., 2022), and the Elman and McRae model (2019). The\nanalysis focuses on their approaches to hierarchical processing, prediction\nmechanisms, and representation learning. Key themes that emerge include the use\nof hierarchical structures as inductive biases, the importance of prediction in\ncomprehension, and diverse strategies for learning event dynamics. The review\nidentifies critical areas for future research, including the need for more\nsophisticated approaches to learning structured representations, integrating\nepisodic memory mechanisms, and developing adaptive updating algorithms for\nworking event models. By synthesizing insights from both theoretical frameworks\nand computational implementations, this review aims to advance our\nunderstanding of human event comprehension and guide future modeling efforts in\ncognitive science.\n","authors":["Tan T. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2409.18992v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10371v2","updated":"2024-11-25T16:55:09Z","published":"2024-11-15T17:19:42Z","title":"A Survey of Event Causality Identification: Principles, Taxonomy,\n  Challenges, and Assessment","summary":"  Event Causality Identification (ECI) has become a crucial task in Natural\nLanguage Processing (NLP), aimed at automatically extracting causalities from\ntextual data. In this survey, we systematically address the foundational\nprinciples, technical frameworks, and challenges of ECI, offering a\ncomprehensive taxonomy to categorize and clarify current research\nmethodologies, as well as a quantitative assessment of existing models. We\nfirst establish a conceptual framework for ECI, outlining key definitions,\nproblem formulations, and evaluation standards. Our taxonomy classifies ECI\nmethods according to the two primary tasks of sentence-level (SECI) and\ndocument-level (DECI) event causality identification. For SECI, we examine\nfeature pattern-based matching, deep semantic encoding, causal knowledge\npre-training and prompt-based fine-tuning, and external knowledge enhancement\nmethods. For DECI, we highlight approaches focused on event graph reasoning and\nprompt-based techniques to address the complexity of cross-sentence causal\ninference. Additionally, we analyze the strengths, limitations, and open\nchallenges of each approach. We further conduct an extensive quantitative\nevaluation of various ECI methods on two benchmark datasets. Finally, we\nexplore future research directions, highlighting promising pathways to overcome\ncurrent limitations and broaden ECI applications.\n","authors":["Qing Cheng","Zefan Zeng","Xingchen Hu","Yuehang Si","Zhong Liu"],"pdf_url":"https://arxiv.org/pdf/2411.10371v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16561v1","updated":"2024-11-25T16:47:10Z","published":"2024-11-25T16:47:10Z","title":"EnStack: An Ensemble Stacking Framework of Large Language Models for\n  Enhanced Vulnerability Detection in Source Code","summary":"  Automated detection of software vulnerabilities is critical for enhancing\nsecurity, yet existing methods often struggle with the complexity and diversity\nof modern codebases. In this paper, we introduce EnStack, a novel ensemble\nstacking framework that enhances vulnerability detection using natural language\nprocessing (NLP) techniques. Our approach synergizes multiple pre-trained large\nlanguage models (LLMs) specialized in code understanding CodeBERT for semantic\nanalysis, GraphCodeBERT for structural representation, and UniXcoder for\ncross-modal capabilities. By fine-tuning these models on the Draper VDISC\ndataset and integrating their outputs through meta-classifiers such as Logistic\nRegression, Support Vector Machines (SVM), Random Forest, and XGBoost, EnStack\neffectively captures intricate code patterns and vulnerabilities that\nindividual models may overlook. The meta-classifiers consolidate the strengths\nof each LLM, resulting in a comprehensive model that excels in detecting subtle\nand complex vulnerabilities across diverse programming contexts. Experimental\nresults demonstrate that EnStack significantly outperforms existing methods,\nachieving notable improvements in accuracy, precision, recall, and F1-score.\nThis work highlights the potential of ensemble LLM approaches in code analysis\ntasks and offers valuable insights into applying NLP techniques for advancing\nautomated vulnerability detection.\n","authors":["Shahriyar Zaman Ridoy","Md. Shazzad Hossain Shaon","Alfredo Cuzzocrea","Mst Shapna Akter"],"pdf_url":"https://arxiv.org/pdf/2411.16561v1.pdf","comment":"Accepted in 2024 IEEE International Conference on Big Data (IEEE\n  BigData 2024)"},{"id":"http://arxiv.org/abs/2411.16537v1","updated":"2024-11-25T16:21:34Z","published":"2024-11-25T16:21:34Z","title":"RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language\n  Models for Robotics","summary":"  Spatial understanding is a crucial capability for robots to make grounded\ndecisions based on their environment. This foundational skill enables robots\nnot only to perceive their surroundings but also to reason about and interact\nmeaningfully within the world. In modern robotics, these capabilities are taken\non by visual language models, and they face significant challenges when applied\nto spatial reasoning context due to their training data sources. These sources\nutilize general-purpose image datasets, and they often lack sophisticated\nspatial scene understanding capabilities. For example, the datasets do not\naddress reference frame comprehension - spatial relationships require clear\ncontextual understanding, whether from an ego-centric, object-centric, or\nworld-centric perspective, which allow for effective real-world interaction. To\naddress this issue, we introduce RoboSpatial, a large-scale spatial\nunderstanding dataset consisting of real indoor and tabletop scenes captured as\n3D scans and egocentric images, annotated with rich spatial information\nrelevant to robotics. The dataset includes 1M images, 5K 3D scans, and 3M\nannotated spatial relationships, with paired 2D egocentric images and 3D scans\nto make it both 2D and 3D ready. Our experiments show that models trained with\nRoboSpatial outperform baselines on downstream tasks such as spatial affordance\nprediction, spatial relationship prediction, and robotics manipulation.\n","authors":["Chan Hee Song","Valts Blukis","Jonathan Tremblay","Stephen Tyree","Yu Su","Stan Birchfield"],"pdf_url":"https://arxiv.org/pdf/2411.16537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16527v1","updated":"2024-11-25T16:14:45Z","published":"2024-11-25T16:14:45Z","title":"Profiling Bias in LLMs: Stereotype Dimensions in Contextual Word\n  Embeddings","summary":"  Large language models (LLMs) are the foundation of the current successes of\nartificial intelligence (AI), however, they are unavoidably biased. To\neffectively communicate the risks and encourage mitigation efforts these models\nneed adequate and intuitive descriptions of their discriminatory properties,\nappropriate for all audiences of AI. We suggest bias profiles with respect to\nstereotype dimensions based on dictionaries from social psychology research.\nAlong these dimensions we investigate gender bias in contextual embeddings,\nacross contexts and layers, and generate stereotype profiles for twelve\ndifferent LLMs, demonstrating their intuition and use case for exposing and\nvisualizing bias.\n","authors":["Carolin M. Schuster","Maria-Alexandra Dinisor","Shashwat Ghatiwala","Georg Groh"],"pdf_url":"https://arxiv.org/pdf/2411.16527v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16525v1","updated":"2024-11-25T16:12:17Z","published":"2024-11-25T16:12:17Z","title":"Fundamental Limits of Prompt Tuning Transformers: Universality, Capacity\n  and Efficiency","summary":"  We investigate the statistical and computational limits of prompt tuning for\ntransformer-based foundation models. Our key contributions are prompt tuning on\n\\textit{single-head} transformers with only a \\textit{single} self-attention\nlayer: (i) is universal, and (ii) supports efficient (even almost-linear time)\nalgorithms under the Strong Exponential Time Hypothesis (SETH). Statistically,\nwe prove that prompt tuning on such simplest possible transformers are\nuniversal approximators for sequence-to-sequence Lipschitz functions. In\naddition, we provide an exponential-in-$dL$ and -in-$(1/\\epsilon)$ lower bound\non the required soft-prompt tokens for prompt tuning to memorize any dataset\nwith 1-layer, 1-head transformers. Computationally, we identify a phase\ntransition in the efficiency of prompt tuning, determined by the norm of the\n\\textit{soft-prompt-induced} keys and queries, and provide an upper bound\ncriterion. Beyond this criterion, no sub-quadratic (efficient) algorithm for\nprompt tuning exists under SETH. Within this criterion, we showcase our theory\nby proving the existence of almost-linear time prompt tuning inference\nalgorithms. These fundamental limits provide important necessary conditions for\ndesigning expressive and efficient prompt tuning methods for practitioners.\n","authors":["Jerry Yao-Chieh Hu","Wei-Po Wang","Ammar Gilani","Chenyang Li","Zhao Song","Han Liu"],"pdf_url":"https://arxiv.org/pdf/2411.16525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16523v1","updated":"2024-11-25T16:10:05Z","published":"2024-11-25T16:10:05Z","title":"LaB-RAG: Label Boosted Retrieval Augmented Generation for Radiology\n  Report Generation","summary":"  In the current paradigm of image captioning, deep learning models are trained\nto generate text from image embeddings of latent features. We challenge the\nassumption that these latent features ought to be high-dimensional vectors\nwhich require model fine tuning to handle. Here we propose Label Boosted\nRetrieval Augmented Generation (LaB-RAG), a text-based approach to image\ncaptioning that leverages image descriptors in the form of categorical labels\nto boost standard retrieval augmented generation (RAG) with pretrained large\nlanguage models (LLMs). We study our method in the context of radiology report\ngeneration (RRG), where the task is to generate a clinician's report detailing\ntheir observations from a set of radiological images, such as X-rays. We argue\nthat simple linear classifiers over extracted image embeddings can effectively\ntransform X-rays into text-space as radiology-specific labels. In combination\nwith standard RAG, we show that these derived text labels can be used with\ngeneral-domain LLMs to generate radiology reports. Without ever training our\ngenerative language model or image feature encoder models, and without ever\ndirectly \"showing\" the LLM an X-ray, we demonstrate that LaB-RAG achieves\nbetter results across natural language and radiology language metrics compared\nwith other retrieval-based RRG methods, while attaining competitive results\ncompared to other fine-tuned vision-language RRG models. We further present\nresults of our experiments with various components of LaB-RAG to better\nunderstand our method. Finally, we critique the use of a popular RRG metric,\narguing it is possible to artificially inflate its results without true\ndata-leakage.\n","authors":["Steven Song","Anirudh Subramanyam","Irene Madejski","Robert L. Grossman"],"pdf_url":"https://arxiv.org/pdf/2411.16523v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16508v1","updated":"2024-11-25T15:44:42Z","published":"2024-11-25T15:44:42Z","title":"All Languages Matter: Evaluating LMMs on Culturally Diverse 100\n  Languages","summary":"  Existing Large Multimodal Models (LMMs) generally focus on only a few regions\nand languages. As LMMs continue to improve, it is increasingly important to\nensure they understand cultural contexts, respect local sensitivities, and\nsupport low-resource languages, all while effectively integrating corresponding\nvisual cues. In pursuit of culturally diverse global multimodal models, our\nproposed All Languages Matter Benchmark (ALM-bench) represents the largest and\nmost comprehensive effort to date for evaluating LMMs across 100 languages.\nALM-bench challenges existing models by testing their ability to understand and\nreason about culturally diverse images paired with text in various languages,\nincluding many low-resource languages traditionally underrepresented in LMM\nresearch. The benchmark offers a robust and nuanced evaluation framework\nfeaturing various question formats, including true/false, multiple choice, and\nopen-ended questions, which are further divided into short and long-answer\ncategories. ALM-bench design ensures a comprehensive assessment of a model's\nability to handle varied levels of difficulty in visual and linguistic\nreasoning. To capture the rich tapestry of global cultures, ALM-bench carefully\ncurates content from 13 distinct cultural aspects, ranging from traditions and\nrituals to famous personalities and celebrations. Through this, ALM-bench not\nonly provides a rigorous testing ground for state-of-the-art open and\nclosed-source LMMs but also highlights the importance of cultural and\nlinguistic inclusivity, encouraging the development of models that can serve\ndiverse global populations effectively. Our benchmark is publicly available.\n","authors":["Ashmal Vayani","Dinura Dissanayake","Hasindri Watawana","Noor Ahsan","Nevasini Sasikumar","Omkar Thawakar","Henok Biadglign Ademtew","Yahya Hmaiti","Amandeep Kumar","Kartik Kuckreja","Mykola Maslych","Wafa Al Ghallabi","Mihail Mihaylov","Chao Qin","Abdelrahman M Shaker","Mike Zhang","Mahardika Krisna Ihsani","Amiel Esplana","Monil Gokani","Shachar Mirkin","Harsh Singh","Ashay Srivastava","Endre Hamerlik","Fathinah Asma Izzati","Fadillah Adamsyah Maani","Sebastian Cavada","Jenny Chim","Rohit Gupta","Sanjay Manjunath","Kamila Zhumakhanova","Feno Heriniaina Rabevohitra","Azril Amirudin","Muhammad Ridzuan","Daniya Kareem","Ketan More","Kunyang Li","Pramesh Shakya","Muhammad Saad","Amirpouya Ghasemaghaei","Amirbek Djanibekov","Dilshod Azizov","Branislava Jankovic","Naman Bhatia","Alvaro Cabrera","Johan Obando-Ceron","Olympiah Otieno","Fabian Farestam","Muztoba Rabbani","Sanoojan Baliah","Santosh Sanjeev","Abduragim Shtanchaev","Maheen Fatima","Thao Nguyen","Amrin Kareem","Toluwani Aremu","Nathan Xavier","Amit Bhatkal","Hawau Toyin","Aman Chadha","Hisham Cholakkal","Rao Muhammad Anwer","Michael Felsberg","Jorma Laaksonen","Thamar Solorio","Monojit Choudhury","Ivan Laptev","Mubarak Shah","Salman Khan","Fahad Khan"],"pdf_url":"https://arxiv.org/pdf/2411.16508v1.pdf","comment":"A Multilingual Multimodal cultural benchmark for 100 languages"},{"id":"http://arxiv.org/abs/2311.05591v2","updated":"2024-11-25T15:38:17Z","published":"2023-11-09T18:48:02Z","title":"Multimodal Foundation Models Exploit Text to Make Medical Image\n  Predictions","summary":"  Multimodal foundation models have shown compelling but conflicting\nperformance in medical image interpretation. However, the mechanisms by which\nthese models integrate and prioritize different data modalities, including\nimages and text, remain poorly understood. Here, using a diverse collection of\n1014 multimodal medical cases, we evaluate the unimodal and multimodal image\ninterpretation abilities of proprietary (GPT-4, Gemini Pro 1.0) and open-source\n(Llama-3.2-90B, LLaVA-Med-v1.5) multimodal foundational models with and without\nthe use of text descriptions. Across all models, image predictions were largely\ndriven by exploiting text, with accuracy increasing monotonically with the\namount of informative text. By contrast, human performance on medical image\ninterpretation did not improve with informative text. Exploitation of text is a\ndouble-edged sword; we show that even mild suggestions of an incorrect\ndiagnosis in text diminishes image-based classification, reducing performance\ndramatically in cases the model could previously answer with images alone.\nFinally, we conducted a physician evaluation of model performance on long-form\nmedical cases, finding that the provision of images either reduced or had no\neffect on model performance when text is already highly informative. Our\nresults suggest that multimodal AI models may be useful in medical diagnostic\nreasoning but that their accuracy is largely driven, for better and worse, by\ntheir exploitation of text.\n","authors":["Thomas Buckley","James A. Diao","Pranav Rajpurkar","Adam Rodman","Arjun K. Manrai"],"pdf_url":"https://arxiv.org/pdf/2311.05591v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16495v1","updated":"2024-11-25T15:35:51Z","published":"2024-11-25T15:35:51Z","title":"AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous\n  Knowledge Reasoning","summary":"  Recent advancements in large language models (LLMs) have led to significant\nimprovements in various natural language processing tasks, but it is still\nchallenging for LLMs to perform knowledge-intensive complex question answering\ndue to LLMs' inefficacy in reasoning planning and the hallucination problem. A\ntypical solution is to employ retrieval-augmented generation (RAG) coupled with\nchain-of-thought (CoT) reasoning, which decomposes complex questions into\nchain-like sub-questions and applies iterative RAG at each sub-question.\nHowever, prior works exhibit sub-optimal reasoning planning and overlook\ndynamic knowledge retrieval from heterogeneous sources. In this paper, we\npropose AtomR, a novel heterogeneous knowledge reasoning framework that\nconducts multi-source reasoning at the atomic level. Drawing inspiration from\nthe graph modeling of knowledge, AtomR leverages large language models (LLMs)\nto decompose complex questions into combinations of three atomic knowledge\noperators, significantly enhancing the reasoning process at both the planning\nand execution stages. We also introduce BlendQA, a novel evaluation benchmark\ntailored to assess complex heterogeneous knowledge reasoning. Experiments show\nthat AtomR significantly outperforms state-of-the-art baselines across three\nsingle-source and two multi-source reasoning benchmarks, with notable\nperformance gains of 9.4% on 2WikiMultihop and 9.5% on BlendQA.\n","authors":["Amy Xin","Jinxin Liu","Zijun Yao","Zhicheng Li","Shulin Cao","Lei Hou","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2411.16495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16489v1","updated":"2024-11-25T15:31:27Z","published":"2024-11-25T15:31:27Z","title":"O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple\n  Distillation, Big Progress or Bitter Lesson?","summary":"  This paper presents a critical examination of current approaches to\nreplicating OpenAI's O1 model capabilities, with particular focus on the\nwidespread but often undisclosed use of knowledge distillation techniques.\nWhile our previous work explored the fundamental technical path to O1\nreplication, this study reveals how simple distillation from O1's API, combined\nwith supervised fine-tuning, can achieve superior performance on complex\nmathematical reasoning tasks. Through extensive experiments, we show that a\nbase model fine-tuned on simply tens of thousands of samples O1-distilled\nlong-thought chains outperforms O1-preview on the American Invitational\nMathematics Examination (AIME) with minimal technical complexity. Moreover, our\ninvestigation extends beyond mathematical reasoning to explore the\ngeneralization capabilities of O1-distilled models across diverse tasks:\nhallucination, safety and open-domain QA. Notably, despite training only on\nmathematical problem-solving data, our models demonstrated strong\ngeneralization to open-ended QA tasks and became significantly less susceptible\nto sycophancy after fine-tuning. We deliberately make this finding public to\npromote transparency in AI research and to challenge the current trend of\nobscured technical claims in the field. Our work includes: (1) A detailed\ntechnical exposition of the distillation process and its effectiveness, (2) A\ncomprehensive benchmark framework for evaluating and categorizing O1\nreplication attempts based on their technical transparency and reproducibility,\n(3) A critical discussion of the limitations and potential risks of\nover-relying on distillation approaches, our analysis culminates in a crucial\nbitter lesson: while the pursuit of more capable AI systems is important, the\ndevelopment of researchers grounded in first-principles thinking is paramount.\n","authors":["Zhen Huang","Haoyang Zou","Xuefeng Li","Yixiu Liu","Yuxiang Zheng","Ethan Chern","Shijie Xia","Yiwei Qin","Weizhe Yuan","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2411.16489v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2411.16487v1","updated":"2024-11-25T15:25:31Z","published":"2024-11-25T15:25:31Z","title":"When Babies Teach Babies: Can student knowledge sharing outperform\n  Teacher-Guided Distillation on small datasets?","summary":"  We present our submission to the BabyLM challenge, aiming to push the\nboundaries of data-efficient language model pretraining. Our method builds upon\ndeep mutual learning, introducing a student model search for diverse\ninitialization. We address the limitation of treating students equally by\nformulating weighted mutual learning as a bi-level optimization problem. The\ninner loop learns compact students through online distillation, while the outer\nloop optimizes weights for better knowledge distillation from diverse students.\nThis dynamic weighting strategy eliminates the need for a teacher model,\nreducing computational requirements. Our evaluations show that teacher-less\nmethods can match or surpass teacher-supervised approaches.\n","authors":["Srikrishna Iyer"],"pdf_url":"https://arxiv.org/pdf/2411.16487v1.pdf","comment":"Accepted to BabyLM challenge, CoNLL Workshop, EMNLP 2024"},{"id":"http://arxiv.org/abs/2411.16454v1","updated":"2024-11-25T15:01:25Z","published":"2024-11-25T15:01:25Z","title":"Learning by Analogy: Enhancing Few-Shot Prompting for Math Word Problem\n  Solving with Computational Graph-Based Retrieval","summary":"  Large language models (LLMs) are known to struggle with complicated reasoning\ntasks such as math word problems (MWPs). In this paper, we present how analogy\nfrom similarly structured questions can improve LLMs' problem-solving\ncapabilities for MWPs. Specifically, we rely on the retrieval of problems with\nsimilar computational graphs to the given question to serve as exemplars in the\nprompt, providing the correct reasoning path for the generation model to refer\nto. Empirical results across six math word problem datasets demonstrate the\neffectiveness of our proposed method, which achieves a significant improvement\nof up to 6.7 percent on average in absolute value, compared to baseline\nmethods. These results highlight our method's potential in addressing the\nreasoning challenges in current LLMs.\n","authors":["Xiaocong Yang","Jiacheng Lin","Ziqi Wang","Chengxiang Zhai"],"pdf_url":"https://arxiv.org/pdf/2411.16454v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16433v1","updated":"2024-11-25T14:37:24Z","published":"2024-11-25T14:37:24Z","title":"Finding Structure in Language Models","summary":"  When we speak, write or listen, we continuously make predictions based on our\nknowledge of a language's grammar. Remarkably, children acquire this\ngrammatical knowledge within just a few years, enabling them to understand and\ngeneralise to novel constructions that have never been uttered before. Language\nmodels are powerful tools that create representations of language by\nincrementally predicting the next word in a sentence, and they have had a\ntremendous societal impact in recent years. The central research question of\nthis thesis is whether these models possess a deep understanding of grammatical\nstructure similar to that of humans. This question lies at the intersection of\nnatural language processing, linguistics, and interpretability. To address it,\nwe will develop novel interpretability techniques that enhance our\nunderstanding of the complex nature of large-scale language models. We approach\nour research question from three directions. First, we explore the presence of\nabstract linguistic information through structural priming, a key paradigm in\npsycholinguistics for uncovering grammatical structure in human language\nprocessing. Next, we examine various linguistic phenomena, such as adjective\norder and negative polarity items, and connect a model's comprehension of these\nphenomena to the data distribution on which it was trained. Finally, we\nintroduce a controlled testbed for studying hierarchical structure in language\nmodels using various synthetic languages of increasing complexity and examine\nthe role of feature interactions in modelling this structure. Our findings\noffer a detailed account of the grammatical knowledge embedded in language\nmodel representations and provide several directions for investigating\nfundamental linguistic questions using computational methods.\n","authors":["Jaap Jumelet"],"pdf_url":"https://arxiv.org/pdf/2411.16433v1.pdf","comment":"PhD Thesis at ILLC, University of Amsterdam"},{"id":"http://arxiv.org/abs/2401.12982v2","updated":"2024-11-25T14:32:25Z","published":"2024-01-11T08:17:42Z","title":"A Comprehensive Survey of Text Classification Techniques and Their\n  Research Applications: Observational and Experimental Insights","summary":"  The exponential growth of textual data presents substantial challenges in\nmanagement and analysis, notably due to high storage and processing costs. Text\nclassification, a vital aspect of text mining, provides robust solutions by\nenabling efficient categorization and organization of text data. These\ntechniques allow individuals, researchers, and businesses to derive meaningful\npatterns and insights from large volumes of text. This survey paper introduces\na comprehensive taxonomy specifically designed for text classification based on\nresearch fields. The taxonomy is structured into hierarchical levels: research\nfield-based category, research field-based sub-category, methodology-based\ntechnique, methodology sub-technique, and research field applications. We\nemploy a dual evaluation approach: empirical and experimental. Empirically, we\nassess text classification techniques across four critical criteria.\nExperimentally, we compare and rank the methodology sub-techniques within the\nsame methodology technique and within the same overall research field\nsub-category. This structured taxonomy, coupled with thorough evaluations,\nprovides a detailed and nuanced understanding of text classification algorithms\nand their applications, empowering researchers to make informed decisions based\non precise, field-specific insights.\n","authors":["Kamal Taha","Paul D. Yoo","Chan Yeun","Aya Taha"],"pdf_url":"https://arxiv.org/pdf/2401.12982v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.14931v3","updated":"2024-11-25T14:20:43Z","published":"2023-04-28T15:42:55Z","title":"HQP: A Human-Annotated Dataset for Detecting Online Propaganda","summary":"  Online propaganda poses a severe threat to the integrity of societies.\nHowever, existing datasets for detecting online propaganda have a key\nlimitation: they were annotated using weak labels that can be noisy and even\nincorrect. To address this limitation, our work makes the following\ncontributions: (1) We present HQP: a novel dataset (N = 30,000) for detecting\nonline propaganda with high-quality labels. To the best of our knowledge, HQP\nis the first large-scale dataset for detecting online propaganda that was\ncreated through human annotation. (2) We show empirically that state-of-the-art\nlanguage models fail in detecting online propaganda when trained with weak\nlabels (AUC: 64.03). In contrast, state-of-the-art language models can\naccurately detect online propaganda when trained with our high-quality labels\n(AUC: 92.25), which is an improvement of ~44%. (3) We show that prompt-based\nlearning using a small sample of high-quality labels can still achieve a\nreasonable performance (AUC: 80.27) while significantly reducing the cost of\nlabeling. (4) We extend HQP to HQP+ to test how well propaganda across\ndifferent contexts can be detected. Crucially, our work highlights the\nimportance of high-quality labels for sensitive NLP tasks such as propaganda\ndetection.\n","authors":["Abdurahman Maarouf","Dominik Bär","Dominique Geissler","Stefan Feuerriegel"],"pdf_url":"https://arxiv.org/pdf/2304.14931v3.pdf","comment":"Accepted at ACL Findings 24"},{"id":"http://arxiv.org/abs/2411.16403v1","updated":"2024-11-25T14:10:24Z","published":"2024-11-25T14:10:24Z","title":"Adapter-based Approaches to Knowledge-enhanced Language Models -- A\n  Survey","summary":"  Knowledge-enhanced language models (KELMs) have emerged as promising tools to\nbridge the gap between large-scale language models and domain-specific\nknowledge. KELMs can achieve higher factual accuracy and mitigate\nhallucinations by leveraging knowledge graphs (KGs). They are frequently\ncombined with adapter modules to reduce the computational load and risk of\ncatastrophic forgetting. In this paper, we conduct a systematic literature\nreview (SLR) on adapter-based approaches to KELMs. We provide a structured\noverview of existing methodologies in the field through quantitative and\nqualitative analysis and explore the strengths and potential shortcomings of\nindividual approaches. We show that general knowledge and domain-specific\napproaches have been frequently explored along with various adapter\narchitectures and downstream tasks. We particularly focused on the popular\nbiomedical domain, where we provided an insightful performance comparison of\nexisting KELMs. We outline the main trends and propose promising future\ndirections.\n","authors":["Alexander Fichtl","Juraj Vladika","Georg Groh"],"pdf_url":"https://arxiv.org/pdf/2411.16403v1.pdf","comment":"12 pages, 4 figures. Published at KEOD24 via SciTePress"},{"id":"http://arxiv.org/abs/2411.16391v1","updated":"2024-11-25T13:53:36Z","published":"2024-11-25T13:53:36Z","title":"Human-Calibrated Automated Testing and Validation of Generative Language\n  Models","summary":"  This paper introduces a comprehensive framework for the evaluation and\nvalidation of generative language models (GLMs), with a focus on\nRetrieval-Augmented Generation (RAG) systems deployed in high-stakes domains\nsuch as banking. GLM evaluation is challenging due to open-ended outputs and\nsubjective quality assessments. Leveraging the structured nature of RAG\nsystems, where generated responses are grounded in a predefined document\ncollection, we propose the Human-Calibrated Automated Testing (HCAT) framework.\nHCAT integrates a) automated test generation using stratified sampling, b)\nembedding-based metrics for explainable assessment of functionality, risk and\nsafety attributes, and c) a two-stage calibration approach that aligns\nmachine-generated evaluations with human judgments through probability\ncalibration and conformal prediction.\n  In addition, the framework includes robustness testing to evaluate model\nperformance against adversarial, out-of-distribution, and varied input\nconditions, as well as targeted weakness identification using marginal and\nbivariate analysis to pinpoint specific areas for improvement. This\nhuman-calibrated, multi-layered evaluation framework offers a scalable,\ntransparent, and interpretable approach to GLM assessment, providing a\npractical and reliable solution for deploying GLMs in applications where\naccuracy, transparency, and regulatory compliance are paramount.\n","authors":["Agus Sudjianto","Aijun Zhang","Srinivas Neppalli","Tarun Joshi","Michal Malohlava"],"pdf_url":"https://arxiv.org/pdf/2411.16391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16387v1","updated":"2024-11-25T13:49:45Z","published":"2024-11-25T13:49:45Z","title":"FineWeb-zhtw: Scalable Curation of Traditional Chinese Text Data from\n  the Web","summary":"  The quality and size of a pretraining dataset significantly influence the\nperformance of large language models (LLMs). While there have been numerous\nefforts in the curation of such a dataset for English users, there is a\nrelative lack of similar initiatives for Traditional Chinese. Building upon\nthis foundation of FineWeb, we introduce FineWeb-zhtw, a dataset tailored\nspecifically for Traditional Chinese users. We came up with multiple stages of\nmeticulously designed filters to cater to the linguistic difference between\nEnglish and Traditional Chinese, to ensure comprehensiveness and quality. We\ndetermined effectiveness from querying dataset samples with three main\nobjectives. Our code and datasets are publicly available.\n","authors":["Cheng-Wei Lin","Wan-Hsuan Hsieh","Kai-Xin Guan","Chan-Jan Hsu","Chia-Chen Kuo","Chuan-Lin Lai","Chung-Wei Chung","Ming-Jen Wang","Da-Shan Shiu"],"pdf_url":"https://arxiv.org/pdf/2411.16387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10310v3","updated":"2024-11-25T13:35:47Z","published":"2024-06-14T06:22:47Z","title":"TEG-DB: A Comprehensive Dataset and Benchmark of Textual-Edge Graphs","summary":"  Text-Attributed Graphs (TAGs) augment graph structures with natural language\ndescriptions, facilitating detailed depictions of data and their\ninterconnections across various real-world settings. However, existing TAG\ndatasets predominantly feature textual information only at the nodes, with\nedges typically represented by mere binary or categorical attributes. This lack\nof rich textual edge annotations significantly limits the exploration of\ncontextual relationships between entities, hindering deeper insights into\ngraph-structured data. To address this gap, we introduce Textual-Edge Graphs\nDatasets and Benchmark (TEG-DB), a comprehensive and diverse collection of\nbenchmark textual-edge datasets featuring rich textual descriptions on nodes\nand edges. The TEG-DB datasets are large-scale and encompass a wide range of\ndomains, from citation networks to social networks. In addition, we conduct\nextensive benchmark experiments on TEG-DB to assess the extent to which current\ntechniques, including pre-trained language models, graph neural networks, and\ntheir combinations, can utilize textual node and edge information. Our goal is\nto elicit advancements in textual-edge graph research, specifically in\ndeveloping methodologies that exploit rich textual node and edge descriptions\nto enhance graph analysis and provide deeper insights into complex real-world\nnetworks. The entire TEG-DB project is publicly accessible as an open-source\nrepository on Github, accessible at\nhttps://github.com/Zhuofeng-Li/TEG-Benchmark.\n","authors":["Zhuofeng Li","Zixing Gou","Xiangnan Zhang","Zhongyuan Liu","Sirui Li","Yuntong Hu","Chen Ling","Zheng Zhang","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.10310v3.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.16365v1","updated":"2024-11-25T13:20:19Z","published":"2024-11-25T13:20:19Z","title":"Multi-modal Retrieval Augmented Multi-modal Generation: A Benchmark,\n  Evaluate Metrics and Strong Baselines","summary":"  This paper investigates an intriguing task of Multi-modal Retrieval Augmented\nMulti-modal Generation (M$^2$RAG). This task requires foundation models to\nbrowse multi-modal web pages, with mixed text and images, and generate\nmulti-modal responses for solving user queries, which exhibits better\ninformation density and readability. Given the early researching stage of\nM$^2$RAG task, there is a lack of systematic studies and analysis. To fill this\ngap, we construct a benchmark for M$^2$RAG task, equipped with a suite of\ntext-modal metrics and multi-modal metrics to analyze the capabilities of\nexisting foundation models. Besides, we also propose several effective methods\nfor foundation models to accomplish this task, based on the comprehensive\nevaluation results on our benchmark. Extensive experimental results reveal\nseveral intriguing phenomena worth further research.\n","authors":["Zi-Ao Ma","Tian Lan","Rong-Cheng Tu","Yong Hu","Heyan Huang","Xian-Ling Mao"],"pdf_url":"https://arxiv.org/pdf/2411.16365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12977v2","updated":"2024-11-25T13:17:01Z","published":"2024-11-20T02:10:44Z","title":"MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong\n  Collaborative Learning","summary":"  Contemporary embodied agents, such as Voyager in Minecraft, have demonstrated\npromising capabilities in open-ended individual learning. However, when powered\nwith open large language models (LLMs), these agents often struggle with\nrudimentary tasks, even when fine-tuned on domain-specific knowledge. Inspired\nby human cultural learning, we present \\collabvoyager, a novel framework that\nenhances Voyager with lifelong collaborative learning through explicit\nperspective-taking. \\collabvoyager introduces three key innovations: (1) theory\nof mind representations linking percepts, beliefs, desires, and actions; (2)\nnatural language communication between agents; and (3) semantic memory of task\nand environment knowledge and episodic memory of collaboration episodes. These\nadvancements enable agents to reason about their and others' mental states,\nempirically addressing two prevalent failure modes: false beliefs and faulty\ntask executions. In mixed-expertise Minecraft experiments, \\collabvoyager\nagents outperform Voyager counterparts, significantly improving task completion\nrate by $66.6\\% (+39.4\\%)$ for collecting one block of dirt and $70.8\\%\n(+20.8\\%)$ for collecting one wood block. They exhibit emergent behaviors like\nknowledge transfer from expert to novice agents and collaborative code\ncorrection. \\collabvoyager agents also demonstrate the ability to adapt to\nout-of-distribution tasks by using their previous experiences and beliefs\nobtained through collaboration. In this open-ended social learning paradigm,\n\\collabvoyager paves the way for the democratic development of embodied AI,\nwhere agents learn in deployment from both peer and environmental feedback.\n","authors":["Mircea Lică","Ojas Shirekar","Baptiste Colle","Chirag Raman"],"pdf_url":"https://arxiv.org/pdf/2411.12977v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06120v2","updated":"2024-11-25T13:12:45Z","published":"2024-08-12T13:02:31Z","title":"How ChatGPT Changed the Media's Narratives on AI: A Semi-Automated\n  Narrative Analysis Through Frame Semantics","summary":"  We perform a mixed-method frame semantics-based analysis on a dataset of more\nthan 49,000 sentences collected from 5846 news articles that mention AI. The\ndataset covers the twelve-month period centred around the launch of OpenAI's\nchatbot ChatGPT and is collected from the most visited open-access\nEnglish-language news publishers. Our findings indicate that during the six\nmonths succeeding the launch, media attention rose tenfold$\\unicode{x2014}$from\nalready historically high levels. During this period, discourse has become\nincreasingly centred around experts and political leaders, and AI has become\nmore closely associated with dangers and risks. A deeper review of the data\nalso suggests a qualitative shift in the types of threat AI is thought to\nrepresent, as well as the anthropomorphic qualities ascribed to it.\n","authors":["Igor Ryazanov","Carl Öhman","Johanna Björklund"],"pdf_url":"https://arxiv.org/pdf/2408.06120v2.pdf","comment":"19 pages, 6 figures and 2 appendices (5 pages) Minds & Machines,\n  published in November 2024"},{"id":"http://arxiv.org/abs/2409.15690v2","updated":"2024-11-25T13:05:39Z","published":"2024-09-24T03:06:25Z","title":"A Survey of Stance Detection on Social Media: New Directions and\n  Perspectives","summary":"  In modern digital environments, users frequently express opinions on\ncontentious topics, providing a wealth of information on prevailing attitudes.\nThe systematic analysis of these opinions offers valuable insights for\ndecision-making in various sectors, including marketing and politics. As a\nresult, stance detection has emerged as a crucial subfield within affective\ncomputing, enabling the automatic detection of user stances in social media\nconversations and providing a nuanced understanding of public sentiment on\ncomplex issues. Recent years have seen a surge of research interest in\ndeveloping effective stance detection methods, with contributions from multiple\ncommunities, including natural language processing, web science, and social\ncomputing. This paper provides a comprehensive survey of stance detection\ntechniques on social media, covering task definitions, datasets, approaches,\nand future works. We review traditional stance detection models, as well as\nstate-of-the-art methods based on large language models, and discuss their\nstrengths and limitations. Our survey highlights the importance of stance\ndetection in understanding public opinion and sentiment, and identifies gaps in\ncurrent research. We conclude by outlining potential future directions for\nstance detection on social media, including the need for more robust and\ngeneralizable models, and the importance of addressing emerging challenges such\nas multi-modal stance detection and stance detection in low-resource languages.\n","authors":["Bowen Zhang","Genan Dai","Fuqiang Niu","Nan Yin","Xiaomao Fan","Senzhang Wang","Xiaochun Cao","Hu Huang"],"pdf_url":"https://arxiv.org/pdf/2409.15690v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16353v1","updated":"2024-11-25T13:04:28Z","published":"2024-11-25T13:04:28Z","title":"The Two-Hop Curse: LLMs trained on A->B, B->C fail to learn A-->C","summary":"  While LLMs excel at multi-hop questions (e.g. \"Who is the spouse of the\nperformer of Imagine?\") when using chain-of-thought reasoning (CoT), they\nstruggle when forced to reason internally (without CoT). Previous work on the\nsize and nature of this gap produced mixed evidence with inconclusive results.\nIn this paper, we introduce a controlled setting for investigating two-hop\nreasoning in LLMs, where the above-chance performance constitutes undeniable\nevidence for latent reasoning. We fine-tune LLMs (including Llama 3 8B Instruct\nand GPT-4o) on fictional facts and confirm that they generalize to answering\ntwo-hop questions about them using CoT. We find that models can perform latent\nreasoning when facts appear together during training or in the prompt. However,\nto our surprise, models completely fail at two-hop reasoning without CoT when\nlearned facts only appear in different documents, achieving chance-level\naccuracy and chance-level test loss. We call this complete failure to compose\nseparately learned facts the Two-Hop Curse. Moreover, we evaluate 9 frontier\nLLMs on real-world facts, finding that models completely fail at two-hop no-CoT\nreasoning for over half of question categories while maintaining partial\nsuccess with CoT across most categories. These results suggest that LLMs lack a\ngeneral capability for latent multi-hop reasoning independent of the question\ntype.\n","authors":["Mikita Balesni","Tomek Korbak","Owain Evans"],"pdf_url":"https://arxiv.org/pdf/2411.16353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14522v2","updated":"2024-11-25T12:58:08Z","published":"2024-06-14T14:24:02Z","title":"Learning thresholds lead to stable language coexistence","summary":"  We introduce a language competition model that is based on the\nAbrams-Strogatz model and incorporates the effects of memory and learning in\nthe language shift dynamics. On a coarse grained time scale, the effects of\nmemory and learning can be expressed as thresholds on the speakers fractions of\nthe competing languages. In its simplest form, the resulting model is exactly\nsolvable. Besides the consensus on one of the two languages, the model\ndescribes additional equilibrium states that are not present in the\nAbrams-Strogatz model: a stable dynamical coexistence of the two languages and\na frozen state coinciding with the initial state. We show numerically that\nthese results are preserved for threshold functions of a more general shape.\nThe comparison of the model predictions with historical datasets demonstrates\nthat while the Abrams-Strogatz model fails to describe some relevant language\ncompetition situations, the proposed model provides a good fitting.\n","authors":["Mikhail V. Tamm","Els Heinsalu","Stefano Scialla","Marco Patriarca"],"pdf_url":"https://arxiv.org/pdf/2406.14522v2.pdf","comment":"15 pages, 6 figures and 1 table"},{"id":"http://arxiv.org/abs/2407.19474v2","updated":"2024-11-25T12:53:44Z","published":"2024-07-28T11:56:03Z","title":"Visual Riddles: a Commonsense and World Knowledge Challenge for Large\n  Vision and Language Models","summary":"  Imagine observing someone scratching their arm; to understand why, additional\ncontext would be necessary. However, spotting a mosquito nearby would\nimmediately offer a likely explanation for the person's discomfort, thereby\nalleviating the need for further information. This example illustrates how\nsubtle visual cues can challenge our cognitive skills and demonstrates the\ncomplexity of interpreting visual scenarios. To study these skills, we present\nVisual Riddles, a benchmark aimed to test vision and language models on visual\nriddles requiring commonsense and world knowledge. The benchmark comprises 400\nvisual riddles, each featuring a unique image created by a variety of\ntext-to-image models, question, ground-truth answer, textual hint, and\nattribution. Human evaluation reveals that existing models lag significantly\nbehind human performance, which is at 82% accuracy, with Gemini-Pro-1.5 leading\nwith 40% accuracy. Our benchmark comes with automatic evaluation tasks to make\nassessment scalable. These findings underscore the potential of Visual Riddles\nas a valuable resource for enhancing vision and language models' capabilities\nin interpreting complex visual scenarios.\n","authors":["Nitzan Bitton-Guetta","Aviv Slobodkin","Aviya Maimon","Eliya Habba","Royi Rassin","Yonatan Bitton","Idan Szpektor","Amir Globerson","Yuval Elovici"],"pdf_url":"https://arxiv.org/pdf/2407.19474v2.pdf","comment":"https://visual-riddles.github.io/"},{"id":"http://arxiv.org/abs/2406.01775v2","updated":"2024-11-25T12:45:19Z","published":"2024-06-03T20:37:27Z","title":"OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models","summary":"  The advent of large language models (LLMs) has revolutionized natural\nlanguage processing, enabling unprecedented capabilities in understanding and\ngenerating human-like text. However, the computational cost and convergence\ntimes associated with fine-tuning these models remain significant challenges.\nLow-Rank Adaptation (LoRA) has emerged as a promising method to mitigate these\nissues by introducing efficient fine-tuning techniques with a reduced number of\ntrainable parameters. In this paper, we present OLoRA, an enhancement to the\nLoRA method that leverages orthonormal matrix initialization through QR\ndecomposition. OLoRA significantly accelerates the convergence of LLM training\nwhile preserving the efficiency benefits of LoRA, such as the number of\ntrainable parameters and GPU memory footprint. Our empirical evaluations\ndemonstrate that OLoRA not only converges faster but also exhibits improved\nperformance compared to standard LoRA across a variety of language modeling\ntasks. This advancement opens new avenues for more efficient and accessible\nfine-tuning of LLMs, potentially enabling broader adoption and innovation in\nnatural language applications.\n","authors":["Kerim Büyükakyüz"],"pdf_url":"https://arxiv.org/pdf/2406.01775v2.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2411.16345v1","updated":"2024-11-25T12:44:02Z","published":"2024-11-25T12:44:02Z","title":"Preference Optimization for Reasoning with Pseudo Feedback","summary":"  Preference optimization techniques, such as Direct Preference Optimization\n(DPO), are frequently employed to enhance the reasoning capabilities of large\nlanguage models (LLMs) in domains like mathematical reasoning and coding,\ntypically following supervised fine-tuning. These methods rely on high-quality\nlabels for reasoning tasks to generate preference pairs; however, the\navailability of reasoning datasets with human-verified labels is limited. In\nthis study, we introduce a novel approach to generate pseudo feedback for\nreasoning tasks by framing the labeling of solutions to reason problems as an\nevaluation against associated test cases. We explore two forms of pseudo\nfeedback based on test cases: one generated by frontier LLMs and the other by\nextending self-consistency to multi-test-case. We conduct experiments on both\nmathematical reasoning and coding tasks using pseudo feedback for preference\noptimization, and observe improvements across both tasks. Specifically, using\nMathstral-7B as our base model, we improve MATH results from 58.3 to 68.6,\nsurpassing both NuminaMath-72B and GPT-4-Turbo-1106-preview. In GSM8K and\nCollege Math, our scores increase from 85.6 to 90.3 and from 34.3 to 42.3,\nrespectively. Building on Deepseek-coder-7B-v1.5, we achieve a score of 24.6 on\nLiveCodeBench (from 21.1), surpassing Claude-3-Haiku.\n","authors":["Fangkai Jiao","Geyang Guo","Xingxing Zhang","Nancy F. Chen","Shafiq Joty","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2411.16345v1.pdf","comment":"28 pages, 11 figures"},{"id":"http://arxiv.org/abs/2411.16337v1","updated":"2024-11-25T12:33:14Z","published":"2024-11-25T12:33:14Z","title":"Can AI grade your essays? A comparative analysis of large language\n  models and teacher ratings in multidimensional essay scoring","summary":"  The manual assessment and grading of student writing is a time-consuming yet\ncritical task for teachers. Recent developments in generative AI, such as large\nlanguage models, offer potential solutions to facilitate essay-scoring tasks\nfor teachers. In our study, we evaluate the performance and reliability of both\nopen-source and closed-source LLMs in assessing German student essays,\ncomparing their evaluations to those of 37 teachers across 10 pre-defined\ncriteria (i.e., plot logic, expression). A corpus of 20 real-world essays from\nYear 7 and 8 students was analyzed using five LLMs: GPT-3.5, GPT-4, o1, LLaMA\n3-70B, and Mixtral 8x7B, aiming to provide in-depth insights into LLMs' scoring\ncapabilities. Closed-source GPT models outperform open-source models in both\ninternal consistency and alignment with human ratings, particularly excelling\nin language-related criteria. The novel o1 model outperforms all other LLMs,\nachieving Spearman's $r = .74$ with human assessments in the overall score, and\nan internal consistency of $ICC=.80$. These findings indicate that LLM-based\nassessment can be a useful tool to reduce teacher workload by supporting the\nevaluation of essays, especially with regard to language-related criteria.\nHowever, due to their tendency for higher scores, the models require further\nrefinement to better capture aspects of content quality.\n","authors":["Kathrin Seßler","Maurice Fürstenberg","Babette Bühler","Enkelejda Kasneci"],"pdf_url":"https://arxiv.org/pdf/2411.16337v1.pdf","comment":"Accepted at LAK '25"},{"id":"http://arxiv.org/abs/2411.11581v3","updated":"2024-11-25T12:16:00Z","published":"2024-11-18T13:57:35Z","title":"OASIS: Open Agent Social Interaction Simulations with One Million Agents","summary":"  There has been a growing interest in enhancing rule-based agent-based models\n(ABMs) for social media platforms (i.e., X, Reddit) with more realistic large\nlanguage model (LLM) agents, thereby allowing for a more nuanced study of\ncomplex systems. As a result, several LLM-based ABMs have been proposed in the\npast year. While they hold promise, each simulator is specifically designed to\nstudy a particular scenario, making it time-consuming and resource-intensive to\nexplore other phenomena using the same ABM. Additionally, these models simulate\nonly a limited number of agents, whereas real-world social media platforms\ninvolve millions of users. To this end, we propose OASIS, a generalizable and\nscalable social media simulator. OASIS is designed based on real-world social\nmedia platforms, incorporating dynamically updated environments (i.e., dynamic\nsocial networks and post information), diverse action spaces (i.e., following,\ncommenting), and recommendation systems (i.e., interest-based and\nhot-score-based). Additionally, OASIS supports large-scale user simulations,\ncapable of modeling up to one million users. With these features, OASIS can be\neasily extended to different social media platforms to study large-scale group\nphenomena and behaviors. We replicate various social phenomena, including\ninformation spreading, group polarization, and herd effects across X and Reddit\nplatforms. Moreover, we provide observations of social phenomena at different\nagent group scales. We observe that the larger agent group scale leads to more\nenhanced group dynamics and more diverse and helpful agents' opinions. These\nfindings demonstrate OASIS's potential as a powerful tool for studying complex\nsystems in digital environments.\n","authors":["Ziyi Yang","Zaibin Zhang","Zirui Zheng","Yuxian Jiang","Ziyue Gan","Zhiyu Wang","Zijian Ling","Jinsong Chen","Martz Ma","Bowen Dong","Prateek Gupta","Shuyue Hu","Zhenfei Yin","Guohao Li","Xu Jia","Lijun Wang","Bernard Ghanem","Huchuan Lu","Wanli Ouyang","Yu Qiao","Philip Torr","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2411.11581v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12788v2","updated":"2024-11-25T11:54:03Z","published":"2024-10-16T17:59:32Z","title":"Meta-Chunking: Learning Efficient Text Segmentation via Logical\n  Perception","summary":"  Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline, which impacts the quality of knowledge-intensive\ntasks. This paper introduces the concept of Meta-Chunking, which refers to a\ngranularity between sentences and paragraphs, consisting of a collection of\nsentences within a paragraph that have deep linguistic logical connections. To\nimplement Meta-Chunking, we designed Perplexity (PPL) Chunking, which balances\nperformance and speed, and precisely identifies the boundaries of text chunks\nby analyzing the characteristics of context perplexity distribution.\nAdditionally, considering the inherent complexity of different texts, we\npropose a strategy that combines PPL Chunking with dynamic merging to achieve a\nbalance between fine-grained and coarse-grained text chunking. Experiments\nconducted on eleven datasets demonstrate that Meta-Chunking can more\nefficiently improve the performance of single-hop and multi-hop question\nanswering based on RAG. For instance, on the 2WikiMultihopQA dataset, it\noutperforms similarity chunking by 1.32 while only consuming 45.8% of the time.\nFurthermore, through the analysis of models of various scales and types, we\nobserved that PPL Chunking exhibits notable flexibility and adaptability. Our\ncode is available at https://github.com/IAAR-Shanghai/Meta-Chunking.\n","authors":["Jihao Zhao","Zhiyuan Ji","Yuchen Feng","Pengnian Qi","Simin Niu","Bo Tang","Feiyu Xiong","Zhiyu Li"],"pdf_url":"https://arxiv.org/pdf/2410.12788v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16305v1","updated":"2024-11-25T11:47:31Z","published":"2024-11-25T11:47:31Z","title":"Learning from Relevant Subgoals in Successful Dialogs using Iterative\n  Training for Task-oriented Dialog Systems","summary":"  Task-oriented Dialog (ToD) systems have to solve multiple subgoals to\naccomplish user goals, whereas feedback is often obtained only at the end of\nthe dialog. In this work, we propose SUIT (SUbgoal-aware ITerative Training),\nan iterative training approach for improving ToD systems. We sample dialogs\nfrom the model we aim to improve and determine subgoals that contribute to\ndialog success using distant supervision to obtain high quality training\nsamples. We show how this data improves supervised fine-tuning or,\nalternatively, preference learning results. SUIT is able to iteratively\ngenerate more data instead of relying on fixed static sets. SUIT reaches new\nstate-of-the-art performance on a popular ToD benchmark.\n","authors":["Magdalena Kaiser","Patrick Ernst","György Szarvas"],"pdf_url":"https://arxiv.org/pdf/2411.16305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16300v1","updated":"2024-11-25T11:35:08Z","published":"2024-11-25T11:35:08Z","title":"BayLing 2: A Multilingual Large Language Model with Efficient Language\n  Alignment","summary":"  Large language models (LLMs), with their powerful generative capabilities and\nvast knowledge, empower various tasks in everyday life. However, these\nabilities are primarily concentrated in high-resource languages, leaving\nlow-resource languages with weaker generative capabilities and relatively\nlimited knowledge. Enhancing the multilingual capabilities of LLMs is therefore\ncrucial for serving over 100 linguistic communities worldwide. An intuitive\napproach to enhance the multilingual capabilities would be to construct\ninstruction data for various languages, but constructing instruction data for\nover 100 languages is prohibitively costly. In this paper, we introduce BayLing\n2, which efficiently transfers generative capabilities and knowledge from\nhigh-resource languages to low-resource languages through language alignment.\nTo achieve this, we constructed a dataset of 3.2 million instructions,\ncomprising high-resource language instructions (Chinese and English) and\ncross-lingual instructions for 100+ languages and performed instruction tuning\nbased on the dataset to facilitate the capability transfer between languages.\nUsing Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B,\nand BayLing-3-8B, and conducted a comprehensive evaluation of BayLing. For\nmultilingual translation across 100+ languages, BayLing shows superior\nperformance compared to open-source models of similar scale. For multilingual\nknowledge and understanding benchmarks, BayLing achieves significant\nimprovements across over 20 low-resource languages, demonstrating its\ncapability of effective knowledge transfer from high-resource to low-resource\nlanguages. Furthermore, results on English benchmarks indicate that BayLing\nmaintains high performance in highresource languages while enhancing the\nperformance in low-resource languages. Demo, homepage, code and models of\nBayLing are available.\n","authors":["Shaolei Zhang","Kehao Zhang","Qingkai Fang","Shoutao Guo","Yan Zhou","Xiaodong Liu","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2411.16300v1.pdf","comment":"BayLing 2's online demo: http://nlp.ict.ac.cn/bayling/demo. BayLing\n  2's code and models: https://github.com/ictnlp/BayLing"},{"id":"http://arxiv.org/abs/2211.11483v5","updated":"2024-11-25T10:55:29Z","published":"2022-11-21T14:18:25Z","title":"Deanthropomorphising NLP: Can a Language Model Be Conscious?","summary":"  This work is intended as a voice in the discussion over previous claims that\na pretrained large language model (LLM) based on the Transformer model\narchitecture can be sentient. Such claims have been made concerning the LaMDA\nmodel and also concerning the current wave of LLM-powered chatbots, such as\nChatGPT. This claim, if confirmed, would have serious ramifications in the\nNatural Language Processing (NLP) community due to wide-spread use of similar\nmodels. However, here we take the position that such a large language model\ncannot be sentient, or conscious, and that LaMDA in particular exhibits no\nadvances over other similar models that would qualify it. We justify this by\nanalysing the Transformer architecture through Integrated Information Theory of\nconsciousness. We see the claims of sentience as part of a wider tendency to\nuse anthropomorphic language in NLP reporting. Regardless of the veracity of\nthe claims, we consider this an opportune moment to take stock of progress in\nlanguage modelling and consider the ethical implications of the task. In order\nto make this work helpful for readers outside the NLP community, we also\npresent the necessary background in language modelling.\n","authors":["Matthew Shardlow","Piotr Przybyła"],"pdf_url":"https://arxiv.org/pdf/2211.11483v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16260v1","updated":"2024-11-25T10:23:11Z","published":"2024-11-25T10:23:11Z","title":"Unraveling Arithmetic in Large Language Models: The Role of Algebraic\n  Structures","summary":"  Large language models (LLMs) have demonstrated remarkable mathematical\ncapabilities, largely driven by chain-of-thought (CoT) prompting, which\ndecomposes complex reasoning into step-by-step solutions. This approach has\nenabled significant advancements, as evidenced by performance on benchmarks\nlike GSM8K and MATH. However, the mechanisms underlying LLMs' ability to\nperform arithmetic in a single step of CoT remain poorly understood. Existing\nstudies debate whether LLMs encode numerical values or rely on symbolic\nreasoning, while others explore attention and multi-layered processing in\narithmetic tasks. In this work, we propose that LLMs learn arithmetic by\ncapturing algebraic structures, such as \\emph{Commutativity} and\n\\emph{Identity} properties. Since these structures are observable through\ninput-output relationships, they can generalize to unseen data. We empirically\ndemonstrate that LLMs can learn algebraic structures using a custom dataset of\narithmetic problems. Our findings indicate that leveraging algebraic structures\ncan enhance the LLMs' arithmetic capabilities, offering insights into improving\ntheir arithmetic performance.\n","authors":["Fu-Chieh Chang","Pei-Yuan Wu"],"pdf_url":"https://arxiv.org/pdf/2411.16260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16252v1","updated":"2024-11-25T10:12:27Z","published":"2024-11-25T10:12:27Z","title":"NormXLogit: The Head-on-Top Never Lies","summary":"  The Transformer architecture has emerged as the dominant choice for building\nlarge language models (LLMs). However, with new LLMs emerging on a frequent\nbasis, it is important to consider the potential value of architecture-agnostic\napproaches that can provide interpretability across a variety of architectures.\nDespite recent successes in the interpretability of LLMs, many existing\napproaches rely on complex methods that are often tied to a specific model\ndesign and come with a significant computational cost. To address these\nlimitations, we propose a novel technique, called NormXLogit, for assessing the\nsignificance of individual input tokens. This method operates based on the\ninput and output representations associated with each token. First, we\ndemonstrate that during the pre-training of LLMs, the norms of word embeddings\ncapture the importance of input tokens. Second, we reveal a significant\nrelationship between a token's importance and the extent to which its\nrepresentation can resemble the model's final prediction. Through extensive\nanalysis, we show that our approach consistently outperforms existing\ngradient-based methods in terms of faithfulness. Additionally, our method\nachieves better performance in layer-wise explanations compared to the most\nprominent architecture-specific methods.\n","authors":["Sina Abbasi","Mohammad Reza Modarres","Mohammad Taher Pilehvar"],"pdf_url":"https://arxiv.org/pdf/2411.16252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16251v1","updated":"2024-11-25T10:10:09Z","published":"2024-11-25T10:10:09Z","title":"Transparent Neighborhood Approximation for Text Classifier Explanation","summary":"  Recent literature highlights the critical role of neighborhood construction\nin deriving model-agnostic explanations, with a growing trend toward deploying\ngenerative models to improve synthetic instance quality, especially for\nexplaining text classifiers. These approaches overcome the challenges in\nneighborhood construction posed by the unstructured nature of texts, thereby\nimproving the quality of explanations. However, the deployed generators are\nusually implemented via neural networks and lack inherent explainability,\nsparking arguments over the transparency of the explanation process itself. To\naddress this limitation while preserving neighborhood quality, this paper\nintroduces a probability-based editing method as an alternative to black-box\ntext generators. This approach generates neighboring texts by implementing\nmanipulations based on in-text contexts. Substituting the generator-based\nconstruction process with recursive probability-based editing, the resultant\nexplanation method, XPROB (explainer with probability-based editing), exhibits\ncompetitive performance according to the evaluation conducted on two real-world\ndatasets. Additionally, XPROB's fully transparent and more controllable\nconstruction process leads to superior stability compared to the\ngenerator-based explainers.\n","authors":["Yi Cai","Arthur Zimek","Eirini Ntoutsi","Gerhard Wunder"],"pdf_url":"https://arxiv.org/pdf/2411.16251v1.pdf","comment":"IEEE DSAA'24"},{"id":"http://arxiv.org/abs/2411.16236v1","updated":"2024-11-25T09:52:28Z","published":"2024-11-25T09:52:28Z","title":"DoubleCCA: Improving Foundation Model Group Robustness with Random\n  Sentence Embeddings","summary":"  This paper presents a novel method to improve the robustness of foundation\nmodels to group-based biases. We propose a simple yet effective method, called\nDoubleCCA, that leverages random sentences and Canonical Correlation Analysis\n(CCA) to enrich the text embeddings of the foundation model. First, we generate\nvarious random sentences that augment the original prompts, which extends the\noriginal prompts with random words or character sequences. Second, we use an\nadditional sentence embedding model to generate different text embeddings with\nrespect to these random sentences. We then use CCA double twice to align the\nrepresentations and reconstruct them back to the original representation space.\nWe demonstrate the effectiveness of our method on a variety of tasks and\ndatasets, showing that it outperforms existing methods in terms of both\nperformance and robustness. Our method is simple to implement and can be easily\nintegrated into existing models, making it a practical solution for improving\nthe robustness of foundation models to group-based biases.\n","authors":["Hong Liu","Yitong Lu"],"pdf_url":"https://arxiv.org/pdf/2411.16236v1.pdf","comment":"18 pages, 6 figures, 2 tables"},{"id":"http://arxiv.org/abs/2411.14957v2","updated":"2024-11-25T09:47:20Z","published":"2024-11-22T14:16:09Z","title":"Information Extraction from Heterogeneous Documents without Ground Truth\n  Labels using Synthetic Label Generation and Knowledge Distillation","summary":"  Invoices and receipts submitted by employees are visually rich documents\n(VRDs) with textual, visual and layout information. To protect against the risk\nof fraud and abuse, it is crucial for organizations to efficiently extract\ndesired information from submitted receipts. This helps in the assessment of\nkey factors such as appropriateness of the expense claim, adherence to spending\nand transaction policies, the validity of the receipt, as well as downstream\nanomaly detection at various levels. These documents are heterogeneous, with\nmultiple formats and languages, uploaded with different image qualities, and\noften do not contain ground truth labels for the efficient training of models.\nIn this paper we propose Task Aware Instruction-based Labelling (TAIL), a\nmethod for synthetic label generation in VRD corpuses without labels, and\nfine-tune a multimodal Visually Rich Document Understanding Model (VRDU) on\nTAIL labels using response-based knowledge distillation without using the\nteacher model's weights or training dataset to conditionally generate\nannotations in the appropriate format. Using a benchmark external dataset where\nground truth labels are available, we demonstrate conditions under which our\napproach performs at par with Claude 3 Sonnet through empirical studies. We\nthen show that the resulting model performs at par or better on the internal\nexpense documents of a large multinational organization than state-of-the-art\nLMM (large multimodal model) Claude 3 Sonnet while being 85% less costly and\n~5X faster, and outperforms layout-aware baselines by more than 10% in Average\nNormalized Levenshtein Similarity (ANLS) scores due to its ability to reason\nand extract information from rare formats. Finally, we illustrate the usage of\nour approach in overpayment prevention.\n","authors":["Aniket Bhattacharyya","Anurag Tripathi"],"pdf_url":"https://arxiv.org/pdf/2411.14957v2.pdf","comment":"Accepted to WACV 2025"},{"id":"http://arxiv.org/abs/2411.16205v1","updated":"2024-11-25T09:05:36Z","published":"2024-11-25T09:05:36Z","title":"MH-MoE:Multi-Head Mixture-of-Experts","summary":"  Multi-Head Mixture-of-Experts (MH-MoE) demonstrates superior performance by\nusing the multi-head mechanism to collectively attend to information from\nvarious representation spaces within different experts. In this paper, we\npresent a novel implementation of MH-MoE that maintains both FLOPs and\nparameter parity with sparse Mixture of Experts models. Experimental results on\nlanguage models show that the new implementation yields quality improvements\nover both vanilla MoE and fine-grained MoE models. Additionally, our\nexperiments demonstrate that MH-MoE is compatible with 1-bit Large Language\nModels (LLMs) such as BitNet.\n","authors":["Shaohan Huang","Xun Wu","Shuming Ma","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2411.16205v1.pdf","comment":"7 pages, 0 figures"},{"id":"http://arxiv.org/abs/2411.16201v1","updated":"2024-11-25T08:59:39Z","published":"2024-11-25T08:59:39Z","title":"Video-Text Dataset Construction from Multi-AI Feedback: Promoting\n  Weak-to-Strong Preference Learning for Video Large Language Models","summary":"  High-quality video-text preference data is crucial for Multimodal Large\nLanguage Models (MLLMs) alignment. However, existing preference data is very\nscarce. Obtaining VQA preference data for preference training is costly, and\nmanually annotating responses is highly unreliable, which could result in\nlow-quality pairs. Meanwhile, AI-generated responses controlled by temperature\nadjustment lack diversity. To address these issues, we propose a high-quality\nVQA preference dataset, called \\textit{\\textbf{M}ultiple \\textbf{M}ultimodal\n\\textbf{A}rtificial \\textbf{I}ntelligence \\textbf{P}reference Datasets in\n\\textbf{V}QA} (\\textbf{MMAIP-V}), which is constructed by sampling from the\nresponse distribution set and using an external scoring function for response\nevaluation. Furthermore, to fully leverage the preference knowledge in MMAIP-V\nand ensure sufficient optimization, we propose \\textit{\\textbf{Iter}ative\n\\textbf{W}eak-to-\\textbf{S}trong \\textbf{R}einforcement \\textbf{L}earning from\n\\textbf{AI} \\textbf{F}eedback for video MLLMs} (\\textbf{Iter-W2S-RLAIF}), a\nframework that gradually enhances MLLMs' alignment capabilities by iteratively\nupdating the reference model and performing parameter extrapolation. Finally,\nwe propose an unbiased and information-complete evaluation scheme in VQA\nevaluation. Experiments demonstrate that MMAIP-V is beneficial for MLLMs in\npreference learning and Iter-W2S-RLAIF fully exploits the alignment information\nin MMAIP-V. We believe that the proposed automatic VQA preference data\ngeneration pipeline based on AI feedback can greatly promote future work in the\nMLLMs alignment. \\textbf{Code and dataset are available}\n\\href{https://anonymous.4open.science/r/MMAIP-V_Iter-W2S-RLAIF-702F}{MMAIP-V\\_Iter-W2S-RLAIF-702F}.\n","authors":["Hao Yi","Qingyang Li","Yulan Hu","Fuzheng Zhang","Di Zhang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2411.16201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19198v2","updated":"2024-11-25T08:57:20Z","published":"2024-07-27T07:34:49Z","title":"Towards the Dynamics of a DNN Learning Symbolic Interactions","summary":"  This study proves the two-phase dynamics of a deep neural network (DNN)\nlearning interactions. Despite the long disappointing view of the faithfulness\nof post-hoc explanation of a DNN, a series of theorems have been proven in\nrecent years to show that for a given input sample, a small set of interactions\nbetween input variables can be considered as primitive inference patterns that\nfaithfully represent a DNN's detailed inference logic on that sample.\nParticularly, Zhang et al. have observed that various DNNs all learn\ninteractions of different complexities in two distinct phases, and this\ntwo-phase dynamics well explains how a DNN changes from under-fitting to\nover-fitting. Therefore, in this study, we mathematically prove the two-phase\ndynamics of interactions, providing a theoretical mechanism for how the\ngeneralization power of a DNN changes during the training process. Experiments\nshow that our theory well predicts the real dynamics of interactions on\ndifferent DNNs trained for various tasks.\n","authors":["Qihan Ren","Junpeng Zhang","Yang Xu","Yue Xin","Dongrui Liu","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.19198v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16189v1","updated":"2024-11-25T08:42:33Z","published":"2024-11-25T08:42:33Z","title":"Enhancing Multi-Agent Consensus through Third-Party LLM Integration:\n  Analyzing Uncertainty and Mitigating Hallucinations in Large Language Models","summary":"  Large Language Models (LLMs) still face challenges when dealing with complex\nreasoning tasks, often resulting in hallucinations, which limit the practical\napplication of LLMs. To alleviate this issue, this paper proposes a new method\nthat integrates different LLMs to expand the knowledge boundary, reduce\ndependence on a single model, and promote in-depth debate among agents. The\nmain contributions include: 1) Introducing third-party LLMs to adjust the\nattention weights of agents through uncertainty estimation and confidence\nanalysis, optimizing consensus formation in multi-agent systems; 2) Experiments\non arithmetic datasets have validated the effectiveness of the method,\nsurpassing traditional multi-agent baselines. This research provides a new\nperspective for large models to alleviate hallucination phenomena when dealing\nwith complex tasks.\n","authors":["Zhihua Duan","Jialin Wang"],"pdf_url":"https://arxiv.org/pdf/2411.16189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07745v2","updated":"2024-11-25T08:38:49Z","published":"2024-10-10T09:23:26Z","title":"StepTool: A Step-grained Reinforcement Learning Framework for Tool\n  Learning in LLMs","summary":"  Despite having powerful reasoning and inference capabilities, Large Language\nModels (LLMs) still need external tools to acquire real-time information\nretrieval or domain-specific expertise to solve complex tasks, which is\nreferred to as tool learning. Existing tool learning methods primarily rely on\ntuning with expert trajectories, focusing on token-sequence learning from a\nlinguistic perspective. However, there are several challenges: 1) imitating\nstatic trajectories limits their ability to generalize to new tasks. 2) even\nexpert trajectories can be suboptimal, and better solution paths may exist. In\nthis work, we introduce StepTool, a novel step-grained reinforcement learning\nframework to improve tool learning in LLMs. It consists of two components:\nStep-grained Reward Shaping, which assigns rewards at each tool interaction\nbased on tool invocation success and its contribution to the task, and\nStep-grained Optimization, which uses policy gradient methods to optimize the\nmodel in a multi-step manner. Experimental results demonstrate that StepTool\nsignificantly outperforms existing methods in multi-step, tool-based tasks,\nproviding a robust solution for complex task environments. Codes are available\nat https://github.com/yuyq18/StepTool.\n","authors":["Yuanqing Yu","Zhefan Wang","Weizhi Ma","Zhicheng Guo","Jingtao Zhan","Shuai Wang","Chuhan Wu","Zhiqiang Guo","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.07745v2.pdf","comment":"Ongoning Work"},{"id":"http://arxiv.org/abs/2411.07965v2","updated":"2024-11-25T08:13:00Z","published":"2024-11-12T17:41:16Z","title":"From General to Specific: Utilizing General Hallucination to Benchmark\n  Specific Role-Playing Agents","summary":"  The advanced role-playing capabilities of Large Language Models (LLMs) have\npaved the way for developing Role-Playing Agents (RPAs). However, existing\nbenchmarks in this domain, such as HPD and SocialBench face limitations like\npoor generalizability, implicit and inaccurate judgments, and the risk of model\nforgetting. To address the above issues, we propose an automatic, scalable, and\ngeneralizable paradigm. Specifically, we construct a benchmark, SHARP, by\nextracting relations from a general knowledge graph and leveraging the inherent\nhallucination properties of RPAs to simulate interactions across roles. We\nemploy ChatGPT for stance detection and define relationship hallucination along\nwith three related metrics based on stance transfer. Extensive experiments\nvalidate the effectiveness and stability of our paradigm. Our findings further\nexplore the factors influencing these metrics and discuss the trade-off between\nblind loyalty to relationships and adherence to facts in RPAs.\n","authors":["Chuyi Kong","Ziyang Luo","Hongzhan Lin","Zhiyuan Fan","Yaxin Fan","Yuxi Sun","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2411.07965v2.pdf","comment":"Revise three typos in the abstract and methodology sections of the\n  introduction"},{"id":"http://arxiv.org/abs/2411.05547v2","updated":"2024-11-25T07:18:33Z","published":"2024-11-08T13:09:14Z","title":"Assessing the Answerability of Queries in Retrieval-Augmented Code\n  Generation","summary":"  Thanks to unprecedented language understanding and generation capabilities of\nlarge language model (LLM), Retrieval-augmented Code Generation (RaCG) has\nrecently been widely utilized among software developers. While this has\nincreased productivity, there are still frequent instances of incorrect codes\nbeing provided. In particular, there are cases where plausible yet incorrect\ncodes are generated for queries from users that cannot be answered with the\ngiven queries and API descriptions. This study proposes a task for evaluating\nanswerability, which assesses whether valid answers can be generated based on\nusers' queries and retrieved APIs in RaCG. Additionally, we build a benchmark\ndataset called Retrieval-augmented Code Generability Evaluation (RaCGEval) to\nevaluate the performance of models performing this task. Experimental results\nshow that this task remains at a very challenging level, with baseline models\nexhibiting a low performance of 46.7%. Furthermore, this study discusses\nmethods that could significantly improve performance.\n","authors":["Geonmin Kim","Jaeyeon Kim","Hancheol Park","Wooksu Shin","Tae-Ho Kim"],"pdf_url":"https://arxiv.org/pdf/2411.05547v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16710v2","updated":"2024-11-25T07:12:35Z","published":"2024-09-25T07:55:36Z","title":"Beyond Turing Test: Can GPT-4 Sway Experts' Decisions?","summary":"  In the post-Turing era, evaluating large language models (LLMs) involves\nassessing generated text based on readers' reactions rather than merely its\nindistinguishability from human-produced content. This paper explores how\nLLM-generated text impacts readers' decisions, focusing on both amateur and\nexpert audiences. Our findings indicate that GPT-4 can generate persuasive\nanalyses affecting the decisions of both amateurs and professionals.\nFurthermore, we evaluate the generated text from the aspects of grammar,\nconvincingness, logical coherence, and usefulness. The results highlight a high\ncorrelation between real-world evaluation through audience reactions and the\ncurrent multi-dimensional evaluators commonly used for generative models.\nOverall, this paper shows the potential and risk of using generated text to\nsway human decisions and also points out a new direction for evaluating\ngenerated text, i.e., leveraging the reactions and decisions of readers. We\nrelease our dataset to assist future research.\n","authors":["Takehiro Takayanagi","Hiroya Takamura","Kiyoshi Izumi","Chung-Chi Chen"],"pdf_url":"https://arxiv.org/pdf/2409.16710v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02684v3","updated":"2024-11-25T07:07:45Z","published":"2023-11-05T15:48:29Z","title":"Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE","summary":"  Recent studies have demonstrated Large Language Models (LLMs) can extend\ntheir zero-shot generalization capabilities to multimodal learning through\ninstruction tuning. As more modalities and downstream tasks are introduced,\nnegative conflicts and interference may have a worse impact on performance.\nWhile this phenomenon has been overlooked in previous work, we propose a novel\nand extensible framework, called Octavius, for comprehensive studies and\nexperimentation on multimodal learning with Multimodal Large Language Models\n(MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and\none of the representative PEFT techniques, i.e., LoRA, designing a novel\nLLM-based decoder, called LoRA-MoE, for multimodal learning. To the best of our\nknowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to\naddress this problem. The experimental results (about 20% improvement) have\nshown the effectiveness and versatility of our design in various 2D and 3D\ndownstream tasks. Code and datasets are available at\nhttps://openlamm.github.io/tutorial/.\n","authors":["Zeren Chen","Ziqin Wang","Zhen Wang","Huayang Liu","Zhenfei Yin","Si Liu","Lu Sheng","Wanli Ouyang","Yu Qiao","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2311.02684v3.pdf","comment":"22 pages, 12 figures. Accepted in ICLR 2024"},{"id":"http://arxiv.org/abs/2409.19458v2","updated":"2024-11-25T06:09:45Z","published":"2024-09-28T21:26:50Z","title":"Scalable Fine-tuning from Multiple Data Sources: A First-Order\n  Approximation Approach","summary":"  We study the problem of fine-tuning a language model (LM) for a target task\nby optimally using the information from $n$ auxiliary tasks. This problem has\nbroad applications in NLP, such as targeted instruction tuning and data\nselection in chain-of-thought fine-tuning. The key challenge of this problem is\nthat not all auxiliary tasks are useful to improve the performance of the\ntarget task. Thus, choosing the right subset of auxiliary tasks is crucial.\nConventional subset selection methods, such as forward and backward stepwise\nselection, are unsuitable for LM fine-tuning because they require repeated\ntraining on subsets of auxiliary tasks. This paper introduces a new algorithm\nto estimate model fine-tuning performances without repeated training. Our\nalgorithm first performs multitask training using the data of all the tasks to\nobtain a meta initialization. Then, we approximate the model fine-tuning loss\nof a subset using functional values and gradients from the meta initialization.\nEmpirically, we find that this gradient-based approximation holds with\nremarkable accuracy for twelve transformer-based LMs. Thus, we can now estimate\nfine-tuning performances on CPUs within a few seconds. Finally, we fine-tune\nthe pretrained base model for once on the selected subset of tasks. We conduct\nextensive experiments to validate this approach, delivering a speedup of\n$30\\times$ over conventional subset selection while incurring only $1\\%$ error\nof the true fine-tuning performances. In downstream evaluations involving both\ninstruction tuning and chain-of-thought fine-tuning, this loss-based selection\napproach improves over prior gradient or representation similarity-based\nmethods for subset selection by up to $3.8\\%$.\n","authors":["Dongyue Li","Ziniu Zhang","Lu Wang","Hongyang R. Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.19458v2.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2411.16116v1","updated":"2024-11-25T06:00:42Z","published":"2024-11-25T06:00:42Z","title":"LLM Augmentations to support Analytical Reasoning over Multiple\n  Documents","summary":"  Building on their demonstrated ability to perform a variety of tasks, we\ninvestigate the application of large language models (LLMs) to enhance in-depth\nanalytical reasoning within the context of intelligence analysis. Intelligence\nanalysts typically work with massive dossiers to draw connections between\nseemingly unrelated entities, and uncover adversaries' plans and motives. We\nexplore if and how LLMs can be helpful to analysts for this task and develop an\narchitecture to augment the capabilities of an LLM with a memory module called\ndynamic evidence trees (DETs) to develop and track multiple investigation\nthreads. Through extensive experiments on multiple datasets, we highlight how\nLLMs, as-is, are still inadequate to support intelligence analysts and offer\nrecommendations to improve LLMs for such intricate reasoning applications.\n","authors":["Raquib Bin Yousuf","Nicholas Defelice","Mandar Sharma","Shengzhe Xu","Naren Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2411.16116v1.pdf","comment":"2024 IEEE International Conference on Big Data (IEEE BigData 2024)"},{"id":"http://arxiv.org/abs/2411.16105v1","updated":"2024-11-25T05:32:34Z","published":"2024-11-25T05:32:34Z","title":"Adaptive Circuit Behavior and Generalization in Mechanistic\n  Interpretability","summary":"  Mechanistic interpretability aims to understand the inner workings of large\nneural networks by identifying circuits, or minimal subgraphs within the model\nthat implement algorithms responsible for performing specific tasks. These\ncircuits are typically discovered and analyzed using a narrowly defined prompt\nformat. However, given the abilities of large language models (LLMs) to\ngeneralize across various prompt formats for the same task, it remains unclear\nhow well these circuits generalize. For instance, it is unclear whether the\nmodels generalization results from reusing the same circuit components, the\ncomponents behaving differently, or the use of entirely different components.\nIn this paper, we investigate the generality of the indirect object\nidentification (IOI) circuit in GPT-2 small, which is well-studied and believed\nto implement a simple, interpretable algorithm. We evaluate its performance on\nprompt variants that challenge the assumptions of this algorithm. Our findings\nreveal that the circuit generalizes surprisingly well, reusing all of its\ncomponents and mechanisms while only adding additional input edges. Notably,\nthe circuit generalizes even to prompt variants where the original algorithm\nshould fail; we discover a mechanism that explains this which we term S2\nHacking. Our findings indicate that circuits within LLMs may be more flexible\nand general than previously recognized, underscoring the importance of studying\ncircuit generalization to better understand the broader capabilities of these\nmodels.\n","authors":["Jatin Nainani","Sankaran Vaidyanathan","AJ Yeung","Kartik Gupta","David Jensen"],"pdf_url":"https://arxiv.org/pdf/2411.16105v1.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2404.16789v3","updated":"2024-11-25T05:27:13Z","published":"2024-04-25T17:38:57Z","title":"Continual Learning of Large Language Models: A Comprehensive Survey","summary":"  The recent success of large language models (LLMs) trained on static,\npre-collected, general datasets has sparked numerous research directions and\napplications. One such direction addresses the non-trivial challenge of\nintegrating pre-trained LLMs into dynamic data distributions, task structures,\nand user preferences. Pre-trained LLMs, when tailored for specific needs, often\nexperience significant performance degradation in previous knowledge domains --\na phenomenon known as \"catastrophic forgetting\". While extensively studied in\nthe continual learning (CL) community, it presents new manifestations in the\nrealm of LLMs. In this survey, we provide a comprehensive overview of the\ncurrent research progress on LLMs within the context of CL. This survey is\nstructured into four main sections: we first describe an overview of\ncontinually learning LLMs, consisting of two directions of continuity: vertical\ncontinuity (or vertical continual learning), i.e., continual adaptation from\ngeneral to specific capabilities, and horizontal continuity (or horizontal\ncontinual learning), i.e., continual adaptation across time and domains\n(Section 3). We then summarize three stages of learning LLMs in the context of\nmodern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP),\nand Continual Fine-Tuning (CFT) (Section 4). Then we provide an overview of\nevaluation protocols for continual learning with LLMs, along with the current\navailable data sources (Section 5). Finally, we discuss intriguing questions\npertaining to continual learning for LLMs (Section 6). The full list of papers\nexamined in this survey is available at\nhttps://github.com/Wang-ML-Lab/llm-continual-learning-survey.\n","authors":["Haizhou Shi","Zihao Xu","Hengyi Wang","Weiyi Qin","Wenyuan Wang","Yibin Wang","Zifeng Wang","Sayna Ebrahimi","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2404.16789v3.pdf","comment":"44 pages, 2 figures, 4 tables; Work in progress"},{"id":"http://arxiv.org/abs/2410.09449v2","updated":"2024-11-25T04:57:04Z","published":"2024-10-12T09:06:09Z","title":"Interpretable Video based Stress Detection with Self-Refine\n  Chain-of-thought Reasoning","summary":"  Stress detection is a critical area of research with significant implications\nfor health monitoring and intervention systems. In this paper, we propose a\nnovel interpretable approach for video-based stress detection, leveraging\nself-refine chain-of-thought reasoning to enhance both accuracy and\ntransparency in decision-making processes. Our method focuses on extracting\nsubtle behavioral and physiological cues from video sequences that indicate\nstress levels. By incorporating a chain-of-thought reasoning mechanism, the\nsystem refines its predictions iteratively, ensuring that the decision-making\nprocess can be traced and explained. The model also learns to self-refine\nthrough feedback loops, improving its reasoning capabilities over time.\n  We evaluate our approach on several public and private datasets,\ndemonstrating its superior performance in comparison to traditional video-based\nstress detection methods. Additionally, we provide comprehensive insights into\nthe interpretability of the model's predictions, making the system highly\nvaluable for applications in both healthcare and human-computer interaction\ndomains.\n","authors":["Yi Dai"],"pdf_url":"https://arxiv.org/pdf/2410.09449v2.pdf","comment":"submitted to ICDE 2025 for review"},{"id":"http://arxiv.org/abs/2411.14790v2","updated":"2024-11-25T04:51:57Z","published":"2024-11-22T08:21:03Z","title":"KBAlign: Efficient Self Adaptation on Specific Knowledge Bases","summary":"  Humans can utilize techniques to quickly acquire knowledge from specific\nmaterials in advance, such as creating self-assessment questions, enabling us\nto achieving related tasks more efficiently. In contrast, large language models\n(LLMs) usually relies on retrieval-augmented generation to exploit knowledge\nmaterials in an instant manner, or requires external signals such as human\npreference data and stronger LLM annotations to conduct knowledge adaptation.\nTo unleash the self-learning potential of LLMs, we propose KBAlign, an approach\ndesigned for efficient adaptation to downstream tasks involving knowledge\nbases. Our method utilizes iterative training with self-annotated data such as\nQ&A pairs and revision suggestions, enabling the model to grasp the knowledge\ncontent efficiently. Experimental results on multiple datasets demonstrate the\neffectiveness of our approach, significantly boosting model performance in\ndownstream tasks that require specific knowledge at a low cost. Notably, our\napproach achieves over 90% of the performance improvement that can be obtained\nby using GPT-4-turbo annotation, while relying entirely on self-supervision. We\nrelease our experimental data, models, and process analyses to the community\nfor further exploration (https://github.com/thunlp/KBAlign).\n","authors":["Zheni Zeng","Yuxuan Chen","Shi Yu","Yukun Yan","Zhenghao Liu","Shuo Wang","Xu Han","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2411.14790v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16085v1","updated":"2024-11-25T04:36:01Z","published":"2024-11-25T04:36:01Z","title":"Cautious Optimizers: Improving Training with One Line of Code","summary":"  AdamW has been the default optimizer for transformer pretraining. For many\nyears, our community searches for faster and more stable optimizers with only\nconstraint positive outcomes. In this work, we propose a \\textbf{single-line\nmodification in Pytorch} to any momentum-based optimizer, which we rename\nCautious Optimizer, e.g. C-AdamW and C-Lion. Our theoretical result shows that\nthis modification preserves Adam's Hamiltonian function and it does not break\nthe convergence guarantee under the Lyapunov analysis. In addition, a whole new\nfamily of optimizers is revealed by our theoretical insight. Among them, we\npick the simplest one for empirical experiments, showing speed-up on Llama and\nMAE pretraining up to $1.47\\times$. Code is available at\nhttps://github.com/kyleliang919/C-Optim\n","authors":["Kaizhao Liang","Lizhang Chen","Bo Liu","Qiang Liu"],"pdf_url":"https://arxiv.org/pdf/2411.16085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16077v1","updated":"2024-11-25T04:07:16Z","published":"2024-11-25T04:07:16Z","title":"SAGEval: The frontiers of Satisfactory Agent based NLG Evaluation for\n  reference-free open-ended text","summary":"  Large Language Model (LLM) integrations into applications like Microsoft365\nsuite and Google Workspace for creating/processing documents, emails,\npresentations, etc. has led to considerable enhancements in productivity and\ntime savings. But as these integrations become more more complex, it is\nparamount to ensure that the quality of output from the LLM-integrated\napplications are relevant and appropriate for use. Identifying the need to\ndevelop robust evaluation approaches for natural language generation, wherein\nreferences/ground labels doesn't exist or isn't amply available, this paper\nintroduces a novel framework called \"SAGEval\" which utilizes a critiquing Agent\nto provide feedback on scores generated by LLM evaluators. We show that the\ncritiquing Agent is able to rectify scores from LLM evaluators, in absence of\nreferences/ground-truth labels, thereby reducing the need for labeled data even\nfor complex NLG evaluation scenarios, like the generation of JSON-structured\nforms/surveys with responses in different styles like multiple choice, likert\nratings, single choice questions, etc.\n","authors":["Reshmi Ghosh","Tianyi Yao","Lizzy Chen","Sadid Hasan","Tianwei Chen","Dario Bernal","Huitian Jiao","H M Sajjad Hossain"],"pdf_url":"https://arxiv.org/pdf/2411.16077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16035v1","updated":"2024-11-25T01:48:09Z","published":"2024-11-25T01:48:09Z","title":"Predicting Emergent Capabilities by Finetuning","summary":"  A fundamental open challenge in modern LLM scaling is the lack of\nunderstanding around emergent capabilities. In particular, language model\npretraining loss is known to be highly predictable as a function of compute.\nHowever, downstream capabilities are far less predictable -- sometimes even\nexhibiting emergent jumps -- which makes it challenging to anticipate the\ncapabilities of future models. In this work, we first pose the task of\nemergence prediction: given access to current LLMs that have random few-shot\naccuracy on a task, can we predict whether future models (GPT-N+1) will have\nnon-trivial accuracy on that task? We then discover a simple insight for this\nproblem: finetuning LLMs on a given task can shift the point in scaling at\nwhich emergence occurs towards less capable models. To operationalize this\ninsight, we can finetune LLMs with varying amounts of data and fit a parametric\nfunction that predicts when emergence will occur (i.e., \"emergence laws\"). We\nvalidate this approach using four standard NLP benchmarks where large-scale\nopen-source LLMs already demonstrate emergence (MMLU, GSM8K, CommonsenseQA, and\nCoLA). Using only small-scale LLMs, we find that, in some cases, we can\naccurately predict whether models trained with up to 4x more compute have\nemerged. Finally, we present a case study of two realistic uses for emergence\nprediction.\n","authors":["Charlie Snell","Eric Wallace","Dan Klein","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2411.16035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16767v2","updated":"2024-11-25T01:13:12Z","published":"2024-06-24T16:24:18Z","title":"The GPT-WritingPrompts Dataset: A Comparative Analysis of Character\n  Portrayal in Short Stories","summary":"  The improved generative capabilities of large language models have made them\na powerful tool for creative writing and storytelling. It is therefore\nimportant to quantitatively understand the nature of generated stories, and how\nthey differ from human storytelling. We augment the Reddit WritingPrompts\ndataset with short stories generated by GPT-3.5, given the same prompts. We\nquantify and compare the emotional and descriptive features of storytelling\nfrom both generative processes, human and machine, along a set of six\ndimensions. We find that generated stories differ significantly from human\nstories along all six dimensions, and that human and machine generations\ndisplay similar biases when grouped according to the narrative point-of-view\nand gender of the main protagonist. We release our dataset and code at\nhttps://github.com/KristinHuangg/gpt-writing-prompts.\n","authors":["Xi Yu Huang","Krishnapriya Vishnubhotla","Frank Rudzicz"],"pdf_url":"https://arxiv.org/pdf/2406.16767v2.pdf","comment":"9 pages plus appendices; published at the 6th Workshop on Narrative\n  Understanding, EMNLP 2024"},{"id":"http://arxiv.org/abs/2403.02281v2","updated":"2024-11-25T01:05:20Z","published":"2024-03-04T18:12:10Z","title":"Emotion Granularity from Text: An Aggregate-Level Indicator of Mental\n  Health","summary":"  We are united in how emotions are central to shaping our experiences; and\nyet, individuals differ greatly in how we each identify, categorize, and\nexpress emotions. In psychology, variation in the ability of individuals to\ndifferentiate between emotion concepts is called emotion granularity\n(determined through self-reports of one's emotions). High emotion granularity\nhas been linked with better mental and physical health; whereas low emotion\ngranularity has been linked with maladaptive emotion regulation strategies and\npoor health outcomes. In this work, we propose computational measures of\nemotion granularity derived from temporally-ordered speaker utterances in\nsocial media (in lieu of self-reports that suffer from various biases). We then\ninvestigate the effectiveness of such text-derived measures of emotion\ngranularity in functioning as markers of various mental health conditions\n(MHCs). We establish baseline measures of emotion granularity derived from\ntextual utterances, and show that, at an aggregate level, emotion granularities\nare significantly lower for people self-reporting as having an MHC than for the\ncontrol population. This paves the way towards a better understanding of the\nMHCs, and specifically the role emotions play in our well-being.\n","authors":["Krishnapriya Vishnubhotla","Daniela Teodorescu","Mallory J. Feldman","Kristen A. Lindquist","Saif M. Mohammad"],"pdf_url":"https://arxiv.org/pdf/2403.02281v2.pdf","comment":"9 pages plus appendices; published as a long paper at EMNLP 2024"},{"id":"http://arxiv.org/abs/2411.16020v1","updated":"2024-11-25T00:32:20Z","published":"2024-11-25T00:32:20Z","title":"TransCompressor: LLM-Powered Multimodal Data Compression for Smart\n  Transportation","summary":"  The incorporation of Large Language Models (LLMs) into smart transportation\nsystems has paved the way for improving data management and operational\nefficiency. This study introduces TransCompressor, a novel framework that\nleverages LLMs for efficient compression and decompression of multimodal\ntransportation sensor data. TransCompressor has undergone thorough evaluation\nwith diverse sensor data types, including barometer, speed, and altitude\nmeasurements, across various transportation modes like buses, taxis, and MTRs.\nComprehensive evaluation illustrates the effectiveness of TransCompressor in\nreconstructing transportation sensor data at different compression ratios. The\nresults highlight that, with well-crafted prompts, LLMs can utilize their vast\nknowledge base to contribute to data compression processes, enhancing data\nstorage, analysis, and retrieval in smart transportation settings.\n","authors":["Huanqi Yang","Rucheng Wu","Weitao Xu"],"pdf_url":"https://arxiv.org/pdf/2411.16020v1.pdf","comment":"6 pages"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2411.16683v1","updated":"2024-11-25T18:59:57Z","published":"2024-11-25T18:59:57Z","title":"Generative Omnimatte: Learning to Decompose Video into Layers","summary":"  Given a video and a set of input object masks, an omnimatte method aims to\ndecompose the video into semantically meaningful layers containing individual\nobjects along with their associated effects, such as shadows and reflections.\nExisting omnimatte methods assume a static background or accurate pose and\ndepth estimation and produce poor decompositions when these assumptions are\nviolated. Furthermore, due to the lack of generative prior on natural videos,\nexisting methods cannot complete dynamic occluded regions. We present a novel\ngenerative layered video decomposition framework to address the omnimatte\nproblem. Our method does not assume a stationary scene or require camera pose\nor depth information and produces clean, complete layers, including convincing\ncompletions of occluded dynamic regions. Our core idea is to train a video\ndiffusion model to identify and remove scene effects caused by a specific\nobject. We show that this model can be finetuned from an existing video\ninpainting model with a small, carefully curated dataset, and demonstrate\nhigh-quality decompositions and editing results for a wide range of casually\ncaptured videos containing soft shadows, glossy reflections, splashing water,\nand more.\n","authors":["Yao-Chih Lee","Erika Lu","Sarah Rumbley","Michal Geyer","Jia-Bin Huang","Tali Dekel","Forrester Cole"],"pdf_url":"https://arxiv.org/pdf/2411.16683v1.pdf","comment":"Project page: https://gen-omnimatte.github.io/"},{"id":"http://arxiv.org/abs/2411.16681v1","updated":"2024-11-25T18:59:53Z","published":"2024-11-25T18:59:53Z","title":"Factorized Visual Tokenization and Generation","summary":"  Visual tokenizers are fundamental to image generation. They convert visual\ndata into discrete tokens, enabling transformer-based models to excel at image\ngeneration. Despite their success, VQ-based tokenizers like VQGAN face\nsignificant limitations due to constrained vocabulary sizes. Simply expanding\nthe codebook often leads to training instability and diminishing performance\ngains, making scalability a critical challenge. In this work, we introduce\nFactorized Quantization (FQ), a novel approach that revitalizes VQ-based\ntokenizers by decomposing a large codebook into multiple independent\nsub-codebooks. This factorization reduces the lookup complexity of large\ncodebooks, enabling more efficient and scalable visual tokenization. To ensure\neach sub-codebook captures distinct and complementary information, we propose a\ndisentanglement regularization that explicitly reduces redundancy, promoting\ndiversity across the sub-codebooks. Furthermore, we integrate representation\nlearning into the training process, leveraging pretrained vision models like\nCLIP and DINO to infuse semantic richness into the learned representations.\nThis design ensures our tokenizer captures diverse semantic levels, leading to\nmore expressive and disentangled representations. Experiments show that the\nproposed FQGAN model substantially improves the reconstruction quality of\nvisual tokenizers, achieving state-of-the-art performance. We further\ndemonstrate that this tokenizer can be effectively adapted into auto-regressive\nimage generation. https://showlab.github.io/FQGAN\n","authors":["Zechen Bai","Jianxiong Gao","Ziteng Gao","Pichao Wang","Zheng Zhang","Tong He","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2411.16681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16680v1","updated":"2024-11-25T18:59:50Z","published":"2024-11-25T18:59:50Z","title":"Quark: Real-time, High-resolution, and General Neural View Synthesis","summary":"  We present a novel neural algorithm for performing high-quality,\nhigh-resolution, real-time novel view synthesis. From a sparse set of input RGB\nimages or videos streams, our network both reconstructs the 3D scene and\nrenders novel views at 1080p resolution at 30fps on an NVIDIA A100. Our\nfeed-forward network generalizes across a wide variety of datasets and scenes\nand produces state-of-the-art quality for a real-time method. Our quality\napproaches, and in some cases surpasses, the quality of some of the top offline\nmethods. In order to achieve these results we use a novel combination of\nseveral key concepts, and tie them together into a cohesive and effective\nalgorithm. We build on previous works that represent the scene using\nsemi-transparent layers and use an iterative learned render-and-refine approach\nto improve those layers. Instead of flat layers, our method reconstructs\nlayered depth maps (LDMs) that efficiently represent scenes with complex depth\nand occlusions. The iterative update steps are embedded in a multi-scale,\nUNet-style architecture to perform as much compute as possible at reduced\nresolution. Within each update step, to better aggregate the information from\nmultiple input views, we use a specialized Transformer-based network component.\nThis allows the majority of the per-input image processing to be performed in\nthe input image space, as opposed to layer space, further increasing\nefficiency. Finally, due to the real-time nature of our reconstruction and\nrendering, we dynamically create and discard the internal 3D geometry for each\nframe, generating the LDM for each view. Taken together, this produces a novel\nand effective algorithm for view synthesis. Through extensive evaluation, we\ndemonstrate that we achieve state-of-the-art quality at real-time rates.\nProject page: https://quark-3d.github.io/\n","authors":["John Flynn","Michael Broxton","Lukas Murmann","Lucy Chai","Matthew DuVall","Clément Godard","Kathryn Heal","Srinivas Kaza","Stephen Lombardi","Xuan Luo","Supreeth Achar","Kira Prabhu","Tiancheng Sun","Lynn Tsai","Ryan Overbeck"],"pdf_url":"https://arxiv.org/pdf/2411.16680v1.pdf","comment":"SIGGRAPH Asia 2024 camera ready version; project page\n  https://quark-3d.github.io/"},{"id":"http://arxiv.org/abs/2404.06507v3","updated":"2024-11-25T18:58:07Z","published":"2024-04-09T17:55:41Z","title":"Reconstructing Hand-Held Objects in 3D from Images and Videos","summary":"  Objects manipulated by the hand (i.e., manipulanda) are particularly\nchallenging to reconstruct from Internet videos. Not only does the hand occlude\nmuch of the object, but also the object is often only visible in a small number\nof image pixels. At the same time, two strong anchors emerge in this setting:\n(1) estimated 3D hands help disambiguate the location and scale of the object,\nand (2) the set of manipulanda is small relative to all possible objects. With\nthese insights in mind, we present a scalable paradigm for hand-held object\nreconstruction that builds on recent breakthroughs in large language/vision\nmodels and 3D object datasets. Given a monocular RGB video, we aim to\nreconstruct hand-held object geometry in 3D, over time. In order to obtain the\nbest performing single frame model, we first present MCC-Hand-Object (MCC-HO),\nwhich jointly reconstructs hand and object geometry given a single RGB image\nand inferred 3D hand as inputs. Subsequently, we prompt a text-to-3D generative\nmodel using GPT-4(V) to retrieve a 3D object model that matches the object in\nthe image(s); we call this alignment Retrieval-Augmented Reconstruction (RAR).\nRAR provides unified object geometry across all frames, and the result is\nrigidly aligned with both the input images and 3D MCC-HO observations in a\ntemporally consistent manner. Experiments demonstrate that our approach\nachieves state-of-the-art performance on lab and Internet image/video datasets.\nWe make our code and models available on the project website:\nhttps://janehwu.github.io/mcc-ho\n","authors":["Jane Wu","Georgios Pavlakos","Georgia Gkioxari","Jitendra Malik"],"pdf_url":"https://arxiv.org/pdf/2404.06507v3.pdf","comment":"Project page: https://janehwu.github.io/mcc-ho"},{"id":"http://arxiv.org/abs/2411.16668v1","updated":"2024-11-25T18:53:56Z","published":"2024-11-25T18:53:56Z","title":"Diffusion Features for Zero-Shot 6DoF Object Pose Estimation","summary":"  Zero-shot object pose estimation enables the retrieval of object poses from\nimages without necessitating object-specific training. In recent approaches\nthis is facilitated by vision foundation models (VFM), which are pre-trained\nmodels that are effectively general-purpose feature extractors. The\ncharacteristics exhibited by these VFMs vary depending on the training data,\nnetwork architecture, and training paradigm. The prevailing choice in this\nfield are self-supervised Vision Transformers (ViT). This study assesses the\ninfluence of Latent Diffusion Model (LDM) backbones on zero-shot pose\nestimation. In order to facilitate a comparison between the two families of\nmodels on a common ground we adopt and modify a recent approach. Therefore, a\ntemplate-based multi-staged method for estimating poses in a zero-shot fashion\nusing LDMs is presented. The efficacy of the proposed approach is empirically\nevaluated on three standard datasets for object-specific 6DoF pose estimation.\nThe experiments demonstrate an Average Recall improvement of up to 27% over the\nViT baseline. The source code is available at: https://github.com/BvG1993/DZOP.\n","authors":["Bernd Von Gimborn","Philipp Ausserlechner","Markus Vincze","Stefan Thalhammer"],"pdf_url":"https://arxiv.org/pdf/2411.16668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16665v1","updated":"2024-11-25T18:53:09Z","published":"2024-11-25T18:53:09Z","title":"Edge Weight Prediction For Category-Agnostic Pose Estimation","summary":"  Category-Agnostic Pose Estimation (CAPE) localizes keypoints across diverse\nobject categories with a single model, using one or a few annotated support\nimages. Recent works have shown that using a pose graph (i.e., treating\nkeypoints as nodes in a graph rather than isolated points) helps handle\nocclusions and break symmetry. However, these methods assume a static pose\ngraph with equal-weight edges, leading to suboptimal results. We introduce\nEdgeCape, a novel framework that overcomes these limitations by predicting the\ngraph's edge weights which optimizes localization. To further leverage\nstructural priors, we propose integrating Markovian Structural Bias, which\nmodulates the self-attention interaction between nodes based on the number of\nhops between them. We show that this improves the model's ability to capture\nglobal spatial dependencies. Evaluated on the MP-100 benchmark, which includes\n100 categories and over 20K images, EdgeCape achieves state-of-the-art results\nin the 1-shot setting and leads among similar-sized methods in the 5-shot\nsetting, significantly improving keypoint localization accuracy. Our code is\npublicly available.\n","authors":["Or Hirschorn","Shai Avidan"],"pdf_url":"https://arxiv.org/pdf/2411.16665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16657v1","updated":"2024-11-25T18:41:56Z","published":"2024-11-25T18:41:56Z","title":"DreamRunner: Fine-Grained Storytelling Video Generation with\n  Retrieval-Augmented Motion Adaptation","summary":"  Storytelling video generation (SVG) has recently emerged as a task to create\nlong, multi-motion, multi-scene videos that consistently represent the story\ndescribed in the input text script. SVG holds great potential for diverse\ncontent creation in media and entertainment; however, it also presents\nsignificant challenges: (1) objects must exhibit a range of fine-grained,\ncomplex motions, (2) multiple objects need to appear consistently across\nscenes, and (3) subjects may require multiple motions with seamless transitions\nwithin a single scene. To address these challenges, we propose DreamRunner, a\nnovel story-to-video generation method: First, we structure the input script\nusing a large language model (LLM) to facilitate both coarse-grained scene\nplanning as well as fine-grained object-level layout and motion planning. Next,\nDreamRunner presents retrieval-augmented test-time adaptation to capture target\nmotion priors for objects in each scene, supporting diverse motion\ncustomization based on retrieved videos, thus facilitating the generation of\nnew videos with complex, scripted motions. Lastly, we propose a novel\nspatial-temporal region-based 3D attention and prior injection module SR3AI for\nfine-grained object-motion binding and frame-by-frame semantic control. We\ncompare DreamRunner with various SVG baselines, demonstrating state-of-the-art\nperformance in character consistency, text alignment, and smooth transitions.\nAdditionally, DreamRunner exhibits strong fine-grained condition-following\nability in compositional text-to-video generation, significantly outperforming\nbaselines on T2V-ComBench. Finally, we validate DreamRunner's robust ability to\ngenerate multi-object interactions with qualitative examples.\n","authors":["Zun Wang","Jialu Li","Han Lin","Jaehong Yoon","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2411.16657v1.pdf","comment":"Project website: https://dreamrunner-story2video.github.io/"},{"id":"http://arxiv.org/abs/2410.17494v2","updated":"2024-11-25T18:35:14Z","published":"2024-10-23T01:25:25Z","title":"Enhancing Multimodal Medical Image Classification using Cross-Graph\n  Modal Contrastive Learning","summary":"  The classification of medical images is a pivotal aspect of disease\ndiagnosis, often enhanced by deep learning techniques. However, traditional\napproaches typically focus on unimodal medical image data, neglecting the\nintegration of diverse non-image patient data. This paper proposes a novel\nCross-Graph Modal Contrastive Learning (CGMCL) framework for multimodal medical\nimage classification. The model effectively integrates both image and non-image\ndata by constructing cross-modality graphs and leveraging contrastive learning\nto align multimodal features in a shared latent space. An inter-modality\nfeature scaling module further optimizes the representation learning process by\nreducing the gap between heterogeneous modalities. The proposed approach is\nevaluated on two datasets: a Parkinson's disease (PD) dataset and a public\nmelanoma dataset. Results demonstrate that CGMCL outperforms conventional\nunimodal methods in accuracy, interpretability, and early disease prediction.\nAdditionally, the method shows superior performance in multi-class melanoma\nclassification. The CGMCL framework provides valuable insights into medical\nimage classification while offering improved disease interpretability and\npredictive capabilities.\n","authors":["Jun-En Ding","Chien-Chin Hsu","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.17494v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11810v4","updated":"2024-11-25T18:17:12Z","published":"2023-11-20T14:42:25Z","title":"DocPedia: Unleashing the Power of Large Multimodal Model in the\n  Frequency Domain for Versatile Document Understanding","summary":"  This work presents DocPedia, a novel large multimodal model (LMM) for\nversatile OCR-free document understanding, capable of parsing images up to\n2,560$\\times$2,560 resolution. Unlike existing work either struggle with\nhigh-resolution documents or give up the large language model thus vision or\nlanguage ability constrained, our DocPedia directly processes visual input in\nthe frequency domain rather than the pixel space. The unique characteristic\nenables DocPedia to capture a greater amount of visual and textual information\nusing a limited number of visual tokens. To consistently enhance both\nperception and comprehension abilities of our model, we develop a dual-stage\ntraining strategy and enrich instructions/annotations of all training tasks\ncovering multiple document types. Extensive quantitative and qualitative\nexperiments conducted on various publicly available benchmarks confirm the\nmutual benefits of jointly learning perception and comprehension tasks. The\nresults provide further evidence of the effectiveness and superior performance\nof our DocPedia over other methods.\n","authors":["Hao Feng","Qi Liu","Hao Liu","Jingqun Tang","Wengang Zhou","Houqiang Li","Can Huang"],"pdf_url":"https://arxiv.org/pdf/2311.11810v4.pdf","comment":"Accepted by Science China Information Sciences (SCIS)"},{"id":"http://arxiv.org/abs/2311.16515v3","updated":"2024-11-25T18:11:18Z","published":"2023-11-25T14:24:49Z","title":"Word4Per: Zero-shot Composed Person Retrieval","summary":"  Searching for specific person has great social benefits and security value,\nand it often involves a combination of visual and textual information.\nConventional person retrieval methods, whether image-based or text-based,\nusually fall short in effectively harnessing both types of information, leading\nto the loss of accuracy. In this paper, a whole new task called Composed Person\nRetrieval (CPR) is proposed to jointly utilize both image and text information\nfor target person retrieval. However, the supervised CPR requires very costly\nmanual annotation dataset, while there are currently no available resources. To\nmitigate this issue, we firstly introduce the Zero-shot Composed Person\nRetrieval (ZS-CPR), which leverages existing domain-related data to resolve the\nCPR problem without expensive annotations. Secondly, to learn ZS-CPR model, we\npropose a two-stage learning framework, Word4Per, where a lightweight Textual\nInversion Network (TINet) and a text-based person retrieval model based on\nfine-tuned Contrastive Language-Image Pre-training (CLIP) network are learned\nwithout utilizing any CPR data. Thirdly, a finely annotated Image-Text Composed\nPerson Retrieval (ITCPR) dataset is built as the benchmark to assess the\nperformance of the proposed Word4Per framework. Extensive experiments under\nboth Rank-1 and mAP demonstrate the effectiveness of Word4Per for the ZS-CPR\ntask, surpassing the comparative methods by over 10\\%. The code and ITCPR\ndataset will be publicly available at\nhttps://github.com/Delong-liu-bupt/Word4Per.\n","authors":["Delong Liu","Haiwen Li","Zhicheng Zhao","Fei Su","Yuan Dong"],"pdf_url":"https://arxiv.org/pdf/2311.16515v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16629v1","updated":"2024-11-25T18:05:34Z","published":"2024-11-25T18:05:34Z","title":"LegoPET: Hierarchical Feature Guided Conditional Diffusion for PET Image\n  Reconstruction","summary":"  Positron emission tomography (PET) is widely utilized for cancer detection\ndue to its ability to visualize functional and biological processes in vivo.\nPET images are usually reconstructed from histogrammed raw data (sinograms)\nusing traditional iterative techniques (e.g., OSEM, MLEM). Recently, deep\nlearning (DL) methods have shown promise by directly mapping raw sinogram data\nto PET images. However, DL approaches that are regression-based or GAN-based\noften produce overly smoothed images or introduce various artifacts\nrespectively. Image-conditioned diffusion probabilistic models (cDPMs) are\nanother class of likelihood-based DL techniques capable of generating highly\nrealistic and controllable images. While cDPMs have notable strengths, they\nstill face challenges such as maintain correspondence and consistency between\ninput and output images when they are from different domains (e.g., sinogram\nvs. image domain) as well as slow convergence rates. To address these\nlimitations, we introduce LegoPET, a hierarchical feature guided conditional\ndiffusion model for high-perceptual quality PET image reconstruction from\nsinograms. We conducted several experiments demonstrating that LegoPET not only\nimproves the performance of cDPMs but also surpasses recent DL-based PET image\nreconstruction techniques in terms of visual quality and pixel-level PSNR/SSIM\nmetrics. Our code is available at https://github.com/yransun/LegoPET.\n","authors":["Yiran Sun","Osama Mawlawi"],"pdf_url":"https://arxiv.org/pdf/2411.16629v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.16622v1","updated":"2024-11-25T18:02:23Z","published":"2024-11-25T18:02:23Z","title":"Imperceptible Adversarial Examples in the Physical World","summary":"  Adversarial examples in the digital domain against deep learning-based\ncomputer vision models allow for perturbations that are imperceptible to human\neyes. However, producing similar adversarial examples in the physical world has\nbeen difficult due to the non-differentiable image distortion functions in\nvisual sensing systems. The existing algorithms for generating physically\nrealizable adversarial examples often loosen their definition of adversarial\nexamples by allowing unbounded perturbations, resulting in obvious or even\nstrange visual patterns. In this work, we make adversarial examples\nimperceptible in the physical world using a straight-through estimator (STE,\na.k.a. BPDA). We employ STE to overcome the non-differentiability -- applying\nexact, non-differentiable distortions in the forward pass of the\nbackpropagation step, and using the identity function in the backward pass. Our\ndifferentiable rendering extension to STE also enables imperceptible\nadversarial patches in the physical world. Using printout photos, and\nexperiments in the CARLA simulator, we show that STE enables fast generation of\n$\\ell_\\infty$ bounded adversarial examples despite the non-differentiable\ndistortions. To the best of our knowledge, this is the first work demonstrating\nimperceptible adversarial examples bounded by small $\\ell_\\infty$ norms in the\nphysical world that force zero classification accuracy in the global\nperturbation threat model and cause near-zero ($4.22\\%$) AP50 in object\ndetection in the patch perturbation threat model. We urge the community to\nre-evaluate the threat of adversarial examples in the physical world.\n","authors":["Weilin Xu","Sebastian Szyller","Cory Cornelius","Luis Murillo Rojas","Marius Arvinte","Alvaro Velasquez","Jason Martin","Nageen Himayat"],"pdf_url":"https://arxiv.org/pdf/2411.16622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.09059v2","updated":"2024-11-25T18:01:56Z","published":"2023-07-18T08:23:46Z","title":"Text-guided Image Restoration and Semantic Enhancement for Text-to-Image\n  Person Retrieval","summary":"  The goal of Text-to-Image Person Retrieval (TIPR) is to retrieve specific\nperson images according to the given textual descriptions. A primary challenge\nin this task is bridging the substantial representational gap between visual\nand textual modalities. The prevailing methods map texts and images into\nunified embedding space for matching, while the intricate semantic\ncorrespondences between texts and images are still not effectively constructed.\nTo address this issue, we propose a novel TIPR framework to build fine-grained\ninteractions and alignment between person images and the corresponding texts.\nSpecifically, via fine-tuning the Contrastive Language-Image Pre-training\n(CLIP) model, a visual-textual dual encoder is firstly constructed, to\npreliminarily align the image and text features. Secondly, a Text-guided Image\nRestoration (TIR) auxiliary task is proposed to map abstract textual entities\nto specific image regions, improving the alignment between local textual and\nvisual embeddings. Additionally, a cross-modal triplet loss is presented to\nhandle hard samples, and further enhance the model's discriminability for minor\ndifferences. Moreover, a pruning-based text data augmentation approach is\nproposed to enhance focus on essential elements in descriptions, thereby\navoiding excessive model attention to less significant information. The\nexperimental results show our proposed method outperforms state-of-the-art\nmethods on three popular benchmark datasets, and the code will be made publicly\navailable at https://github.com/Delong-liu-bupt/SEN.\n","authors":["Delong Liu","Haiwen Li","Zhicheng Zhao","Yuan Dong","Nikolaos V. Boulgouris"],"pdf_url":"https://arxiv.org/pdf/2307.09059v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16619v1","updated":"2024-11-25T17:58:43Z","published":"2024-11-25T17:58:43Z","title":"Human-Activity AGV Quality Assessment: A Benchmark Dataset and an\n  Objective Evaluation Metric","summary":"  AI-driven video generation techniques have made significant progress in\nrecent years. However, AI-generated videos (AGVs) involving human activities\noften exhibit substantial visual and semantic distortions, hindering the\npractical application of video generation technologies in real-world scenarios.\nTo address this challenge, we conduct a pioneering study on human activity AGV\nquality assessment, focusing on visual quality evaluation and the\nidentification of semantic distortions. First, we construct the AI-Generated\nHuman activity Video Quality Assessment (Human-AGVQA) dataset, consisting of\n3,200 AGVs derived from 8 popular text-to-video (T2V) models using 400 text\nprompts that describe diverse human activities. We conduct a subjective study\nto evaluate the human appearance quality, action continuity quality, and\noverall video quality of AGVs, and identify semantic issues of human body\nparts. Based on Human-AGVQA, we benchmark the performance of T2V models and\nanalyze their strengths and weaknesses in generating different categories of\nhuman activities. Second, we develop an objective evaluation metric, named\nAI-Generated Human activity Video Quality metric (GHVQ), to automatically\nanalyze the quality of human activity AGVs. GHVQ systematically extracts\nhuman-focused quality features, AI-generated content-aware quality features,\nand temporal continuity features, making it a comprehensive and explainable\nquality metric for human activity AGVs. The extensive experimental results show\nthat GHVQ outperforms existing quality metrics on the Human-AGVQA dataset by a\nlarge margin, demonstrating its efficacy in assessing the quality of human\nactivity AGVs. The Human-AGVQA dataset and GHVQ metric will be released in\npublic at https://github.com/zczhang-sjtu/GHVQ.git\n","authors":["Zhichao Zhang","Wei Sun","Xinyue Li","Yunhao Li","Qihang Ge","Jun Jia","Zicheng Zhang","Zhongpeng Ji","Fengyu Sun","Shangling Jui","Xiongkuo Min","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2411.16619v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16616v1","updated":"2024-11-25T17:54:44Z","published":"2024-11-25T17:54:44Z","title":"GeoFormer: A Multi-Polygon Segmentation Transformer","summary":"  In remote sensing there exists a common need for learning scale invariant\nshapes of objects like buildings. Prior works relies on tweaking multiple loss\nfunctions to convert segmentation maps into the final scale invariant\nrepresentation, necessitating arduous design and optimization. For this purpose\nwe introduce the GeoFormer, a novel architecture which presents a remedy to the\nsaid challenges, learning to generate multipolygons end-to-end. By modeling\nkeypoints as spatially dependent tokens in an auto-regressive manner, the\nGeoFormer outperforms existing works in delineating building objects from\nsatellite imagery. We evaluate the robustness of the GeoFormer against former\nmethods through a variety of parameter ablations and highlight the advantages\nof optimizing a single likelihood function. Our study presents the first\nsuccessful application of auto-regressive transformer models for multi-polygon\npredictions in remote sensing, suggesting a promising methodological\nalternative for building vectorization.\n","authors":["Maxim Khomiakov","Michael Riis Andersen","Jes Frellsen"],"pdf_url":"https://arxiv.org/pdf/2411.16616v1.pdf","comment":"21 pages, 5 figures, in proceedings of British Machine Vision\n  Conference 2024"},{"id":"http://arxiv.org/abs/2411.15098v2","updated":"2024-11-25T17:46:35Z","published":"2024-11-22T17:55:15Z","title":"OminiControl: Minimal and Universal Control for Diffusion Transformer","summary":"  In this paper, we introduce OminiControl, a highly versatile and\nparameter-efficient framework that integrates image conditions into pre-trained\nDiffusion Transformer (DiT) models. At its core, OminiControl leverages a\nparameter reuse mechanism, enabling the DiT to encode image conditions using\nitself as a powerful backbone and process them with its flexible multi-modal\nattention processors. Unlike existing methods, which rely heavily on additional\nencoder modules with complex architectures, OminiControl (1) effectively and\nefficiently incorporates injected image conditions with only ~0.1% additional\nparameters, and (2) addresses a wide range of image conditioning tasks in a\nunified manner, including subject-driven generation and spatially-aligned\nconditions such as edges, depth, and more. Remarkably, these capabilities are\nachieved by training on images generated by the DiT itself, which is\nparticularly beneficial for subject-driven generation. Extensive evaluations\ndemonstrate that OminiControl outperforms existing UNet-based and DiT-adapted\nmodels in both subject-driven and spatially-aligned conditional generation.\nAdditionally, we release our training dataset, Subjects200K, a diverse\ncollection of over 200,000 identity-consistent images, along with an efficient\ndata synthesis pipeline to advance research in subject-consistent generation.\n","authors":["Zhenxiong Tan","Songhua Liu","Xingyi Yang","Qiaochu Xue","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2411.15098v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16602v1","updated":"2024-11-25T17:31:57Z","published":"2024-11-25T17:31:57Z","title":"Chat2SVG: Vector Graphics Generation with Large Language Models and\n  Image Diffusion Models","summary":"  Scalable Vector Graphics (SVG) has become the de facto standard for vector\ngraphics in digital design, offering resolution independence and precise\ncontrol over individual elements. Despite their advantages, creating\nhigh-quality SVG content remains challenging, as it demands technical expertise\nwith professional editing software and a considerable time investment to craft\ncomplex shapes. Recent text-to-SVG generation methods aim to make vector\ngraphics creation more accessible, but they still encounter limitations in\nshape regularity, generalization ability, and expressiveness. To address these\nchallenges, we introduce Chat2SVG, a hybrid framework that combines the\nstrengths of Large Language Models (LLMs) and image diffusion models for\ntext-to-SVG generation. Our approach first uses an LLM to generate semantically\nmeaningful SVG templates from basic geometric primitives. Guided by image\ndiffusion models, a dual-stage optimization pipeline refines paths in latent\nspace and adjusts point coordinates to enhance geometric complexity. Extensive\nexperiments show that Chat2SVG outperforms existing methods in visual fidelity,\npath regularity, and semantic alignment. Additionally, our system enables\nintuitive editing through natural language instructions, making professional\nvector graphics creation accessible to all users.\n","authors":["Ronghuan Wu","Wanchao Su","Jing Liao"],"pdf_url":"https://arxiv.org/pdf/2411.16602v1.pdf","comment":"Project Page: https://chat2svg.github.io/"},{"id":"http://arxiv.org/abs/2411.16598v1","updated":"2024-11-25T17:30:32Z","published":"2024-11-25T17:30:32Z","title":"Unlocking The Potential of Adaptive Attacks on Diffusion-Based\n  Purification","summary":"  Diffusion-based purification (DBP) is a defense against adversarial examples\n(AEs), amassing popularity for its ability to protect classifiers in an\nattack-oblivious manner and resistance to strong adversaries with access to the\ndefense. Its robustness has been claimed to ensue from the reliance on\ndiffusion models (DMs) that project the AEs onto the natural distribution. We\nrevisit this claim, focusing on gradient-based strategies that back-propagate\nthe loss gradients through the defense, commonly referred to as ``adaptive\nattacks\". Analytically, we show that such an optimization method invalidates\nDBP's core foundations, effectively targeting the DM rather than the classifier\nand restricting the purified outputs to a distribution over malicious samples\ninstead. Thus, we reassess the reported empirical robustness, uncovering\nimplementation flaws in the gradient back-propagation techniques used thus far\nfor DBP. We fix these issues, providing the first reliable gradient library for\nDBP and demonstrating how adaptive attacks drastically degrade its robustness.\nWe then study a less efficient yet stricter majority-vote setting where the\nclassifier evaluates multiple purified copies of the input to make its\ndecision. Here, DBP's stochasticity enables it to remain partially robust\nagainst traditional norm-bounded AEs. We propose a novel adaptation of a recent\noptimization method against deepfake watermarking that crafts systemic\nmalicious perturbations while ensuring imperceptibility. When integrated with\nthe adaptive attack, it completely defeats DBP, even in the majority-vote\nsetup. Our findings prove that DBP, in its current state, is not a viable\ndefense against AEs.\n","authors":["Andre Kassis","Urs Hengartner","Yaoliang Yu"],"pdf_url":"https://arxiv.org/pdf/2411.16598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15658v2","updated":"2024-11-25T17:14:20Z","published":"2024-05-24T15:53:59Z","title":"CoHD: A Counting-Aware Hierarchical Decoding Framework for Generalized\n  Referring Expression Segmentation","summary":"  The newly proposed Generalized Referring Expression Segmentation (GRES)\namplifies the formulation of classic RES by involving complex\nmultiple/non-target scenarios. Recent approaches address GRES by directly\nextending the well-adopted RES frameworks with object-existence identification.\nHowever, these approaches tend to encode multi-granularity object information\ninto a single representation, which makes it difficult to precisely represent\ncomprehensive objects of different granularity. Moreover, the simple binary\nobject-existence identification across all referent scenarios fails to specify\ntheir inherent differences, incurring ambiguity in object understanding. To\ntackle the above issues, we propose a \\textbf{Co}unting-Aware\n\\textbf{H}ierarchical \\textbf{D}ecoding framework (CoHD) for GRES. By\ndecoupling the intricate referring semantics into different granularity with a\nvisual-linguistic hierarchy, and dynamic aggregating it with intra- and\ninter-selection, CoHD boosts multi-granularity comprehension with the\nreciprocal benefit of the hierarchical nature. Furthermore, we incorporate the\ncounting ability by embodying multiple/single/non-target scenarios into count-\nand category-level supervision, facilitating comprehensive object perception.\nExperimental results on gRefCOCO, Ref-ZOM, R-RefCOCO, and RefCOCO benchmarks\ndemonstrate the effectiveness and rationality of CoHD which outperforms\nstate-of-the-art GRES methods by a remarkable margin. Code is available at\n\\href{https://github.com/RobertLuo1/CoHD}{here}.\n","authors":["Zhuoyan Luo","Yinghao Wu","Tianheng Cheng","Yong Liu","Yicheng Xiao","Hongfa Wang","Xiao-Ping Zhang","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2405.15658v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07610v2","updated":"2024-11-25T17:01:53Z","published":"2024-10-10T04:54:37Z","title":"CSA: Data-efficient Mapping of Unimodal Features to Multimodal Features","summary":"  Multimodal encoders like CLIP excel in tasks such as zero-shot image\nclassification and cross-modal retrieval. However, they require excessive\ntraining data. We propose canonical similarity analysis (CSA), which uses two\nunimodal encoders to replicate multimodal encoders using limited data. CSA maps\nunimodal features into a multimodal space, using a new similarity score to\nretain only the multimodal information. CSA only involves the inference of\nunimodal encoders and a cubic-complexity matrix decomposition, eliminating the\nneed for extensive GPU-based model training. Experiments show that CSA\noutperforms CLIP while requiring $300,000\\times$ fewer multimodal data pairs\nand $6\\times$ fewer unimodal data for ImageNet classification and\nmisinformative news captions detection. CSA surpasses the state-of-the-art\nmethod to map unimodal features to multimodal features. We also demonstrate the\nability of CSA with modalities beyond image and text, paving the way for future\nmodality pairs with limited paired multimodal data but abundant unpaired\nunimodal data, such as lidar and text.\n","authors":["Po-han Li","Sandeep P. Chinchali","Ufuk Topcu"],"pdf_url":"https://arxiv.org/pdf/2410.07610v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16575v1","updated":"2024-11-25T16:59:42Z","published":"2024-11-25T16:59:42Z","title":"Rethinking Diffusion for Text-Driven Human Motion Generation","summary":"  Since 2023, Vector Quantization (VQ)-based discrete generation methods have\nrapidly dominated human motion generation, primarily surpassing diffusion-based\ncontinuous generation methods in standard performance metrics. However,\nVQ-based methods have inherent limitations. Representing continuous motion data\nas limited discrete tokens leads to inevitable information loss, reduces the\ndiversity of generated motions, and restricts their ability to function\neffectively as motion priors or generation guidance. In contrast, the\ncontinuous space generation nature of diffusion-based methods makes them\nwell-suited to address these limitations and with even potential for model\nscalability. In this work, we systematically investigate why current VQ-based\nmethods perform well and explore the limitations of existing diffusion-based\nmethods from the perspective of motion data representation and distribution.\nDrawing on these insights, we preserve the inherent strengths of a\ndiffusion-based human motion generation model and gradually optimize it with\ninspiration from VQ-based approaches. Our approach introduces a human motion\ndiffusion model enabled to perform bidirectional masked autoregression,\noptimized with a reformed data representation and distribution. Additionally,\nwe also propose more robust evaluation methods to fairly assess different-based\nmethods. Extensive experiments on benchmark human motion generation datasets\ndemonstrate that our method excels previous methods and achieves\nstate-of-the-art performances.\n","authors":["Zichong Meng","Yiming Xie","Xiaogang Peng","Zeyu Han","Huaizu Jiang"],"pdf_url":"https://arxiv.org/pdf/2411.16575v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2409.18992v2","updated":"2024-11-25T16:55:33Z","published":"2024-09-17T22:10:05Z","title":"A Review of Mechanistic Models of Event Comprehension","summary":"  This review examines theoretical assumptions and computational models of\nevent comprehension, tracing the evolution from discourse comprehension\ntheories to contemporary event cognition frameworks. The review covers key\ndiscourse comprehension accounts, including Construction-Integration, Event\nIndexing, Causal Network, and Resonance models, highlighting their\ncontributions to understanding cognitive processes in comprehension. I then\ndiscuss contemporary theoretical frameworks of event comprehension, including\nEvent Segmentation Theory (Zacks et al., 2007), the Event Horizon Model\n(Radvansky & Zacks, 2014), and Hierarchical Generative Framework (Kuperberg,\n2021), which emphasize prediction, causality, and multilevel representations in\nevent understanding. Building on these theories, I evaluate five computational\nmodels of event comprehension: REPRISE (Butz et al., 2019), Structured Event\nMemory (SEM; Franklin et al., 2020), the Lu model (Lu et al., 2022), the\nGumbsch model (Gumbsch et al., 2022), and the Elman and McRae model (2019). The\nanalysis focuses on their approaches to hierarchical processing, prediction\nmechanisms, and representation learning. Key themes that emerge include the use\nof hierarchical structures as inductive biases, the importance of prediction in\ncomprehension, and diverse strategies for learning event dynamics. The review\nidentifies critical areas for future research, including the need for more\nsophisticated approaches to learning structured representations, integrating\nepisodic memory mechanisms, and developing adaptive updating algorithms for\nworking event models. By synthesizing insights from both theoretical frameworks\nand computational implementations, this review aims to advance our\nunderstanding of human event comprehension and guide future modeling efforts in\ncognitive science.\n","authors":["Tan T. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2409.18992v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16568v1","updated":"2024-11-25T16:52:21Z","published":"2024-11-25T16:52:21Z","title":"J-CaPA : Joint Channel and Pyramid Attention Improves Medical Image\n  Segmentation","summary":"  Medical image segmentation is crucial for diagnosis and treatment planning.\nTraditional CNN-based models, like U-Net, have shown promising results but\nstruggle to capture long-range dependencies and global context. To address\nthese limitations, we propose a transformer-based architecture that jointly\napplies Channel Attention and Pyramid Attention mechanisms to improve\nmulti-scale feature extraction and enhance segmentation performance for medical\nimages. Increasing model complexity requires more training data, and we further\nimprove model generalization with CutMix data augmentation. Our approach is\nevaluated on the Synapse multi-organ segmentation dataset, achieving a 6.9%\nimprovement in Mean Dice score and a 39.9% improvement in Hausdorff Distance\n(HD95) over an implementation without our enhancements. Our proposed model\ndemonstrates improved segmentation accuracy for complex anatomical structures,\noutperforming existing state-of-the-art methods.\n","authors":["Marzia Binta Nizam","Marian Zlateva","James Davis"],"pdf_url":"https://arxiv.org/pdf/2411.16568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16554v1","updated":"2024-11-25T16:38:17Z","published":"2024-11-25T16:38:17Z","title":"Generating Out-Of-Distribution Scenarios Using Language Models","summary":"  The deployment of autonomous vehicles controlled by machine learning\ntechniques requires extensive testing in diverse real-world environments,\nrobust handling of edge cases and out-of-distribution scenarios, and\ncomprehensive safety validation to ensure that these systems can navigate\nsafely and effectively under unpredictable conditions. Addressing\nOut-Of-Distribution (OOD) driving scenarios is essential for enhancing safety,\nas OOD scenarios help validate the reliability of the models within the\nvehicle's autonomy stack. However, generating OOD scenarios is challenging due\nto their long-tailed distribution and rarity in urban driving dataset.\nRecently, Large Language Models (LLMs) have shown promise in autonomous\ndriving, particularly for their zero-shot generalization and common-sense\nreasoning capabilities. In this paper, we leverage these LLM strengths to\nintroduce a framework for generating diverse OOD driving scenarios. Our\napproach uses LLMs to construct a branching tree, where each branch represents\na unique OOD scenario. These scenarios are then simulated in the CARLA\nsimulator using an automated framework that aligns scene augmentation with the\ncorresponding textual descriptions. We evaluate our framework through extensive\nsimulations, and assess its performance via a diversity metric that measures\nthe richness of the scenarios. Additionally, we introduce a new \"OOD-ness\"\nmetric, which quantifies how much the generated scenarios deviate from typical\nurban driving conditions. Furthermore, we explore the capacity of modern\nVision-Language Models (VLMs) to interpret and safely navigate through the\nsimulated OOD scenarios. Our findings offer valuable insights into the\nreliability of language models in addressing OOD scenarios within the context\nof urban driving.\n","authors":["Erfan Aasi","Phat Nguyen","Shiva Sreeram","Guy Rosman","Sertac Karaman","Daniela Rus"],"pdf_url":"https://arxiv.org/pdf/2411.16554v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17434v3","updated":"2024-11-25T16:23:35Z","published":"2023-11-29T08:26:18Z","title":"GSE: Group-wise Sparse and Explainable Adversarial Attacks","summary":"  Sparse adversarial attacks fool deep neural networks (DNNs) through minimal\npixel perturbations, often regularized by the $\\ell_0$ norm. Recent efforts\nhave replaced this norm with a structural sparsity regularizer, such as the\nnuclear group norm, to craft group-wise sparse adversarial attacks. The\nresulting perturbations are thus explainable and hold significant practical\nrelevance, shedding light on an even greater vulnerability of DNNs. However,\ncrafting such attacks poses an optimization challenge, as it involves computing\nnorms for groups of pixels within a non-convex objective. We address this by\npresenting a two-phase algorithm that generates group-wise sparse attacks\nwithin semantically meaningful areas of an image. Initially, we optimize a\nquasinorm adversarial loss using the $1/2-$quasinorm proximal operator tailored\nfor non-convex programming. Subsequently, the algorithm transitions to a\nprojected Nesterov's accelerated gradient descent with $2-$norm regularization\napplied to perturbation magnitudes. Rigorous evaluations on CIFAR-10 and\nImageNet datasets demonstrate a remarkable increase in group-wise sparsity,\ne.g., $50.9\\%$ on CIFAR-10 and $38.4\\%$ on ImageNet (average case, targeted\nattack). This performance improvement is accompanied by significantly faster\ncomputation times, improved explainability, and a $100\\%$ attack success rate.\n","authors":["Shpresim Sadiku","Moritz Wagner","Sebastian Pokutta"],"pdf_url":"https://arxiv.org/pdf/2311.17434v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16537v1","updated":"2024-11-25T16:21:34Z","published":"2024-11-25T16:21:34Z","title":"RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language\n  Models for Robotics","summary":"  Spatial understanding is a crucial capability for robots to make grounded\ndecisions based on their environment. This foundational skill enables robots\nnot only to perceive their surroundings but also to reason about and interact\nmeaningfully within the world. In modern robotics, these capabilities are taken\non by visual language models, and they face significant challenges when applied\nto spatial reasoning context due to their training data sources. These sources\nutilize general-purpose image datasets, and they often lack sophisticated\nspatial scene understanding capabilities. For example, the datasets do not\naddress reference frame comprehension - spatial relationships require clear\ncontextual understanding, whether from an ego-centric, object-centric, or\nworld-centric perspective, which allow for effective real-world interaction. To\naddress this issue, we introduce RoboSpatial, a large-scale spatial\nunderstanding dataset consisting of real indoor and tabletop scenes captured as\n3D scans and egocentric images, annotated with rich spatial information\nrelevant to robotics. The dataset includes 1M images, 5K 3D scans, and 3M\nannotated spatial relationships, with paired 2D egocentric images and 3D scans\nto make it both 2D and 3D ready. Our experiments show that models trained with\nRoboSpatial outperform baselines on downstream tasks such as spatial affordance\nprediction, spatial relationship prediction, and robotics manipulation.\n","authors":["Chan Hee Song","Valts Blukis","Jonathan Tremblay","Stephen Tyree","Yu Su","Stan Birchfield"],"pdf_url":"https://arxiv.org/pdf/2411.16537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16523v1","updated":"2024-11-25T16:10:05Z","published":"2024-11-25T16:10:05Z","title":"LaB-RAG: Label Boosted Retrieval Augmented Generation for Radiology\n  Report Generation","summary":"  In the current paradigm of image captioning, deep learning models are trained\nto generate text from image embeddings of latent features. We challenge the\nassumption that these latent features ought to be high-dimensional vectors\nwhich require model fine tuning to handle. Here we propose Label Boosted\nRetrieval Augmented Generation (LaB-RAG), a text-based approach to image\ncaptioning that leverages image descriptors in the form of categorical labels\nto boost standard retrieval augmented generation (RAG) with pretrained large\nlanguage models (LLMs). We study our method in the context of radiology report\ngeneration (RRG), where the task is to generate a clinician's report detailing\ntheir observations from a set of radiological images, such as X-rays. We argue\nthat simple linear classifiers over extracted image embeddings can effectively\ntransform X-rays into text-space as radiology-specific labels. In combination\nwith standard RAG, we show that these derived text labels can be used with\ngeneral-domain LLMs to generate radiology reports. Without ever training our\ngenerative language model or image feature encoder models, and without ever\ndirectly \"showing\" the LLM an X-ray, we demonstrate that LaB-RAG achieves\nbetter results across natural language and radiology language metrics compared\nwith other retrieval-based RRG methods, while attaining competitive results\ncompared to other fine-tuned vision-language RRG models. We further present\nresults of our experiments with various components of LaB-RAG to better\nunderstand our method. Finally, we critique the use of a popular RRG metric,\narguing it is possible to artificially inflate its results without true\ndata-leakage.\n","authors":["Steven Song","Anirudh Subramanyam","Irene Madejski","Robert L. Grossman"],"pdf_url":"https://arxiv.org/pdf/2411.16523v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14385v2","updated":"2024-11-25T16:07:16Z","published":"2024-11-21T18:21:42Z","title":"Enhancing Diagnostic Precision in Gastric Bleeding through Automated\n  Lesion Segmentation: A Deep DuS-KFCM Approach","summary":"  Timely and precise classification and segmentation of gastric bleeding in\nendoscopic imagery are pivotal for the rapid diagnosis and intervention of\ngastric complications, which is critical in life-saving medical procedures.\nTraditional methods grapple with the challenge posed by the indistinguishable\nintensity values of bleeding tissues adjacent to other gastric structures. Our\nstudy seeks to revolutionize this domain by introducing a novel deep learning\nmodel, the Dual Spatial Kernelized Constrained Fuzzy C-Means (Deep DuS-KFCM)\nclustering algorithm. This Hybrid Neuro-Fuzzy system synergizes Neural Networks\nwith Fuzzy Logic to offer a highly precise and efficient identification of\nbleeding regions. Implementing a two-fold coarse-to-fine strategy for\nsegmentation, this model initially employs the Spatial Kernelized Fuzzy C-Means\n(SKFCM) algorithm enhanced with spatial intensity profiles and subsequently\nharnesses the state-of-the-art DeepLabv3+ with ResNet50 architecture to refine\nthe segmentation output. Through extensive experiments across mainstream\ngastric bleeding and red spots datasets, our Deep DuS-KFCM model demonstrated\nunprecedented accuracy rates of 87.95%, coupled with a specificity of 96.33%,\noutperforming contemporary segmentation methods. The findings underscore the\nmodel's robustness against noise and its outstanding segmentation capabilities,\nparticularly for identifying subtle bleeding symptoms, thereby presenting a\nsignificant leap forward in medical image processing.\n","authors":["Xian-Xian Liu","Mingkun Xu","Yuanyuan Wei","Huafeng Qin","Qun Song","Simon Fong","Feng Tien","Wei Luo","Juntao Gao","Zhihua Zhang","Shirley Siu"],"pdf_url":"https://arxiv.org/pdf/2411.14385v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15472v3","updated":"2024-11-25T16:06:27Z","published":"2024-07-22T08:35:41Z","title":"Learning deep illumination-robust features from multispectral filter\n  array images","summary":"  Multispectral (MS) snapshot cameras equipped with a MS filter array (MSFA),\ncapture multiple spectral bands in a single shot, resulting in a raw mosaic\nimage where each pixel holds only one channel value. The fully-defined MS image\nis estimated from the raw one through \\textit{demosaicing}, which inevitably\nintroduces spatio-spectral artifacts. Moreover, training on fully-defined MS\nimages can be computationally intensive, particularly with deep neural networks\n(DNNs), and may result in features lacking discrimination power due to\nsuboptimal learning of spatio-spectral interactions. Furthermore, outdoor MS\nimage acquisition occurs under varying lighting conditions, leading to\nillumination-dependent features. This paper presents an original approach to\nlearn discriminant and illumination-robust features directly from raw images.\nIt involves: \\textit{raw spectral constancy} to mitigate the impact of\nillumination, \\textit{MSFA-preserving} transformations suited for raw image\naugmentation to train DNNs on diverse raw textures, and \\textit{raw-mixing} to\ncapture discriminant spatio-spectral interactions in raw images. Experiments on\nMS image classification show that our approach outperforms both handcrafted and\nrecent deep learning-based methods, while also requiring significantly less\ncomputational effort. The source code is available at\nhttps://github.com/AnisAmziane/RawTexture.\n","authors":["Anis Amziane"],"pdf_url":"https://arxiv.org/pdf/2407.15472v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16515v1","updated":"2024-11-25T15:57:19Z","published":"2024-11-25T15:57:19Z","title":"PriorPath: Coarse-To-Fine Approach for Controlled De-Novo Pathology\n  Semantic Masks Generation","summary":"  Incorporating artificial intelligence (AI) into digital pathology offers\npromising prospects for automating and enhancing tasks such as image analysis\nand diagnostic processes. However, the diversity of tissue samples and the\nnecessity for meticulous image labeling often result in biased datasets,\nconstraining the applicability of algorithms trained on them. To harness\nsynthetic histopathological images to cope with this challenge, it is essential\nnot only to produce photorealistic images but also to be able to exert control\nover the cellular characteristics they depict. Previous studies used methods to\ngenerate, from random noise, semantic masks that captured the spatial\ndistribution of the tissue. These masks were then used as a prior for\nconditional generative approaches to produce photorealistic histopathological\nimages. However, as with many other generative models, this solution exhibits\nmode collapse as the model fails to capture the full diversity of the\nunderlying data distribution. In this work, we present a pipeline, coined\nPriorPath, that generates detailed, realistic, semantic masks derived from\ncoarse-grained images delineating tissue regions. This approach enables control\nover the spatial arrangement of the generated masks and, consequently, the\nresulting synthetic images. We demonstrated the efficacy of our method across\nthree cancer types, skin, prostate, and lung, showcasing PriorPath's capability\nto cover the semantic mask space and to provide better similarity to real masks\ncompared to previous methods. Our approach allows for specifying desired tissue\ndistributions and obtaining both photorealistic masks and images within a\nsingle platform, thus providing a state-of-the-art, controllable solution for\ngenerating histopathological images to facilitate AI for computational\npathology.\n","authors":["Nati Daniel","May Nathan","Eden Azeroual","Yael Fisher","Yonatan Savir"],"pdf_url":"https://arxiv.org/pdf/2411.16515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16512v1","updated":"2024-11-25T15:55:06Z","published":"2024-11-25T15:55:06Z","title":"Guarding the Gate: ConceptGuard Battles Concept-Level Backdoors in\n  Concept Bottleneck Models","summary":"  The increasing complexity of AI models, especially in deep learning, has\nraised concerns about transparency and accountability, particularly in\nhigh-stakes applications like medical diagnostics, where opaque models can\nundermine trust. Explainable Artificial Intelligence (XAI) aims to address\nthese issues by providing clear, interpretable models. Among XAI techniques,\nConcept Bottleneck Models (CBMs) enhance transparency by using high-level\nsemantic concepts. However, CBMs are vulnerable to concept-level backdoor\nattacks, which inject hidden triggers into these concepts, leading to\nundetectable anomalous behavior. To address this critical security gap, we\nintroduce ConceptGuard, a novel defense framework specifically designed to\nprotect CBMs from concept-level backdoor attacks. ConceptGuard employs a\nmulti-stage approach, including concept clustering based on text distance\nmeasurements and a voting mechanism among classifiers trained on different\nconcept subgroups, to isolate and mitigate potential triggers. Our\ncontributions are threefold: (i) we present ConceptGuard as the first defense\nmechanism tailored for concept-level backdoor attacks in CBMs; (ii) we provide\ntheoretical guarantees that ConceptGuard can effectively defend against such\nattacks within a certain trigger size threshold, ensuring robustness; and (iii)\nwe demonstrate that ConceptGuard maintains the high performance and\ninterpretability of CBMs, crucial for trustworthiness. Through comprehensive\nexperiments and theoretical proofs, we show that ConceptGuard significantly\nenhances the security and trustworthiness of CBMs, paving the way for their\nsecure deployment in critical applications.\n","authors":["Songning Lai","Yu Huang","Jiayu Yang","Gaoxiang Huang","Wenshuo Chen","Yutao Yue"],"pdf_url":"https://arxiv.org/pdf/2411.16512v1.pdf","comment":"17pages, 4 figures"},{"id":"http://arxiv.org/abs/2404.05911v2","updated":"2024-11-25T15:50:52Z","published":"2024-04-09T00:05:45Z","title":"LATUP-Net: A Lightweight 3D Attention U-Net with Parallel Convolutions\n  for Brain Tumor Segmentation","summary":"  Early-stage 3D brain tumor segmentation from magnetic resonance imaging (MRI)\nscans is crucial for prompt and effective treatment. However, this process\nfaces the challenge of precise delineation due to the tumors' complex\nheterogeneity. Moreover, energy sustainability targets and resource\nlimitations, especially in developing countries, require efficient and\naccessible medical imaging solutions. The proposed architecture, a Lightweight\n3D ATtention U-Net with Parallel convolutions, LATUP-Net, addresses these\nissues. It is specifically designed to reduce computational requirements\nsignificantly while maintaining high segmentation performance. By incorporating\nparallel convolutions, it enhances feature representation by capturing\nmulti-scale information. It further integrates an attention mechanism to refine\nsegmentation through selective feature recalibration. LATUP-Net achieves\npromising segmentation performance: the average Dice scores for the whole\ntumor, tumor core, and enhancing tumor on the BraTS 2020 dataset are 88.41%,\n83.82%, and 73.67%, and on the BraTS 2021 dataset, they are 90.29%, 89.54%, and\n83.92%, respectively. Hausdorff distance metrics further indicate its improved\nability to delineate tumor boundaries. With its significantly reduced\ncomputational demand using only 3.07M parameters, about 59 times fewer than\nother state-of-the-art models, and running on a single NVIDIA GeForce RTX3060\n12GB GPU, LATUP-Net requires just 15.79 GFLOPs. This makes it a promising\nsolution for real-world clinical applications, particularly in settings with\nlimited resources. Investigations into the model's interpretability, utilizing\ngradient-weighted class activation mapping and confusion matrices, reveal that\nwhile attention mechanisms enhance the segmentation of small regions, their\nimpact is nuanced. Achieving the most [...]. The code is available at\nhttps://qyber.black/ca/code-bca.\n","authors":["Ebtihal J. Alwadee","Xianfang Sun","Yipeng Qin","Frank C. Langbein"],"pdf_url":"https://arxiv.org/pdf/2404.05911v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16508v1","updated":"2024-11-25T15:44:42Z","published":"2024-11-25T15:44:42Z","title":"All Languages Matter: Evaluating LMMs on Culturally Diverse 100\n  Languages","summary":"  Existing Large Multimodal Models (LMMs) generally focus on only a few regions\nand languages. As LMMs continue to improve, it is increasingly important to\nensure they understand cultural contexts, respect local sensitivities, and\nsupport low-resource languages, all while effectively integrating corresponding\nvisual cues. In pursuit of culturally diverse global multimodal models, our\nproposed All Languages Matter Benchmark (ALM-bench) represents the largest and\nmost comprehensive effort to date for evaluating LMMs across 100 languages.\nALM-bench challenges existing models by testing their ability to understand and\nreason about culturally diverse images paired with text in various languages,\nincluding many low-resource languages traditionally underrepresented in LMM\nresearch. The benchmark offers a robust and nuanced evaluation framework\nfeaturing various question formats, including true/false, multiple choice, and\nopen-ended questions, which are further divided into short and long-answer\ncategories. ALM-bench design ensures a comprehensive assessment of a model's\nability to handle varied levels of difficulty in visual and linguistic\nreasoning. To capture the rich tapestry of global cultures, ALM-bench carefully\ncurates content from 13 distinct cultural aspects, ranging from traditions and\nrituals to famous personalities and celebrations. Through this, ALM-bench not\nonly provides a rigorous testing ground for state-of-the-art open and\nclosed-source LMMs but also highlights the importance of cultural and\nlinguistic inclusivity, encouraging the development of models that can serve\ndiverse global populations effectively. Our benchmark is publicly available.\n","authors":["Ashmal Vayani","Dinura Dissanayake","Hasindri Watawana","Noor Ahsan","Nevasini Sasikumar","Omkar Thawakar","Henok Biadglign Ademtew","Yahya Hmaiti","Amandeep Kumar","Kartik Kuckreja","Mykola Maslych","Wafa Al Ghallabi","Mihail Mihaylov","Chao Qin","Abdelrahman M Shaker","Mike Zhang","Mahardika Krisna Ihsani","Amiel Esplana","Monil Gokani","Shachar Mirkin","Harsh Singh","Ashay Srivastava","Endre Hamerlik","Fathinah Asma Izzati","Fadillah Adamsyah Maani","Sebastian Cavada","Jenny Chim","Rohit Gupta","Sanjay Manjunath","Kamila Zhumakhanova","Feno Heriniaina Rabevohitra","Azril Amirudin","Muhammad Ridzuan","Daniya Kareem","Ketan More","Kunyang Li","Pramesh Shakya","Muhammad Saad","Amirpouya Ghasemaghaei","Amirbek Djanibekov","Dilshod Azizov","Branislava Jankovic","Naman Bhatia","Alvaro Cabrera","Johan Obando-Ceron","Olympiah Otieno","Fabian Farestam","Muztoba Rabbani","Sanoojan Baliah","Santosh Sanjeev","Abduragim Shtanchaev","Maheen Fatima","Thao Nguyen","Amrin Kareem","Toluwani Aremu","Nathan Xavier","Amit Bhatkal","Hawau Toyin","Aman Chadha","Hisham Cholakkal","Rao Muhammad Anwer","Michael Felsberg","Jorma Laaksonen","Thamar Solorio","Monojit Choudhury","Ivan Laptev","Mubarak Shah","Salman Khan","Fahad Khan"],"pdf_url":"https://arxiv.org/pdf/2411.16508v1.pdf","comment":"A Multilingual Multimodal cultural benchmark for 100 languages"},{"id":"http://arxiv.org/abs/2411.16503v1","updated":"2024-11-25T15:40:47Z","published":"2024-11-25T15:40:47Z","title":"Noise Diffusion for Enhancing Semantic Faithfulness in Text-to-Image\n  Synthesis","summary":"  Diffusion models have achieved impressive success in generating\nphotorealistic images, but challenges remain in ensuring precise semantic\nalignment with input prompts. Optimizing the initial noisy latent offers a more\nefficient alternative to modifying model architectures or prompt engineering\nfor improving semantic alignment. A latest approach, InitNo, refines the\ninitial noisy latent by leveraging attention maps; however, these maps capture\nonly limited information, and the effectiveness of InitNo is highly dependent\non the initial starting point, as it tends to converge on a local optimum near\nthis point. To this end, this paper proposes leveraging the language\ncomprehension capabilities of large vision-language models (LVLMs) to guide the\noptimization of the initial noisy latent, and introduces the Noise Diffusion\nprocess, which updates the noisy latent to generate semantically faithful\nimages while preserving distribution consistency. Furthermore, we provide a\ntheoretical analysis of the condition under which the update improves semantic\nfaithfulness. Experimental results demonstrate the effectiveness and\nadaptability of our framework, consistently enhancing semantic alignment across\nvarious diffusion models. The code is available at\nhttps://github.com/Bomingmiao/NoiseDiffusion.\n","authors":["Boming Miao","Chunxiao Li","Xiaoxiao Wang","Andi Zhang","Rui Sun","Zizhe Wang","Yao Zhu"],"pdf_url":"https://arxiv.org/pdf/2411.16503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05591v2","updated":"2024-11-25T15:38:17Z","published":"2023-11-09T18:48:02Z","title":"Multimodal Foundation Models Exploit Text to Make Medical Image\n  Predictions","summary":"  Multimodal foundation models have shown compelling but conflicting\nperformance in medical image interpretation. However, the mechanisms by which\nthese models integrate and prioritize different data modalities, including\nimages and text, remain poorly understood. Here, using a diverse collection of\n1014 multimodal medical cases, we evaluate the unimodal and multimodal image\ninterpretation abilities of proprietary (GPT-4, Gemini Pro 1.0) and open-source\n(Llama-3.2-90B, LLaVA-Med-v1.5) multimodal foundational models with and without\nthe use of text descriptions. Across all models, image predictions were largely\ndriven by exploiting text, with accuracy increasing monotonically with the\namount of informative text. By contrast, human performance on medical image\ninterpretation did not improve with informative text. Exploitation of text is a\ndouble-edged sword; we show that even mild suggestions of an incorrect\ndiagnosis in text diminishes image-based classification, reducing performance\ndramatically in cases the model could previously answer with images alone.\nFinally, we conducted a physician evaluation of model performance on long-form\nmedical cases, finding that the provision of images either reduced or had no\neffect on model performance when text is already highly informative. Our\nresults suggest that multimodal AI models may be useful in medical diagnostic\nreasoning but that their accuracy is largely driven, for better and worse, by\ntheir exploitation of text.\n","authors":["Thomas Buckley","James A. Diao","Pranav Rajpurkar","Adam Rodman","Arjun K. Manrai"],"pdf_url":"https://arxiv.org/pdf/2311.05591v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14215v2","updated":"2024-11-25T15:36:32Z","published":"2024-09-21T18:30:17Z","title":"@Bench: Benchmarking Vision-Language Models for Human-centered Assistive\n  Technology","summary":"  As Vision-Language Models (VLMs) advance, human-centered Assistive\nTechnologies (ATs) for helping People with Visual Impairments (PVIs) are\nevolving into generalists, capable of performing multiple tasks simultaneously.\nHowever, benchmarking VLMs for ATs remains under-explored. To bridge this gap,\nwe first create a novel AT benchmark (@Bench). Guided by a pre-design user\nstudy with PVIs, our benchmark includes the five most crucial vision-language\ntasks: Panoptic Segmentation, Depth Estimation, Optical Character Recognition\n(OCR), Image Captioning, and Visual Question Answering (VQA). Besides, we\npropose a novel AT model (@Model) that addresses all tasks simultaneously and\ncan be expanded to more assistive functions for helping PVIs. Our framework\nexhibits outstanding performance across tasks by integrating multi-modal\ninformation, and it offers PVIs a more comprehensive assistance. Extensive\nexperiments prove the effectiveness and generalizability of our framework.\n","authors":["Xin Jiang","Junwei Zheng","Ruiping Liu","Jiahang Li","Jiaming Zhang","Sven Matthiesen","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2409.14215v2.pdf","comment":"Accepted by WACV 2025, project page:\n  https://junweizheng93.github.io/publications/ATBench/ATBench.html"},{"id":"http://arxiv.org/abs/2411.16498v1","updated":"2024-11-25T15:36:29Z","published":"2024-11-25T15:36:29Z","title":"Multi-Resolution Generative Modeling of Human Motion from Limited Data","summary":"  We present a generative model that learns to synthesize human motion from\nlimited training sequences. Our framework provides conditional generation and\nblending across multiple temporal resolutions. The model adeptly captures human\nmotion patterns by integrating skeletal convolution layers and a multi-scale\narchitecture. Our model contains a set of generative and adversarial networks,\nalong with embedding modules, each tailored for generating motions at specific\nframe rates while exerting control over their content and details. Notably, our\napproach also extends to the synthesis of co-speech gestures, demonstrating its\nability to generate synchronized gestures from speech inputs, even with limited\npaired data. Through direct synthesis of SMPL pose parameters, our approach\navoids test-time adjustments to fit human body meshes. Experimental results\nshowcase our model's ability to achieve extensive coverage of training\nexamples, while generating diverse motions, as indicated by local and global\ndiversity metrics.\n","authors":["David Eduardo Moreno-Villamarín","Anna Hilsmann","Peter Eisert"],"pdf_url":"https://arxiv.org/pdf/2411.16498v1.pdf","comment":"1O pages, 7 figures, published in European Conference on Visual Media\n  Production CVMP 24"},{"id":"http://arxiv.org/abs/2411.16481v1","updated":"2024-11-25T15:21:48Z","published":"2024-11-25T15:21:48Z","title":"Deformable Mamba for Wide Field of View Segmentation","summary":"  Wide-FoV cameras, like fisheye and panoramic setups, are essential for\nbroader perception but introduce significant distortions in 180{\\deg} and\n360{\\deg} images, complicating dense prediction tasks. For instance, existing\nMAMBA models lacking distortion-aware capacity cannot perform well in panoramic\nsemantic segmentation. To address this problem, this work presents Deformable\nMamba, a unified framework specifically designed to address imaging distortions\nwithin the context of panoramic and fisheye semantic segmentation. At the core\nis a decoder constructed with a series of Deformable Mamba Fusion (DMF) blocks,\nmaking the whole framework more deformable, efficient, and accurate, when\nhandling extreme distortions. Extensive evaluations across five datasets\ndemonstrate that our method consistently improves segmentation accuracy\ncompared to the previous state-of-the-art methods tailored for specific FoVs.\nNotably, Deformable Mamba achieves a +2.5% performance improvement on the\n360{\\deg} Stanford2D3D dataset, and shows better results across FoVs from\n60{\\deg} to 360{\\deg}.\n","authors":["Jie Hu","Junwei Zheng","Jiale Wei","Jiaming Zhang","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2411.16481v1.pdf","comment":"Models and code will be made publicly available at:\n  https://github.com/JieHu1996/DeformableMamba"},{"id":"http://arxiv.org/abs/2411.16468v1","updated":"2024-11-25T15:14:36Z","published":"2024-11-25T15:14:36Z","title":"Efficient Video Face Enhancement with Enhanced Spatial-Temporal\n  Consistency","summary":"  As a very common type of video, face videos often appear in movies, talk\nshows, live broadcasts, and other scenes. Real-world online videos are often\nplagued by degradations such as blurring and quantization noise, due to the\nhigh compression ratio caused by high communication costs and limited\ntransmission bandwidth. These degradations have a particularly serious impact\non face videos because the human visual system is highly sensitive to facial\ndetails. Despite the significant advancement in video face enhancement, current\nmethods still suffer from $i)$ long processing time and $ii)$ inconsistent\nspatial-temporal visual effects (e.g., flickering). This study proposes a novel\nand efficient blind video face enhancement method to overcome the above two\nchallenges, restoring high-quality videos from their compressed low-quality\nversions with an effective de-flickering mechanism. In particular, the proposed\nmethod develops upon a 3D-VQGAN backbone associated with spatial-temporal\ncodebooks recording high-quality portrait features and residual-based temporal\ninformation. We develop a two-stage learning framework for the model. In Stage\n\\Rmnum{1}, we learn the model with a regularizer mitigating the codebook\ncollapse problem. In Stage \\Rmnum{2}, we learn two transformers to lookup code\nfrom the codebooks and further update the encoder of low-quality videos.\nExperiments conducted on the VFHQ-Test dataset demonstrate that our method\nsurpasses the current state-of-the-art blind face video restoration and\nde-flickering methods on both efficiency and effectiveness. Code is available\nat \\url{https://github.com/Dixin-Lab/BFVR-STC}.\n","authors":["Yutong Wang","Jiajie Teng","Jiajiong Cao","Yuming Li","Chenguang Ma","Hongteng Xu","Dixin Luo"],"pdf_url":"https://arxiv.org/pdf/2411.16468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16466v1","updated":"2024-11-25T15:13:17Z","published":"2024-11-25T15:13:17Z","title":"No Identity, no problem: Motion through detection for people tracking","summary":"  Tracking-by-detection has become the de facto standard approach to people\ntracking. To increase robustness, some approaches incorporate re-identification\nusing appearance models and regressing motion offset, which requires costly\nidentity annotations. In this paper, we propose exploiting motion clues while\nproviding supervision only for the detections, which is much easier to do. Our\nalgorithm predicts detection heatmaps at two different times, along with a 2D\nmotion estimate between the two images. It then warps one heatmap using the\nmotion estimate and enforces consistency with the other one. This provides the\nrequired supervisory signal on the motion without the need for any motion\nannotations. In this manner, we couple the information obtained from different\nimages during training and increase accuracy, especially in crowded scenes and\nwhen using low frame-rate sequences. We show that our approach delivers\nstate-of-the-art results for single- and multi-view multi-target tracking on\nthe MOT17 and WILDTRACK datasets.\n","authors":["Martin Engilberge","F. Wilke Grosche","Pascal Fua"],"pdf_url":"https://arxiv.org/pdf/2411.16466v1.pdf","comment":"Accepted in TMLR November 2024"},{"id":"http://arxiv.org/abs/2411.10979v3","updated":"2024-11-25T15:12:24Z","published":"2024-11-17T06:23:46Z","title":"VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?","summary":"  The advancement of Multimodal Large Language Models (MLLMs) has enabled\nsignificant progress in multimodal understanding, expanding their capacity to\nanalyze video content. However, existing evaluation benchmarks for MLLMs\nprimarily focus on abstract video comprehension, lacking a detailed assessment\nof their ability to understand video compositions, the nuanced interpretation\nof how visual elements combine and interact within highly compiled video\ncontexts. We introduce VidComposition, a new benchmark specifically designed to\nevaluate the video composition understanding capabilities of MLLMs using\ncarefully curated compiled videos and cinematic-level annotations.\nVidComposition includes 982 videos with 1706 multiple-choice questions,\ncovering various compositional aspects such as camera movement, angle, shot\nsize, narrative structure, character actions and emotions, etc. Our\ncomprehensive evaluation of 33 open-source and proprietary MLLMs reveals a\nsignificant performance gap between human and model capabilities. This\nhighlights the limitations of current MLLMs in understanding complex, compiled\nvideo compositions and offers insights into areas for further improvement. The\nleaderboard and evaluation code are available at\nhttps://yunlong10.github.io/VidComposition/.\n","authors":["Yunlong Tang","Junjia Guo","Hang Hua","Susan Liang","Mingqian Feng","Xinyang Li","Rui Mao","Chao Huang","Jing Bi","Zeliang Zhang","Pooyan Fazli","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2411.10979v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16446v1","updated":"2024-11-25T14:51:22Z","published":"2024-11-25T14:51:22Z","title":"VQ-SGen: A Vector Quantized Stroke Representation for Sketch Generation","summary":"  This paper presents VQ-SGen, a novel algorithm for high-quality sketch\ngeneration. Recent approaches have often framed the task as pixel-based\ngeneration either as a whole or part-by-part, neglecting the intrinsic and\ncontextual relationships among individual strokes, such as the shape and\nspatial positioning of both proximal and distant strokes. To overcome these\nlimitations, we propose treating each stroke within a sketch as an entity and\nintroducing a vector-quantized (VQ) stroke representation for fine-grained\nsketch generation. Our method follows a two-stage framework - in the first\nstage, we decouple each stroke's shape and location information to ensure the\nVQ representation prioritizes stroke shape learning. In the second stage, we\nfeed the precise and compact representation into an auto-decoding Transformer\nto incorporate stroke semantics, positions, and shapes into the generation\nprocess. By utilizing tokenized stroke representation, our approach generates\nstrokes with high fidelity and facilitates novel applications, such as\nconditional generation and semantic-aware stroke editing. Comprehensive\nexperiments demonstrate our method surpasses existing state-of-the-art\ntechniques, underscoring its effectiveness. The code and model will be made\npublicly available upon publication.\n","authors":["Jiawei Wang","Zhiming Cui","Changjian Li"],"pdf_url":"https://arxiv.org/pdf/2411.16446v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16443v1","updated":"2024-11-25T14:46:17Z","published":"2024-11-25T14:46:17Z","title":"SplatFlow: Multi-View Rectified Flow Model for 3D Gaussian Splatting\n  Synthesis","summary":"  Text-based generation and editing of 3D scenes hold significant potential for\nstreamlining content creation through intuitive user interactions. While recent\nadvances leverage 3D Gaussian Splatting (3DGS) for high-fidelity and real-time\nrendering, existing methods are often specialized and task-focused, lacking a\nunified framework for both generation and editing. In this paper, we introduce\nSplatFlow, a comprehensive framework that addresses this gap by enabling direct\n3DGS generation and editing. SplatFlow comprises two main components: a\nmulti-view rectified flow (RF) model and a Gaussian Splatting Decoder\n(GSDecoder). The multi-view RF model operates in latent space, generating\nmulti-view images, depths, and camera poses simultaneously, conditioned on text\nprompts, thus addressing challenges like diverse scene scales and complex\ncamera trajectories in real-world settings. Then, the GSDecoder efficiently\ntranslates these latent outputs into 3DGS representations through a\nfeed-forward 3DGS method. Leveraging training-free inversion and inpainting\ntechniques, SplatFlow enables seamless 3DGS editing and supports a broad range\nof 3D tasks-including object editing, novel view synthesis, and camera pose\nestimation-within a unified framework without requiring additional complex\npipelines. We validate SplatFlow's capabilities on the MVImgNet and DL3DV-7K\ndatasets, demonstrating its versatility and effectiveness in various 3D\ngeneration, editing, and inpainting-based tasks.\n","authors":["Hyojun Go","Byeongjun Park","Jiho Jang","Jin-Young Kim","Soonwoo Kwon","Changick Kim"],"pdf_url":"https://arxiv.org/pdf/2411.16443v1.pdf","comment":"Project Page: https://gohyojun15.github.io/SplatFlow/"},{"id":"http://arxiv.org/abs/2411.16440v1","updated":"2024-11-25T14:43:03Z","published":"2024-11-25T14:43:03Z","title":"AnonyNoise: Anonymizing Event Data with Smart Noise to Outsmart\n  Re-Identification and Preserve Privacy","summary":"  The increasing capabilities of deep neural networks for re-identification,\ncombined with the rise in public surveillance in recent years, pose a\nsubstantial threat to individual privacy. Event cameras were initially\nconsidered as a promising solution since their output is sparse and therefore\ndifficult for humans to interpret. However, recent advances in deep learning\nproof that neural networks are able to reconstruct high-quality grayscale\nimages and re-identify individuals using data from event cameras. In our paper,\nwe contribute a crucial ethical discussion on data privacy and present the\nfirst event anonymization pipeline to prevent re-identification not only by\nhumans but also by neural networks. Our method effectively introduces learnable\ndata-dependent noise to cover personally identifiable information in raw event\ndata, reducing attackers' re-identification capabilities by up to 60%, while\nmaintaining substantial information for the performing of downstream tasks.\nMoreover, our anonymization generalizes well on unseen data and is robust\nagainst image reconstruction and inversion attacks. Code:\nhttps://github.com/dfki-av/AnonyNoise\n","authors":["Katharina Bendig","René Schuster","Nicole Thiemer","Karen Joisten","Didier Stricker"],"pdf_url":"https://arxiv.org/pdf/2411.16440v1.pdf","comment":"Accepted at WACV25"},{"id":"http://arxiv.org/abs/2411.16438v1","updated":"2024-11-25T14:39:52Z","published":"2024-11-25T14:39:52Z","title":"Harnessing Superclasses for Learning from Hierarchical Databases","summary":"  In many large-scale classification problems, classes are organized in a known\nhierarchy, typically represented as a tree expressing the inclusion of classes\nin superclasses. We introduce a loss for this type of supervised hierarchical\nclassification. It utilizes the knowledge of the hierarchy to assign each\nexample not only to a class but also to all encompassing superclasses.\nApplicable to any feedforward architecture with a softmax output layer, this\nloss is a proper scoring rule, in that its expectation is minimized by the true\nposterior class probabilities. This property allows us to simultaneously pursue\nconsistent classification objectives between superclasses and fine-grained\nclasses, and eliminates the need for a performance trade-off between different\ngranularities. We conduct an experimental study on three reference benchmarks,\nin which we vary the size of the training sets to cover a diverse set of\nlearning scenarios. Our approach does not entail any significant additional\ncomputational cost compared with the loss of cross-entropy. It improves\naccuracy and reduces the number of coarse errors, with predicted labels that\nare distant from ground-truth labels in the tree.\n","authors":["Nicolas Urbani","Sylvain Rousseau","Yves Grandvalet","Leonardo Tanzi"],"pdf_url":"https://arxiv.org/pdf/2411.16438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16437v1","updated":"2024-11-25T14:39:18Z","published":"2024-11-25T14:39:18Z","title":"Privacy Protection in Personalized Diffusion Models via Targeted\n  Cross-Attention Adversarial Attack","summary":"  The growing demand for customized visual content has led to the rise of\npersonalized text-to-image (T2I) diffusion models. Despite their remarkable\npotential, they pose significant privacy risk when misused for malicious\npurposes. In this paper, we propose a novel and efficient adversarial attack\nmethod, Concept Protection by Selective Attention Manipulation (CoPSAM) which\ntargets only the cross-attention layers of a T2I diffusion model. For this\npurpose, we carefully construct an imperceptible noise to be added to clean\nsamples to get their adversarial counterparts. This is obtained during the\nfine-tuning process by maximizing the discrepancy between the corresponding\ncross-attention maps of the user-specific token and the class-specific token,\nrespectively. Experimental validation on a subset of CelebA-HQ face images\ndataset demonstrates that our approach outperforms existing methods. Besides\nthis, our method presents two important advantages derived from the qualitative\nevaluation: (i) we obtain better protection results for lower noise levels than\nour competitors; and (ii) we protect the content from unauthorized use thereby\nprotecting the individual's identity from potential misuse.\n","authors":["Xide Xu","Muhammad Atif Butt","Sandesh Kamath","Bogdan Raducanu"],"pdf_url":"https://arxiv.org/pdf/2411.16437v1.pdf","comment":"Accepted at Safe Generative AI Workshop (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2411.16425v1","updated":"2024-11-25T14:27:55Z","published":"2024-11-25T14:27:55Z","title":"TopV-Nav: Unlocking the Top-View Spatial Reasoning Potential of MLLM for\n  Zero-shot Object Navigation","summary":"  The Zero-Shot Object Navigation (ZSON) task requires embodied agents to find\na previously unseen object by navigating in unfamiliar environments. Such a\ngoal-oriented exploration heavily relies on the ability to perceive,\nunderstand, and reason based on the spatial information of the environment.\nHowever, current LLM-based approaches convert visual observations to language\ndescriptions and reason in the linguistic space, leading to the loss of spatial\ninformation. In this paper, we introduce TopV-Nav, a MLLM-based method that\ndirectly reasons on the top-view map with complete spatial information. To\nfully unlock the MLLM's spatial reasoning potential in top-view perspective, we\npropose the Adaptive Visual Prompt Generation (AVPG) method to adaptively\nconstruct semantically-rich top-view map. It enables the agent to directly\nutilize spatial information contained in the top-view map to conduct thorough\nreasoning. Besides, we design a Dynamic Map Scaling (DMS) mechanism to\ndynamically zoom top-view map at preferred scales, enhancing local fine-grained\nreasoning. Additionally, we devise a Target-Guided Navigation (TGN) mechanism\nto predict and to utilize target locations, facilitating global and human-like\nexploration. Experiments on MP3D and HM3D benchmarks demonstrate the\nsuperiority of our TopV-Nav, e.g., $+3.9\\%$ SR and $+2.0\\%$ SPL absolute\nimprovements on HM3D.\n","authors":["Linqing Zhong","Chen Gao","Zihan Ding","Yue Liao","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2411.16425v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2411.16421v1","updated":"2024-11-25T14:25:39Z","published":"2024-11-25T14:25:39Z","title":"Machine Learning for the Digital Typhoon Dataset: Extensions to Multiple\n  Basins and New Developments in Representations and Tasks","summary":"  This paper presents the Digital Typhoon Dataset V2, a new version of the\nlongest typhoon satellite image dataset for 40+ years aimed at benchmarking\nmachine learning models for long-term spatio-temporal data. The new addition in\nDataset V2 is tropical cyclone data from the southern hemisphere, in addition\nto the northern hemisphere data in Dataset V1. Having data from two hemispheres\nallows us to ask new research questions about regional differences across\nbasins and hemispheres. We also discuss new developments in representations and\ntasks of the dataset. We first introduce a self-supervised learning framework\nfor representation learning. Combined with the LSTM model, we discuss\nperformance on intensity forecasting and extra-tropical transition forecasting\ntasks. We then propose new tasks, such as the typhoon center estimation task.\nWe show that an object detection-based model performs better for stronger\ntyphoons. Finally, we study how machine learning models can generalize across\nbasins and hemispheres, by training the model on the northern hemisphere data\nand testing it on the southern hemisphere data. The dataset is publicly\navailable at \\url{http://agora.ex.nii.ac.jp/digital-typhoon/dataset/} and\n\\url{https://github.com/kitamoto-lab/digital-typhoon/}.\n","authors":["Asanobu Kitamoto","Erwan Dzik","Gaspar Faure"],"pdf_url":"https://arxiv.org/pdf/2411.16421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17818v2","updated":"2024-11-25T14:24:39Z","published":"2024-05-28T04:29:23Z","title":"Hyperspectral and multispectral image fusion with arbitrary resolution\n  through self-supervised representations","summary":"  The fusion of a low-resolution hyperspectral image (LR-HSI) with a\nhigh-resolution multispectral image (HR-MSI) has emerged as an effective\ntechnique for achieving HSI super-resolution (SR). Previous studies have mainly\nconcentrated on estimating the posterior distribution of the latent\nhigh-resolution hyperspectral image (HR-HSI), leveraging an appropriate image\nprior and likelihood computed from the discrepancy between the latent HSI and\nobserved images. Low rankness stands out for preserving latent HSI\ncharacteristics through matrix factorization among the various priors. However,\nthe primary limitation in previous studies lies in the generalization of a\nfusion model with fixed resolution scales, which necessitates retraining\nwhenever output resolutions are changed. To overcome this limitation, we\npropose a novel continuous low-rank factorization (CLoRF) by integrating two\nneural representations into the matrix factorization, capturing spatial and\nspectral information, respectively. This approach enables us to harness both\nthe low rankness from the matrix factorization and the continuity from neural\nrepresentation in a self-supervised manner.Theoretically, we prove the low-rank\nproperty and Lipschitz continuity in the proposed continuous low-rank\nfactorization. Experimentally, our method significantly surpasses existing\ntechniques and achieves user-desired resolutions without the need for neural\nnetwork retraining. Code is available at\nhttps://github.com/wangting1907/CLoRF-Fusion.\n","authors":["Ting Wang","Zipei Yan","Jizhou Li","Xile Zhao","Chao Wang","Michael Ng"],"pdf_url":"https://arxiv.org/pdf/2405.17818v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16417v1","updated":"2024-11-25T14:20:53Z","published":"2024-11-25T14:20:53Z","title":"Comparison of Generative Learning Methods for Turbulence Modeling","summary":"  Numerical simulations of turbulent flows present significant challenges in\nfluid dynamics due to their complexity and high computational cost. High\nresolution techniques such as Direct Numerical Simulation (DNS) and Large Eddy\nSimulation (LES) are generally not computationally affordable, particularly for\ntechnologically relevant problems. Recent advances in machine learning,\nspecifically in generative probabilistic models, offer promising alternatives\nfor turbulence modeling. This paper investigates the application of three\ngenerative models - Variational Autoencoders (VAE), Deep Convolutional\nGenerative Adversarial Networks (DCGAN), and Denoising Diffusion Probabilistic\nModels (DDPM) - in simulating a 2D K\\'arm\\'an vortex street around a fixed\ncylinder. Training data was obtained by means of LES. We evaluate each model's\nability to capture the statistical properties and spatial structures of the\nturbulent flow. Our results demonstrate that DDPM and DCGAN effectively\nreplicate the flow distribution, highlighting their potential as efficient and\naccurate tools for turbulence modeling. We find a strong argument for DCGAN, as\nalthough they are more difficult to train (due to problems such as mode\ncollapse), they gave the fastest inference and training time, require less data\nto train compared to VAE and DDPM, and provide the results most closely aligned\nwith the input stream. In contrast, VAE train quickly (and can generate samples\nquickly) but do not produce adequate results, and DDPM, whilst effective, is\nsignificantly slower at both inference and training time.\n","authors":["Claudia Drygala","Edmund Ross","Francesca di Mare","Hanno Gottschalk"],"pdf_url":"https://arxiv.org/pdf/2411.16417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16408v1","updated":"2024-11-25T14:14:25Z","published":"2024-11-25T14:14:25Z","title":"Low-Data Classification of Historical Music Manuscripts: A Few-Shot\n  Learning Approach","summary":"  In this paper, we explore the intersection of technology and cultural\npreservation by developing a self-supervised learning framework for the\nclassification of musical symbols in historical manuscripts. Optical Music\nRecognition (OMR) plays a vital role in digitising and preserving musical\nheritage, but historical documents often lack the labelled data required by\ntraditional methods. We overcome this challenge by training a neural-based\nfeature extractor on unlabelled data, enabling effective classification with\nminimal samples. Key contributions include optimising crop preprocessing for a\nself-supervised Convolutional Neural Network and evaluating classification\nmethods, including SVM, multilayer perceptrons, and prototypical networks. Our\nexperiments yield an accuracy of 87.66\\%, showcasing the potential of AI-driven\nmethods to ensure the survival of historical music for future generations\nthrough advanced digital archiving techniques.\n","authors":["Elona Shatri","Daniel Raymond","George Fazekas"],"pdf_url":"https://arxiv.org/pdf/2411.16408v1.pdf","comment":"6 pages, The Sixth IEEE international conference on Image Processing\n  Applications and Systems"},{"id":"http://arxiv.org/abs/2411.16407v1","updated":"2024-11-25T14:12:24Z","published":"2024-11-25T14:12:24Z","title":"A Study on Unsupervised Domain Adaptation for Semantic Segmentation in\n  the Era of Vision-Language Models","summary":"  Despite the recent progress in deep learning based computer vision, domain\nshifts are still one of the major challenges. Semantic segmentation for\nautonomous driving faces a wide range of domain shifts, e.g. caused by changing\nweather conditions, new geolocations and the frequent use of synthetic data in\nmodel training. Unsupervised domain adaptation (UDA) methods have emerged which\nadapt a model to a new target domain by only using unlabeled data of that\ndomain. The variety of UDA methods is large but all of them use ImageNet\npre-trained models. Recently, vision-language models have demonstrated strong\ngeneralization capabilities which may facilitate domain adaptation. We show\nthat simply replacing the encoder of existing UDA methods like DACS by a\nvision-language pre-trained encoder can result in significant performance\nimprovements of up to 10.0% mIoU on the GTA5-to-Cityscapes domain shift. For\nthe generalization performance to unseen domains, the newly employed\nvision-language pre-trained encoder provides a gain of up to 13.7% mIoU across\nthree unseen datasets. However, we find that not all UDA methods can be easily\npaired with the new encoder and that the UDA performance does not always\nlikewise transfer into generalization performance. Finally, we perform our\nexperiments on an adverse weather condition domain shift to further verify our\nfindings on a pure real-to-real domain shift.\n","authors":["Manuel Schwonberg","Claus Werner","Hanno Gottschalk","Carsten Meyer"],"pdf_url":"https://arxiv.org/pdf/2411.16407v1.pdf","comment":"Accepted to British Machine Vision Conference (BMVC) 2024: Workshop\n  on Robust Recognition in the Open World (RROW)"},{"id":"http://arxiv.org/abs/2411.16405v1","updated":"2024-11-25T14:10:43Z","published":"2024-11-25T14:10:43Z","title":"Synthesising Handwritten Music with GANs: A Comprehensive Evaluation of\n  CycleWGAN, ProGAN, and DCGAN","summary":"  The generation of handwritten music sheets is a crucial step toward enhancing\nOptical Music Recognition (OMR) systems, which rely on large and diverse\ndatasets for optimal performance. However, handwritten music sheets, often\nfound in archives, present challenges for digitisation due to their fragility,\nvaried handwriting styles, and image quality. This paper addresses the data\nscarcity problem by applying Generative Adversarial Networks (GANs) to\nsynthesise realistic handwritten music sheets. We provide a comprehensive\nevaluation of three GAN models - DCGAN, ProGAN, and CycleWGAN - comparing their\nability to generate diverse and high-quality handwritten music images. The\nproposed CycleWGAN model, which enhances style transfer and training stability,\nsignificantly outperforms DCGAN and ProGAN in both qualitative and quantitative\nevaluations. CycleWGAN achieves superior performance, with an FID score of\n41.87, an IS of 2.29, and a KID of 0.05, making it a promising solution for\nimproving OMR systems.\n","authors":["Elona Shatri","Kalikidhar Palavala","George Fazekas"],"pdf_url":"https://arxiv.org/pdf/2411.16405v1.pdf","comment":"10 pages, one page references, to appear on the IEEE Big Data 2024\n  2nd Workshop on AI Music Generation (AIMG 2024)"},{"id":"http://arxiv.org/abs/2411.16392v1","updated":"2024-11-25T13:55:00Z","published":"2024-11-25T13:55:00Z","title":"Quadratic Gaussian Splatting for Efficient and Detailed Surface\n  Reconstruction","summary":"  Recently, 3D Gaussian Splatting (3DGS) has attracted attention for its\nsuperior rendering quality and speed over Neural Radiance Fields (NeRF). To\naddress 3DGS's limitations in surface representation, 2D Gaussian Splatting\n(2DGS) introduced disks as scene primitives to model and reconstruct geometries\nfrom multi-view images, offering view-consistent geometry. However, the disk's\nfirst-order linear approximation often leads to over-smoothed results. We\npropose Quadratic Gaussian Splatting (QGS), a novel method that replaces disks\nwith quadric surfaces, enhancing geometric fitting, whose code will be\nopen-sourced. QGS defines Gaussian distributions in non-Euclidean space,\nallowing primitives to capture more complex textures. As a second-order surface\napproximation, QGS also renders spatial curvature to guide the normal\nconsistency term, to effectively reduce over-smoothing. Moreover, QGS is a\ngeneralized version of 2DGS that achieves more accurate and detailed\nreconstructions, as verified by experiments on DTU and TNT, demonstrating its\neffectiveness in surpassing current state-of-the-art methods in geometry\nreconstruction. Our code willbe released as open source.\n","authors":["Ziyu Zhang","Binbin Huang","Hanqing Jiang","Liyang Zhou","Xiaojun Xiang","Shunhan Shen"],"pdf_url":"https://arxiv.org/pdf/2411.16392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16380v1","updated":"2024-11-25T13:40:11Z","published":"2024-11-25T13:40:11Z","title":"Privacy-Preserving Federated Foundation Model for Generalist Ultrasound\n  Artificial Intelligence","summary":"  Ultrasound imaging is widely used in clinical diagnosis due to its\nnon-invasive nature and real-time capabilities. However, conventional\nultrasound diagnostics face several limitations, including high dependence on\nphysician expertise and suboptimal image quality, which complicates\ninterpretation and increases the likelihood of diagnostic errors. Artificial\nintelligence (AI) has emerged as a promising solution to enhance clinical\ndiagnosis, particularly in detecting abnormalities across various biomedical\nimaging modalities. Nonetheless, current AI models for ultrasound imaging face\ncritical challenges. First, these models often require large volumes of labeled\nmedical data, raising concerns over patient privacy breaches. Second, most\nexisting models are task-specific, which restricts their broader clinical\nutility. To overcome these challenges, we present UltraFedFM, an innovative\nprivacy-preserving ultrasound foundation model. UltraFedFM is collaboratively\npre-trained using federated learning across 16 distributed medical institutions\nin 9 countries, leveraging a dataset of over 1 million ultrasound images\ncovering 19 organs and 10 ultrasound modalities. This extensive and diverse\ndata, combined with a secure training framework, enables UltraFedFM to exhibit\nstrong generalization and diagnostic capabilities. It achieves an average area\nunder the receiver operating characteristic curve of 0.927 for disease\ndiagnosis and a dice similarity coefficient of 0.878 for lesion segmentation.\nNotably, UltraFedFM surpasses the diagnostic accuracy of mid-level\nultrasonographers and matches the performance of expert-level sonographers in\nthe joint diagnosis of 8 common systemic diseases. These findings indicate that\nUltraFedFM can significantly enhance clinical diagnostics while safeguarding\npatient privacy, marking an advancement in AI-driven ultrasound imaging for\nfuture clinical applications.\n","authors":["Yuncheng Jiang","Chun-Mei Feng","Jinke Ren","Jun Wei","Zixun Zhang","Yiwen Hu","Yunbi Liu","Rui Sun","Xuemei Tang","Juan Du","Xiang Wan","Yong Xu","Bo Du","Xin Gao","Guangyu Wang","Shaohua Zhou","Shuguang Cui","Rick Siow Mong Goh","Yong Liu","Zhen Li"],"pdf_url":"https://arxiv.org/pdf/2411.16380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13615v2","updated":"2024-11-25T13:33:49Z","published":"2024-11-20T08:09:35Z","title":"A Deep Learning Approach to Predict the Fall [of Price] of\n  Cryptocurrency Long Before its Actual Fall","summary":"  In modern times, the cryptocurrency market is one of the world's most rapidly\nrising financial markets. The cryptocurrency market is regarded to be more\nvolatile and illiquid than traditional markets such as equities, foreign\nexchange, and commodities. The risk of this market creates an uncertain\ncondition among the investors. The purpose of this research is to predict the\nmagnitude of the risk factor of the cryptocurrency market. Risk factor is also\ncalled volatility. Our approach will assist people who invest in the\ncryptocurrency market by overcoming the problems and difficulties they\nexperience. Our approach starts with calculating the risk factor of the\ncryptocurrency market from the existing parameters. In twenty elements of the\ncryptocurrency market, the risk factor has been predicted using different\nmachine learning algorithms such as CNN, LSTM, BiLSTM, and GRU. All of the\nmodels have been applied to the calculated risk factor parameter. A new model\nhas been developed to predict better than the existing models. Our proposed\nmodel gives the highest RMSE value of 1.3229 and the lowest RMSE value of\n0.0089. Following our model, it will be easier for investors to trade in\ncomplicated and challenging financial assets like bitcoin, Ethereum, dogecoin,\netc. Where the other existing models, the highest RMSE was 14.5092, and the\nlower was 0.02769. So, the proposed model performs much better than models with\nproper generalization. Using our approach, it will be easier for investors to\ntrade in complicated and challenging financial assets like Bitcoin, Ethereum,\nand Dogecoin.\n","authors":["Anika Tahsin Meem"],"pdf_url":"https://arxiv.org/pdf/2411.13615v2.pdf","comment":"22 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.16375v1","updated":"2024-11-25T13:33:41Z","published":"2024-11-25T13:33:41Z","title":"Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing","summary":"  With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM\n","authors":["Kaifeng Gao","Jiaxin Shi","Hanwang Zhang","Chunping Wang","Jun Xiao","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2411.16375v1.pdf","comment":"Technical Report. Code is available at\n  https://github.com/Dawn-LX/CausalCache-VDM"},{"id":"http://arxiv.org/abs/2409.17682v2","updated":"2024-11-25T13:31:05Z","published":"2024-09-26T09:48:24Z","title":"Dark Miner: Defend against undesired generation for text-to-image\n  diffusion models","summary":"  Text-to-image diffusion models have been demonstrated with undesired\ngeneration due to unfiltered large-scale training data, such as sexual images\nand copyrights, necessitating the erasure of undesired concepts. Most existing\nmethods focus on modifying the generation probabilities conditioned on the\ntexts containing target concepts. However, they fail to guarantee the desired\ngeneration of texts unseen in the training phase, especially for the\nadversarial texts from malicious attacks. In this paper, we analyze the erasure\ntask and point out that existing methods cannot guarantee the minimization of\nthe total probabilities of undesired generation. To tackle this problem, we\npropose Dark Miner. It entails a recurring three-stage process that comprises\nmining, verifying, and circumventing. This method greedily mines embeddings\nwith maximum generation probabilities of target concepts and more effectively\nreduces their generation. In the experiments, we evaluate its performance on\nthe inappropriateness, object, and style concepts. Compared with the previous\nmethods, our method achieves better erasure and defense results, especially\nunder multiple adversarial attacks, while preserving the native generation\ncapability of the models. Our code will be available at\nhttps://github.com/RichardSunnyMeng/DarkMiner-offical-codes.\n","authors":["Zheling Meng","Bo Peng","Xiaochuan Jin","Yue Jiang","Jing Dong","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2409.17682v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21964v2","updated":"2024-11-25T13:26:30Z","published":"2024-10-29T11:36:49Z","title":"FakeFormer: Efficient Vulnerability-Driven Transformers for\n  Generalisable Deepfake Detection","summary":"  Recently, Vision Transformers (ViTs) have achieved unprecedented\neffectiveness in the general domain of image classification. Nonetheless, these\nmodels remain underexplored in the field of deepfake detection, given their\nlower performance as compared to Convolution Neural Networks (CNNs) in that\nspecific context. In this paper, we start by investigating why plain ViT\narchitectures exhibit a suboptimal performance when dealing with the detection\nof facial forgeries. Our analysis reveals that, as compared to CNNs, ViT\nstruggles to model localized forgery artifacts that typically characterize\ndeepfakes. Based on this observation, we propose a deepfake detection framework\ncalled FakeFormer, which extends ViTs to enforce the extraction of subtle\ninconsistency-prone information. For that purpose, an explicit attention\nlearning guided by artifact-vulnerable patches and tailored to ViTs is\nintroduced. Extensive experiments are conducted on diverse well-known datasets,\nincluding FF++, Celeb-DF, WildDeepfake, DFD, DFDCP, and DFDC. The results show\nthat FakeFormer outperforms the state-of-the-art in terms of generalization and\ncomputational cost, without the need for large-scale training datasets. The\ncode is available at \\url{https://github.com/10Ring/FakeFormer}.\n","authors":["Dat Nguyen","Marcella Astrid","Enjie Ghorbel","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2410.21964v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16370v1","updated":"2024-11-25T13:26:09Z","published":"2024-11-25T13:26:09Z","title":"A Review of Bayesian Uncertainty Quantification in Deep Probabilistic\n  Image Segmentation","summary":"  Advancements in image segmentation play an integral role within the greater\nscope of Deep Learning-based computer vision. Furthermore, their widespread\napplicability in critical real-world tasks has given rise to challenges related\nto the reliability of such algorithms. Hence, uncertainty quantification has\nbeen extensively studied within this context, enabling expression of model\nignorance (epistemic uncertainty) or data ambiguity (aleatoric uncertainty) to\nprevent uninformed decision making. Due to the rapid adoption of Convolutional\nNeural Network (CNN)-based segmentation models in high-stake applications, a\nsubstantial body of research has been published on this very topic, causing its\nswift expansion into a distinct field. This work provides a comprehensive\noverview of probabilistic segmentation by discussing fundamental concepts in\nuncertainty that govern advancements in the field as well as the application to\nvarious tasks. We identify that quantifying aleatoric and epistemic uncertainty\napproximates Bayesian inference w.r.t. to either latent variables or model\nparameters, respectively. Moreover, literature on both uncertainties trace back\nto four key applications; (1) to quantify statistical inconsistencies in the\nannotation process due ambiguous images, (2) correlating prediction error with\nuncertainty, (3) expanding the model hypothesis space for better\ngeneralization, and (4) active learning. Then, a discussion follows that\nincludes an overview of utilized datasets for each of the applications and\ncomparison of the available methods. We also highlight challenges related to\narchitectures, uncertainty-based active learning, standardization and\nbenchmarking, and recommendations for future work such as methods based on\nsingle forward passes and models that appropriately leverage volumetric data.\n","authors":["M. M. A. Valiuddin","R. J. G. van Sloun","C. G. A. Viviers","P. H. N. de With","F. van der Sommen"],"pdf_url":"https://arxiv.org/pdf/2411.16370v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2411.03795v3","updated":"2024-11-25T13:19:16Z","published":"2024-11-06T09:39:52Z","title":"VQA$^2$: Visual Question Answering for Video Quality Assessment","summary":"  The advent and proliferation of large multi-modal models (LMMs) have\nintroduced new paradigms to computer vision, transforming various tasks into a\nunified visual question answering framework. Video Quality Assessment (VQA), a\nclassic field in low-level visual perception, focused initially on quantitative\nvideo quality scoring. However, driven by advances in LMMs, it is now\nprogressing toward more holistic visual quality understanding tasks. Recent\nstudies in the image domain have demonstrated that Visual Question Answering\n(VQA) can markedly enhance low-level visual quality evaluation. Nevertheless,\nrelated work has not been explored in the video domain, leaving substantial\nroom for improvement. To address this gap, we introduce the VQA2 Instruction\nDataset - the first visual question answering instruction dataset that focuses\non video quality assessment. This dataset consists of 3 subsets and covers\nvarious video types, containing 157,755 instruction question-answer pairs.\nThen, leveraging this foundation, we present the VQA2 series models. The VQA2\nseries models interleave visual and motion tokens to enhance the perception of\nspatial-temporal quality details in videos. We conduct extensive experiments on\nvideo quality scoring and understanding tasks, and results demonstrate that the\nVQA2series models achieve excellent performance in both tasks. Notably, our\nfinal model, the VQA2-Assistant, exceeds the renowned GPT-4o in visual quality\nunderstanding tasks while maintaining strong competitiveness in quality scoring\ntasks. Our work provides a foundation and feasible approach for integrating\nlow-level video quality assessment and understanding with LMMs.\n","authors":["Ziheng Jia","Zicheng Zhang","Jiaying Qian","Haoning Wu","Wei Sun","Chunyi Li","Xiaohong Liu","Weisi Lin","Guangtao Zhai","Xiongkuo Min"],"pdf_url":"https://arxiv.org/pdf/2411.03795v3.pdf","comment":"23 pages 12 figures"},{"id":"http://arxiv.org/abs/2407.19474v2","updated":"2024-11-25T12:53:44Z","published":"2024-07-28T11:56:03Z","title":"Visual Riddles: a Commonsense and World Knowledge Challenge for Large\n  Vision and Language Models","summary":"  Imagine observing someone scratching their arm; to understand why, additional\ncontext would be necessary. However, spotting a mosquito nearby would\nimmediately offer a likely explanation for the person's discomfort, thereby\nalleviating the need for further information. This example illustrates how\nsubtle visual cues can challenge our cognitive skills and demonstrates the\ncomplexity of interpreting visual scenarios. To study these skills, we present\nVisual Riddles, a benchmark aimed to test vision and language models on visual\nriddles requiring commonsense and world knowledge. The benchmark comprises 400\nvisual riddles, each featuring a unique image created by a variety of\ntext-to-image models, question, ground-truth answer, textual hint, and\nattribution. Human evaluation reveals that existing models lag significantly\nbehind human performance, which is at 82% accuracy, with Gemini-Pro-1.5 leading\nwith 40% accuracy. Our benchmark comes with automatic evaluation tasks to make\nassessment scalable. These findings underscore the potential of Visual Riddles\nas a valuable resource for enhancing vision and language models' capabilities\nin interpreting complex visual scenarios.\n","authors":["Nitzan Bitton-Guetta","Aviv Slobodkin","Aviya Maimon","Eliya Habba","Royi Rassin","Yonatan Bitton","Idan Szpektor","Amir Globerson","Yuval Elovici"],"pdf_url":"https://arxiv.org/pdf/2407.19474v2.pdf","comment":"https://visual-riddles.github.io/"},{"id":"http://arxiv.org/abs/2406.02037v2","updated":"2024-11-25T12:49:46Z","published":"2024-06-04T07:23:09Z","title":"Multi-Scale Direction-Aware Network for Infrared Small Target Detection","summary":"  Infrared small target detection faces the problem that it is difficult to\neffectively separate the background and the target. Existing deep\nlearning-based methods focus on appearance features and ignore high-frequency\ndirectional features. Therefore, we propose a multi-scale direction-aware\nnetwork (MSDA-Net), which is the first attempt to integrate the high-frequency\ndirectional features of infrared small targets as domain prior knowledge into\nneural networks. Specifically, an innovative multi-directional feature\nawareness (MDFA) module is constructed, which fully utilizes the prior\nknowledge of targets and emphasizes the focus on high-frequency directional\nfeatures. On this basis, combined with the multi-scale local relation learning\n(MLRL) module, a multi-scale direction-aware (MSDA) module is further\nconstructed. The MSDA module promotes the full extraction of local relations at\ndifferent scales and the full perception of key features in different\ndirections. Meanwhile, a high-frequency direction injection (HFDI) module\nwithout training parameters is constructed to inject the high-frequency\ndirectional information of the original image into the network. This helps\nguide the network to pay attention to detailed information such as target edges\nand shapes. In addition, we propose a feature aggregation (FA) structure that\naggregates multi-level features to solve the problem of small targets\ndisappearing in deep feature maps. Furthermore, a lightweight feature alignment\nfusion (FAF) module is constructed, which can effectively alleviate the pixel\noffset existing in multi-level feature map fusion. Extensive experimental\nresults show that our MSDA-Net achieves state-of-the-art (SOTA) results on the\npublic NUDT-SIRST, SIRST and IRSTD-1k datasets.\n","authors":["Jinmiao Zhao","Zelin Shi","Chuang Yu","Yunpeng Liu"],"pdf_url":"https://arxiv.org/pdf/2406.02037v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15403v2","updated":"2024-11-25T12:48:23Z","published":"2024-10-20T14:31:05Z","title":"MMDS: A Multimodal Medical Diagnosis System Integrating Image Analysis\n  and Knowledge-based Departmental Consultation","summary":"  We present MMDS, a system capable of recognizing medical images and patient\nfacial details, and providing professional medical diagnoses. The system\nconsists of two core components:The first component is the analysis of medical\nimages and videos. We trained a specialized multimodal medical model capable of\ninterpreting medical images and accurately analyzing patients' facial emotions\nand facial paralysis conditions. The model achieved an accuracy of 72.59% on\nthe FER2013 facial emotion recognition dataset, with a 91.1% accuracy in\nrecognizing the \"happy\" emotion. In facial paralysis recognition, the model\nreached an accuracy of 92%, which is 30% higher than that of GPT-4o. Based on\nthis model, we developed a parser for analyzing facial movement videos of\npatients with facial paralysis, achieving precise grading of the paralysis\nseverity. In tests on 30 videos of facial paralysis patients, the system\ndemonstrated a grading accuracy of 83.3%.The second component is the generation\nof professional medical responses. We employed a large language model,\nintegrated with a medical knowledge base, to generate professional diagnoses\nbased on the analysis of medical images or videos. The core innovation lies in\nour development of a department-specific knowledge base routing management\nmechanism, in which the large language model categorizes data by medical\ndepartments and, during the retrieval process, determines the appropriate\nknowledge base to query. This significantly improves retrieval accuracy in the\nRAG (retrieval-augmented generation) process.\n","authors":["Yi Ren","HanZhi Zhang","Weibin Li","Jun Fu","Diandong Liu","Tianyi Zhang","Jie He","Licheng Jiao"],"pdf_url":"https://arxiv.org/pdf/2410.15403v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.09841v2","updated":"2024-11-25T12:40:39Z","published":"2021-10-19T10:54:01Z","title":"Cutting Voxel Projector a New Approach to Construct 3D Cone Beam CT\n  Operator","summary":"  In this paper, we introduce a novel class of projectors for 3D cone beam\ntomographic reconstruction. Analytical formulas are derived to compute the\nrelationship between the volume of a voxel projected onto a detector pixel and\nits contribution to the line integral of attenuation recorded by that pixel.\nBased on these formulas, we construct a near-exact projector and backprojector,\nparticularly suited for algebraic reconstruction techniques and hierarchical\nreconstruction approaches with nonuniform voxel grids. Unlike traditional\nprojectors, which assume a uniform grid with fixed voxel sizes, our method\nenables local refinement of voxels, allowing for adaptive grid resolution and\nimproved reconstruction quality in regions of interest. We have implemented\nthis cutting voxel projector along with a relaxed, speed-optimized version and\ncompared them to two established projectors: a ray-tracing projector based on\nSiddon's algorithm and a TT footprint projector. Our results demonstrate that\nthe cutting voxel projector achieves higher accuracy than the TT projector,\nespecially for large cone beam angles. Furthermore, the relaxed version of the\ncutting voxel projector offers a significant speed advantage over current\nfootprint projector implementations, while maintaining comparable accuracy. In\ncontrast, Siddon's algorithm, when achieving similar accuracy, is considerably\nslower than the cutting voxel projector. All algorithms are implemented in an\nopen-source framework for algebraic reconstruction using OpenCL and C++,\noptimized for efficient GPU computation. GitHub repository of the project\nhttps://github.com/kulvait/KCT_cbct.\n","authors":["Vojtěch Kulvait","Julian Moosmann","Georg Rose"],"pdf_url":"https://arxiv.org/pdf/2110.09841v2.pdf","comment":"18 pages, 4 figures"},{"id":"http://arxiv.org/abs/2312.13377v3","updated":"2024-11-25T12:38:52Z","published":"2023-12-20T19:08:49Z","title":"SADA: Semantic adversarial unsupervised domain adaptation for Temporal\n  Action Localization","summary":"  Temporal Action Localization (TAL) is a complex task that poses relevant\nchallenges, particularly when attempting to generalize on new -- unseen --\ndomains in real-world applications. These scenarios, despite realistic, are\noften neglected in the literature, exposing these solutions to important\nperformance degradation. In this work, we tackle this issue by introducing, for\nthe first time, an approach for Unsupervised Domain Adaptation (UDA) in sparse\nTAL, which we refer to as Semantic Adversarial unsupervised Domain Adaptation\n(SADA). Our contributions are threefold: (1) we pioneer the development of a\ndomain adaptation model that operates on realistic sparse action detection\nbenchmarks; (2) we tackle the limitations of global-distribution alignment\ntechniques by introducing a novel adversarial loss that is sensitive to local\nclass distributions, ensuring finer-grained adaptation; and (3) we present a\nnovel set of benchmarks based on EpicKitchens100 and CharadesEgo, that evaluate\nmultiple domain shifts in a comprehensive manner. Our experiments indicate that\nSADA improves the adaptation across domains when compared to fully supervised\nstate-of-the-art and alternative UDA methods, attaining a performance boost of\nup to 6.14% mAP.\n","authors":["David Pujol-Perich","Albert Clapés","Sergio Escalera"],"pdf_url":"https://arxiv.org/pdf/2312.13377v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16336v1","updated":"2024-11-25T12:31:03Z","published":"2024-11-25T12:31:03Z","title":"WTDUN: Wavelet Tree-Structured Sampling and Deep Unfolding Network for\n  Image Compressed Sensing","summary":"  Deep unfolding networks have gained increasing attention in the field of\ncompressed sensing (CS) owing to their theoretical interpretability and\nsuperior reconstruction performance. However, most existing deep unfolding\nmethods often face the following issues: 1) they learn directly from\nsingle-channel images, leading to a simple feature representation that does not\nfully capture complex features; and 2) they treat various image components\nuniformly, ignoring the characteristics of different components. To address\nthese issues, we propose a novel wavelet-domain deep unfolding framework named\nWTDUN, which operates directly on the multi-scale wavelet subbands. Our method\nutilizes the intrinsic sparsity and multi-scale structure of wavelet\ncoefficients to achieve a tree-structured sampling and reconstruction,\neffectively capturing and highlighting the most important features within\nimages. Specifically, the design of tree-structured reconstruction aims to\ncapture the inter-dependencies among the multi-scale subbands, enabling the\nidentification of both fine and coarse features, which can lead to a marked\nimprovement in reconstruction quality. Furthermore, a wavelet domain adaptive\nsampling method is proposed to greatly improve the sampling capability, which\nis realized by assigning measurements to each wavelet subband based on its\nimportance. Unlike pure deep learning methods that treat all components\nuniformly, our method introduces a targeted focus on important subbands,\nconsidering their energy and sparsity. This targeted strategy lets us capture\nkey information more efficiently while discarding less important information,\nresulting in a more effective and detailed reconstruction. Extensive\nexperimental results on various datasets validate the superior performance of\nour proposed method.\n","authors":["Kai Han","Jin Wang","Yunhui Shi","Hanqin Cai","Nam Ling","Baocai Yin"],"pdf_url":"https://arxiv.org/pdf/2411.16336v1.pdf","comment":"20pages,Accepted by ACM Transactions on Multimedia Computing\n  Communications and Applications (TOMM)"},{"id":"http://arxiv.org/abs/2411.16332v1","updated":"2024-11-25T12:26:48Z","published":"2024-11-25T12:26:48Z","title":"Cluster-based human-in-the-loop strategy for improving machine\n  learning-based circulating tumor cell detection in liquid biopsy","summary":"  Detection and differentiation of circulating tumor cells (CTCs) and non-CTCs\nin blood draws of cancer patients pose multiple challenges. While the gold\nstandard relies on tedious manual evaluation of an automatically generated\nselection of images, machine learning (ML) techniques offer the potential to\nautomate these processes. However, human assessment remains indispensable when\nthe ML system arrives at uncertain or wrong decisions due to an insufficient\nset of labeled training data. This study introduces a human-in-the-loop (HiL)\nstrategy for improving ML-based CTC detection. We combine self-supervised deep\nlearning and a conventional ML-based classifier and propose iterative targeted\nsampling and labeling of new unlabeled training samples by human experts. The\nsampling strategy is based on the classification performance of local latent\nspace clusters. The advantages of the proposed approach compared to naive\nrandom sampling are demonstrated for liquid biopsy data from patients with\nmetastatic breast cancer.\n","authors":["Hümeyra Husseini-Wüsthoff","Sabine Riethdorf","Andreas Schneeweiss","Andreas Trumpp","Klaus Pantel","Harriet Wikman","Maximilian Nielsen","René Werner"],"pdf_url":"https://arxiv.org/pdf/2411.16332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04398v3","updated":"2024-11-25T12:26:11Z","published":"2023-12-07T16:10:10Z","title":"Intelligent Anomaly Detection for Lane Rendering Using Transformer with\n  Self-Supervised Pre-Training and Customized Fine-Tuning","summary":"  The burgeoning navigation services using digital maps provide great\nconvenience to drivers. Nevertheless, the presence of anomalies in lane\nrendering map images occasionally introduces potential hazards, as such\nanomalies can be misleading to human drivers and consequently contribute to\nunsafe driving conditions. In response to this concern and to accurately and\neffectively detect the anomalies, this paper transforms lane rendering image\nanomaly detection into a classification problem and proposes a four-phase\npipeline consisting of data pre-processing, self-supervised pre-training with\nthe masked image modeling (MiM) method, customized fine-tuning using\ncross-entropy based loss with label smoothing, and post-processing to tackle it\nleveraging state-of-the-art deep learning techniques, especially those\ninvolving Transformer models. Various experiments verify the effectiveness of\nthe proposed pipeline. Results indicate that the proposed pipeline exhibits\nsuperior performance in lane rendering image anomaly detection, and notably,\nthe self-supervised pre-training with MiM can greatly enhance the detection\naccuracy while significantly reducing the total training time. For instance,\nemploying the Swin Transformer with Uniform Masking as self-supervised\npretraining (Swin-Trans-UM) yielded a heightened accuracy at 94.77% and an\nimproved Area Under The Curve (AUC) score of 0.9743 compared with the pure Swin\nTransformer without pre-training (Swin-Trans) with an accuracy of 94.01% and an\nAUC of 0.9498. The fine-tuning epochs were dramatically reduced to 41 from the\noriginal 280. In conclusion, the proposed pipeline, with its incorporation of\nself-supervised pre-training using MiM and other advanced deep learning\ntechniques, emerges as a robust solution for enhancing the accuracy and\nefficiency of lane rendering image anomaly detection in digital navigation\nsystems.\n","authors":["Yongqi Dong","Xingmin Lu","Ruohan Li","Wei Song","Bart van Arem","Haneen Farah"],"pdf_url":"https://arxiv.org/pdf/2312.04398v3.pdf","comment":"25 pages, 7 figures, accepted by the 103rd Transportation Research\n  Board (TRB) Annual Meeting, under review by Transportation Research Record:\n  Journal of the Transportation Research Board"},{"id":"http://arxiv.org/abs/2411.16327v1","updated":"2024-11-25T12:23:14Z","published":"2024-11-25T12:23:14Z","title":"CapHDR2IR: Caption-Driven Transfer from Visible Light to Infrared Domain","summary":"  Infrared (IR) imaging offers advantages in several fields due to its unique\nability of capturing content in extreme light conditions. However, the\ndemanding hardware requirements of high-resolution IR sensors limit its\nwidespread application. As an alternative, visible light can be used to\nsynthesize IR images but this causes a loss of fidelity in image details and\nintroduces inconsistencies due to lack of contextual awareness of the scene.\nThis stems from a combination of using visible light with a standard dynamic\nrange, especially under extreme lighting, and a lack of contextual awareness\ncan result in pseudo-thermal-crossover artifacts. This occurs when multiple\nobjects with similar temperatures appear indistinguishable in the training\ndata, further exacerbating the loss of fidelity. To solve this challenge, this\npaper proposes CapHDR2IR, a novel framework incorporating vision-language\nmodels using high dynamic range (HDR) images as inputs to generate IR images.\nHDR images capture a wider range of luminance variations, ensuring reliable IR\nimage generation in different light conditions. Additionally, a dense caption\nbranch integrates semantic understanding, resulting in more meaningful and\ndiscernible IR outputs. Extensive experiments on the HDRT dataset show that the\nproposed CapHDR2IR achieves state-of-the-art performance compared with existing\ngeneral domain transfer methods and those tailored for visible-to-infrared\nimage translation.\n","authors":["Jingchao Peng","Thomas Bashford-Rogers","Zhuang Shao","Haitao Zhao","Aru Ranjan Singh","Abhishek Goswami","Kurt Debattista"],"pdf_url":"https://arxiv.org/pdf/2411.16327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16326v1","updated":"2024-11-25T12:22:36Z","published":"2024-11-25T12:22:36Z","title":"Brain-like emergent properties in deep networks: impact of network\n  architecture, datasets and training","summary":"  Despite the rapid pace at which deep networks are improving on standardized\nvision benchmarks, they are still outperformed by humans on real-world vision\ntasks. This paradoxical lack of generalization could be addressed by making\ndeep networks more brain-like. Although several benchmarks have compared the\nability of deep networks to predict brain responses to natural images, they do\nnot capture subtle but important brain-like emergent properties. To resolve\nthis issue, we report several well-known perceptual and neural emergent\nproperties that can be tested on deep networks. To evaluate how various design\nfactors impact brain-like properties, we systematically evaluated over 30\nstate-of-the-art networks with varying network architectures, training datasets\nand training regimes. Our main findings are as follows. First, network\narchitecture had the strongest impact on brain-like properties compared to\ndataset and training regime variations. Second, networks varied widely in their\nalignment to the brain with no single network outperforming all others. Taken\ntogether, our results complement existing benchmarks by revealing brain-like\nproperties that are either emergent or lacking in state-of-the-art deep\nnetworks.\n","authors":["Niranjan Rajesh","Georgin Jacob","SP Arun"],"pdf_url":"https://arxiv.org/pdf/2411.16326v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16325v1","updated":"2024-11-25T12:21:58Z","published":"2024-11-25T12:21:58Z","title":"Luminance Component Analysis for Exposure Correction","summary":"  Exposure correction methods aim to adjust the luminance while maintaining\nother luminance-unrelated information. However, current exposure correction\nmethods have difficulty in fully separating luminance-related and\nluminance-unrelated components, leading to distortions in color, loss of\ndetail, and requiring extra restoration procedures. Inspired by principal\ncomponent analysis (PCA), this paper proposes an exposure correction method\ncalled luminance component analysis (LCA). LCA applies the orthogonal\nconstraint to a U-Net structure to decouple luminance-related and\nluminance-unrelated features. With decoupled luminance-related features, LCA\nadjusts only the luminance-related components while keeping the\nluminance-unrelated components unchanged. To optimize the orthogonal constraint\nproblem, LCA employs a geometric optimization algorithm, which converts the\nconstrained problem in Euclidean space to an unconstrained problem in\northogonal Stiefel manifolds. Extensive experiments show that LCA can decouple\nthe luminance feature from the RGB color space. Moreover, LCA achieves the best\nPSNR (21.33) and SSIM (0.88) in the exposure correction dataset with 28.72 FPS.\n","authors":["Jingchao Peng","Thomas Bashford-Rogers","Jingkun Chen","Haitao Zhao","Zhengwei Hu","Kurt Debattista"],"pdf_url":"https://arxiv.org/pdf/2411.16325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12980v2","updated":"2024-11-25T12:18:42Z","published":"2024-11-20T02:14:07Z","title":"LaVida Drive: Vision-Text Interaction VLM for Autonomous Driving with\n  Token Selection, Recovery and Enhancement","summary":"  Recent advancements in Visual Language Models (VLMs) have made them crucial\nfor visual question answering (VQA) in autonomous driving, enabling natural\nhuman-vehicle interactions. However, existing methods often struggle in dynamic\ndriving environments, as they usually focus on static images or videos and rely\non downsampling to manage computational costs. This results in the loss of\ncritical details and the difficulty in effectively integrating spatial and\ntemporal information, undermining fine-grained perception and temporal\ncoherence essential for effective decision-making. To tackle these challenges,\nwe introduce LaVida Drive, a novel and efficient VQA framework for autonomous\ndriving. LaVida Drive seamlessly integrates temporal data while maintaining\nhigh-resolution inputs for detailed visual perception. It optimizes spatial\nprocessing by retaining high-resolution data for intricate details and using\nlower-resolution inputs for temporal analysis to focus on motion-related\nfeatures, thereby boosting computational efficiency. The core of LaVida Drive\nconsists of two modules: the \\textit{Query-aware Token Selection} module and\nthe \\textit{Spatial-Temporal Token Recovery and Enhancement} module. The former\ndynamically selects the most relevant visual tokens based on semantic alignment\nwith the input query, reducing the token count from high-resolution spatial\ninput. The latter ensures smooth and coherent interactions between spatial and\ntemporal information, preserving contextual continuity across frames. Extensive\nexperiments on various autonomous driving question-answering benchmarks show\nthat LaVida Drive significantly reduces visual tokens, enhances efficiency, and\nimproves overall performance.\n","authors":["Siwen Jiao","Yangyi Fang","Baoyun Peng","Wangqun Chen","Bharadwaj Veeravalli"],"pdf_url":"https://arxiv.org/pdf/2411.12980v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20807v2","updated":"2024-11-25T12:14:11Z","published":"2024-10-28T07:54:29Z","title":"Long-Tailed Out-of-Distribution Detection via Normalized Outlier\n  Distribution Adaptation","summary":"  One key challenge in Out-of-Distribution (OOD) detection is the absence of\nground-truth OOD samples during training. One principled approach to address\nthis issue is to use samples from external datasets as outliers (i.e., pseudo\nOOD samples) to train OOD detectors. However, we find empirically that the\noutlier samples often present a distribution shift compared to the true OOD\nsamples, especially in Long-Tailed Recognition (LTR) scenarios, where ID\nclasses are heavily imbalanced, \\ie, the true OOD samples exhibit very\ndifferent probability distribution to the head and tailed ID classes from the\noutliers. In this work, we propose a novel approach, namely normalized outlier\ndistribution adaptation (AdaptOD), to tackle this distribution shift problem.\nOne of its key components is dynamic outlier distribution adaptation that\neffectively adapts a vanilla outlier distribution based on the outlier samples\nto the true OOD distribution by utilizing the OOD knowledge in the predicted\nOOD samples during inference. Further, to obtain a more reliable set of\npredicted OOD samples on long-tailed ID data, a novel dual-normalized energy\nloss is introduced in AdaptOD, which leverages class- and sample-wise\nnormalized energy to enforce a more balanced prediction energy on imbalanced ID\nsamples. This helps avoid bias toward the head samples and learn a\nsubstantially better vanilla outlier distribution than existing energy losses\nduring training. It also eliminates the need of manually tuning the sensitive\nmargin hyperparameters in energy losses. Empirical results on three popular\nbenchmarks for OOD detection in LTR show the superior performance of AdaptOD\nover state-of-the-art methods. Code is available at\nhttps://github.com/mala-lab/AdaptOD.\n","authors":["Wenjun Miao","Guansong Pang","Jin Zheng","Xiao Bai"],"pdf_url":"https://arxiv.org/pdf/2410.20807v2.pdf","comment":"NeurIPS2024"},{"id":"http://arxiv.org/abs/2411.16319v1","updated":"2024-11-25T12:11:27Z","published":"2024-11-25T12:11:27Z","title":"CutS3D: Cutting Semantics in 3D for 2D Unsupervised Instance\n  Segmentation","summary":"  Traditionally, algorithms that learn to segment object instances in 2D images\nhave heavily relied on large amounts of human-annotated data. Only recently,\nnovel approaches have emerged tackling this problem in an unsupervised fashion.\nGenerally, these approaches first generate pseudo-masks and then train a\nclass-agnostic detector. While such methods deliver the current state of the\nart, they often fail to correctly separate instances overlapping in 2D image\nspace since only semantics are considered. To tackle this issue, we instead\npropose to cut the semantic masks in 3D to obtain the final 2D instances by\nutilizing a point cloud representation of the scene. Furthermore, we derive a\nSpatial Importance function, which we use to resharpen the semantics along the\n3D borders of instances. Nevertheless, these pseudo-masks are still subject to\nmask ambiguity. To address this issue, we further propose to augment the\ntraining of a class-agnostic detector with three Spatial Confidence components\naiming to isolate a clean learning signal. With these contributions, our\napproach outperforms competing methods across multiple standard benchmarks for\nunsupervised instance segmentation and object detection.\n","authors":["Leon Sick","Dominik Engel","Sebastian Hartwig","Pedro Hermosilla","Timo Ropinski"],"pdf_url":"https://arxiv.org/pdf/2411.16319v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16318v1","updated":"2024-11-25T12:11:05Z","published":"2024-11-25T12:11:05Z","title":"One Diffusion to Generate Them All","summary":"  We introduce OneDiffusion, a versatile, large-scale diffusion model that\nseamlessly supports bidirectional image synthesis and understanding across\ndiverse tasks. It enables conditional generation from inputs such as text,\ndepth, pose, layout, and semantic maps, while also handling tasks like image\ndeblurring, upscaling, and reverse processes such as depth estimation and\nsegmentation. Additionally, OneDiffusion allows for multi-view generation,\ncamera pose estimation, and instant personalization using sequential image\ninputs. Our model takes a straightforward yet effective approach by treating\nall tasks as frame sequences with varying noise scales during training,\nallowing any frame to act as a conditioning image at inference time. Our\nunified training framework removes the need for specialized architectures,\nsupports scalable multi-task training, and adapts smoothly to any resolution,\nenhancing both generalization and scalability. Experimental results demonstrate\ncompetitive performance across tasks in both generation and prediction such as\ntext-to-image, multiview generation, ID preservation, depth estimation and\ncamera pose estimation despite relatively small training dataset. Our code and\ncheckpoint are freely available at https://github.com/lehduong/OneDiffusion\n","authors":["Duong H. Le","Tuan Pham","Sangho Lee","Christopher Clark","Aniruddha Kembhavi","Stephan Mandt","Ranjay Krishna","Jiasen Lu"],"pdf_url":"https://arxiv.org/pdf/2411.16318v1.pdf","comment":"two first authors contribute equally"},{"id":"http://arxiv.org/abs/2411.08460v2","updated":"2024-11-25T12:10:05Z","published":"2024-11-13T09:31:06Z","title":"Trap-MID: Trapdoor-based Defense against Model Inversion Attacks","summary":"  Model Inversion (MI) attacks pose a significant threat to the privacy of Deep\nNeural Networks by recovering training data distribution from well-trained\nmodels. While existing defenses often rely on regularization techniques to\nreduce information leakage, they remain vulnerable to recent attacks. In this\npaper, we propose the Trapdoor-based Model Inversion Defense (Trap-MID) to\nmislead MI attacks. A trapdoor is integrated into the model to predict a\nspecific label when the input is injected with the corresponding trigger.\nConsequently, this trapdoor information serves as the \"shortcut\" for MI\nattacks, leading them to extract trapdoor triggers rather than private data. We\nprovide theoretical insights into the impacts of trapdoor's effectiveness and\nnaturalness on deceiving MI attacks. In addition, empirical experiments\ndemonstrate the state-of-the-art defense performance of Trap-MID against\nvarious MI attacks without the requirements for extra data or large\ncomputational overhead. Our source code is publicly available at\nhttps://github.com/ntuaislab/Trap-MID.\n","authors":["Zhen-Ting Liu","Shang-Tse Chen"],"pdf_url":"https://arxiv.org/pdf/2411.08460v2.pdf","comment":"Accepted by Neural Information Processing Systems (NeurIPS) 2024"},{"id":"http://arxiv.org/abs/2411.16316v1","updated":"2024-11-25T12:09:43Z","published":"2024-11-25T12:09:43Z","title":"Monocular Lane Detection Based on Deep Learning: A Survey","summary":"  Lane detection plays an important role in autonomous driving perception\nsystem. As deep learning algorithms gain popularity, monocular lane detection\nmethods based on deep learning have demonstrated superior performance and\nemerged as a key research direction in autonomous driving perception. The core\ndesign of these algorithmic frameworks can be summarized as follows: (1) Task\nparadigm, focusing on lane instance-level discrimination; (2) Lane modeling,\nrepresenting lanes as a set of learnable parameters in the neural network; (3)\nGlobal context supplementation, enhancing the detection of obscured lanes; (4)\nPerspective effect elimination, providing 3D lanes usable for downstream\napplications. From these perspectives, this paper presents a comprehensive\noverview of existing methods, encompassing both the increasingly mature 2D lane\ndetection approaches and the developing 3D lane detection works. For a\nrelatively fair comparison, in addition to comparing the performance of\nmainstream methods on different benchmarks, their inference speed is also\ninvestigated under a unified setting. Moreover, we present some extended works\non lane detection, including multi-task perception, video lane detection,\nonline high-definition (HD) map construction, and lane topology reasoning, to\noffer readers a comprehensive roadmap for the evolution of lane detection.\nFinally, we point out some potential future research directions in this field.\nWe exhaustively collect the papers and codes of existing works at\nhttps://github.com/Core9724/Awesome-Lane-Detection and will keep tracing the\nresearch.\n","authors":["Xin He","Haiyun Guo","Kuan Zhu","Bingke Zhu","Xu Zhao","Jianwu Fang","Jinqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2411.16316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16314v1","updated":"2024-11-25T12:05:57Z","published":"2024-11-25T12:05:57Z","title":"Oriented histogram-based vector field embedding for characterizing 4D CT\n  data sets in radiotherapy","summary":"  In lung radiotherapy, the primary objective is to optimize treatment outcomes\nby minimizing exposure to healthy tissues while delivering the prescribed dose\nto the target volume. The challenge lies in accounting for lung tissue motion\ndue to breathing, which impacts precise treatment alignment. To address this,\nthe paper proposes a prospective approach that relies solely on pre-treatment\ninformation, such as planning CT scans and derived data like vector fields from\ndeformable image registration. This data is compared to analogous patient data\nto tailor treatment strategies, i.e., to be able to review treatment parameters\nand success for similar patients. To allow for such a comparison, an embedding\nand clustering strategy of prospective patient data is needed. Therefore, the\nmain focus of this study lies on reducing the dimensionality of deformable\nregistration-based vector fields by employing a voxel-wise spherical coordinate\ntransformation and a low-dimensional 2D oriented histogram representation.\nAfterwards, a fully unsupervised UMAP embedding of the encoded vector fields\n(i.e., patient-specific motion information) becomes applicable. The\nfunctionality of the proposed method is demonstrated with 71 in-house acquired\n4D CT data sets and 33 external 4D CT data sets. A comprehensive analysis of\nthe patient clusters is conducted, focusing on the similarity of breathing\npatterns of clustered patients. The proposed general approach of reducing the\ndimensionality of registration vector fields by encoding the inherent\ninformation into oriented histograms is, however, applicable to other tasks.\n","authors":["Frederic Madesta","Lukas Wimmert","Tobias Gauer","René Werner","Thilo Sentker"],"pdf_url":"https://arxiv.org/pdf/2411.16314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16312v1","updated":"2024-11-25T12:01:57Z","published":"2024-11-25T12:01:57Z","title":"EPS: Efficient Patch Sampling for Video Overfitting in Deep\n  Super-Resolution Model Training","summary":"  Leveraging the overfitting property of deep neural networks (DNNs) is\ntrending in video delivery systems to enhance quality within bandwidth limits.\nExisting approaches transmit overfitted super-resolution (SR) model streams for\nlow-resolution (LR) bitstreams, which are used to reconstruct high-resolution\n(HR) videos at the decoder. Although these approaches show promising results,\nthe huge computational costs of training a large number of video frames limit\ntheir practical applications. To overcome this challenge, we propose an\nefficient patch sampling method named EPS for video SR network overfitting,\nwhich identifies the most valuable training patches from video frames. To this\nend, we first present two low-complexity Discrete Cosine Transform (DCT)-based\nspatial-temporal features to measure the complexity score of each patch\ndirectly. By analyzing the histogram distribution of these features, we then\ncategorize all possible patches into different clusters and select training\npatches from the cluster with the highest spatial-temporal information. The\nnumber of sampled patches is adaptive based on the video content, addressing\nthe trade-off between training complexity and efficiency. Our method reduces\nthe number of patches for the training to 4% to 25%, depending on the\nresolution and number of clusters, while maintaining high video quality and\nsignificantly enhancing training efficiency. Compared to the state-of-the-art\npatch sampling method, EMT, our approach achieves an 83% decrease in overall\nrun time.\n","authors":["Yiying Wei","Hadi Amirpour","Jong Hwan Ko","Christian Timmerer"],"pdf_url":"https://arxiv.org/pdf/2411.16312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16310v1","updated":"2024-11-25T11:57:48Z","published":"2024-11-25T11:57:48Z","title":"Functionality understanding and segmentation in 3D scenes","summary":"  Understanding functionalities in 3D scenes involves interpreting natural\nlanguage descriptions to locate functional interactive objects, such as handles\nand buttons, in a 3D environment. Functionality understanding is highly\nchallenging, as it requires both world knowledge to interpret language and\nspatial perception to identify fine-grained objects. For example, given a task\nlike 'turn on the ceiling light', an embodied AI agent must infer that it needs\nto locate the light switch, even though the switch is not explicitly mentioned\nin the task description. To date, no dedicated methods have been developed for\nthis problem. In this paper, we introduce Fun3DU, the first approach designed\nfor functionality understanding in 3D scenes. Fun3DU uses a language model to\nparse the task description through Chain-of-Thought reasoning in order to\nidentify the object of interest. The identified object is segmented across\nmultiple views of the captured scene by using a vision and language model. The\nsegmentation results from each view are lifted in 3D and aggregated into the\npoint cloud using geometric information. Fun3DU is training-free, relying\nentirely on pre-trained models. We evaluate Fun3DU on SceneFun3D, the most\nrecent and only dataset to benchmark this task, which comprises over 3000 task\ndescriptions on 230 scenes. Our method significantly outperforms\nstate-of-the-art open-vocabulary 3D segmentation approaches. Code will be\nreleased publicly.\n","authors":["Jaime Corsetti","Francesco Giuliari","Alice Fasoli","Davide Boscaini","Fabio Poiesi"],"pdf_url":"https://arxiv.org/pdf/2411.16310v1.pdf","comment":"Technical report. 20 pages, 12 figures, 7 tables"},{"id":"http://arxiv.org/abs/2411.14514v2","updated":"2024-11-25T11:54:28Z","published":"2024-11-21T14:15:15Z","title":"NexusSplats: Efficient 3D Gaussian Splatting in the Wild","summary":"  While 3D Gaussian Splatting (3DGS) has recently demonstrated remarkable\nrendering quality and efficiency in 3D scene reconstruction, it struggles with\nvarying lighting conditions and incidental occlusions in real-world scenarios.\nTo accommodate varying lighting conditions, existing 3DGS extensions apply\ncolor mapping to the massive Gaussian primitives with individually optimized\nappearance embeddings. To handle occlusions, they predict pixel-wise\nuncertainties via 2D image features for occlusion capture. Nevertheless, such\nmassive color mapping and pixel-wise uncertainty prediction strategies suffer\nfrom not only additional computational costs but also coarse-grained lighting\nand occlusion handling. In this work, we propose a nexus kernel-driven\napproach, termed NexusSplats, for efficient and finer 3D scene reconstruction\nunder complex lighting and occlusion conditions. In particular, NexusSplats\nleverages a novel light decoupling strategy where appearance embeddings are\noptimized based on nexus kernels instead of massive Gaussian primitives, thus\naccelerating reconstruction speeds while ensuring local color consistency for\nfiner textures. Additionally, a Gaussian-wise uncertainty mechanism is\ndeveloped, aligning 3D structures with 2D image features for fine-grained\nocclusion handling. Experimental results demonstrate that NexusSplats achieves\nstate-of-the-art rendering quality while reducing reconstruction time by up to\n70.4% compared to the current best in quality.\n","authors":["Yuzhou Tang","Dejun Xu","Yongjie Hou","Zhenzhong Wang","Min Jiang"],"pdf_url":"https://arxiv.org/pdf/2411.14514v2.pdf","comment":"Project page: https://nexus-splats.github.io/"},{"id":"http://arxiv.org/abs/2411.16308v1","updated":"2024-11-25T11:53:55Z","published":"2024-11-25T11:53:55Z","title":"An End-to-End Robust Point Cloud Semantic Segmentation Network with\n  Single-Step Conditional Diffusion Models","summary":"  Existing conditional Denoising Diffusion Probabilistic Models (DDPMs) with a\nNoise-Conditional Framework (NCF) remain challenging for 3D scene understanding\ntasks, as the complex geometric details in scenes increase the difficulty of\nfitting the gradients of the data distribution (the scores) from semantic\nlabels. This also results in longer training and inference time for DDPMs\ncompared to non-DDPMs. From a different perspective, we delve deeply into the\nmodel paradigm dominated by the Conditional Network. In this paper, we propose\nan end-to-end robust semantic \\textbf{Seg}mentation \\textbf{Net}work based on a\n\\textbf{C}onditional-Noise Framework (CNF) of D\\textbf{D}PMs, named\n\\textbf{CDSegNet}. Specifically, CDSegNet models the Noise Network (NN) as a\nlearnable noise-feature generator. This enables the Conditional Network (CN) to\nunderstand 3D scene semantics under multi-level feature perturbations,\nenhancing the generalization in unseen scenes. Meanwhile, benefiting from the\nnoise system of DDPMs, CDSegNet exhibits strong noise and sparsity robustness\nin experiments. Moreover, thanks to CNF, CDSegNet can generate the semantic\nlabels in a single-step inference like non-DDPMs, due to avoiding directly\nfitting the scores from semantic labels in the dominant network of CDSegNet. On\npublic indoor and outdoor benchmarks, CDSegNet significantly outperforms\nexisting methods, achieving state-of-the-art performance.\n","authors":["Wentao Qu","Jing Wang","YongShun Gong","Xiaoshui Huang","Liang Xiao"],"pdf_url":"https://arxiv.org/pdf/2411.16308v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16301v1","updated":"2024-11-25T11:36:34Z","published":"2024-11-25T11:36:34Z","title":"DiffDesign: Controllable Diffusion with Meta Prior for Efficient\n  Interior Design Generation","summary":"  Interior design is a complex and creative discipline involving aesthetics,\nfunctionality, ergonomics, and materials science. Effective solutions must meet\ndiverse requirements, typically producing multiple deliverables such as\nrenderings and design drawings from various perspectives. Consequently,\ninterior design processes are often inefficient and demand significant\ncreativity. With advances in machine learning, generative models have emerged\nas a promising means of improving efficiency by creating designs from text\ndescriptions or sketches. However, few generative works focus on interior\ndesign, leading to substantial discrepancies between outputs and practical\nneeds, such as differences in size, spatial scope, and the lack of controllable\ngeneration quality. To address these challenges, we propose DiffDesign, a\ncontrollable diffusion model with meta priors for efficient interior design\ngeneration. Specifically, we utilize the generative priors of a 2D diffusion\nmodel pre-trained on a large image dataset as our rendering backbone. We\nfurther guide the denoising process by disentangling cross-attention control\nover design attributes, such as appearance, pose, and size, and introduce an\noptimal transfer-based alignment module to enforce view consistency.\nSimultaneously, we construct an interior design-specific dataset, DesignHelper,\nconsisting of over 400 solutions across more than 15 spatial types and 15\ndesign styles. This dataset helps fine-tune DiffDesign. Extensive experiments\nconducted on various benchmark datasets demonstrate the effectiveness and\nrobustness of DiffDesign.\n","authors":["Yuxuan Yang","Jingyao Wang","Tao Geng","Wenwen Qiang","Changwen Zheng","Fuchun Sun"],"pdf_url":"https://arxiv.org/pdf/2411.16301v1.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2411.16295v1","updated":"2024-11-25T11:27:42Z","published":"2024-11-25T11:27:42Z","title":"A Performance Increment Strategy for Semantic Segmentation of\n  Low-Resolution Images from Damaged Roads","summary":"  Autonomous driving needs good roads, but 85% of Brazilian roads have damages\nthat deep learning models may not regard as most semantic segmentation datasets\nfor autonomous driving are high-resolution images of well-maintained urban\nroads. A representative dataset for emerging countries consists of\nlow-resolution images of poorly maintained roads and includes labels of damage\nclasses; in this scenario, three challenges arise: objects with few pixels,\nobjects with undefined shapes, and highly underrepresented classes. To tackle\nthese challenges, this work proposes the Performance Increment Strategy for\nSemantic Segmentation (PISSS) as a methodology of 14 training experiments to\nboost performance. With PISSS, we reached state-of-the-art results of 79.8 and\n68.8 mIoU on the Road Traversing Knowledge (RTK) and Technik Autonomer Systeme\n500 (TAS500) test sets, respectively. Furthermore, we also offer an analysis of\nDeepLabV3+ pitfalls for small object segmentation.\n","authors":["Rafael S. Toledo","Cristiano S. Oliveira","Vitor H. T. Oliveira","Eric A. Antonelo","Aldo von Wangenheim"],"pdf_url":"https://arxiv.org/pdf/2411.16295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12564v2","updated":"2024-11-25T11:24:23Z","published":"2024-10-16T13:38:31Z","title":"FTII-Bench: A Comprehensive Multimodal Benchmark for Flow Text with\n  Image Insertion","summary":"  Benefiting from the revolutionary advances in large language models (LLMs)\nand foundational vision models, large vision-language models (LVLMs) have also\nmade significant progress. However, current benchmarks focus on tasks that\nevaluating only a single aspect of LVLM capabilities (e.g., recognition,\ndetection, understanding). These tasks fail to fully demonstrate LVLMs'\npotential in complex application scenarios. To comprehensively assess the\nperformance of existing LVLMs, we propose a more challenging task called the\nFlow Text with Image Insertion task (FTII). This task requires LVLMs to\nsimultaneously possess outstanding abilities in image comprehension,\ninstruction understanding, and long-text interpretation. Specifically, given\nseveral text paragraphs and a set of candidate images, as the text paragraphs\naccumulate, the LVLMs are required to select the most suitable image from the\ncandidates to insert after the corresponding paragraph. Constructing a\nbenchmark for such a task is highly challenging, particularly in determining\nthe sequence of flowing text and images. To address this challenge, we turn to\nprofessional news reports, which naturally contain a gold standard for\nimage-text sequences. Based on this, we introduce the Flow Text with Image\nInsertion Benchmark (FTII-Bench), which includes 318 high-quality Chinese\nimage-text news articles and 307 high-quality English image-text news articles,\ncovering 10 different news domains. Using these 625 high-quality articles, we\nconstruct problems of two different types with multiple levels of difficulty.\nFurthermore, we establish two different evaluation pipelines based on the CLIP\nmodel and existing LVLMs. We evaluate 9 open-source and 2 closed-source LVLMs\nas well as 2 CLIP-based models. Results indicate that even the most advanced\nmodels (e.g., GPT-4o) face significant challenges when tackling the FTII task.\n","authors":["Jiacheng Ruan","Yebin Yang","Zehao Lin","Yuchen Feng","Feiyu Xiong","Zeyun Tang","Zhiyu Li"],"pdf_url":"https://arxiv.org/pdf/2410.12564v2.pdf","comment":"Work in progress. 9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.16289v1","updated":"2024-11-25T11:13:12Z","published":"2024-11-25T11:13:12Z","title":"Utilizing Uncertainty in 2D Pose Detectors for Probabilistic 3D Human\n  Mesh Recovery","summary":"  Monocular 3D human pose and shape estimation is an inherently ill-posed\nproblem due to depth ambiguities, occlusions, and truncations. Recent\nprobabilistic approaches learn a distribution over plausible 3D human meshes by\nmaximizing the likelihood of the ground-truth pose given an image. We show that\nthis objective function alone is not sufficient to best capture the full\ndistributions. Instead, we propose to additionally supervise the learned\ndistributions by minimizing the distance to distributions encoded in heatmaps\nof a 2D pose detector. Moreover, we reveal that current methods often generate\nincorrect hypotheses for invisible joints which is not detected by the\nevaluation protocols. We demonstrate that person segmentation masks can be\nutilized during training to significantly decrease the number of invalid\nsamples and introduce two metrics to evaluate it. Our normalizing flow-based\napproach predicts plausible 3D human mesh hypotheses that are consistent with\nthe image evidence while maintaining high diversity for ambiguous body parts.\nExperiments on 3DPW and EMDB show that we outperform other state-of-the-art\nprobabilistic methods. Code is available for research purposes at\nhttps://github.com/twehrbein/humr.\n","authors":["Tom Wehrbein","Marco Rudolph","Bodo Rosenhahn","Bastian Wandt"],"pdf_url":"https://arxiv.org/pdf/2411.16289v1.pdf","comment":"WACV 2025"},{"id":"http://arxiv.org/abs/2411.12070v2","updated":"2024-11-25T10:36:37Z","published":"2024-11-18T21:29:50Z","title":"Autoassociative Learning of Structural Representations for Modeling and\n  Classification in Medical Imaging","summary":"  Deep learning architectures based on convolutional neural networks tend to\nrely on continuous, smooth features. While this characteristics provides\nsignificant robustness and proves useful in many real-world tasks, it is\nstrikingly incompatible with the physical characteristic of the world, which,\nat the scale in which humans operate, comprises crisp objects, typically\nrepresenting well-defined categories. This study proposes a class of\nneurosymbolic systems that learn by reconstructing the observed images in terms\nof visual primitives and are thus forced to form high-level, structural\nexplanations of them. When applied to the task of diagnosing abnormalities in\nhistological imaging, the method proved superior to a conventional deep\nlearning architecture in terms of classification accuracy, while being more\ntransparent.\n","authors":["Zuzanna Buchnajzer","Kacper Dobek","Stanisław Hapke","Daniel Jankowski","Krzysztof Krawiec"],"pdf_url":"https://arxiv.org/pdf/2411.12070v2.pdf","comment":"16 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.07838v2","updated":"2024-11-25T10:30:36Z","published":"2024-10-10T11:56:09Z","title":"Minority-Focused Text-to-Image Generation via Prompt Optimization","summary":"  We investigate the generation of minority samples using pretrained\ntext-to-image (T2I) latent diffusion models. Minority instances, in the context\nof T2I generation, can be defined as ones living on low-density regions of\ntext-conditional data distributions. They are valuable for various applications\nof modern T2I generators, such as data augmentation and creative AI.\nUnfortunately, existing pretrained T2I diffusion models primarily focus on\nhigh-density regions, largely due to the influence of guided samplers (like\nCFG) that are essential for producing high-quality generations. To address\nthis, we present a novel framework to counter the high-density-focus of T2I\ndiffusion models. Specifically, we first develop an online prompt optimization\nframework that can encourage the emergence of desired properties during\ninference while preserving semantic contents of user-provided prompts. We\nsubsequently tailor this generic prompt optimizer into a specialized solver\nthat promotes the generation of minority features by incorporating a\ncarefully-crafted likelihood objective. Our comprehensive experiments,\nconducted across various types of T2I models, demonstrate that our approach\nsignificantly enhances the capability to produce high-quality minority\ninstances compared to existing samplers.\n","authors":["Soobin Um","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2410.07838v2.pdf","comment":"20 pages, 9 figures"},{"id":"http://arxiv.org/abs/2411.16253v1","updated":"2024-11-25T10:14:10Z","published":"2024-11-25T10:14:10Z","title":"Open-Vocabulary Octree-Graph for 3D Scene Understanding","summary":"  Open-vocabulary 3D scene understanding is indispensable for embodied agents.\nRecent works leverage pretrained vision-language models (VLMs) for object\nsegmentation and project them to point clouds to build 3D maps. Despite\nprogress, a point cloud is a set of unordered coordinates that requires\nsubstantial storage space and does not directly convey occupancy information or\nspatial relation, making existing methods inefficient for downstream tasks,\ne.g., path planning and complex text-based object retrieval. To address these\nissues, we propose Octree-Graph, a novel scene representation for\nopen-vocabulary 3D scene understanding. Specifically, a Chronological\nGroup-wise Segment Merging (CGSM) strategy and an Instance Feature Aggregation\n(IFA) algorithm are first designed to get 3D instances and corresponding\nsemantic features. Subsequently, an adaptive-octree structure is developed that\nstores semantics and depicts the occupancy of an object adjustably according to\nits shape. Finally, the Octree-Graph is constructed where each adaptive-octree\nacts as a graph node, and edges describe the spatial relations among nodes.\nExtensive experiments on various tasks are conducted on several widely-used\ndatasets, demonstrating the versatility and effectiveness of our method.\n","authors":["Zhigang Wang","Yifei Su","Chenhui Li","Dong Wang","Yan Huang","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2411.16253v1.pdf","comment":"11pages,7figures"},{"id":"http://arxiv.org/abs/2411.16250v1","updated":"2024-11-25T10:09:37Z","published":"2024-11-25T10:09:37Z","title":"Diagnosis of diabetic retinopathy using machine learning & deep learning\n  technique","summary":"  Fundus images are widely used for diagnosing various eye diseases, such as\ndiabetic retinopathy, glaucoma, and age-related macular degeneration. However,\nmanual analysis of fundus images is time-consuming and prone to errors. In this\nreport, we propose a novel method for fundus detection using object detection\nand machine learning classification techniques. We use a YOLO_V8 to perform\nobject detection on fundus images and locate the regions of interest (ROIs)\nsuch as optic disc, optic cup and lesions. We then use machine learning SVM\nclassification algorithms to classify the ROIs into different DR stages based\non the presence or absence of pathological signs such as exudates,\nmicroaneurysms, and haemorrhages etc. Our method achieves 84% accuracy and\nefficiency for fundus detection and can be applied for retinal fundus disease\ntriage, especially in remote areas around the world.\n","authors":["Eric Shah","Jay Patel","Mr. Vishal Katheriya","Parth Pataliya"],"pdf_url":"https://arxiv.org/pdf/2411.16250v1.pdf","comment":"9 pages, 11 figures, Journal Paper"},{"id":"http://arxiv.org/abs/2411.16236v1","updated":"2024-11-25T09:52:28Z","published":"2024-11-25T09:52:28Z","title":"DoubleCCA: Improving Foundation Model Group Robustness with Random\n  Sentence Embeddings","summary":"  This paper presents a novel method to improve the robustness of foundation\nmodels to group-based biases. We propose a simple yet effective method, called\nDoubleCCA, that leverages random sentences and Canonical Correlation Analysis\n(CCA) to enrich the text embeddings of the foundation model. First, we generate\nvarious random sentences that augment the original prompts, which extends the\noriginal prompts with random words or character sequences. Second, we use an\nadditional sentence embedding model to generate different text embeddings with\nrespect to these random sentences. We then use CCA double twice to align the\nrepresentations and reconstruct them back to the original representation space.\nWe demonstrate the effectiveness of our method on a variety of tasks and\ndatasets, showing that it outperforms existing methods in terms of both\nperformance and robustness. Our method is simple to implement and can be easily\nintegrated into existing models, making it a practical solution for improving\nthe robustness of foundation models to group-based biases.\n","authors":["Hong Liu","Yitong Lu"],"pdf_url":"https://arxiv.org/pdf/2411.16236v1.pdf","comment":"18 pages, 6 figures, 2 tables"},{"id":"http://arxiv.org/abs/2411.16227v1","updated":"2024-11-25T09:41:20Z","published":"2024-11-25T09:41:20Z","title":"EigenHearts: Cardiac Diseases Classification Using EigenFaces Approach","summary":"  In the realm of cardiovascular medicine, medical imaging plays a crucial role\nin accurately classifying cardiac diseases and making precise diagnoses.\nHowever, the field faces significant challenges when integrating data science\ntechniques, as a significant volume of images is required for these techniques.\nAs a consequence, it is necessary to investigate different avenues to overcome\nthis challenge. In this contribution, we offer an innovative tool to conquer\nthis limitation. In particular, we delve into the application of a well\nrecognized method known as the EigenFaces approach to classify cardiac\ndiseases. This approach was originally motivated for efficiently representing\npictures of faces using principal component analysis, which provides a set of\neigenvectors (aka eigenfaces), explaining the variation between face images. As\nthis approach proven to be efficient for face recognition, it motivated us to\nexplore its efficiency on more complicated data bases. In particular, we\nintegrate this approach, with convolutional neural networks (CNNs) to classify\nechocardiography images taken from mice in five distinct cardiac conditions\n(healthy, diabetic cardiomyopathy, myocardial infarction, obesity and TAC\nhypertension). Performing a preprocessing step inspired from the eigenfaces\napproach on the echocardiography datasets, yields sets of pod modes, which we\nwill call eigenhearts. To demonstrate the proposed approach, we compare two\ntestcases: (i) supplying the CNN with the original images directly, (ii)\nsupplying the CNN with images projected into the obtained pod modes. The\nresults show a substantial and noteworthy enhancement when employing SVD for\npre-processing, with classification accuracy increasing by approximately 50%.\n","authors":["Nourelhouda Groun","Maria Villalba-Orero","Lucia Casado-Martin","Enrique Lara-Pezzi","Eusebio Valero","Soledad Le Clainche","Jesus Garicano-Mena"],"pdf_url":"https://arxiv.org/pdf/2411.16227v1.pdf","comment":"16 pages, 9 figures, 3 tables"},{"id":"http://arxiv.org/abs/2411.16222v1","updated":"2024-11-25T09:33:44Z","published":"2024-11-25T09:33:44Z","title":"UltraSam: A Foundation Model for Ultrasound using Large Open-Access\n  Segmentation Datasets","summary":"  Purpose: Automated ultrasound image analysis is challenging due to anatomical\ncomplexity and limited annotated data. To tackle this, we take a data-centric\napproach, assembling the largest public ultrasound segmentation dataset and\ntraining a versatile visual foundation model tailored for ultrasound.\n  Methods: We compile US-43d, a large-scale collection of 43 open-access\nultrasound datasets with over 280,000 images and segmentation masks for more\nthan 50 anatomical structures. We then introduce UltraSam, an adaptation of the\nSegment Anything Model (SAM) that is trained on US-43d and supports both point-\nand box-prompts. Finally, we introduce a new use case for SAM-style models by\nusing UltraSam as a model initialization that can be fine-tuned for various\ndownstream analysis tasks, demonstrating UltraSam's foundational capabilities.\n  Results: UltraSam achieves vastly improved performance over existing\nSAM-style models for prompt-based segmentation on three diverse public\ndatasets. Moreover, an UltraSam-initialized Vision Transformer surpasses\nImageNet-, SAM-, and MedSAM-initialized models in various downstream\nsegmentation and classification tasks, highlighting UltraSam's effectiveness as\na foundation model.\n  Conclusion: We compile US-43d, a large-scale unified ultrasound dataset, and\nintroduce UltraSam, a powerful multi-purpose SAM-style model for ultrasound\nimages. We release our code and pretrained models at\nhttps://github.com/CAMMA-public/UltraSam and invite the community to further\nthis effort by contributing high-quality datasets.\n","authors":["Adrien Meyer","Aditya Murali","Didier Mutter","Nicolas Padoy"],"pdf_url":"https://arxiv.org/pdf/2411.16222v1.pdf","comment":"7 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2411.16219v1","updated":"2024-11-25T09:28:58Z","published":"2024-11-25T09:28:58Z","title":"Weakly supervised image segmentation for defect-based grading of fresh\n  produce","summary":"  Implementing image-based machine learning in agriculture is often limited by\nscarce data and annotations, making it hard to achieve high-quality model\npredictions. This study tackles the issue of postharvest quality assessment of\nbananas in decentralized supply chains. We propose a method to detect and\nsegment surface defects in banana images using panoptic segmentation to\nquantify defect size and number. Instead of time-consuming pixel-level\nannotations, we use weak supervision with coarse labels. A dataset of 476\nsmartphone images of bananas was collected under real-world field conditions\nand annotated for bruises and scars. Using the Segment Anything Model (SAM), a\nrecently published foundation model for image segmentation, we generated dense\nannotations from coarse bounding boxes to train a segmentation model,\nsignificantly reducing manual effort while achieving a panoptic quality score\nof 77.6%. This demonstrates SAM's potential for low-effort, accurate\nsegmentation in agricultural settings with limited data.\n","authors":["Manuel Knott","Divinefavour Odion","Sameer Sontakke","Anup Karwa","Thijs Defraeye"],"pdf_url":"https://arxiv.org/pdf/2411.16219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16217v1","updated":"2024-11-25T09:26:34Z","published":"2024-11-25T09:26:34Z","title":"Mixed Degradation Image Restoration via Local Dynamic Optimization and\n  Conditional Embedding","summary":"  Multiple-in-one image restoration (IR) has made significant progress, aiming\nto handle all types of single degraded image restoration with a single model.\nHowever, in real-world scenarios, images often suffer from combinations of\nmultiple degradation factors. Existing multiple-in-one IR models encounter\nchallenges related to degradation diversity and prompt singularity when\naddressing this issue. In this paper, we propose a novel multiple-in-one IR\nmodel that can effectively restore images with both single and mixed\ndegradations. To address degradation diversity, we design a Local Dynamic\nOptimization (LDO) module which dynamically processes degraded areas of varying\ntypes and granularities. To tackle the prompt singularity issue, we develop an\nefficient Conditional Feature Embedding (CFE) module that guides the decoder in\nleveraging degradation-type-related features, significantly improving the\nmodel's performance in mixed degradation restoration scenarios. To validate the\neffectiveness of our model, we introduce a new dataset containing both single\nand mixed degradation elements. Experimental results demonstrate that our\nproposed model achieves state-of-the-art (SOTA) performance not only on mixed\ndegradation tasks but also on classic single-task restoration benchmarks.\n","authors":["Yubin Gu","Yuan Meng","Xiaoshuai Sun","Jiayi Ji","Weijian Ruan","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2411.16217v1.pdf","comment":"10 pages, 3 figures, 8 tables"},{"id":"http://arxiv.org/abs/2411.16216v1","updated":"2024-11-25T09:25:53Z","published":"2024-11-25T09:25:53Z","title":"SMGDiff: Soccer Motion Generation using diffusion probabilistic models","summary":"  Soccer is a globally renowned sport with significant applications in video\ngames and VR/AR. However, generating realistic soccer motions remains\nchallenging due to the intricate interactions between the human player and the\nball. In this paper, we introduce SMGDiff, a novel two-stage framework for\ngenerating real-time and user-controllable soccer motions. Our key idea is to\nintegrate real-time character control with a powerful diffusion-based\ngenerative model, ensuring high-quality and diverse output motion. In the first\nstage, we instantly transform coarse user controls into diverse global\ntrajectories of the character. In the second stage, we employ a\ntransformer-based autoregressive diffusion model to generate soccer motions\nbased on trajectory conditioning. We further incorporate a contact guidance\nmodule during inference to optimize the contact details for realistic ball-foot\ninteractions. Moreover, we contribute a large-scale soccer motion dataset\nconsisting of over 1.08 million frames of diverse soccer motions. Extensive\nexperiments demonstrate that our SMGDiff significantly outperforms existing\nmethods in terms of motion quality and condition alignment.\n","authors":["Hongdi Yang","Chengyang Li","Zhenxuan Wu","Gaozheng Li","Jingya Wang","Jingyi Yu","Zhuo Su","Lan Xu"],"pdf_url":"https://arxiv.org/pdf/2411.16216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16213v1","updated":"2024-11-25T09:22:13Z","published":"2024-11-25T09:22:13Z","title":"SAVEn-Vid: Synergistic Audio-Visual Integration for Enhanced\n  Understanding in Long Video Context","summary":"  Endeavors have been made to explore Large Language Models for video analysis\n(Video-LLMs), particularly in understanding and interpreting long videos.\nHowever, existing Video-LLMs still face challenges in effectively integrating\nthe rich and diverse audio-visual information inherent in long videos, which is\ncrucial for comprehensive understanding. This raises the question: how can we\nleverage embedded audio-visual information to enhance long video understanding?\nTherefore, (i) we introduce SAVEn-Vid, the first-ever long audio-visual video\ndataset comprising over 58k audio-visual instructions. (ii) From the model\nperspective, we propose a time-aware Audio-Visual Large Language Model\n(AV-LLM), SAVEnVideo, fine-tuned on SAVEn-Vid. (iii) Besides, we present\nAVBench, a benchmark containing 2,500 QAs designed to evaluate models on\nenhanced audio-visual comprehension tasks within long video, challenging their\nability to handle intricate audio-visual interactions. Experiments on AVBench\nreveal the limitations of current AV-LLMs. Experiments also demonstrate that\nSAVEnVideo outperforms the best Video-LLM by 3.61% on the zero-shot long video\ntask (Video-MME) and surpasses the leading audio-visual LLM by 1.29% on the\nzero-shot audio-visual task (Music-AVQA). Consequently, at the 7B parameter\nscale, SAVEnVideo can achieve state-of-the-art performance. Our dataset and\ncode will be released at https://ljungang.github.io/SAVEn-Vid/ upon acceptance.\n","authors":["Jungang Li","Sicheng Tao","Yibo Yan","Xiaojie Gu","Haodong Xu","Xu Zheng","Yuanhuiyi Lyu","Linfeng Zhang","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2411.16213v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08379v4","updated":"2024-11-25T09:19:38Z","published":"2024-06-12T16:29:45Z","title":"Gazing Into Missteps: Leveraging Eye-Gaze for Unsupervised Mistake\n  Detection in Egocentric Videos of Skilled Human Activities","summary":"  We address the challenge of unsupervised mistake detection in egocentric\nvideo of skilled human activities through the analysis of gaze signals. While\ntraditional methods rely on manually labeled mistakes, our approach does not\nrequire mistake annotations, hence overcoming the need of domain-specific\nlabeled data. Based on the observation that eye movements closely follow object\nmanipulation activities, we assess to what extent eye-gaze signals can support\nmistake detection, proposing to identify deviations in attention patterns\nmeasured through a gaze tracker with respect to those estimated by a gaze\nprediction model. Since predicting gaze in video is characterized by high\nuncertainty, we propose a novel gaze completion task, where eye fixations are\npredicted from visual observations and partial gaze trajectories, and\ncontribute a novel gaze completion approach which explicitly models\ncorrelations between gaze information and local visual tokens. Inconsistencies\nbetween predicted and observed gaze trajectories act as an indicator to\nidentify mistakes. Experiments highlight the effectiveness of the proposed\napproach in different settings, with relative gains up to +14%, +11%, and +5%\nin EPIC-Tent, HoloAssist and IndustReal respectively, remarkably matching\nresults of supervised approaches without seeing any labels. We further show\nthat gaze-based analysis is particularly useful in the presence of skilled\nactions, low action execution confidence, and actions requiring hand-eye\ncoordination and object manipulation skills. Our method is ranked first on the\nHoloAssist Mistake Detection challenge.\n","authors":["Michele Mazzamuto","Antonino Furnari","Yoichi Sato","Giovanni Maria Farinella"],"pdf_url":"https://arxiv.org/pdf/2406.08379v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11221v2","updated":"2024-11-25T09:03:41Z","published":"2023-11-19T04:26:16Z","title":"GaussianDiffusion: 3D Gaussian Splatting for Denoising Diffusion\n  Probabilistic Models with Structured Noise","summary":"  Text-to-3D, known for its efficient generation methods and expansive creative\npotential, has garnered significant attention in the AIGC domain. However, the\npixel-wise rendering of NeRF and its ray marching light sampling constrain the\nrendering speed, impacting its utility in downstream industrial applications.\nGaussian splatting has recently shown a trend of replacing the traditional\npointwise sampling technique commonly used in NeRF-based methodologies, and it\nis changing various aspects of 3D reconstruction. This paper introduces a novel\ntext to 3D content generation framework based on Gaussian splatting and\nproduces more realistic renderings. The challenge of achieving multi-view\nconsistency in 3D generation significantly impedes modeling complexity and\naccuracy. Taking inspiration from SJC, we explore employing multi-view noise\ndistributions to perturb images generated by 3D Gaussian splatting, aiming to\nrectify inconsistencies in multi-view geometry. We ingeniously devise an\nefficient method to generate noise that produces Gaussian noise from diverse\nviewpoints, all originating from a shared noise source. Furthermore, vanilla 3D\nGaussian-based generation tends to trap models in local minima, causing\nartifacts like floaters, burrs, or proliferative elements. To mitigate these\nissues, we propose the variational Gaussian splatting technique to enhance the\nquality and stability of 3D appearance. To our knowledge, our approach\nrepresents the first comprehensive utilization of Gaussian splatting across the\nentire spectrum of 3D content generation processes.\n","authors":["Xinhai Li","Huaibin Wang","Kuo-Kun Tseng"],"pdf_url":"https://arxiv.org/pdf/2311.11221v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16201v1","updated":"2024-11-25T08:59:39Z","published":"2024-11-25T08:59:39Z","title":"Video-Text Dataset Construction from Multi-AI Feedback: Promoting\n  Weak-to-Strong Preference Learning for Video Large Language Models","summary":"  High-quality video-text preference data is crucial for Multimodal Large\nLanguage Models (MLLMs) alignment. However, existing preference data is very\nscarce. Obtaining VQA preference data for preference training is costly, and\nmanually annotating responses is highly unreliable, which could result in\nlow-quality pairs. Meanwhile, AI-generated responses controlled by temperature\nadjustment lack diversity. To address these issues, we propose a high-quality\nVQA preference dataset, called \\textit{\\textbf{M}ultiple \\textbf{M}ultimodal\n\\textbf{A}rtificial \\textbf{I}ntelligence \\textbf{P}reference Datasets in\n\\textbf{V}QA} (\\textbf{MMAIP-V}), which is constructed by sampling from the\nresponse distribution set and using an external scoring function for response\nevaluation. Furthermore, to fully leverage the preference knowledge in MMAIP-V\nand ensure sufficient optimization, we propose \\textit{\\textbf{Iter}ative\n\\textbf{W}eak-to-\\textbf{S}trong \\textbf{R}einforcement \\textbf{L}earning from\n\\textbf{AI} \\textbf{F}eedback for video MLLMs} (\\textbf{Iter-W2S-RLAIF}), a\nframework that gradually enhances MLLMs' alignment capabilities by iteratively\nupdating the reference model and performing parameter extrapolation. Finally,\nwe propose an unbiased and information-complete evaluation scheme in VQA\nevaluation. Experiments demonstrate that MMAIP-V is beneficial for MLLMs in\npreference learning and Iter-W2S-RLAIF fully exploits the alignment information\nin MMAIP-V. We believe that the proposed automatic VQA preference data\ngeneration pipeline based on AI feedback can greatly promote future work in the\nMLLMs alignment. \\textbf{Code and dataset are available}\n\\href{https://anonymous.4open.science/r/MMAIP-V_Iter-W2S-RLAIF-702F}{MMAIP-V\\_Iter-W2S-RLAIF-702F}.\n","authors":["Hao Yi","Qingyang Li","Yulan Hu","Fuzheng Zhang","Di Zhang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2411.16201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19198v2","updated":"2024-11-25T08:57:20Z","published":"2024-07-27T07:34:49Z","title":"Towards the Dynamics of a DNN Learning Symbolic Interactions","summary":"  This study proves the two-phase dynamics of a deep neural network (DNN)\nlearning interactions. Despite the long disappointing view of the faithfulness\nof post-hoc explanation of a DNN, a series of theorems have been proven in\nrecent years to show that for a given input sample, a small set of interactions\nbetween input variables can be considered as primitive inference patterns that\nfaithfully represent a DNN's detailed inference logic on that sample.\nParticularly, Zhang et al. have observed that various DNNs all learn\ninteractions of different complexities in two distinct phases, and this\ntwo-phase dynamics well explains how a DNN changes from under-fitting to\nover-fitting. Therefore, in this study, we mathematically prove the two-phase\ndynamics of interactions, providing a theoretical mechanism for how the\ngeneralization power of a DNN changes during the training process. Experiments\nshow that our theory well predicts the real dynamics of interactions on\ndifferent DNNs trained for various tasks.\n","authors":["Qihan Ren","Junpeng Zhang","Yang Xu","Yue Xin","Dongrui Liu","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.19198v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16199v1","updated":"2024-11-25T08:55:41Z","published":"2024-11-25T08:55:41Z","title":"VIRES: Video Instance Repainting with Sketch and Text Guidance","summary":"  We introduce VIRES, a video instance repainting method with sketch and text\nguidance, enabling video instance repainting, replacement, generation, and\nremoval. Existing approaches struggle with temporal consistency and accurate\nalignment with the provided sketch sequence. VIRES leverages the generative\npriors of text-to-video models to maintain temporal consistency and produce\nvisually pleasing results. We propose the Sequential ControlNet with the\nstandardized self-scaling, which effectively extracts structure layouts and\nadaptively captures high-contrast sketch details. We further augment the\ndiffusion transformer backbone with the sketch attention to interpret and\ninject fine-grained sketch semantics. A sketch-aware encoder ensures that\nrepainted results are aligned with the provided sketch sequence. Additionally,\nwe contribute the VireSet, a dataset with detailed annotations tailored for\ntraining and evaluating video instance editing methods. Experimental results\ndemonstrate the effectiveness of VIRES, which outperforms state-of-the-art\nmethods in visual quality, temporal consistency, condition alignment, and human\nratings.\n","authors":["Shuchen Weng","Haojie Zheng","Peixuan Zhan","Yuchen Hong","Han Jiang","Si Li","Boxin Shi"],"pdf_url":"https://arxiv.org/pdf/2411.16199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16198v1","updated":"2024-11-25T08:54:54Z","published":"2024-11-25T08:54:54Z","title":"Interpreting Object-level Foundation Models via Visual Precision Search","summary":"  Advances in multimodal pre-training have propelled object-level foundation\nmodels, such as Grounding DINO and Florence-2, in tasks like visual grounding\nand object detection. However, interpreting these models\\' decisions has grown\nincreasingly challenging. Existing interpretable attribution methods for\nobject-level task interpretation have notable limitations: (1) gradient-based\nmethods lack precise localization due to visual-textual fusion in foundation\nmodels, and (2) perturbation-based methods produce noisy saliency maps,\nlimiting fine-grained interpretability. To address these, we propose a Visual\nPrecision Search method that generates accurate attribution maps with fewer\nregions. Our method bypasses internal model parameters to overcome attribution\nissues from multimodal fusion, dividing inputs into sparse sub-regions and\nusing consistency and collaboration scores to accurately identify critical\ndecision-making regions. We also conducted a theoretical analysis of the\nboundary guarantees and scope of applicability of our method. Experiments on\nRefCOCO, MS COCO, and LVIS show our approach enhances object-level task\ninterpretability over SOTA for Grounding DINO and Florence-2 across various\nevaluation metrics, with faithfulness gains of 23.7\\%, 31.6\\%, and 20.1\\% on MS\nCOCO, LVIS, and RefCOCO for Grounding DINO, and 102.9\\% and 66.9\\% on MS COCO\nand RefCOCO for Florence-2. Additionally, our method can interpret failures in\nvisual grounding and object detection tasks, surpassing existing methods across\nmultiple evaluation metrics. The code will be released at\n\\url{https://github.com/RuoyuChen10/VPS}.\n","authors":["Ruoyu Chen","Siyuan Liang","Jingzhi Li","Shiming Liu","Maosen Li","Zheng Huang","Hua Zhang","Xiaochun Cao"],"pdf_url":"https://arxiv.org/pdf/2411.16198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.11470v3","updated":"2024-11-25T08:53:06Z","published":"2023-07-21T10:10:18Z","title":"Semi-supervised Underwater Image Enhancement Using A Physics-Aware\n  Triple-Stream Network","summary":"  Underwater images normally suffer from degradation due to the transmission\nmedium of water bodies. Both traditional prior-based approaches and deep\nlearning-based methods have been used to address this problem. However, the\ninflexible assumption of the former often impairs their effectiveness in\nhandling diverse underwater scenes, while the generalization of the latter to\nunseen images is usually weakened by insufficient data. In this study, we\nleverage both the physics-based Image Formation Model (IFM) and deep learning\ntechniques for Underwater Image Enhancement (UIE). To this end, we propose a\nnovel Physics-Aware Triple-Stream Underwater Image Enhancement Network, i.e.,\nPATS-UIENet, which comprises a Direct Signal Transmission Estimation Steam\n(D-Stream), a Backscatter Signal Transmission Estimation Steam (B-Stream) and\nan Ambient Light Estimation Stream (A-Stream). This network fulfills the UIE\ntask by explicitly estimating the degradation parameters of a revised IFM. We\nalso adopt an IFM-inspired semi-supervised learning framework, which exploits\nboth the labeled and unlabeled images, to address the issue of insufficient\ndata. To our knowledge, such a physics-aware deep network and the IFM-inspired\nsemi-supervised learning framework have not been used for the UIE task before.\nOur method performs better than, or at least comparably to, sixteen baselines\nacross six testing sets in the degradation estimation and UIE tasks. These\npromising results should be due to the fact that the proposed method can not\nonly model the degradation but also learn the characteristics of diverse\nunderwater scenes.\n","authors":["Hao Qi","Shixuan Xu","Xinghui Dong"],"pdf_url":"https://arxiv.org/pdf/2307.11470v3.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2411.16196v1","updated":"2024-11-25T08:52:46Z","published":"2024-11-25T08:52:46Z","title":"Learn from Foundation Model: Fruit Detection Model without Manual\n  Annotation","summary":"  Recent breakthroughs in large foundation models have enabled the possibility\nof transferring knowledge pre-trained on vast datasets to domains with limited\ndata availability. Agriculture is one of the domains that lacks sufficient\ndata. This study proposes a framework to train effective, domain-specific,\nsmall models from foundation models without manual annotation. Our approach\nbegins with SDM (Segmentation-Description-Matching), a stage that leverages two\nfoundation models: SAM2 (Segment Anything in Images and Videos) for\nsegmentation and OpenCLIP (Open Contrastive Language-Image Pretraining) for\nzero-shot open-vocabulary classification. In the second stage, a novel\nknowledge distillation mechanism is utilized to distill compact,\nedge-deployable models from SDM, enhancing both inference speed and perception\naccuracy. The complete method, termed SDM-D\n(Segmentation-Description-Matching-Distilling), demonstrates strong performance\nacross various fruit detection tasks object detection, semantic segmentation,\nand instance segmentation) without manual annotation. It nearly matches the\nperformance of models trained with abundant labels. Notably, SDM-D outperforms\nopen-set detection methods such as Grounding SAM and YOLO-World on all tested\nfruit detection datasets. Additionally, we introduce MegaFruits, a\ncomprehensive fruit segmentation dataset encompassing over 25,000 images, and\nall code and datasets are made publicly available at\nhttps://github.com/AgRoboticsResearch/SDM-D.git.\n","authors":["Yanan Wang","Zhenghao Fei","Ruichen Li","Yibin Ying"],"pdf_url":"https://arxiv.org/pdf/2411.16196v1.pdf","comment":"17 pages, 12 figures, conference or other essential info"},{"id":"http://arxiv.org/abs/2411.04269v2","updated":"2024-11-25T08:44:32Z","published":"2024-11-06T21:22:46Z","title":"Increasing the scalability of graph convolution for FPGA-implemented\n  event-based vision","summary":"  Event cameras are becoming increasingly popular as an alternative to\ntraditional frame-based vision sensors, especially in mobile robotics. Taking\nfull advantage of their high temporal resolution, high dynamic range, low power\nconsumption and sparsity of event data, which only reflects changes in the\nobserved scene, requires both an efficient algorithm and a specialised hardware\nplatform. A recent trend involves using Graph Convolutional Neural Networks\n(GCNNs) implemented on a heterogeneous SoC FPGA. In this paper we focus on\noptimising hardware modules for graph convolution to allow flexible selection\nof the FPGA resource (BlockRAM, DSP and LUT) for their implementation. We\npropose a ''two-step convolution'' approach that utilises additional BRAM\nbuffers in order to reduce up to 94% of LUT usage for multiplications. This\nmethod significantly improves the scalability of GCNNs, enabling the deployment\nof models with more layers, larger graphs sizes and their application for more\ndynamic scenarios.\n","authors":["Piotr Wzorek","Kamil Jeziorek","Tomasz Kryjak","Andrea Pinna"],"pdf_url":"https://arxiv.org/pdf/2411.04269v2.pdf","comment":"Accepted for the PhD forum during FPT 2024 (International Conference\n  on Field Programmable Technology), 10-12 December 2024, Sydney, Australia"},{"id":"http://arxiv.org/abs/2411.16185v1","updated":"2024-11-25T08:31:55Z","published":"2024-11-25T08:31:55Z","title":"Fancy123: One Image to High-Quality 3D Mesh Generation via Plug-and-Play\n  Deformation","summary":"  Generating 3D meshes from a single image is an important but ill-posed task.\nExisting methods mainly adopt 2D multiview diffusion models to generate\nintermediate multiview images, and use the Large Reconstruction Model (LRM) to\ncreate the final meshes. However, the multiview images exhibit local\ninconsistencies, and the meshes often lack fidelity to the input image or look\nblurry. We propose Fancy123, featuring two enhancement modules and an\nunprojection operation to address the above three issues, respectively. The\nappearance enhancement module deforms the 2D multiview images to realign\nmisaligned pixels for better multiview consistency. The fidelity enhancement\nmodule deforms the 3D mesh to match the input image. The unprojection of the\ninput image and deformed multiview images onto LRM's generated mesh ensures\nhigh clarity, discarding LRM's predicted blurry-looking mesh colors. Extensive\nqualitative and quantitative experiments verify Fancy123's SoTA performance\nwith significant improvement. Also, the two enhancement modules are\nplug-and-play and work at inference time, allowing seamless integration into\nvarious existing single-image-to-3D methods.\n","authors":["Qiao Yu","Xianzhi Li","Yuan Tang","Xu Han","Long Hu","Yixue Hao","Min Chen"],"pdf_url":"https://arxiv.org/pdf/2411.16185v1.pdf","comment":"Project page: https://github.com/YuQiao0303/Fancy123"},{"id":"http://arxiv.org/abs/2411.16183v1","updated":"2024-11-25T08:26:31Z","published":"2024-11-25T08:26:31Z","title":"Any3DIS: Class-Agnostic 3D Instance Segmentation by 2D Mask Tracking","summary":"  Existing 3D instance segmentation methods frequently encounter issues with\nover-segmentation, leading to redundant and inaccurate 3D proposals that\ncomplicate downstream tasks. This challenge arises from their unsupervised\nmerging approach, where dense 2D instance masks are lifted across frames into\npoint clouds to form 3D candidate proposals without direct supervision. These\ncandidates are then hierarchically merged based on heuristic criteria, often\nresulting in numerous redundant segments that fail to combine into precise 3D\nproposals. To overcome these limitations, we propose a 3D-Aware 2D Mask\nTracking module that uses robust 3D priors from a 2D mask segmentation and\ntracking foundation model (SAM-2) to ensure consistent object masks across\nvideo frames. Rather than merging all visible superpoints across views to\ncreate a 3D mask, our 3D Mask Optimization module leverages a dynamic\nprogramming algorithm to select an optimal set of views, refining the\nsuperpoints to produce a final 3D proposal for each object. Our approach\nachieves comprehensive object coverage within the scene while reducing\nunnecessary proposals, which could otherwise impair downstream applications.\nEvaluations on ScanNet200 and ScanNet++ confirm the effectiveness of our\nmethod, with improvements across Class-Agnostic, Open-Vocabulary, and\nOpen-Ended 3D Instance Segmentation tasks.\n","authors":["Phuc Nguyen","Minh Luu","Anh Tran","Cuong Pham","Khoi Nguyen"],"pdf_url":"https://arxiv.org/pdf/2411.16183v1.pdf","comment":"Project page: https://any3dis.github.io/"},{"id":"http://arxiv.org/abs/2411.16180v1","updated":"2024-11-25T08:23:38Z","published":"2024-11-25T08:23:38Z","title":"Event-boosted Deformable 3D Gaussians for Fast Dynamic Scene\n  Reconstruction","summary":"  3D Gaussian Splatting (3D-GS) enables real-time rendering but struggles with\nfast motion due to low temporal resolution of RGB cameras. To address this, we\nintroduce the first approach combining event cameras, which capture\nhigh-temporal-resolution, continuous motion data, with deformable 3D-GS for\nfast dynamic scene reconstruction. We observe that threshold modeling for\nevents plays a crucial role in achieving high-quality reconstruction.\nTherefore, we propose a GS-Threshold Joint Modeling (GTJM) strategy, creating a\nmutually reinforcing process that greatly improves both 3D reconstruction and\nthreshold modeling. Moreover, we introduce a Dynamic-Static Decomposition (DSD)\nstrategy that first identifies dynamic areas by exploiting the inability of\nstatic Gaussians to represent motions, then applies a buffer-based soft\ndecomposition to separate dynamic and static areas. This strategy accelerates\nrendering by avoiding unnecessary deformation in static areas, and focuses on\ndynamic areas to enhance fidelity. Our approach achieves high-fidelity dynamic\nreconstruction at 156 FPS with a 400$\\times$400 resolution on an RTX 3090 GPU.\n","authors":["Wenhao Xu","Wenming Weng","Yueyi Zhang","Ruikang Xu","Zhiwei Xiong"],"pdf_url":"https://arxiv.org/pdf/2411.16180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16175v1","updated":"2024-11-25T08:13:32Z","published":"2024-11-25T08:13:32Z","title":"High-Resolution Be Aware! Improving the Self-Supervised Real-World\n  Super-Resolution","summary":"  Self-supervised learning is crucial for super-resolution because ground-truth\nimages are usually unavailable for real-world settings. Existing methods derive\nself-supervision from low-resolution images by creating pseudo-pairs or by\nenforcing a low-resolution reconstruction objective. These methods struggle\nwith insufficient modeling of real-world degradations and the lack of knowledge\nabout high-resolution imagery, resulting in unnatural super-resolved results.\nThis paper strengthens awareness of the high-resolution image to improve the\nself-supervised real-world super-resolution. We propose a controller to adjust\nthe degradation modeling based on the quality of super-resolution results. We\nalso introduce a novel feature-alignment regularizer that directly constrains\nthe distribution of super-resolved images. Our method finetunes the\noff-the-shelf SR models for a target real-world domain. Experiments show that\nit produces natural super-resolved images with state-of-the-art perceptual\nperformance.\n","authors":["Yuehan Zhang","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2411.16175v1.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2411.16173v1","updated":"2024-11-25T08:04:47Z","published":"2024-11-25T08:04:47Z","title":"SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval\n  and Routing in Long-Form Video Analysis","summary":"  Despite advances in Large Multi-modal Models, applying them to long and\nuntrimmed video content remains challenging due to limitations in context\nlength and substantial memory overhead. These constraints often lead to\nsignificant information loss and reduced relevance in the model responses. With\nthe exponential growth of video data across web platforms, understanding\nlong-form video is crucial for advancing generalized intelligence. In this\npaper, we introduce SALOVA: Segment-Augmented LOng Video Assistant, a novel\nvideo-LLM framework designed to enhance the comprehension of lengthy video\ncontent through targeted retrieval process. We address two main challenges to\nachieve it: (i) We present the SceneWalk dataset, a high-quality collection of\n87.8K long videos, each densely captioned at the segment level to enable models\nto capture scene continuity and maintain rich descriptive context. (ii) We\ndevelop robust architectural designs integrating dynamic routing mechanism and\nspatio-temporal projector to efficiently retrieve and process relevant video\nsegments based on user queries. Our framework mitigates the limitations of\ncurrent video-LMMs by allowing for precise identification and retrieval of\nrelevant video segments in response to queries, thereby improving the\ncontextual relevance of the generated responses. Through extensive experiments,\nSALOVA demonstrates enhanced capability in processing complex long-form videos,\nshowing significant capability to maintain contextual integrity across extended\nsequences.\n","authors":["Junho Kim","Hyunjun Kim","Hosu Lee","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2411.16173v1.pdf","comment":"Project page: https://ivy-lvlm.github.io/SALOVA/"},{"id":"http://arxiv.org/abs/2411.16172v1","updated":"2024-11-25T08:02:28Z","published":"2024-11-25T08:02:28Z","title":"U2NeRF: Unsupervised Underwater Image Restoration and Neural Radiance\n  Fields","summary":"  Underwater images suffer from colour shifts, low contrast, and haziness due\nto light absorption, refraction, scattering and restoring these images has\nwarranted much attention. In this work, we present Unsupervised Underwater\nNeural Radiance Field U2NeRF, a transformer-based architecture that learns to\nrender and restore novel views conditioned on multi-view geometry\nsimultaneously. Due to the absence of supervision, we attempt to implicitly\nbake restoring capabilities onto the NeRF pipeline and disentangle the\npredicted color into several components - scene radiance, direct transmission\nmap, backscatter transmission map, and global background light, and when\ncombined reconstruct the underwater image in a self-supervised manner. In\naddition, we release an Underwater View Synthesis UVS dataset consisting of 12\nunderwater scenes, containing both synthetically-generated and real-world data.\nOur experiments demonstrate that when optimized on a single scene, U2NeRF\noutperforms several baselines by as much LPIPS 11%, UIQM 5%, UCIQE 4% (on\naverage) and showcases improved rendering and restoration capabilities. Code\nwill be made available upon acceptance.\n","authors":["Vinayak Gupta","Manoj S","Mukund Varma T","Kaushik Mitra"],"pdf_url":"https://arxiv.org/pdf/2411.16172v1.pdf","comment":"ICLR Tiny Papers 2024. arXiv admin note: text overlap with\n  arXiv:2207.13298"},{"id":"http://arxiv.org/abs/2410.15851v2","updated":"2024-11-25T08:01:15Z","published":"2024-10-21T10:27:57Z","title":"R2I-rPPG: A Robust Region of Interest Selection Method for Remote\n  Photoplethysmography to Extract Heart Rate","summary":"  The COVID-19 pandemic has underscored the need for low-cost, scalable\napproaches to measuring contactless vital signs, either during initial triage\nat a healthcare facility or virtual telemedicine visits. Remote\nphotoplethysmography (rPPG) can accurately estimate heart rate (HR) when\napplied to close-up videos of healthy volunteers in well-lit laboratory\nsettings. However, results from such highly optimized laboratory studies may\nnot be readily translated to healthcare settings. One significant barrier to\nthe practical application of rPPG in health care is the accurate localization\nof the region of interest (ROI). Clinical or telemedicine visits may involve\nsub-optimal lighting, movement artifacts, variable camera angle, and subject\ndistance. This paper presents an rPPG ROI selection method based on 3D facial\nlandmarks and patient head yaw angle. We then demonstrate the robustness of\nthis ROI selection method when coupled to the Plane-Orthogonal-to-Skin (POS)\nrPPG method when applied to videos of patients presenting to an Emergency\nDepartment for respiratory complaints. Our results demonstrate the\neffectiveness of our proposed approach in improving the accuracy and robustness\nof rPPG in a challenging clinical environment.\n","authors":["Sandeep Nagar","Mark Hasegawa-Johnson","David G. Beiser","Narendra Ahuja"],"pdf_url":"https://arxiv.org/pdf/2410.15851v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2411.16171v1","updated":"2024-11-25T08:00:21Z","published":"2024-11-25T08:00:21Z","title":"Image Generation Diversity Issues and How to Tame Them","summary":"  Generative methods now produce outputs nearly indistinguishable from real\ndata but often fail to fully capture the data distribution. Unlike quality\nissues, diversity limitations in generative models are hard to detect visually,\nrequiring specific metrics for assessment. In this paper, we draw attention to\nthe current lack of diversity in generative models and the inability of common\nmetrics to measure this. We achieve this by framing diversity as an image\nretrieval problem, where we measure how many real images can be retrieved using\nsynthetic data as queries. This yields the Image Retrieval Score (IRS), an\ninterpretable, hyperparameter-free metric that quantifies the diversity of a\ngenerative model's output. IRS requires only a subset of synthetic samples and\nprovides a statistical measure of confidence. Our experiments indicate that\ncurrent feature extractors commonly used in generative model assessment are\ninadequate for evaluating diversity effectively. Consequently, we perform an\nextensive search for the best feature extractors to assess diversity.\nEvaluation reveals that current diffusion models converge to limited subsets of\nthe real distribution, with no current state-of-the-art models superpassing 77%\nof the diversity of the training data. To address this limitation, we introduce\nDiversity-Aware Diffusion Models (DiADM), a novel approach that improves\ndiversity of unconditional diffusion models without loss of image quality. We\ndo this by disentangling diversity from image quality by using a diversity\naware module that uses pseudo-unconditional features as input. We provide a\nPython package offering unified feature extraction and metric computation to\nfurther facilitate the evaluation of generative models\nhttps://github.com/MischaD/beyondfid.\n","authors":["Mischa Dombrowski","Weitong Zhang","Sarah Cechnicka","Hadrien Reynaud","Bernhard Kainz"],"pdf_url":"https://arxiv.org/pdf/2411.16171v1.pdf","comment":"17 pages, 6 tables, 12 figures"},{"id":"http://arxiv.org/abs/2409.04298v2","updated":"2024-11-25T07:58:24Z","published":"2024-09-06T14:17:09Z","title":"RevSAM2: Prompt SAM2 for Medical Image Segmentation via\n  Reverse-Propagation without Fine-tuning","summary":"  The Segment Anything Model 2 (SAM2) has recently demonstrated exceptional\nperformance in zero-shot prompt segmentation for natural images and videos.\nHowever, when the propagation mechanism of SAM2 is applied to medical images,\nit often results in spatial inconsistencies, leading to significantly different\nsegmentation outcomes for very similar images. In this paper, we introduce\nRevSAM2, a simple yet effective self-correction framework that enables SAM2 to\nachieve superior performance in unseen 3D medical image segmentation tasks\nwithout the need for fine-tuning. Specifically, to segment a 3D query volume\nusing a limited number of support image-label pairs that define a new\nsegmentation task, we propose reverse propagation strategy as a query\ninformation selection mechanism. Instead of simply maintaining a\nfirst-in-first-out (FIFO) queue of memories to predict query slices\nsequentially, reverse propagation selects high-quality query information by\nleveraging support images to evaluate the quality of each predicted query slice\nmask. The selected high-quality masks are then used as prompts to propagate\nacross the entire query volume, thereby enhancing generalization to unseen\ntasks. Notably, we are the first to explore the potential of SAM2 in\nlabel-efficient medical image segmentation without fine-tuning. Compared to\nfine-tuning on large labeled datasets, the label-efficient scenario provides a\ncost-effective alternative for medical segmentation tasks, particularly for\nrare diseases or when dealing with unseen classes. Experiments on four public\ndatasets demonstrate the superiority of RevSAM2 in scenarios with limited\nlabels, surpassing state-of-the-arts by 12.18% in Dice. The code will be\nreleased.\n","authors":["Yunhao Bai","Boxiang Yun","Zeli Chen","Qinji Yu","Yingda Xia","Yan Wang"],"pdf_url":"https://arxiv.org/pdf/2409.04298v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16170v1","updated":"2024-11-25T07:56:13Z","published":"2024-11-25T07:56:13Z","title":"CARE Transformer: Mobile-Friendly Linear Visual Transformer via\n  Decoupled Dual Interaction","summary":"  Recently, large efforts have been made to design efficient linear-complexity\nvisual Transformers. However, current linear attention models are generally\nunsuitable to be deployed in resource-constrained mobile devices, due to\nsuffering from either few efficiency gains or significant accuracy drops. In\nthis paper, we propose a new de\\textbf{C}oupled du\\textbf{A}l-interactive\nlinea\\textbf{R} att\\textbf{E}ntion (CARE) mechanism, revealing that features'\ndecoupling and interaction can fully unleash the power of linear attention. We\nfirst propose an asymmetrical feature decoupling strategy that asymmetrically\ndecouples the learning process for local inductive bias and long-range\ndependencies, thereby preserving sufficient local and global information while\neffectively enhancing the efficiency of models. Then, a dynamic memory unit is\nemployed to maintain critical information along the network pipeline. Moreover,\nwe design a dual interaction module to effectively facilitate interaction\nbetween local inductive bias and long-range information as well as among\nfeatures at different layers. By adopting a decoupled learning way and fully\nexploiting complementarity across features, our method can achieve both high\nefficiency and accuracy. Extensive experiments on ImageNet-1K, COCO, and ADE20K\ndatasets demonstrate the effectiveness of our approach, e.g., achieving\n$78.4/82.1\\%$ top-1 accuracy on ImagegNet-1K at the cost of only $0.7/1.9$\nGMACs. Codes will be released on \\href{..}{github}.\n","authors":["Yuan Zhou","Qingshan Xu","Jiequan Cui","Junbao Zhou","Jing Zhang","Richang Hong","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.16170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16169v1","updated":"2024-11-25T07:55:57Z","published":"2024-11-25T07:55:57Z","title":"Local and Global Feature Attention Fusion Network for Face Recognition","summary":"  Recognition of low-quality face images remains a challenge due to invisible\nor deformation in partial facial regions. For low-quality images dominated by\nmissing partial facial regions, local region similarity contributes more to\nface recognition (FR). Conversely, in cases dominated by local face\ndeformation, excessive attention to local regions may lead to misjudgments,\nwhile global features exhibit better robustness. However, most of the existing\nFR methods neglect the bias in feature quality of low-quality images introduced\nby different factors. To address this issue, we propose a Local and Global\nFeature Attention Fusion (LGAF) network based on feature quality. The network\nadaptively allocates attention between local and global features according to\nfeature quality and obtains more discriminative and high-quality face features\nthrough local and global information complementarity. In addition, to\neffectively obtain fine-grained information at various scales and increase the\nseparability of facial features in high-dimensional space, we introduce a\nMulti-Head Multi-Scale Local Feature Extraction (MHMS) module. Experimental\nresults demonstrate that the LGAF achieves the best average performance on $4$\nvalidation sets (CFP-FP, CPLFW, AgeDB, and CALFW), and the performance on\nTinyFace and SCFace outperforms the state-of-the-art methods (SoTA).\n","authors":["Wang Yu","Wei Wei"],"pdf_url":"https://arxiv.org/pdf/2411.16169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16164v1","updated":"2024-11-25T07:40:32Z","published":"2024-11-25T07:40:32Z","title":"Text-to-Image Synthesis: A Decade Survey","summary":"  When humans read a specific text, they often visualize the corresponding\nimages, and we hope that computers can do the same. Text-to-image synthesis\n(T2I), which focuses on generating high-quality images from textual\ndescriptions, has become a significant aspect of Artificial Intelligence\nGenerated Content (AIGC) and a transformative direction in artificial\nintelligence research. Foundation models play a crucial role in T2I. In this\nsurvey, we review over 440 recent works on T2I. We start by briefly introducing\nhow GANs, autoregressive models, and diffusion models have been used for image\ngeneration. Building on this foundation, we discuss the development of these\nmodels for T2I, focusing on their generative capabilities and diversity when\nconditioned on text. We also explore cutting-edge research on various aspects\nof T2I, including performance, controllability, personalized generation, safety\nconcerns, and consistency in content and spatial relationships. Furthermore, we\nsummarize the datasets and evaluation metrics commonly used in T2I research.\nFinally, we discuss the potential applications of T2I within AIGC, along with\nthe challenges and future research opportunities in this field.\n","authors":["Nonghai Zhang","Hao Tang"],"pdf_url":"https://arxiv.org/pdf/2411.16164v1.pdf","comment":"In this survey, we review over 440 recent works on T2I"},{"id":"http://arxiv.org/abs/2411.16162v1","updated":"2024-11-25T07:36:46Z","published":"2024-11-25T07:36:46Z","title":"Sparse patches adversarial attacks via extrapolating point-wise\n  information","summary":"  Sparse and patch adversarial attacks were previously shown to be applicable\nin realistic settings and are considered a security risk to autonomous systems.\nSparse adversarial perturbations constitute a setting in which the adversarial\nperturbations are limited to affecting a relatively small number of points in\nthe input. Patch adversarial attacks denote the setting where the sparse\nattacks are limited to a given structure, i.e., sparse patches with a given\nshape and number. However, previous patch adversarial attacks do not\nsimultaneously optimize multiple patches' locations and perturbations. This\nwork suggests a novel approach for sparse patches adversarial attacks via\npoint-wise trimming dense adversarial perturbations. Our approach enables\nsimultaneous optimization of multiple sparse patches' locations and\nperturbations for any given number and shape. Moreover, our approach is also\napplicable for standard sparse adversarial attacks, where we show that it\nsignificantly improves the state-of-the-art over multiple extensive settings. A\nreference implementation of the proposed method and the reported experiments is\nprovided at \\url{https://github.com/yanemcovsky/SparsePatches.git}\n","authors":["Yaniv Nemcovsky","Avi Mendelson","Chaim Baskin"],"pdf_url":"https://arxiv.org/pdf/2411.16162v1.pdf","comment":"AdvML-Frontiers 24: The 3nd Workshop on New Frontiers in Adversarial\n  Machine Learning, NeurIPS 24"},{"id":"http://arxiv.org/abs/2411.16157v1","updated":"2024-11-25T07:34:23Z","published":"2024-11-25T07:34:23Z","title":"MVGenMaster: Scaling Multi-View Generation from Any Image via 3D Priors\n  Enhanced Diffusion Model","summary":"  We introduce MVGenMaster, a multi-view diffusion model enhanced with 3D\npriors to address versatile Novel View Synthesis (NVS) tasks. MVGenMaster\nleverages 3D priors that are warped using metric depth and camera poses,\nsignificantly enhancing both generalization and 3D consistency in NVS. Our\nmodel features a simple yet effective pipeline that can generate up to 100\nnovel views conditioned on arbitrary reference views and camera poses with a\nsingle forward process. Additionally, we have developed a comprehensive\nlarge-scale multi-view image dataset comprising up to 1.2 million scenes,\nequipped with well-aligned metric depth. Moreover, we present several training\nand model modifications to strengthen the model with scaled-up datasets.\nExtensive evaluations across in- and out-of-domain benchmarks demonstrate the\neffectiveness of our proposed method and data formulation. Models and codes\nwill be released at https://github.com/ewrfcas/MVGenMaster/.\n","authors":["Chenjie Cao","Chaohui Yu","Shang Liu","Fan Wang","Xiangyang Xue","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2411.16157v1.pdf","comment":"Models and codes will be released at\n  https://github.com/ewrfcas/MVGenMaster/"},{"id":"http://arxiv.org/abs/2411.16156v1","updated":"2024-11-25T07:32:02Z","published":"2024-11-25T07:32:02Z","title":"VideoOrion: Tokenizing Object Dynamics in Videos","summary":"  We present VideoOrion, a Video Large Language Model (Video-LLM) that\nexplicitly captures the key semantic information in videos--the\nspatial-temporal dynamics of objects throughout the videos. VideoOrion employs\nexpert vision models to extract object dynamics through a detect-segment-track\npipeline, encoding them into a set of object tokens by aggregating\nspatial-temporal object features. Our method addresses the persistent challenge\nin Video-LLMs of efficiently compressing high-dimensional video data into\nsemantic tokens that are comprehensible to LLMs. Compared to prior methods\nwhich resort to downsampling the original video or aggregating visual tokens\nusing resamplers, leading to information loss and entangled semantics,\nVideoOrion not only offers a more natural and efficient way to derive compact,\ndisentangled semantic representations but also enables explicit object modeling\nof video content with minimal computational cost. Moreover, the introduced\nobject tokens naturally allow VideoOrion to accomplish video-based referring\ntasks. Experimental results show that VideoOrion can learn to make good use of\nthe object tokens, and achieves competitive results on both general video\nquestion answering and video-based referring benchmarks.\n","authors":["Yicheng Feng","Yijiang Li","Wanpeng Zhang","Sipeng Zheng","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2411.16156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16148v1","updated":"2024-11-25T07:15:28Z","published":"2024-11-25T07:15:28Z","title":"Revisiting Marr in Face: The Building of 2D--2.5D--3D Representations in\n  Deep Neural Networks","summary":"  David Marr's seminal theory of vision proposes that the human visual system\noperates through a sequence of three stages, known as the 2D sketch, the 2.5D\nsketch, and the 3D model. In recent years, Deep Neural Networks (DNN) have been\nwidely thought to have reached a level comparable to human vision. However, the\nmechanisms by which DNNs accomplish this and whether they adhere to Marr's\n2D--2.5D--3D construction theory remain unexplored. In this paper, we delve\ninto the perception task to explore these questions and find evidence\nsupporting Marr's theory. We introduce a graphics probe, a sub-network crafted\nto reconstruct the original image from the network's intermediate layers. The\nkey to the graphics probe is its flexible architecture that supports image in\nboth 2D and 3D formats, as well as in a transitional state between them. By\ninjecting graphics probes into neural networks, and analyzing their behavior in\nreconstructing images, we find that DNNs initially encode images as 2D\nrepresentations in low-level layers, and finally construct 3D representations\nin high-level layers. Intriguingly, in mid-level layers, DNNs exhibit a hybrid\nstate, building a geometric representation that s sur normals within a narrow\ndepth range, akin to the appearance of a low-relief sculpture. This stage\nresembles the 2.5D representations, providing a view of how DNNs evolve from 2D\nto 3D in the perception process. The graphics probe therefore serves as a tool\nfor peering into the mechanisms of DNN, providing empirical support for Marr's\ntheory.\n","authors":["Xiangyu Zhu","Chang Yu","Jiankuo Zhao","Zhaoxiang Zhang","Stan Z. Li","Zhen Lei"],"pdf_url":"https://arxiv.org/pdf/2411.16148v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09777v2","updated":"2024-11-25T07:09:47Z","published":"2024-09-15T15:55:24Z","title":"DiFSD: Ego-Centric Fully Sparse Paradigm with Uncertainty Denoising and\n  Iterative Refinement for Efficient End-to-End Self-Driving","summary":"  Current end-to-end autonomous driving methods resort to unifying modular\ndesigns for various tasks (e.g. perception, prediction and planning). Although\noptimized in a planning-oriented spirit with a fully differentiable framework,\nexisting end-to-end driving systems without ego-centric designs still suffer\nfrom unsatisfactory performance and inferior efficiency, owing to the\nrasterized scene representation learning and redundant information\ntransmission. In this paper, we revisit the human driving behavior and propose\nan ego-centric fully sparse paradigm, named DiFSD, for end-to-end self-driving.\nSpecifically, DiFSD mainly consists of sparse perception, hierarchical\ninteraction and iterative motion planner. The sparse perception module performs\ndetection, tracking and online mapping based on sparse representation of the\ndriving scene. The hierarchical interaction module aims to select the Closest\nIn-Path Vehicle / Stationary (CIPV / CIPS) from coarse to fine, benefiting from\nan additional geometric prior. As for the iterative motion planner, both\nselected interactive agents and ego-vehicle are considered for joint motion\nprediction, where the output multi-modal ego-trajectories are optimized in an\niterative fashion. Besides, both position-level motion diffusion and\ntrajectory-level planning denoising are introduced for uncertainty modeling,\nthus facilitating the training stability and convergence of the whole\nframework. Extensive experiments conducted on nuScenes and Bench2Drive datasets\ndemonstrate the superior planning performance and great efficiency of DiFSD.\n","authors":["Haisheng Su","Wei Wu","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2409.09777v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02684v3","updated":"2024-11-25T07:07:45Z","published":"2023-11-05T15:48:29Z","title":"Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE","summary":"  Recent studies have demonstrated Large Language Models (LLMs) can extend\ntheir zero-shot generalization capabilities to multimodal learning through\ninstruction tuning. As more modalities and downstream tasks are introduced,\nnegative conflicts and interference may have a worse impact on performance.\nWhile this phenomenon has been overlooked in previous work, we propose a novel\nand extensible framework, called Octavius, for comprehensive studies and\nexperimentation on multimodal learning with Multimodal Large Language Models\n(MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and\none of the representative PEFT techniques, i.e., LoRA, designing a novel\nLLM-based decoder, called LoRA-MoE, for multimodal learning. To the best of our\nknowledge, we are one of the pioneering efforts to introduce MoE into MLLMs to\naddress this problem. The experimental results (about 20% improvement) have\nshown the effectiveness and versatility of our design in various 2D and 3D\ndownstream tasks. Code and datasets are available at\nhttps://openlamm.github.io/tutorial/.\n","authors":["Zeren Chen","Ziqin Wang","Zhen Wang","Huayang Liu","Zhenfei Yin","Si Liu","Lu Sheng","Wanli Ouyang","Yu Qiao","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2311.02684v3.pdf","comment":"22 pages, 12 figures. Accepted in ICLR 2024"},{"id":"http://arxiv.org/abs/2411.09156v2","updated":"2024-11-25T07:03:32Z","published":"2024-11-14T03:19:57Z","title":"DyGASR: Dynamic Generalized Exponential Splatting with Surface Alignment\n  for Accelerated 3D Mesh Reconstruction","summary":"  Recent advancements in 3D Gaussian Splatting (3DGS), which lead to\nhigh-quality novel view synthesis and accelerated rendering, have remarkably\nimproved the quality of radiance field reconstruction. However, the extraction\nof mesh from a massive number of minute 3D Gaussian points remains great\nchallenge due to the large volume of Gaussians and difficulty of representation\nof sharp signals caused by their inherent low-pass characteristics. To address\nthis issue, we propose DyGASR, which utilizes generalized exponential function\ninstead of traditional 3D Gaussian to decrease the number of particles and\ndynamically optimize the representation of the captured signal. In addition, it\nis observed that reconstructing mesh with Generalized Exponential\nSplatting(GES) without modifications frequently leads to failures since the\ngeneralized exponential distribution centroids may not precisely align with the\nscene surface. To overcome this, we adopt Sugar's approach and introduce\nGeneralized Surface Regularization (GSR), which reduces the smallest scaling\nvector of each point cloud to zero and ensures normal alignment perpendicular\nto the surface, facilitating subsequent Poisson surface mesh reconstruction.\nAdditionally, we propose a dynamic resolution adjustment strategy that utilizes\na cosine schedule to gradually increase image resolution from low to high\nduring the training stage, thus avoiding constant full resolution, which\nsignificantly boosts the reconstruction speed. Our approach surpasses existing\n3DGS-based mesh reconstruction methods, as evidenced by extensive evaluations\non various scene datasets, demonstrating a 25\\% increase in speed, and a 30\\%\nreduction in memory usage.\n","authors":["Shengchao Zhao","Yundong Li"],"pdf_url":"https://arxiv.org/pdf/2411.09156v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13571v3","updated":"2024-11-25T07:02:47Z","published":"2024-10-17T14:07:46Z","title":"DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving\n  Scene Representation","summary":"  Closed-loop simulation is essential for advancing end-to-end autonomous\ndriving systems. Contemporary sensor simulation methods, such as NeRF and 3DGS,\nrely predominantly on conditions closely aligned with training data\ndistributions, which are largely confined to forward-driving scenarios.\nConsequently, these methods face limitations when rendering complex maneuvers\n(e.g., lane change, acceleration, deceleration). Recent advancements in\nautonomous-driving world models have demonstrated the potential to generate\ndiverse driving videos. However, these approaches remain constrained to 2D\nvideo generation, inherently lacking the spatiotemporal coherence required to\ncapture intricacies of dynamic driving environments. In this paper, we\nintroduce DriveDreamer4D, which enhances 4D driving scene representation\nleveraging world model priors. Specifically, we utilize the world model as a\ndata machine to synthesize novel trajectory videos, where structured conditions\nare explicitly leveraged to control the spatial-temporal consistency of traffic\nelements. Besides, the cousin data training strategy is proposed to facilitate\nmerging real and synthetic data for optimizing 4DGS. To our knowledge,\nDriveDreamer4D is the first to utilize video generation models for improving 4D\nreconstruction in driving scenarios. Experimental results reveal that\nDriveDreamer4D significantly enhances generation quality under novel trajectory\nviews, achieving a relative improvement in FID by 32.1%, 46.4%, and 16.3%\ncompared to PVG, S3Gaussian, and Deformable-GS. Moreover, DriveDreamer4D\nmarkedly enhances the spatiotemporal coherence of driving agents, which is\nverified by a comprehensive user study and the relative increases of 22.6%,\n43.5%, and 15.6% in the NTA-IoU metric.\n","authors":["Guosheng Zhao","Chaojun Ni","Xiaofeng Wang","Zheng Zhu","Xueyang Zhang","Yida Wang","Guan Huang","Xinze Chen","Boyuan Wang","Youyi Zhang","Wenjun Mei","Xingang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.13571v3.pdf","comment":"Project Page: https://drivedreamer4d.github.io"},{"id":"http://arxiv.org/abs/2411.10814v2","updated":"2024-11-25T06:45:44Z","published":"2024-11-16T14:30:46Z","title":"DEAL: Decoupled Classifier with Adaptive Linear Modulation for Group\n  Robust Early Diagnosis of MCI to AD Conversion","summary":"  While deep learning-based Alzheimer's disease (AD) diagnosis has recently\nmade significant advancements, particularly in predicting the conversion of\nmild cognitive impairment (MCI) to AD based on MRI images, there remains a\ncritical gap in research regarding the group robustness of the diagnosis.\nAlthough numerous studies pointed out that deep learning-based classifiers may\nexhibit poor performance in certain groups by relying on unimportant\nattributes, this issue has been largely overlooked in the early diagnosis of\nMCI to AD conversion. In this paper, we present the first comprehensive\ninvestigation of the group robustness in the early diagnosis of MCI to AD\nconversion using MRI images, focusing on disparities in accuracy between\ngroups, specifically sMCI and pMCI individuals divided by age. Our experiments\nreveal that standard classifiers consistently underperform for certain groups\nacross different architectures, highlighting the need for more tailored\napproaches. To address this, we propose a novel method, dubbed DEAL (DEcoupled\nclassifier with Adaptive Linear modulation), comprising two key components: (1)\na linear modulation of features from the penultimate layer, incorporating\neasily obtainable age and cognitive indicative tabular features, and (2) a\ndecoupled classifier that provides more tailored decision boundaries for each\ngroup, further improving performance. Through extensive experiments and\nevaluations across different architectures, we demonstrate the efficacy of DEAL\nin improving the group robustness of the MCI to AD conversion prediction.\n","authors":["Donggyu Lee","Juhyeon Park","Taesup Moon"],"pdf_url":"https://arxiv.org/pdf/2411.10814v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2411.16132v1","updated":"2024-11-25T06:43:20Z","published":"2024-11-25T06:43:20Z","title":"TreeFormer: Single-view Plant Skeleton Estimation via Tree-constrained\n  Graph Generation","summary":"  Accurate estimation of plant skeletal structure (e.g., branching structure)\nfrom images is essential for smart agriculture and plant science. Unlike human\nskeletons with fixed topology, plant skeleton estimation presents a unique\nchallenge, i.e., estimating arbitrary tree graphs from images. While recent\ngraph generation methods successfully infer thin structures from images, it is\nchallenging to constrain the output graph strictly to a tree structure. To this\nproblem, we present TreeFormer, a plant skeleton estimator via tree-constrained\ngraph generation. Our approach combines learning-based graph generation with\ntraditional graph algorithms to impose the constraints during the training\nloop. Specifically, our method projects an unconstrained graph onto a minimum\nspanning tree (MST) during the training loop and incorporates this prior\nknowledge into the gradient descent optimization by suppressing unwanted\nfeature values. Experiments show that our method accurately estimates target\nplant skeletal structures for multiple domains: Synthetic tree patterns, real\nbotanical roots, and grapevine branches. Our implementations are available at\nhttps://github.com/huntorochi/TreeFormer/.\n","authors":["Xinpeng Liu","Hiroaki Santo","Yosuke Toda","Fumio Okura"],"pdf_url":"https://arxiv.org/pdf/2411.16132v1.pdf","comment":"IEEE/CVF Winter Conference on Applications of Computer Vision (WACV\n  2025)"},{"id":"http://arxiv.org/abs/2411.16129v1","updated":"2024-11-25T06:33:43Z","published":"2024-11-25T06:33:43Z","title":"Three Cars Approaching within 100m! Enhancing Distant Geometry by\n  Tri-Axis Voxel Scanning for Camera-based Semantic Scene Completion","summary":"  Camera-based Semantic Scene Completion (SSC) is gaining attentions in the 3D\nperception field. However, properties such as perspective and occlusion lead to\nthe underestimation of the geometry in distant regions, posing a critical issue\nfor safety-focused autonomous driving systems. To tackle this, we propose\nScanSSC, a novel camera-based SSC model composed of a Scan Module and Scan\nLoss, both designed to enhance distant scenes by leveraging context from\nnear-viewpoint scenes. The Scan Module uses axis-wise masked attention, where\neach axis employing a near-to-far cascade masking that enables distant voxels\nto capture relationships with preceding voxels. In addition, the Scan Loss\ncomputes the cross-entropy along each axis between cumulative logits and\ncorresponding class distributions in a near-to-far direction, thereby\npropagating rich context-aware signals to distant voxels. Leveraging the\nsynergy between these components, ScanSSC achieves state-of-the-art\nperformance, with IoUs of 44.54 and 48.29, and mIoUs of 17.40 and 20.14 on the\nSemanticKITTI and SSCBench-KITTI-360 benchmarks.\n","authors":["Jongseong Bae","Junwoo Ha","Ha Young Kim"],"pdf_url":"https://arxiv.org/pdf/2411.16129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16128v1","updated":"2024-11-25T06:29:51Z","published":"2024-11-25T06:29:51Z","title":"CIA: Controllable Image Augmentation Framework Based on Stable Diffusion","summary":"  Computer vision tasks such as object detection and segmentation rely on the\navailability of extensive, accurately annotated datasets. In this work, We\npresent CIA, a modular pipeline, for (1) generating synthetic images for\ndataset augmentation using Stable Diffusion, (2) filtering out low quality\nsamples using defined quality metrics, (3) forcing the existence of specific\npatterns in generated images using accurate prompting and ControlNet. In order\nto show how CIA can be used to search for an optimal augmentation pipeline of\ntraining data, we study human object detection in a data constrained scenario,\nusing YOLOv8n on COCO and Flickr30k datasets. We have recorded significant\nimprovement using CIA-generated images, approaching the performances obtained\nwhen doubling the amount of real images in the dataset. Our findings suggest\nthat our modular framework can significantly enhance object detection systems,\nand make it possible for future research to be done on data-constrained\nscenarios. The framework is available at: github.com/multitel-ai/CIA.\n","authors":["Mohamed Benkedadra","Dany Rimez","Tiffanie Godelaine","Natarajan Chidambaram","Hamed Razavi Khosroshahi","Horacio Tellez","Matei Mancas","Benoit Macq","Sidi Ahmed Mahmoudi"],"pdf_url":"https://arxiv.org/pdf/2411.16128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15503v4","updated":"2024-11-25T06:24:48Z","published":"2024-08-28T03:17:40Z","title":"RoboSense: Large-scale Dataset and Benchmark for Egocentric Robot\n  Perception and Navigation in Crowded and Unstructured Environments","summary":"  Reliable embodied perception from an egocentric perspective is challenging\nyet essential for autonomous navigation technology of intelligent mobile\nagents. With the growing demand of social robotics, near-field scene\nunderstanding becomes an important research topic in the areas of egocentric\nperceptual tasks related to navigation in both crowded and unstructured\nenvironments. Due to the complexity of environmental conditions and difficulty\nof surrounding obstacles owing to truncation and occlusion, the perception\ncapability under this circumstance is still inferior. To further enhance the\nintelligence of mobile robots, in this paper, we setup an egocentric\nmulti-sensor data collection platform based on 3 main types of sensors (Camera,\nLiDAR and Fisheye), which supports flexible sensor configurations to enable\ndynamic sight of view from ego-perspective, capturing either near or farther\nareas. Meanwhile, a large-scale multimodal dataset is constructed, named\nRoboSense, to facilitate egocentric robot perception. Specifically, RoboSense\ncontains more than 133K synchronized data with 1.4M 3D bounding box and IDs\nannotated in the full $360^{\\circ}$ view, forming 216K trajectories across 7.6K\ntemporal sequences. It has $270\\times$ and $18\\times$ as many annotations of\nsurrounding obstacles within near ranges as the previous datasets collected for\nautonomous driving scenarios such as KITTI and nuScenes. Moreover, we define a\nnovel matching criterion for near-field 3D perception and prediction metrics.\nBased on RoboSense, we formulate 6 popular tasks to facilitate the future\nresearch development, where the detailed analysis as well as benchmarks are\nalso provided accordingly. Data desensitization measures have been conducted\nfor privacy protection.\n","authors":["Haisheng Su","Feixiang Song","Cong Ma","Wei Wu","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2408.15503v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16123v1","updated":"2024-11-25T06:16:17Z","published":"2024-11-25T06:16:17Z","title":"Med-PerSAM: One-Shot Visual Prompt Tuning for Personalized Segment\n  Anything Model in Medical Domain","summary":"  Leveraging pre-trained models with tailored prompts for in-context learning\nhas proven highly effective in NLP tasks. Building on this success, recent\nstudies have applied a similar approach to the Segment Anything Model (SAM)\nwithin a ``one-shot\" framework, where only a single reference image and its\nlabel are employed. However, these methods face limitations in the medical\ndomain, primarily due to SAM's essential requirement for visual prompts and the\nover-reliance on pixel similarity for generating them. This dependency may lead\nto (1) inaccurate prompt generation and (2) clustering of point prompts,\nresulting in suboptimal outcomes. To address these challenges, we introduce\n\\textbf{Med-PerSAM}, a novel and straightforward one-shot framework designed\nfor the medical domain. Med-PerSAM uses only visual prompt engineering and\neliminates the need for additional training of the pretrained SAM or human\nintervention, owing to our novel automated prompt generation process. By\nintegrating our lightweight warping-based prompt tuning model with SAM, we\nenable the extraction and iterative refinement of visual prompts, enhancing the\nperformance of the pre-trained SAM. This advancement is particularly meaningful\nin the medical domain, where creating visual prompts poses notable challenges\nfor individuals lacking medical expertise. Our model outperforms various\nfoundational models and previous SAM-based approaches across diverse 2D medical\nimaging datasets.\n","authors":["Hangyul Yoon","Doohyuk Jang","Jungeun Kim","Eunho Yang"],"pdf_url":"https://arxiv.org/pdf/2411.16123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08604v2","updated":"2024-11-25T06:15:53Z","published":"2024-08-16T08:45:25Z","title":"Bi-Directional Deep Contextual Video Compression","summary":"  Deep video compression has made remarkable process in recent years, with the\nmajority of advancements concentrated on P-frame coding. Although efforts to\nenhance B-frame coding are ongoing, their compression performance is still far\nbehind that of traditional bi-directional video codecs. In this paper, we\nintroduce a bi-directional deep contextual video compression scheme tailored\nfor B-frames, termed DCVC-B, to improve the compression performance of deep\nB-frame coding. Our scheme mainly has three key innovations. First, we develop\na bi-directional motion difference context propagation method for effective\nmotion difference coding, which significantly reduces the bit cost of\nbi-directional motions. Second, we propose a bi-directional contextual\ncompression model and a corresponding bi-directional temporal entropy model, to\nmake better use of the multi-scale temporal contexts. Third, we propose a\nhierarchical quality structure-based training strategy, leading to an effective\nbit allocation across large groups of pictures (GOP). Experimental results show\nthat our DCVC-B achieves an average reduction of 26.6% in BD-Rate compared to\nthe reference software for H.265/HEVC under random access conditions.\nRemarkably, it surpasses the performance of the H.266/VVC reference software on\ncertain test datasets under the same configuration. We anticipate our work can\nprovide valuable insights and bring up deep B-frame coding to the next level.\n","authors":["Xihua Sheng","Li Li","Dong Liu","Shiqi Wang"],"pdf_url":"https://arxiv.org/pdf/2408.08604v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16119v1","updated":"2024-11-25T06:05:08Z","published":"2024-11-25T06:05:08Z","title":"Learning Optimal Lattice Vector Quantizers for End-to-end Neural Image\n  Compression","summary":"  It is customary to deploy uniform scalar quantization in the end-to-end\noptimized Neural image compression methods, instead of more powerful vector\nquantization, due to the high complexity of the latter. Lattice vector\nquantization (LVQ), on the other hand, presents a compelling alternative, which\ncan exploit inter-feature dependencies more effectively while keeping\ncomputational efficiency almost the same as scalar quantization. However,\ntraditional LVQ structures are designed/optimized for uniform source\ndistributions, hence nonadaptive and suboptimal for real source distributions\nof latent code space for Neural image compression tasks. In this paper, we\npropose a novel learning method to overcome this weakness by designing the\nrate-distortion optimal lattice vector quantization (OLVQ) codebooks with\nrespect to the sample statistics of the latent features to be compressed. By\nbeing able to better fit the LVQ structures to any given latent sample\ndistribution, the proposed OLVQ method improves the rate-distortion\nperformances of the existing quantization schemes in neural image compression\nsignificantly, while retaining the amenability of uniform scalar quantization.\n","authors":["Xi Zhang","Xiaolin Wu"],"pdf_url":"https://arxiv.org/pdf/2411.16119v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2405.19644v2","updated":"2024-11-25T06:03:02Z","published":"2024-05-30T02:53:19Z","title":"EgoSurgery-Phase: A Dataset of Surgical Phase Recognition from\n  Egocentric Open Surgery Videos","summary":"  Surgical phase recognition has gained significant attention due to its\npotential to offer solutions to numerous demands of the modern operating room.\nHowever, most existing methods concentrate on minimally invasive surgery (MIS),\nleaving surgical phase recognition for open surgery understudied. This\ndiscrepancy is primarily attributed to the scarcity of publicly available open\nsurgery video datasets for surgical phase recognition. To address this issue,\nwe introduce a new egocentric open surgery video dataset for phase recognition,\nnamed EgoSurgery-Phase. This dataset comprises 15 hours of real open surgery\nvideos spanning 9 distinct surgical phases all captured using an egocentric\ncamera attached to the surgeon's head. In addition to video, the\nEgoSurgery-Phase offers eye gaze. As far as we know, it is the first real open\nsurgery video dataset for surgical phase recognition publicly available.\nFurthermore, inspired by the notable success of masked autoencoders (MAEs) in\nvideo understanding tasks (e.g., action recognition), we propose a gaze-guided\nmasked autoencoder (GGMAE). Considering the regions where surgeons' gaze\nfocuses are often critical for surgical phase recognition (e.g., surgical\nfield), in our GGMAE, the gaze information acts as an empirical semantic\nrichness prior to guiding the masking process, promoting better attention to\nsemantically rich spatial regions. GGMAE significantly improves the previous\nstate-of-the-art recognition method (6.4% in Jaccard) and the masked\nautoencoder-based method (3.1% in Jaccard) on EgoSurgery-Phase.\n","authors":["Ryo Fujii","Masashi Hatano","Hideo Saito","Hiroki Kajita"],"pdf_url":"https://arxiv.org/pdf/2405.19644v2.pdf","comment":"Early accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.03095v3","updated":"2024-11-25T05:58:10Z","published":"2024-06-05T09:36:15Z","title":"EgoSurgery-Tool: A Dataset of Surgical Tool and Hand Detection from\n  Egocentric Open Surgery Videos","summary":"  Surgical tool detection is a fundamental task for understanding egocentric\nopen surgery videos. However, detecting surgical tools presents significant\nchallenges due to their highly imbalanced class distribution, similar shapes\nand similar textures, and heavy occlusion. The lack of a comprehensive\nlarge-scale dataset compounds these challenges. In this paper, we introduce\nEgoSurgery-Tool, an extension of the existing EgoSurgery-Phase dataset, which\ncontains real open surgery videos captured using an egocentric camera attached\nto the surgeon's head, along with phase annotations. EgoSurgery-Tool has been\ndensely annotated with surgical tools and comprises over 49K surgical tool\nbounding boxes across 15 categories, constituting a large-scale surgical tool\ndetection dataset. EgoSurgery-Tool also provides annotations for hand detection\nwith over 46K hand-bounding boxes, capturing hand-object interactions that are\ncrucial for understanding activities in egocentric open surgery.\nEgoSurgery-Tool is superior to existing datasets due to its larger scale,\ngreater variety of surgical tools, more annotations, and denser scenes. We\nconduct a comprehensive analysis of EgoSurgery-Tool using nine popular object\ndetectors to assess their effectiveness in both surgical tool and hand\ndetection.\n","authors":["Ryo Fujii","Hideo Saito","Hiroki Kajita"],"pdf_url":"https://arxiv.org/pdf/2406.03095v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16110v1","updated":"2024-11-25T05:51:38Z","published":"2024-11-25T05:51:38Z","title":"FUN-AD: Fully Unsupervised Learning for Anomaly Detection with Noisy\n  Training Data","summary":"  While the mainstream research in anomaly detection has mainly followed the\none-class classification, practical industrial environments often incur noisy\ntraining data due to annotation errors or lack of labels for new or refurbished\nproducts. To address these issues, we propose a novel learning-based approach\nfor fully unsupervised anomaly detection with unlabeled and potentially\ncontaminated training data. Our method is motivated by two observations, that\ni) the pairwise feature distances between the normal samples are on average\nlikely to be smaller than those between the anomaly samples or heterogeneous\nsamples and ii) pairs of features mutually closest to each other are likely to\nbe homogeneous pairs, which hold if the normal data has smaller variance than\nthe anomaly data. Building on the first observation that nearest-neighbor\ndistances can distinguish between confident normal samples and anomalies, we\npropose a pseudo-labeling strategy using an iteratively reconstructed memory\nbank (IRMB). The second observation is utilized as a new loss function to\npromote class-homogeneity between mutually closest pairs thereby reducing the\nill-posedness of the task. Experimental results on two public industrial\nanomaly benchmarks and semantic anomaly examples validate the effectiveness of\nFUN-AD across different scenarios and anomaly-to-normal ratios. Our code is\navailable at https://github.com/HY-Vision-Lab/FUNAD.\n","authors":["Jiin Im","Yongho Son","Je Hyeong Hong"],"pdf_url":"https://arxiv.org/pdf/2411.16110v1.pdf","comment":"Accepted at WACV 2025. Supplementary material included after\n  references. 17 pages, 7 figures, 14 tables"},{"id":"http://arxiv.org/abs/2311.14262v3","updated":"2024-11-25T05:47:52Z","published":"2023-11-24T03:19:17Z","title":"ZeroPS: High-quality Cross-modal Knowledge Transfer for Zero-Shot 3D\n  Part Segmentation","summary":"  Zero-shot 3D part segmentation is a challenging and fundamental task. In this\nwork, we propose a novel pipeline, ZeroPS, which achieves high-quality\nknowledge transfer from 2D pretrained foundation models (FMs), SAM and GLIP, to\n3D object point clouds. We aim to explore the natural relationship between\nmulti-view correspondence and the FMs' prompt mechanism and build bridges on\nit. In ZeroPS, the relationship manifests as follows: 1) lifting 2D to 3D by\nleveraging co-viewed regions and SAM's prompt mechanism, 2) relating 1D classes\nto 3D parts by leveraging 2D-3D view projection and GLIP's prompt mechanism,\nand 3) enhancing prediction performance by leveraging multi-view observations.\nExtensive evaluations on the PartNetE and AKBSeg benchmarks demonstrate that\nZeroPS significantly outperforms the SOTA method across zero-shot unlabeled and\ninstance segmentation tasks. ZeroPS does not require additional training or\nfine-tuning for the FMs. ZeroPS applies to both simulated and real-world data.\nIt is hardly affected by domain shift. The project page is available at\nhttps://luis2088.github.io/ZeroPS_page.\n","authors":["Yuheng Xue","Nenglun Chen","Jun Liu","Wenyun Sun"],"pdf_url":"https://arxiv.org/pdf/2311.14262v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16106v1","updated":"2024-11-25T05:36:00Z","published":"2024-11-25T05:36:00Z","title":"UNOPose: Unseen Object Pose Estimation with an Unposed RGB-D Reference\n  Image","summary":"  Unseen object pose estimation methods often rely on CAD models or multiple\nreference views, making the onboarding stage costly. To simplify reference\nacquisition, we aim to estimate the unseen object's pose through a single\nunposed RGB-D reference image. While previous works leverage reference images\nas pose anchors to limit the range of relative pose, our scenario presents\nsignificant challenges since the relative transformation could vary across the\nentire SE(3) space. Moreover, factors like occlusion, sensor noise, and extreme\ngeometry could result in low viewpoint overlap. To address these challenges, we\npresent a novel approach and benchmark, termed UNOPose, for unseen\none-reference-based object pose estimation. Building upon a coarse-to-fine\nparadigm, UNOPose constructs an SE(3)-invariant reference frame to standardize\nobject representation despite pose and size variations. To alleviate small\noverlap across viewpoints, we recalibrate the weight of each correspondence\nbased on its predicted likelihood of being within the overlapping region.\nEvaluated on our proposed benchmark based on the BOP Challenge, UNOPose\ndemonstrates superior performance, significantly outperforming traditional and\nlearning-based methods in the one-reference setting and remaining competitive\nwith CAD-model-based methods. The code and dataset will be available.\n","authors":["Xingyu Liu","Gu Wang","Ruida Zhang","Chenyangguang Zhang","Federico Tombari","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2411.16106v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.16096v1","updated":"2024-11-25T05:15:38Z","published":"2024-11-25T05:15:38Z","title":"ENCLIP: Ensembling and Clustering-Based Contrastive Language-Image\n  Pretraining for Fashion Multimodal Search with Limited Data and Low-Quality\n  Images","summary":"  Multimodal search has revolutionized the fashion industry, providing a\nseamless and intuitive way for users to discover and explore fashion items.\nBased on their preferences, style, or specific attributes, users can search for\nproducts by combining text and image information. Text-to-image searches enable\nusers to find visually similar items or describe products using natural\nlanguage. This paper presents an innovative approach called ENCLIP, for\nenhancing the performance of the Contrastive Language-Image Pretraining (CLIP)\nmodel, specifically in Multimodal Search targeted towards the domain of fashion\nintelligence. This method focuses on addressing the challenges posed by limited\ndata availability and low-quality images. This paper proposes an algorithm that\ninvolves training and ensembling multiple instances of the CLIP model, and\nleveraging clustering techniques to group similar images together. The\nexperimental findings presented in this study provide evidence of the\neffectiveness of the methodology. This approach unlocks the potential of CLIP\nin the domain of fashion intelligence, where data scarcity and image quality\nissues are prevalent. Overall, the ENCLIP method represents a valuable\ncontribution to the field of fashion intelligence and provides a practical\nsolution for optimizing the CLIP model in scenarios with limited data and\nlow-quality images.\n","authors":["Prithviraj Purushottam Naik","Rohit Agarwal"],"pdf_url":"https://arxiv.org/pdf/2411.16096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16094v1","updated":"2024-11-25T05:02:35Z","published":"2024-11-25T05:02:35Z","title":"Very Basics of Tensors with Graphical Notations: Unfolding,\n  Calculations, and Decompositions","summary":"  Tensor network diagram (graphical notation) is a useful tool that graphically\nrepresents multiplications between multiple tensors using nodes and edges.\nUsing the graphical notation, complex multiplications between tensors can be\ndescribed simply and intuitively, and it also helps to understand the essence\nof tensor products. In fact, most of matrix/tensor products including inner\nproduct, outer product, Hadamard product, Kronecker product, and Khatri-Rao\nproduct can be written in graphical notation. These matrix/tensor operations\nare essential building blocks for the use of matrix/tensor decompositions in\nsignal processing and machine learning. The purpose of this lecture note is to\nlearn the very basics of tensors and how to represent them in mathematical\nsymbols and graphical notation. Many papers using tensors omit these detailed\ndefinitions and explanations, which can be difficult for the reader. I hope\nthis note will be of help to such readers.\n","authors":["Tatsuya Yokota"],"pdf_url":"https://arxiv.org/pdf/2411.16094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12471v2","updated":"2024-11-25T04:58:09Z","published":"2024-11-19T12:52:37Z","title":"SCIGS: 3D Gaussians Splatting from a Snapshot Compressive Image","summary":"  Snapshot Compressive Imaging (SCI) offers a possibility for capturing\ninformation in high-speed dynamic scenes, requiring efficient reconstruction\nmethod to recover scene information. Despite promising results, current deep\nlearning-based and NeRF-based reconstruction methods face challenges: 1) deep\nlearning-based reconstruction methods struggle to maintain 3D structural\nconsistency within scenes, and 2) NeRF-based reconstruction methods still face\nlimitations in handling dynamic scenes. To address these challenges, we propose\nSCIGS, a variant of 3DGS, and develop a primitive-level transformation network\nthat utilizes camera pose stamps and Gaussian primitive coordinates as\nembedding vectors. This approach resolves the necessity of camera pose in\nvanilla 3DGS and enhances multi-view 3D structural consistency in dynamic\nscenes by utilizing transformed primitives. Additionally, a high-frequency\nfilter is introduced to eliminate the artifacts generated during the\ntransformation. The proposed SCIGS is the first to reconstruct a 3D explicit\nscene from a single compressed image, extending its application to dynamic 3D\nscenes. Experiments on both static and dynamic scenes demonstrate that SCIGS\nnot only enhances SCI decoding but also outperforms current state-of-the-art\nmethods in reconstructing dynamic 3D scenes from a single compressed image. The\ncode will be made available upon publication.\n","authors":["Zixu Wang","Hao Yang","Yu Guo","Fei Wang"],"pdf_url":"https://arxiv.org/pdf/2411.12471v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16087v1","updated":"2024-11-25T04:47:53Z","published":"2024-11-25T04:47:53Z","title":"AI-Generated Image Quality Assessment Based on Task-Specific Prompt and\n  Multi-Granularity Similarity","summary":"  Recently, AI-generated images (AIGIs) created by given prompts (initial\nprompts) have garnered widespread attention. Nevertheless, due to technical\nnonproficiency, they often suffer from poor perception quality and\nText-to-Image misalignment. Therefore, assessing the perception quality and\nalignment quality of AIGIs is crucial to improving the generative model's\nperformance. Existing assessment methods overly rely on the initial prompts in\nthe task prompt design and use the same prompts to guide both perceptual and\nalignment quality evaluation, overlooking the distinctions between the two\ntasks. To address this limitation, we propose a novel quality assessment method\nfor AIGIs named TSP-MGS, which designs task-specific prompts and measures\nmulti-granularity similarity between AIGIs and the prompts. Specifically,\ntask-specific prompts are first constructed to describe perception and\nalignment quality degrees separately, and the initial prompt is introduced for\ndetailed quality perception. Then, the coarse-grained similarity between AIGIs\nand task-specific prompts is calculated, which facilitates holistic quality\nawareness. In addition, to improve the understanding of AIGI details, the\nfine-grained similarity between the image and the initial prompt is measured.\nFinally, precise quality prediction is acquired by integrating the\nmulti-granularity similarities. Experiments on the commonly used AGIQA-1K and\nAGIQA-3K benchmarks demonstrate the superiority of the proposed TSP-MGS.\n","authors":["Jili Xia","Lihuo He","Fei Gao","Kaifan Zhang","Leida Li","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2411.16087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16085v1","updated":"2024-11-25T04:36:01Z","published":"2024-11-25T04:36:01Z","title":"Cautious Optimizers: Improving Training with One Line of Code","summary":"  AdamW has been the default optimizer for transformer pretraining. For many\nyears, our community searches for faster and more stable optimizers with only\nconstraint positive outcomes. In this work, we propose a \\textbf{single-line\nmodification in Pytorch} to any momentum-based optimizer, which we rename\nCautious Optimizer, e.g. C-AdamW and C-Lion. Our theoretical result shows that\nthis modification preserves Adam's Hamiltonian function and it does not break\nthe convergence guarantee under the Lyapunov analysis. In addition, a whole new\nfamily of optimizers is revealed by our theoretical insight. Among them, we\npick the simplest one for empirical experiments, showing speed-up on Llama and\nMAE pretraining up to $1.47\\times$. Code is available at\nhttps://github.com/kyleliang919/C-Optim\n","authors":["Kaizhao Liang","Lizhang Chen","Bo Liu","Qiang Liu"],"pdf_url":"https://arxiv.org/pdf/2411.16085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16082v1","updated":"2024-11-25T04:22:33Z","published":"2024-11-25T04:22:33Z","title":"Leverage Task Context for Object Affordance Ranking","summary":"  Intelligent agents accomplish different tasks by utilizing various objects\nbased on their affordance, but how to select appropriate objects according to\ntask context is not well-explored. Current studies treat objects within the\naffordance category as equivalent, ignoring that object affordances vary in\npriority with different task contexts, hindering accurate decision-making in\ncomplex environments. To enable agents to develop a deeper understanding of the\nobjects required to perform tasks, we propose to leverage task context for\nobject affordance ranking, i.e., given image of a complex scene and the textual\ndescription of the affordance and task context, revealing task-object\nrelationships and clarifying the priority rank of detected objects. To this\nend, we propose a novel Context-embed Group Ranking Framework with task\nrelation mining module and graph group update module to deeply integrate task\ncontext and perform global relative relationship transmission. Due to the lack\nof such data, we construct the first large-scale task-oriented affordance\nranking dataset with 25 common tasks, over 50k images and more than 661k\nobjects. Experimental results demonstrate the feasibility of the task context\nbased affordance learning paradigm and the superiority of our model over\nstate-of-the-art models in the fields of saliency ranking and multimodal object\ndetection. The source code and dataset will be made available to the public.\n","authors":["Haojie Huang","Hongchen Luo","Wei Zhai","Yang Cao","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2411.16082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11178v2","updated":"2024-11-25T04:21:50Z","published":"2023-10-17T11:53:32Z","title":"FocDepthFormer: Transformer with latent LSTM for Depth Estimation from\n  Focal Stack","summary":"  Most existing methods for depth estimation from a focal stack of images\nemploy convolutional neural networks (CNNs) using 2D or 3D convolutions over a\nfixed set of images. However, their effectiveness is constrained by the local\nproperties of CNN kernels, which restricts them to process only focal stacks of\nfixed number of images during both training and inference. This limitation\nhampers their ability to generalize to stacks of arbitrary lengths. To overcome\nthese limitations, we present a novel Transformer-based network,\nFocDepthFormer, which integrates a Transformer with an LSTM module and a CNN\ndecoder. The Transformer's self-attention mechanism allows for the learning of\nmore informative spatial features by implicitly performing non-local\ncross-referencing. The LSTM module is designed to integrate representations\nacross image stacks of varying lengths. Additionally, we employ multi-scale\nconvolutional kernels in an early-stage encoder to capture low-level features\nat different degrees of focus/defocus. By incorporating the LSTM,\nFocDepthFormer can be pre-trained on large-scale monocular RGB depth estimation\ndatasets, improving visual pattern learning and reducing reliance on\ndifficult-to-obtain focal stack data. Extensive experiments on diverse focal\nstack benchmark datasets demonstrate that our model outperforms\nstate-of-the-art approaches across multiple evaluation metrics.\n","authors":["Xueyang Kang","Fengze Han","Abdur R. Fayjie","Patrick Vandewalle","Kourosh Khoshelham","Dong Gong"],"pdf_url":"https://arxiv.org/pdf/2310.11178v2.pdf","comment":"30 pages, 20 figures, Conference paper"},{"id":"http://arxiv.org/abs/2411.16080v1","updated":"2024-11-25T04:20:52Z","published":"2024-11-25T04:20:52Z","title":"Boosting 3D Object Generation through PBR Materials","summary":"  Automatic 3D content creation has gained increasing attention recently, due\nto its potential in various applications such as video games, film industry,\nand AR/VR. Recent advancements in diffusion models and multimodal models have\nnotably improved the quality and efficiency of 3D object generation given a\nsingle RGB image. However, 3D objects generated even by state-of-the-art\nmethods are still unsatisfactory compared to human-created assets. Considering\nonly textures instead of materials makes these methods encounter challenges in\nphoto-realistic rendering, relighting, and flexible appearance editing. And\nthey also suffer from severe misalignment between geometry and high-frequency\ntexture details. In this work, we propose a novel approach to boost the quality\nof generated 3D objects from the perspective of Physics-Based Rendering (PBR)\nmaterials. By analyzing the components of PBR materials, we choose to consider\nalbedo, roughness, metalness, and bump maps. For albedo and bump maps, we\nleverage Stable Diffusion fine-tuned on synthetic data to extract these values,\nwith novel usages of these fine-tuned models to obtain 3D consistent albedo UV\nand bump UV for generated objects. In terms of roughness and metalness maps, we\nadopt a semi-automatic process to provide room for interactive adjustment,\nwhich we believe is more practical. Extensive experiments demonstrate that our\nmodel is generally beneficial for various state-of-the-art generation methods,\nsignificantly boosting the quality and realism of their generated 3D objects,\nwith natural relighting effects and substantially improved geometry.\n","authors":["Yitong Wang","Xudong Xu","Li Ma","Haoran Wang","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2411.16080v1.pdf","comment":"Accepted to SIGGRAPH Asia 2024 Conference Papers"},{"id":"http://arxiv.org/abs/2411.16079v1","updated":"2024-11-25T04:11:16Z","published":"2024-11-25T04:11:16Z","title":"Debiasing Classifiers by Amplifying Bias with Latent Diffusion and Large\n  Language Models","summary":"  Neural networks struggle with image classification when biases are learned\nand misleads correlations, affecting their generalization and performance.\nPrevious methods require attribute labels (e.g. background, color) or utilizes\nGenerative Adversarial Networks (GANs) to mitigate biases. We introduce\nDiffuBias, a novel pipeline for text-to-image generation that enhances\nclassifier robustness by generating bias-conflict samples, without requiring\ntraining during the generation phase. Utilizing pretrained diffusion and image\ncaptioning models, DiffuBias generates images that challenge the biases of\nclassifiers, using the top-$K$ losses from a biased classifier ($f_B$) to\ncreate more representative data samples. This method not only debiases\neffectively but also boosts classifier generalization capabilities. To the best\nof our knowledge, DiffuBias is the first approach leveraging a stable diffusion\nmodel to generate bias-conflict samples in debiasing tasks. Our comprehensive\nexperimental evaluations demonstrate that DiffuBias achieves state-of-the-art\nperformance on benchmark datasets. We also conduct a comparative analysis of\nvarious generative models in terms of carbon emissions and energy consumption\nto highlight the significance of computational efficiency.\n","authors":["Donggeun Ko","Dongjun Lee","Namjun Park","Wonkyeong Shim","Jaekwang Kim"],"pdf_url":"https://arxiv.org/pdf/2411.16079v1.pdf","comment":"8 pages + Appendix"},{"id":"http://arxiv.org/abs/2402.13575v2","updated":"2024-11-25T04:07:14Z","published":"2024-02-21T07:15:16Z","title":"Flexible Physical Camouflage Generation Based on a Differential Approach","summary":"  This study introduces a novel approach to neural rendering, specifically\ntailored for adversarial camouflage, within an extensive 3D rendering\nframework. Our method, named FPA, goes beyond traditional techniques by\nfaithfully simulating lighting conditions and material variations, ensuring a\nnuanced and realistic representation of textures on a 3D target. To achieve\nthis, we employ a generative approach that learns adversarial patterns from a\ndiffusion model. This involves incorporating a specially designed adversarial\nloss and covert constraint loss to guarantee the adversarial and covert nature\nof the camouflage in the physical world. Furthermore, we showcase the\neffectiveness of the proposed camouflage in sticker mode, demonstrating its\nability to cover the target without compromising adversarial information.\nThrough empirical and physical experiments, FPA exhibits strong performance in\nterms of attack success rate and transferability. Additionally, the designed\nsticker-mode camouflage, coupled with a concealment constraint, adapts to the\nenvironment, yielding diverse styles of texture. Our findings highlight the\nversatility and efficacy of the FPA approach in adversarial camouflage\napplications.\n","authors":["Yang Li","Wenyi Tan","Chenxing Zhao","Shuangju Zhou","Xinkai Liang","Quan Pan"],"pdf_url":"https://arxiv.org/pdf/2402.13575v2.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2411.16645v1","updated":"2024-11-25T18:27:50Z","published":"2024-11-25T18:27:50Z","title":"Recommender Systems for Good (RS4Good): Survey of Use Cases and a Call\n  to Action for Research that Matters","summary":"  In the area of recommender systems, the vast majority of research efforts is\nspent on developing increasingly sophisticated recommendation models, also\nusing increasingly more computational resources. Unfortunately, most of these\nresearch efforts target a very small set of application domains, mostly\ne-commerce and media recommendation. Furthermore, many of these models are\nnever evaluated with users, let alone put into practice. The scientific,\neconomic and societal value of much of these efforts by scholars therefore\nremains largely unclear. To achieve a stronger positive impact resulting from\nthese efforts, we posit that we as a research community should more often\naddress use cases where recommender systems contribute to societal good\n(RS4Good). In this opinion piece, we first discuss a number of examples where\nthe use of recommender systems for problems of societal concern has been\nsuccessfully explored in the literature. We then proceed by outlining a\nparadigmatic shift that is needed to conduct successful RS4Good research, where\nthe key ingredients are interdisciplinary collaborations and longitudinal\nevaluation approaches with humans in the loop.\n","authors":["Dietmar Jannach","Alan Said","Marko Tkalčič","Markus Zanker"],"pdf_url":"https://arxiv.org/pdf/2411.16645v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16515v3","updated":"2024-11-25T18:11:18Z","published":"2023-11-25T14:24:49Z","title":"Word4Per: Zero-shot Composed Person Retrieval","summary":"  Searching for specific person has great social benefits and security value,\nand it often involves a combination of visual and textual information.\nConventional person retrieval methods, whether image-based or text-based,\nusually fall short in effectively harnessing both types of information, leading\nto the loss of accuracy. In this paper, a whole new task called Composed Person\nRetrieval (CPR) is proposed to jointly utilize both image and text information\nfor target person retrieval. However, the supervised CPR requires very costly\nmanual annotation dataset, while there are currently no available resources. To\nmitigate this issue, we firstly introduce the Zero-shot Composed Person\nRetrieval (ZS-CPR), which leverages existing domain-related data to resolve the\nCPR problem without expensive annotations. Secondly, to learn ZS-CPR model, we\npropose a two-stage learning framework, Word4Per, where a lightweight Textual\nInversion Network (TINet) and a text-based person retrieval model based on\nfine-tuned Contrastive Language-Image Pre-training (CLIP) network are learned\nwithout utilizing any CPR data. Thirdly, a finely annotated Image-Text Composed\nPerson Retrieval (ITCPR) dataset is built as the benchmark to assess the\nperformance of the proposed Word4Per framework. Extensive experiments under\nboth Rank-1 and mAP demonstrate the effectiveness of Word4Per for the ZS-CPR\ntask, surpassing the comparative methods by over 10\\%. The code and ITCPR\ndataset will be publicly available at\nhttps://github.com/Delong-liu-bupt/Word4Per.\n","authors":["Delong Liu","Haiwen Li","Zhicheng Zhao","Fei Su","Yuan Dong"],"pdf_url":"https://arxiv.org/pdf/2311.16515v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07610v2","updated":"2024-11-25T17:01:53Z","published":"2024-10-10T04:54:37Z","title":"CSA: Data-efficient Mapping of Unimodal Features to Multimodal Features","summary":"  Multimodal encoders like CLIP excel in tasks such as zero-shot image\nclassification and cross-modal retrieval. However, they require excessive\ntraining data. We propose canonical similarity analysis (CSA), which uses two\nunimodal encoders to replicate multimodal encoders using limited data. CSA maps\nunimodal features into a multimodal space, using a new similarity score to\nretain only the multimodal information. CSA only involves the inference of\nunimodal encoders and a cubic-complexity matrix decomposition, eliminating the\nneed for extensive GPU-based model training. Experiments show that CSA\noutperforms CLIP while requiring $300,000\\times$ fewer multimodal data pairs\nand $6\\times$ fewer unimodal data for ImageNet classification and\nmisinformative news captions detection. CSA surpasses the state-of-the-art\nmethod to map unimodal features to multimodal features. We also demonstrate the\nability of CSA with modalities beyond image and text, paving the way for future\nmodality pairs with limited paired multimodal data but abundant unpaired\nunimodal data, such as lidar and text.\n","authors":["Po-han Li","Sandeep P. Chinchali","Ufuk Topcu"],"pdf_url":"https://arxiv.org/pdf/2410.07610v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16408v1","updated":"2024-11-25T14:14:25Z","published":"2024-11-25T14:14:25Z","title":"Low-Data Classification of Historical Music Manuscripts: A Few-Shot\n  Learning Approach","summary":"  In this paper, we explore the intersection of technology and cultural\npreservation by developing a self-supervised learning framework for the\nclassification of musical symbols in historical manuscripts. Optical Music\nRecognition (OMR) plays a vital role in digitising and preserving musical\nheritage, but historical documents often lack the labelled data required by\ntraditional methods. We overcome this challenge by training a neural-based\nfeature extractor on unlabelled data, enabling effective classification with\nminimal samples. Key contributions include optimising crop preprocessing for a\nself-supervised Convolutional Neural Network and evaluating classification\nmethods, including SVM, multilayer perceptrons, and prototypical networks. Our\nexperiments yield an accuracy of 87.66\\%, showcasing the potential of AI-driven\nmethods to ensure the survival of historical music for future generations\nthrough advanced digital archiving techniques.\n","authors":["Elona Shatri","Daniel Raymond","George Fazekas"],"pdf_url":"https://arxiv.org/pdf/2411.16408v1.pdf","comment":"6 pages, The Sixth IEEE international conference on Image Processing\n  Applications and Systems"},{"id":"http://arxiv.org/abs/2409.15690v2","updated":"2024-11-25T13:05:39Z","published":"2024-09-24T03:06:25Z","title":"A Survey of Stance Detection on Social Media: New Directions and\n  Perspectives","summary":"  In modern digital environments, users frequently express opinions on\ncontentious topics, providing a wealth of information on prevailing attitudes.\nThe systematic analysis of these opinions offers valuable insights for\ndecision-making in various sectors, including marketing and politics. As a\nresult, stance detection has emerged as a crucial subfield within affective\ncomputing, enabling the automatic detection of user stances in social media\nconversations and providing a nuanced understanding of public sentiment on\ncomplex issues. Recent years have seen a surge of research interest in\ndeveloping effective stance detection methods, with contributions from multiple\ncommunities, including natural language processing, web science, and social\ncomputing. This paper provides a comprehensive survey of stance detection\ntechniques on social media, covering task definitions, datasets, approaches,\nand future works. We review traditional stance detection models, as well as\nstate-of-the-art methods based on large language models, and discuss their\nstrengths and limitations. Our survey highlights the importance of stance\ndetection in understanding public opinion and sentiment, and identifies gaps in\ncurrent research. We conclude by outlining potential future directions for\nstance detection on social media, including the need for more robust and\ngeneralizable models, and the importance of addressing emerging challenges such\nas multi-modal stance detection and stance detection in low-resource languages.\n","authors":["Bowen Zhang","Genan Dai","Fuqiang Niu","Nan Yin","Xiaomao Fan","Senzhang Wang","Xiaochun Cao","Hu Huang"],"pdf_url":"https://arxiv.org/pdf/2409.15690v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16160v1","updated":"2024-11-25T07:36:20Z","published":"2024-11-25T07:36:20Z","title":"Stop Playing the Guessing Game! Target-free User Simulation for\n  Evaluating Conversational Recommender Systems","summary":"  Recent approaches in Conversational Recommender Systems (CRSs) have tried to\nsimulate real-world users engaging in conversations with CRSs to create more\nrealistic testing environments that reflect the complexity of human-agent\ndialogue. Despite the significant advancements, reliably evaluating the\ncapability of CRSs to elicit user preferences still faces a significant\nchallenge. Existing evaluation metrics often rely on target-biased user\nsimulators that assume users have predefined preferences, leading to\ninteractions that devolve into simplistic guessing game. These simulators\ntypically guide the CRS toward specific target items based on fixed attributes,\nlimiting the dynamic exploration of user preferences and struggling to capture\nthe evolving nature of real-user interactions. Additionally, current evaluation\nmetrics are predominantly focused on single-turn recall of target items,\nneglecting the intermediate processes of preference elicitation. To address\nthis, we introduce PEPPER, a novel CRS evaluation protocol with target-free\nuser simulators constructed from real-user interaction histories and reviews.\nPEPPER enables realistic user-CRS dialogues without falling into simplistic\nguessing games, allowing users to gradually discover their preferences through\nenriched interactions, thereby providing a more accurate and reliable\nassessment of the CRS's ability to elicit personal preferences. Furthermore,\nPEPPER presents detailed measures for comprehensively evaluating the preference\nelicitation capabilities of CRSs, encompassing both quantitative and\nqualitative measures that capture four distinct aspects of the preference\nelicitation process. Through extensive experiments, we demonstrate the validity\nof PEPPER as a simulation environment and conduct a thorough analysis of how\neffectively existing CRSs perform in preference elicitation and recommendation.\n","authors":["Sunghwan Kim","Tongyoung Kim","Kwangwook Seo","Jinyoung Yeo","Dongha Lee"],"pdf_url":"https://arxiv.org/pdf/2411.16160v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2411.16133v1","updated":"2024-11-25T06:48:38Z","published":"2024-11-25T06:48:38Z","title":"Context Awareness Gate For Retrieval Augmented Generation","summary":"  Retrieval Augmented Generation (RAG) has emerged as a widely adopted approach\nto mitigate the limitations of large language models (LLMs) in answering\ndomain-specific questions. Previous research has predominantly focused on\nimproving the accuracy and quality of retrieved data chunks to enhance the\noverall performance of the generation pipeline. However, despite ongoing\nadvancements, the critical issue of retrieving irrelevant information -- which\ncan impair the ability of the model to utilize its internal knowledge\neffectively -- has received minimal attention. In this work, we investigate the\nimpact of retrieving irrelevant information in open-domain question answering,\nhighlighting its significant detrimental effect on the quality of LLM outputs.\nTo address this challenge, we propose the Context Awareness Gate (CAG)\narchitecture, a novel mechanism that dynamically adjusts the LLMs' input prompt\nbased on whether the user query necessitates external context retrieval.\nAdditionally, we introduce the Vector Candidates method, a core mathematical\ncomponent of CAG that is statistical, LLM-independent, and highly scalable. We\nfurther examine the distributions of relationships between contexts and\nquestions, presenting a statistical analysis of these distributions. This\nanalysis can be leveraged to enhance the context retrieval process in Retrieval\nAugmented Generation (RAG) systems.\n","authors":["Mohammad Hassan Heydari","Arshia Hemmat","Erfan Naman","Afsaneh Fatemi"],"pdf_url":"https://arxiv.org/pdf/2411.16133v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16122v1","updated":"2024-11-25T06:14:20Z","published":"2024-11-25T06:14:20Z","title":"Ensemble Learning via Knowledge Transfer for CTR Prediction","summary":"  Click-through rate (CTR) prediction plays a critical role in recommender\nsystems and web searches. While many existing methods utilize ensemble learning\nto improve model performance, they typically limit the ensemble to two or three\nsub-networks, with little exploration of larger ensembles. In this paper, we\ninvestigate larger ensemble networks and find three inherent limitations in\ncommonly used ensemble learning method: (1) performance degradation with more\nnetworks; (2) sharp decline and high variance in sub-network performance; (3)\nlarge discrepancies between sub-network and ensemble predictions.\n  To simultaneously address the above limitations, this paper investigates\npotential solutions from the perspectives of Knowledge Distillation (KD) and\nDeep Mutual Learning (DML). Based on the empirical performance of these\nmethods, we combine them to propose a novel model-agnostic Ensemble Knowledge\nTransfer Framework (EKTF). Specifically, we employ the collective\ndecision-making of the students as an abstract teacher to guide each student\n(sub-network) towards more effective learning. Additionally, we encourage\nmutual learning among students to enable knowledge acquisition from different\nviews. To address the issue of balancing the loss hyperparameters, we design a\nnovel examination mechanism to ensure tailored teaching from teacher-to-student\nand selective learning in peer-to-peer. Experimental results on five real-world\ndatasets demonstrate the effectiveness and compatibility of EKTF. The code,\nrunning logs, and detailed hyperparameter configurations are available at:\nhttps://github.com/salmon1802/EKTF.\n","authors":["Honghao Li","Yiwen Zhang","Yi Zhang","Lei Sang"],"pdf_url":"https://arxiv.org/pdf/2411.16122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11225v2","updated":"2024-11-25T06:07:41Z","published":"2024-11-18T01:30:34Z","title":"Online Item Cold-Start Recommendation with Popularity-Aware\n  Meta-Learning","summary":"  With the rise of e-commerce and short videos, online recommender systems that\ncan capture users' interests and update new items in real-time play an\nincreasingly important role. In both online and offline recommendation, the\ncold-start problem due to interaction sparsity has been affecting the\nrecommendation effect of cold-start items, which is also known as the long-tail\nproblem of item distribution. Many cold-start scheme based on fine-tuning or\nknowledge transferring shows excellent performance on offline recommendation.\nYet, these schemes are infeasible for online recommendation on streaming data\npipelines due to different training method, computational overhead and time\nconstraints. Inspired by the above questions, we propose a model-agnostic\nrecommendation algorithm called Popularity-Aware Meta-learning (PAM), to\naddress the item cold-start problem under streaming data settings. PAM divides\nthe incoming data into different meta-learning tasks by predefined item\npopularity thresholds. The model can distinguish and reweight behavior-related\nand content-related features in each task based on their different roles in\ndifferent popularity levels, thus adapting to recommendations for cold-start\nsamples. These task-fixing design significantly reduces additional computation\nand storage costs compared to offline methods. Furthermore, PAM also introduced\ndata augmentation and an additional self-supervised loss specifically designed\nfor low-popularity tasks, leveraging insights from high-popularity samples.\nThis approach effectively mitigates the issue of inadequate supervision due to\nthe scarcity of cold-start samples. Experimental results across multiple public\ndatasets demonstrate the superiority of our approach over other baseline\nmethods in addressing cold-start challenges in online streaming data scenarios.\n","authors":["Yunze Luo","Yuezihan Jiang","Yinjie Jiang","Gaode Chen","Jingchi Wang","Kaigui Bian","Peiyi Li","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.11225v2.pdf","comment":"11 pages, 4 figures, to be published in KDD '25"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2411.16680v1","updated":"2024-11-25T18:59:50Z","published":"2024-11-25T18:59:50Z","title":"Quark: Real-time, High-resolution, and General Neural View Synthesis","summary":"  We present a novel neural algorithm for performing high-quality,\nhigh-resolution, real-time novel view synthesis. From a sparse set of input RGB\nimages or videos streams, our network both reconstructs the 3D scene and\nrenders novel views at 1080p resolution at 30fps on an NVIDIA A100. Our\nfeed-forward network generalizes across a wide variety of datasets and scenes\nand produces state-of-the-art quality for a real-time method. Our quality\napproaches, and in some cases surpasses, the quality of some of the top offline\nmethods. In order to achieve these results we use a novel combination of\nseveral key concepts, and tie them together into a cohesive and effective\nalgorithm. We build on previous works that represent the scene using\nsemi-transparent layers and use an iterative learned render-and-refine approach\nto improve those layers. Instead of flat layers, our method reconstructs\nlayered depth maps (LDMs) that efficiently represent scenes with complex depth\nand occlusions. The iterative update steps are embedded in a multi-scale,\nUNet-style architecture to perform as much compute as possible at reduced\nresolution. Within each update step, to better aggregate the information from\nmultiple input views, we use a specialized Transformer-based network component.\nThis allows the majority of the per-input image processing to be performed in\nthe input image space, as opposed to layer space, further increasing\nefficiency. Finally, due to the real-time nature of our reconstruction and\nrendering, we dynamically create and discard the internal 3D geometry for each\nframe, generating the LDM for each view. Taken together, this produces a novel\nand effective algorithm for view synthesis. Through extensive evaluation, we\ndemonstrate that we achieve state-of-the-art quality at real-time rates.\nProject page: https://quark-3d.github.io/\n","authors":["John Flynn","Michael Broxton","Lukas Murmann","Lucy Chai","Matthew DuVall","Clément Godard","Kathryn Heal","Srinivas Kaza","Stephen Lombardi","Xuan Luo","Supreeth Achar","Kira Prabhu","Tiancheng Sun","Lynn Tsai","Ryan Overbeck"],"pdf_url":"https://arxiv.org/pdf/2411.16680v1.pdf","comment":"SIGGRAPH Asia 2024 camera ready version; project page\n  https://quark-3d.github.io/"},{"id":"http://arxiv.org/abs/2402.14081v3","updated":"2024-11-25T18:57:35Z","published":"2024-02-21T19:10:08Z","title":"Motion Code: Robust Time Series Classification and Forecasting via\n  Sparse Variational Multi-Stochastic Processes Learning","summary":"  Despite extensive research, time series classification and forecasting on\nnoisy data remain highly challenging. The main difficulties lie in finding\nsuitable mathematical concepts to describe time series and effectively separate\nnoise from the true signals. Unlike traditional methods treating time series as\nstatic vectors or fixed sequences, we propose a novel framework that views each\ntime series, regardless of length, as a realization of a continuous-time\nstochastic process. This mathematical approach captures dependencies across\ntimestamps and detects hidden, time-varying signals within the noise. However,\nreal-world data often involves multiple distinct dynamics, making it\ninsufficient to model the entire process with a single stochastic model. To\naddress this, we assign each dynamic a unique signature vector and introduce\nthe concept of \"most informative timestamps\" to infer a sparse approximation of\nthe individual dynamics from these vectors. The resulting model, called Motion\nCode, includes parameters that fully capture diverse underlying dynamics in an\nintegrated manner, enabling simultaneous classification and forecasting of time\nseries. Extensive experiments on noisy datasets, including real-world\nParkinson's disease sensor tracking, demonstrate Motion Code's strong\nperformance against established benchmarks for time series classification and\nforecasting.\n","authors":["Chandrajit Bajaj","Minh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2402.14081v3.pdf","comment":"20 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2401.09622v3","updated":"2024-11-25T18:55:38Z","published":"2024-01-17T22:23:29Z","title":"Is Hyper-Parameter Optimization Different for Software Analytics?","summary":"  Yes. SE data can have \"smoother\" boundaries between classes (compared to\ntraditional AI data sets). To be more precise, the magnitude of the second\nderivative of the loss function found in SE data is typically much smaller. A\nnew hyper-parameter optimizer, called SMOOTHIE, can exploit this idiosyncrasy\nof SE data. We compare SMOOTHIE and a state-of-the-art AI hyper-parameter\noptimizer on three tasks: (a) GitHub issue lifetime prediction (b) detecting\nstatic code warnings false alarm; (c) defect prediction. For completeness, we\nalso show experiments on some standard AI datasets. SMOOTHIE runs faster and\npredicts better on the SE data--but ties on non-SE data with the AI tool. Hence\nwe conclude that SE data can be different to other kinds of data; and those\ndifferences mean that we should use different kinds of algorithms for our data.\nTo support open science and other researchers working in this area, all our\nscripts and datasets are available on-line at\nhttps://github.com/yrahul3910/smoothness-hpo/.\n","authors":["Rahul Yedida","Tim Menzies"],"pdf_url":"https://arxiv.org/pdf/2401.09622v3.pdf","comment":"v3, major revisions"},{"id":"http://arxiv.org/abs/2411.16666v1","updated":"2024-11-25T18:53:37Z","published":"2024-11-25T18:53:37Z","title":"CatNet: Effective FDR Control in LSTM with Gaussian Mirrors and SHAP\n  Feature Importance","summary":"  We introduce CatNet, an algorithm that effectively controls False Discovery\nRate (FDR) and selects significant features in LSTM with the Gaussian Mirror\n(GM) method. To evaluate the feature importance of LSTM in time series, we\nintroduce a vector of the derivative of the SHapley Additive exPlanations\n(SHAP) to measure feature importance. We also propose a new kernel-based\ndependence measure to avoid multicollinearity in the GM algorithm, to make a\nrobust feature selection with controlled FDR. We use simulated data to evaluate\nCatNet's performance in both linear models and LSTM models with different link\nfunctions. The algorithm effectively controls the FDR while maintaining a high\nstatistical power in all cases. We also evaluate the algorithm's performance in\ndifferent low-dimensional and high-dimensional cases, demonstrating its\nrobustness in various input dimensions. To evaluate CatNet's performance in\nreal world applications, we construct a multi-factor investment portfolio to\nforecast the prices of S\\&P 500 index components. The results demonstrate that\nour model achieves superior predictive accuracy compared to traditional LSTM\nmodels without feature selection and FDR control. Additionally, CatNet\neffectively captures common market-driving features, which helps informed\ndecision-making in financial markets by enhancing the interpretability of\npredictions. Our study integrates of the Gaussian Mirror algorithm with LSTM\nmodels for the first time, and introduces SHAP values as a new feature\nimportance metric for FDR control methods, marking a significant advancement in\nfeature selection and error control for neural networks.\n","authors":["Jiaan Han","Junxiao Chen","Yanzhe Fu"],"pdf_url":"https://arxiv.org/pdf/2411.16666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16663v1","updated":"2024-11-25T18:48:15Z","published":"2024-11-25T18:48:15Z","title":"Gaussian Process Priors for Boundary Value Problems of Linear Partial\n  Differential Equations","summary":"  Solving systems of partial differential equations (PDEs) is a fundamental\ntask in computational science, traditionally addressed by numerical solvers.\nRecent advancements have introduced neural operators and physics-informed\nneural networks (PINNs) to tackle PDEs, achieving reduced computational costs\nat the expense of solution quality and accuracy. Gaussian processes (GPs) have\nalso been applied to linear PDEs, with the advantage of always yielding precise\nsolutions. In this work, we propose Boundary Ehrenpreis-Palamodov Gaussian\nProcesses (B-EPGPs), a novel framework for constructing GP priors that satisfy\nboth general systems of linear PDEs with constant coefficients and linear\nboundary conditions. We explicitly construct GP priors for representative PDE\nsystems with practical boundary conditions. Formal proofs of correctness are\nprovided and empirical results demonstrating significant accuracy improvements\nover state-of-the-art neural operator approaches.\n","authors":["Jianle iHuang","Marc Härkönen","Markus Lange-Hegermann","Bogdan Raiţă"],"pdf_url":"https://arxiv.org/pdf/2411.16663v1.pdf","comment":"25 pages, 19 figures. Code available at\n  $\\href{https://github.com/Jimmy000207/Boundary-EPGP}{\\text{this https URL}}$.\n  The paper and all ancillary files are released under CC-BY"},{"id":"http://arxiv.org/abs/2411.16658v1","updated":"2024-11-25T18:42:13Z","published":"2024-11-25T18:42:13Z","title":"Fast training of large kernel models with delayed projections","summary":"  Classical kernel machines have historically faced significant challenges in\nscaling to large datasets and model sizes--a key ingredient that has driven the\nsuccess of neural networks. In this paper, we present a new methodology for\nbuilding kernel machines that can scale efficiently with both data size and\nmodel size. Our algorithm introduces delayed projections to Preconditioned\nStochastic Gradient Descent (PSGD) allowing the training of much larger models\nthan was previously feasible, pushing the practical limits of kernel-based\nlearning. We validate our algorithm, EigenPro4, across multiple datasets,\ndemonstrating drastic training speed up over the existing methods while\nmaintaining comparable or better classification accuracy.\n","authors":["Amirhesam Abedsoltan","Siyuan Ma","Parthe Pandit","Mikhail Belkin"],"pdf_url":"https://arxiv.org/pdf/2411.16658v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2302.02605"},{"id":"http://arxiv.org/abs/2411.16646v1","updated":"2024-11-25T18:28:26Z","published":"2024-11-25T18:28:26Z","title":"Self-Generated Critiques Boost Reward Modeling for Language Models","summary":"  Reward modeling is crucial for aligning large language models (LLMs) with\nhuman preferences, especially in reinforcement learning from human feedback\n(RLHF). However, current reward models mainly produce scalar scores and\nstruggle to incorporate critiques in a natural language format. We hypothesize\nthat predicting both critiques and the scalar reward would improve reward\nmodeling ability. Motivated by this, we propose Critic-RM, a framework that\nimproves reward models using self-generated critiques without extra\nsupervision. Critic-RM employs a two-stage process: generating and filtering\nhigh-quality critiques, followed by joint fine-tuning on reward prediction and\ncritique generation. Experiments across benchmarks show that Critic-RM improves\nreward modeling accuracy by 3.7%-7.3% compared to standard reward models and\nLLM judges, demonstrating strong performance and data efficiency. Additional\nstudies further validate the effectiveness of generated critiques in rectifying\nflawed reasoning steps with 2.5%-3.2% gains in improving reasoning accuracy.\n","authors":["Yue Yu","Zhengxing Chen","Aston Zhang","Liang Tan","Chenguang Zhu","Richard Yuanzhe Pang","Yundi Qian","Xuewei Wang","Suchin Gururangan","Chao Zhang","Melanie Kambadur","Dhruv Mahajan","Rui Hou"],"pdf_url":"https://arxiv.org/pdf/2411.16646v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2411.16645v1","updated":"2024-11-25T18:27:50Z","published":"2024-11-25T18:27:50Z","title":"Recommender Systems for Good (RS4Good): Survey of Use Cases and a Call\n  to Action for Research that Matters","summary":"  In the area of recommender systems, the vast majority of research efforts is\nspent on developing increasingly sophisticated recommendation models, also\nusing increasingly more computational resources. Unfortunately, most of these\nresearch efforts target a very small set of application domains, mostly\ne-commerce and media recommendation. Furthermore, many of these models are\nnever evaluated with users, let alone put into practice. The scientific,\neconomic and societal value of much of these efforts by scholars therefore\nremains largely unclear. To achieve a stronger positive impact resulting from\nthese efforts, we posit that we as a research community should more often\naddress use cases where recommender systems contribute to societal good\n(RS4Good). In this opinion piece, we first discuss a number of examples where\nthe use of recommender systems for problems of societal concern has been\nsuccessfully explored in the literature. We then proceed by outlining a\nparadigmatic shift that is needed to conduct successful RS4Good research, where\nthe key ingredients are interdisciplinary collaborations and longitudinal\nevaluation approaches with humans in the loop.\n","authors":["Dietmar Jannach","Alan Said","Marko Tkalčič","Markus Zanker"],"pdf_url":"https://arxiv.org/pdf/2411.16645v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16644v1","updated":"2024-11-25T18:27:39Z","published":"2024-11-25T18:27:39Z","title":"Exploring Discrete Flow Matching for 3D De Novo Molecule Generation","summary":"  Deep generative models that produce novel molecular structures have the\npotential to facilitate chemical discovery. Flow matching is a recently\nproposed generative modeling framework that has achieved impressive performance\non a variety of tasks including those on biomolecular structures. The seminal\nflow matching framework was developed only for continuous data. However, de\nnovo molecular design tasks require generating discrete data such as atomic\nelements or sequences of amino acid residues. Several discrete flow matching\nmethods have been proposed recently to address this gap. In this work we\nbenchmark the performance of existing discrete flow matching methods for 3D de\nnovo small molecule generation and provide explanations of their differing\nbehavior. As a result we present FlowMol-CTMC, an open-source model that\nachieves state of the art performance for 3D de novo design with fewer\nlearnable parameters than existing methods. Additionally, we propose the use of\nmetrics that capture molecule quality beyond local chemical valency constraints\nand towards higher-order structural motifs. These metrics show that even though\nbasic constraints are satisfied, the models tend to produce unusual and\npotentially problematic functional groups outside of the training data\ndistribution. Code and trained models for reproducing this work are available\nat \\url{https://github.com/dunni3/FlowMol}.\n","authors":["Ian Dunn","David R. Koes"],"pdf_url":"https://arxiv.org/pdf/2411.16644v1.pdf","comment":"Presented at the NeurIPS 2024 Machine Learning for Structural Biology\n  Workshop"},{"id":"http://arxiv.org/abs/2411.16627v1","updated":"2024-11-25T18:03:50Z","published":"2024-11-25T18:03:50Z","title":"Inference-Time Policy Steering through Human Interactions","summary":"  Generative policies trained with human demonstrations can autonomously\naccomplish multimodal, long-horizon tasks. However, during inference, humans\nare often removed from the policy execution loop, limiting the ability to guide\na pre-trained policy towards a specific sub-goal or trajectory shape among\nmultiple predictions. Naive human intervention may inadvertently exacerbate\ndistribution shift, leading to constraint violations or execution failures. To\nbetter align policy output with human intent without inducing\nout-of-distribution errors, we propose an Inference-Time Policy Steering (ITPS)\nframework that leverages human interactions to bias the generative sampling\nprocess, rather than fine-tuning the policy on interaction data. We evaluate\nITPS across three simulated and real-world benchmarks, testing three forms of\nhuman interaction and associated alignment distance metrics. Among six sampling\nstrategies, our proposed stochastic sampling with diffusion policy achieves the\nbest trade-off between alignment and distribution shift. Videos are available\nat https://yanweiw.github.io/itps/.\n","authors":["Yanwei Wang","Lirui Wang","Yilun Du","Balakumar Sundaralingam","Xuning Yang","Yu-Wei Chao","Claudia Perez-D'Arpino","Dieter Fox","Julie Shah"],"pdf_url":"https://arxiv.org/pdf/2411.16627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.10740v3","updated":"2024-11-25T18:03:23Z","published":"2023-04-21T04:29:05Z","title":"Multi-Modal Deep Learning for Credit Rating Prediction Using Text and\n  Numerical Data Streams","summary":"  Knowing which factors are significant in credit rating assignment leads to\nbetter decision-making. However, the focus of the literature thus far has been\nmostly on structured data, and fewer studies have addressed unstructured or\nmulti-modal datasets. In this paper, we present an analysis of the most\neffective architectures for the fusion of deep learning models for the\nprediction of company credit rating classes, by using structured and\nunstructured datasets of different types. In these models, we tested different\ncombinations of fusion strategies with different deep learning models,\nincluding CNN, LSTM, GRU, and BERT. We studied data fusion strategies in terms\nof level (including early and intermediate fusion) and techniques (including\nconcatenation and cross-attention). Our results show that a CNN-based\nmulti-modal model with two fusion strategies outperformed other multi-modal\ntechniques. In addition, by comparing simple architectures with more complex\nones, we found that more sophisticated deep learning models do not necessarily\nproduce the highest performance; however, if attention-based models are\nproducing the best results, cross-attention is necessary as a fusion strategy.\nFinally, our comparison of rating agencies on short-, medium-, and long-term\nperformance shows that Moody's credit ratings outperform those of other\nagencies like Standard & Poor's and Fitch Ratings.\n","authors":["Mahsa Tavakoli","Rohitash Chandra","Fengrui Tian","Cristián Bravo"],"pdf_url":"https://arxiv.org/pdf/2304.10740v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16615v1","updated":"2024-11-25T17:54:29Z","published":"2024-11-25T17:54:29Z","title":"Graph Pooling with Local Cluster Selection","summary":"  Graph poolings in GNNs are a family of operations which take graphs as inputs\nand produce coarsened graphs as output. Modern graph poolings are trainable and\nclosely related to GNNs, which learn to pool graphs under different\nassumptions. Though there are various assumptions, the procedure of generating\npooled graphs is relatively similar and limited. This work formalizes a novel\nprocedure of pooling graphs, along with a graph pooling approach for average\nsituations.\n","authors":["Yizhu Chen"],"pdf_url":"https://arxiv.org/pdf/2411.16615v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.19631v2","updated":"2024-11-25T17:51:33Z","published":"2024-10-25T15:34:03Z","title":"Efficient Biological Data Acquisition through Inference Set Design","summary":"  In drug discovery, highly automated high-throughput laboratories are used to\nscreen a large number of compounds in search of effective drugs. These\nexperiments are expensive, so one might hope to reduce their cost by\nexperimenting on a subset of the compounds, and predicting the outcomes of the\nremaining experiments. In this work, we model this scenario as a sequential\nsubset selection problem: we aim to select the smallest set of candidates in\norder to achieve some desired level of accuracy for the system as a whole. Our\nkey observation is that, if there is heterogeneity in the difficulty of the\nprediction problem across the input space, selectively obtaining the labels for\nthe hardest examples in the acquisition pool will leave only the relatively\neasy examples to remain in the inference set, leading to better overall system\nperformance. We call this mechanism inference set design, and propose the use\nof a confidence-based active learning solution to prune out these challenging\nexamples. Our algorithm includes an explicit stopping criterion that stops\nrunning the experiments when it is sufficiently confident that the system has\nreached the target performance. Our empirical studies on image and molecular\ndatasets, as well as a real-world large-scale biological assay, show that\nactive learning for inference set design leads to significant reduction in\nexperimental cost while retaining high system performance.\n","authors":["Ihor Neporozhnii","Julien Roy","Emmanuel Bengio","Jason Hartford"],"pdf_url":"https://arxiv.org/pdf/2410.19631v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15098v2","updated":"2024-11-25T17:46:35Z","published":"2024-11-22T17:55:15Z","title":"OminiControl: Minimal and Universal Control for Diffusion Transformer","summary":"  In this paper, we introduce OminiControl, a highly versatile and\nparameter-efficient framework that integrates image conditions into pre-trained\nDiffusion Transformer (DiT) models. At its core, OminiControl leverages a\nparameter reuse mechanism, enabling the DiT to encode image conditions using\nitself as a powerful backbone and process them with its flexible multi-modal\nattention processors. Unlike existing methods, which rely heavily on additional\nencoder modules with complex architectures, OminiControl (1) effectively and\nefficiently incorporates injected image conditions with only ~0.1% additional\nparameters, and (2) addresses a wide range of image conditioning tasks in a\nunified manner, including subject-driven generation and spatially-aligned\nconditions such as edges, depth, and more. Remarkably, these capabilities are\nachieved by training on images generated by the DiT itself, which is\nparticularly beneficial for subject-driven generation. Extensive evaluations\ndemonstrate that OminiControl outperforms existing UNet-based and DiT-adapted\nmodels in both subject-driven and spatially-aligned conditional generation.\nAdditionally, we release our training dataset, Subjects200K, a diverse\ncollection of over 200,000 identity-consistent images, along with an efficient\ndata synthesis pipeline to advance research in subject-consistent generation.\n","authors":["Zhenxiong Tan","Songhua Liu","Xingyi Yang","Qiaochu Xue","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2411.15098v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10254v2","updated":"2024-11-25T17:36:19Z","published":"2024-11-15T15:02:35Z","title":"Uncertainty in Supply Chain Digital Twins: A Quantum-Classical Hybrid\n  Approach","summary":"  This study investigates uncertainty quantification (UQ) using\nquantum-classical hybrid machine learning (ML) models for applications in\ncomplex and dynamic fields, such as attaining resiliency in supply chain\ndigital twins and financial risk assessment. Although quantum feature\ntransformations have been integrated into ML models for complex data tasks, a\ngap exists in determining their impact on UQ within their hybrid architectures\n(quantum-classical approach). This work applies existing UQ techniques for\ndifferent models within a hybrid framework, examining how quantum feature\ntransformation affects uncertainty propagation. Increasing qubits from 4 to 16\nshows varied model responsiveness to outlier detection (OD) samples, which is a\ncritical factor for resilient decision-making in dynamic environments. This\nwork shows how quantum computing techniques can transform data features for UQ,\nparticularly when combined with traditional methods.\n","authors":["Abdullah Abdullah","Fannya Ratana Sandjaja","Ayesha Abdul Majeed","Gyan Wickremasinghe","Karen Rafferty","Vishal Sharma"],"pdf_url":"https://arxiv.org/pdf/2411.10254v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08509v2","updated":"2024-11-25T17:35:07Z","published":"2024-04-12T14:46:15Z","title":"Efficient Interactive LLM Serving with Proxy Model-based Sequence Length\n  Prediction","summary":"  Large language models (LLMs) have been driving a new wave of interactive AI\napplications across numerous domains. However, efficiently serving LLM\ninference requests is challenging due to their unpredictable execution times\noriginating from the autoregressive nature of generative models. Existing LLM\nserving systems exploit first-come-first-serve (FCFS) scheduling, suffering\nfrom head-of-line blocking issues. To address the non-deterministic nature of\nLLMs and enable efficient interactive LLM serving, we present a speculative\nshortest-job-first (SSJF) scheduler that uses a light proxy model to predict\nLLM output sequence lengths. Our open-source SSJF implementation does not\nrequire changes to memory management or batching strategies. Evaluations on\nreal-world datasets and production workload traces show that SSJF reduces\naverage job completion times by 30.5-39.6% and increases throughput by 2.2-3.6x\ncompared to FCFS schedulers, across no batching, dynamic batching, and\ncontinuous batching settings.\n","authors":["Haoran Qiu","Weichao Mao","Archit Patke","Shengkun Cui","Saurabh Jha","Chen Wang","Hubertus Franke","Zbigniew T. Kalbarczyk","Tamer Başar","Ravishankar K. Iyer"],"pdf_url":"https://arxiv.org/pdf/2404.08509v2.pdf","comment":"Accepted at AIOps'24"},{"id":"http://arxiv.org/abs/2411.16600v1","updated":"2024-11-25T17:31:34Z","published":"2024-11-25T17:31:34Z","title":"Approximation Algorithms for Combinatorial Optimization with Predictions","summary":"  We initiate a systematic study of utilizing predictions to improve over\napproximation guarantees of classic algorithms, without increasing the running\ntime. We propose a systematic method for a wide class of optimization problems\nthat ask to select a feasible subset of input items of minimal (or maximal)\ntotal weight. This gives simple (near-)linear time algorithms for, e.g., Vertex\nCover, Steiner Tree, Min-Weight Perfect Matching, Knapsack, and Clique. Our\nalgorithms produce optimal solutions when provided with perfect predictions and\ntheir approximation ratios smoothly degrade with increasing prediction error.\nWith small enough prediction error we achieve approximation guarantees that are\nbeyond reach without predictions in the given time bounds, as exemplified by\nthe NP-hardness and APX-hardness of many of the above problems. Although we\nshow our approach to be optimal for this class of problems as a whole, there is\na potential for exploiting specific structural properties of individual\nproblems to obtain improved bounds; we demonstrate this on the Steiner Tree\nproblem. We conclude with an empirical evaluation of our approach.\n","authors":["Antonios Antoniadis","Marek Eliáš","Adam Polak","Moritz Venzin"],"pdf_url":"https://arxiv.org/pdf/2411.16600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16598v1","updated":"2024-11-25T17:30:32Z","published":"2024-11-25T17:30:32Z","title":"Unlocking The Potential of Adaptive Attacks on Diffusion-Based\n  Purification","summary":"  Diffusion-based purification (DBP) is a defense against adversarial examples\n(AEs), amassing popularity for its ability to protect classifiers in an\nattack-oblivious manner and resistance to strong adversaries with access to the\ndefense. Its robustness has been claimed to ensue from the reliance on\ndiffusion models (DMs) that project the AEs onto the natural distribution. We\nrevisit this claim, focusing on gradient-based strategies that back-propagate\nthe loss gradients through the defense, commonly referred to as ``adaptive\nattacks\". Analytically, we show that such an optimization method invalidates\nDBP's core foundations, effectively targeting the DM rather than the classifier\nand restricting the purified outputs to a distribution over malicious samples\ninstead. Thus, we reassess the reported empirical robustness, uncovering\nimplementation flaws in the gradient back-propagation techniques used thus far\nfor DBP. We fix these issues, providing the first reliable gradient library for\nDBP and demonstrating how adaptive attacks drastically degrade its robustness.\nWe then study a less efficient yet stricter majority-vote setting where the\nclassifier evaluates multiple purified copies of the input to make its\ndecision. Here, DBP's stochasticity enables it to remain partially robust\nagainst traditional norm-bounded AEs. We propose a novel adaptation of a recent\noptimization method against deepfake watermarking that crafts systemic\nmalicious perturbations while ensuring imperceptibility. When integrated with\nthe adaptive attack, it completely defeats DBP, even in the majority-vote\nsetup. Our findings prove that DBP, in its current state, is not a viable\ndefense against AEs.\n","authors":["Andre Kassis","Urs Hengartner","Yaoliang Yu"],"pdf_url":"https://arxiv.org/pdf/2411.16598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16591v1","updated":"2024-11-25T17:25:00Z","published":"2024-11-25T17:25:00Z","title":"Adversarial Attacks for Drift Detection","summary":"  Concept drift refers to the change of data distributions over time. While\ndrift poses a challenge for learning models, requiring their continual\nadaption, it is also relevant in system monitoring to detect malfunctions,\nsystem failures, and unexpected behavior. In the latter case, the robust and\nreliable detection of drifts is imperative. This work studies the shortcomings\nof commonly used drift detection schemes. We show how to construct data streams\nthat are drifting without being detected. We refer to those as drift\nadversarials. In particular, we compute all possible adversairals for common\ndetection schemes and underpin our theoretical findings with empirical\nevaluations.\n","authors":["Fabian Hinder","Valerie Vaquet","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2411.16591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10842v4","updated":"2024-11-25T17:22:53Z","published":"2024-03-16T07:40:23Z","title":"Twin Transformer using Gated Dynamic Learnable Attention mechanism for\n  Fault Detection and Diagnosis in the Tennessee Eastman Process","summary":"  Fault detection and diagnosis (FDD) is a crucial task for ensuring the safety\nand efficiency of industrial processes. We propose a novel FDD methodology for\nthe Tennessee Eastman Process (TEP), a widely used benchmark for chemical\nprocess control. The model employs two separate Transformer branches, enabling\nindependent processing of input data and potential extraction of diverse\ninformation. A novel attention mechanism, Gated Dynamic Learnable Attention\n(GDLAttention), is introduced which integrates a gating mechanism and dynamic\nlearning capabilities. The gating mechanism modulates the attention weights,\nallowing the model to focus on the most relevant parts of the input. The\ndynamic learning approach adapts the attention strategy during training,\npotentially leading to improved performance. The attention mechanism uses a\nbilinear similarity function, providing greater flexibility in capturing\ncomplex relationships between query and key vectors. In order to assess the\neffectiveness of our approach, we tested it against 21 and 18 distinct fault\nscenarios in TEP, and compared its performance with several established FDD\ntechniques. The outcomes indicate that the method outperforms others in terms\nof accuracy, false alarm rate, and misclassification rate. This underscores the\nrobustness and efficacy of the approach for FDD in intricate industrial\nprocesses.\n","authors":["Mohammad Ali Labbaf-Khaniki","Mohammad Manthouri","Hanieh Ajami"],"pdf_url":"https://arxiv.org/pdf/2403.10842v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16586v1","updated":"2024-11-25T17:19:30Z","published":"2024-11-25T17:19:30Z","title":"Alpha Entropy Search for New Information-based Bayesian Optimization","summary":"  Bayesian optimization (BO) methods based on information theory have obtained\nstate-of-the-art results in several tasks. These techniques heavily rely on the\nKullback-Leibler (KL) divergence to compute the acquisition function. In this\nwork, we introduce a novel information-based class of acquisition functions for\nBO called Alpha Entropy Search (AES). AES is based on the {\\alpha}-divergence,\nthat generalizes the KL divergence. Iteratively, AES selects the next\nevaluation point as the one whose associated target value has the highest level\nof the dependency with respect to the location and associated value of the\nglobal maximum of the optimization problem. Dependency is measured in terms of\nthe {\\alpha}-divergence, as an alternative to the KL divergence. Intuitively,\nthis favors the evaluation of the objective function at the most informative\npoints about the global maximum. The {\\alpha}-divergence has a free parameter\n{\\alpha}, which determines the behavior of the divergence, trading-off\nevaluating differences between distributions at a single mode, and evaluating\ndifferences globally. Therefore, different values of {\\alpha} result in\ndifferent acquisition functions. AES acquisition lacks a closed-form\nexpression. However, we propose an efficient and accurate approximation using a\ntruncated Gaussian distribution. In practice, the value of {\\alpha} can be\nchosen by the practitioner, but here we suggest to use a combination of\nacquisition functions obtained by simultaneously considering a range of values\nof {\\alpha}. We provide an implementation of AES in BOTorch and we evaluate its\nperformance in both synthetic, benchmark and real-world experiments involving\nthe tuning of the hyper-parameters of a deep neural network. These experiments\nshow that the performance of AES is competitive with respect to other\ninformation-based acquisition functions such as JES, MES or PES.\n","authors":["Daniel Fernández-Sánchez","Eduardo C. Garrido-Merchán","Daniel Hernández-Lobato"],"pdf_url":"https://arxiv.org/pdf/2411.16586v1.pdf","comment":"31 pages, 12 figures, 3 tables, Journal KBS"},{"id":"http://arxiv.org/abs/2411.16579v1","updated":"2024-11-25T17:11:54Z","published":"2024-11-25T17:11:54Z","title":"Enhancing LLM Reasoning via Critique Models with Test-Time and\n  Training-Time Supervision","summary":"  Training large language models (LLMs) to spend more time thinking and\nreflection before responding is crucial for effectively solving complex\nreasoning tasks in fields such as science, coding, and mathematics. However,\nthe effectiveness of mechanisms like self-reflection and self-correction\ndepends on the model's capacity to accurately assess its own performance, which\ncan be limited by factors such as initial accuracy, question difficulty, and\nthe lack of external feedback. In this paper, we delve into a two-player\nparadigm that separates the roles of reasoning and critique models, where the\ncritique model provides step-level feedback to supervise the reasoning (actor)\nmodel during both test-time and train-time. We first propose AutoMathCritique,\nan automated and scalable framework for collecting critique data, resulting in\na dataset of $76,321$ responses paired with step-level feedback. Fine-tuning\nlanguage models with this dataset enables them to generate natural language\nfeedback for mathematical reasoning. We demonstrate that the critique models\nconsistently improve the actor's performance on difficult queries at test-time,\nespecially when scaling up inference-time computation. Motivated by these\nfindings, we introduce the critique-based supervision to the actor's\nself-training process, and propose a critique-in-the-loop self-improvement\nmethod. Experiments show that the method improves the actor's exploration\nefficiency and solution diversity, especially on challenging queries, leading\nto a stronger reasoning model. Lastly, we take the preliminary step to explore\ntraining self-talk reasoning models via critique supervision and showcase its\npotential. Our code and datasets are at\n\\href{https://mathcritique.github.io/}{https://mathcritique.github.io/}.\n","authors":["Zhiheng Xi","Dingwen Yang","Jixuan Huang","Jiafu Tang","Guanyu Li","Yiwen Ding","Wei He","Boyang Hong","Shihan Do","Wenyu Zhan","Xiao Wang","Rui Zheng","Tao Ji","Xiaowei Shi","Yitao Zhai","Rongxiang Weng","Jingang Wang","Xunliang Cai","Tao Gui","Zuxuan Wu","Qi Zhang","Xipeng Qiu","Xuanjing Huang","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2411.16579v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.07610v2","updated":"2024-11-25T17:01:53Z","published":"2024-10-10T04:54:37Z","title":"CSA: Data-efficient Mapping of Unimodal Features to Multimodal Features","summary":"  Multimodal encoders like CLIP excel in tasks such as zero-shot image\nclassification and cross-modal retrieval. However, they require excessive\ntraining data. We propose canonical similarity analysis (CSA), which uses two\nunimodal encoders to replicate multimodal encoders using limited data. CSA maps\nunimodal features into a multimodal space, using a new similarity score to\nretain only the multimodal information. CSA only involves the inference of\nunimodal encoders and a cubic-complexity matrix decomposition, eliminating the\nneed for extensive GPU-based model training. Experiments show that CSA\noutperforms CLIP while requiring $300,000\\times$ fewer multimodal data pairs\nand $6\\times$ fewer unimodal data for ImageNet classification and\nmisinformative news captions detection. CSA surpasses the state-of-the-art\nmethod to map unimodal features to multimodal features. We also demonstrate the\nability of CSA with modalities beyond image and text, paving the way for future\nmodality pairs with limited paired multimodal data but abundant unpaired\nunimodal data, such as lidar and text.\n","authors":["Po-han Li","Sandeep P. Chinchali","Ufuk Topcu"],"pdf_url":"https://arxiv.org/pdf/2410.07610v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01472v3","updated":"2024-11-25T16:52:46Z","published":"2023-12-03T18:15:58Z","title":"BenchMARL: Benchmarking Multi-Agent Reinforcement Learning","summary":"  The field of Multi-Agent Reinforcement Learning (MARL) is currently facing a\nreproducibility crisis. While solutions for standardized reporting have been\nproposed to address the issue, we still lack a benchmarking tool that enables\nstandardization and reproducibility, while leveraging cutting-edge\nReinforcement Learning (RL) implementations. In this paper, we introduce\nBenchMARL, the first MARL training library created to enable standardized\nbenchmarking across different algorithms, models, and environments. BenchMARL\nuses TorchRL as its backend, granting it high performance and maintained\nstate-of-the-art implementations while addressing the broad community of MARL\nPyTorch users. Its design enables systematic configuration and reporting, thus\nallowing users to create and run complex benchmarks from simple one-line\ninputs. BenchMARL is open-sourced on GitHub:\nhttps://github.com/facebookresearch/BenchMARL\n","authors":["Matteo Bettini","Amanda Prorok","Vincent Moens"],"pdf_url":"https://arxiv.org/pdf/2312.01472v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16567v1","updated":"2024-11-25T16:51:11Z","published":"2024-11-25T16:51:11Z","title":"Enhancing Few-Shot Learning with Integrated Data and GAN Model\n  Approaches","summary":"  This paper presents an innovative approach to enhancing few-shot learning by\nintegrating data augmentation with model fine-tuning in a framework designed to\ntackle the challenges posed by small-sample data. Recognizing the critical\nlimitations of traditional machine learning models that require large\ndatasets-especially in fields such as drug discovery, target recognition, and\nmalicious traffic detection-this study proposes a novel strategy that leverages\nGenerative Adversarial Networks (GANs) and advanced optimization techniques to\nimprove model performance with limited data. Specifically, the paper addresses\nthe noise and bias issues introduced by data augmentation methods, contrasting\nthem with model-based approaches, such as fine-tuning and metric learning,\nwhich rely heavily on related datasets. By combining Markov Chain Monte Carlo\n(MCMC) sampling and discriminative model ensemble strategies within a GAN\nframework, the proposed model adjusts generative and discriminative\ndistributions to simulate a broader range of relevant data. Furthermore, it\nemploys MHLoss and a reparameterized GAN ensemble to enhance stability and\naccelerate convergence, ultimately leading to improved classification\nperformance on small-sample images and structured datasets. Results confirm\nthat the MhERGAN algorithm developed in this research is highly effective for\nfew-shot learning, offering a practical solution that bridges data scarcity\nwith high-performing model adaptability and generalization.\n","authors":["Yinqiu Feng","Aoran Shen","Jiacheng Hu","Yingbin Liang","Shiru Wang","Junliang Du"],"pdf_url":"https://arxiv.org/pdf/2411.16567v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16560v1","updated":"2024-11-25T16:46:22Z","published":"2024-11-25T16:46:22Z","title":"Quantum Circuit Training with Growth-Based Architectures","summary":"  This study introduces growth-based training strategies that incrementally\nincrease parameterized quantum circuit (PQC) depth during training, mitigating\noverfitting and managing model complexity dynamically. We develop three\ndistinct methods: Block Growth, Sequential Feature Map Growth, and Interleave\nFeature Map Growth, which add reuploader blocks to PQCs adaptively, expanding\nthe accessible frequency spectrum of the model in response to training needs.\nThis approach enables PQCs to achieve more stable convergence and\ngeneralization, even in noisy settings. We evaluate our methods on regression\ntasks and the 2D Laplace equation, demonstrating that dynamic growth methods\noutperform traditional, fixed-depth approaches, achieving lower final losses\nand reduced variance between runs. These findings underscore the potential of\ngrowth-based PQCs for quantum scientific machine learning (QSciML)\napplications, where balancing expressivity and stability is essential.\n","authors":["Callum Duffy","Smit Chaudhary","Gergana V. Velikova"],"pdf_url":"https://arxiv.org/pdf/2411.16560v1.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2411.02998v2","updated":"2024-11-25T16:41:05Z","published":"2024-11-05T11:00:09Z","title":"Accelerating Task Generalisation with Multi-Level Hierarchical Options","summary":"  Creating reinforcement learning agents that generalise effectively to new\ntasks is a key challenge in AI research. This paper introduces Fracture Cluster\nOptions (FraCOs), a multi-level hierarchical reinforcement learning method that\nachieves state-of-the-art performance on difficult generalisation tasks. FraCOs\nidentifies patterns in agent behaviour and forms options based on the expected\nfuture usefulness of those patterns, enabling rapid adaptation to new tasks. In\ntabular settings, FraCOs demonstrates effective transfer and improves\nperformance as it grows in hierarchical depth. We evaluate FraCOs against\nstate-of-the-art deep reinforcement learning algorithms in several complex\nprocedurally generated environments. Our results show that FraCOs achieves\nhigher in-distribution and out-of-distribution performance than competitors.\n","authors":["Thomas P Cannon","Özgür Simsek"],"pdf_url":"https://arxiv.org/pdf/2411.02998v2.pdf","comment":"10 pages, under review for ICLR 2025"},{"id":"http://arxiv.org/abs/2402.01964v2","updated":"2024-11-25T16:41:01Z","published":"2024-02-03T00:12:36Z","title":"Scalable and Efficient Temporal Graph Representation Learning via\n  Forward Recent Sampling","summary":"  Temporal graph representation learning (TGRL) is essential for modeling\ndynamic systems in real-world networks. However, traditional TGRL methods,\ndespite their effectiveness, often face significant computational challenges\nand inference delays due to the inefficient sampling of temporal neighbors.\nConventional sampling methods typically involve backtracking through the\ninteraction history of each node. In this paper, we propose a novel TGRL\nframework, No-Looking-Back (NLB), which overcomes these challenges by\nintroducing a forward recent sampling strategy. This strategy eliminates the\nneed to backtrack through historical interactions by utilizing a\nGPU-executable, size-constrained hash table for each node. The hash table\nrecords a down-sampled set of recent interactions, enabling rapid query\nresponses with minimal inference latency. The maintenance of this hash table is\nhighly efficient, operating with $O(1)$ complexity. Fully compatible with GPU\nprocessing, NLB maximizes programmability, parallelism, and power efficiency.\nEmpirical evaluations demonstrate that NLB not only matches or surpasses\nstate-of-the-art methods in accuracy for tasks like link prediction and node\nclassification across six real-world datasets but also achieves 1.32-4.40x\nfaster training, 1.2-7.94x greater energy efficiency, and 1.63-12.95x lower\ninference latency compared to competitive baselines. The link to the code:\nhttps://github.com/Graph-COM/NLB.\n","authors":["Yuhong Luo","Pan Li"],"pdf_url":"https://arxiv.org/pdf/2402.01964v2.pdf","comment":"Learning on Graphs Conference (LoG 2024)"},{"id":"http://arxiv.org/abs/2411.16556v1","updated":"2024-11-25T16:40:19Z","published":"2024-11-25T16:40:19Z","title":"Anomaly Detection and RFI Classification with Unsupervised Learning in\n  Narrowband Radio Technosignature Searches","summary":"  The search for radio technosignatures is an anomaly detection problem:\ncandidate signals represent needles of interest in the proverbial haystack of\nradio-frequency interference (RFI). Current search frameworks find an enormity\nof false-positive signals, especially in large surveys, requiring manual\nfollow-up to a sometimes prohibitive degree. Unsupervised learning provides an\nalgorithmic way to winnow the most anomalous signals from the chaff, as well as\ngroup together RFI signals that bear morphological similarities. We present\nGLOBULAR (Grouping Low-frequency Observations By Unsupervised Learning After\nReduction) clustering, a signal processing method that uses HDBSCAN to reduce\nthe false-positive rate and isolate outlier signals for further analysis. When\ncombined with a standard narrowband signal detection and spatial filtering\npipeline, such as turboSETI, GLOBULAR clustering offers significant\nimprovements in the false-positive rate over the standard pipeline alone,\nsuggesting dramatic potential for the amelioration of manual follow-up\nrequirements for future large surveys. By removing RFI signals in regions of\nhigh spectral occupancy, GLOBULAR clustering may also enable the detection of\nsignals missed by the standard pipeline. We benchmark our method against the\nChoza et al. (2024) turboSETI-only search of 97 nearby galaxies at L-band,\ndemonstrating a false-positive hit reduction rate of 93.1% and a false-positive\nevent reduction rate of 99.3%.\n","authors":["Ben Jacobson-Bell","Steve Croft","Carmen Choza","Alex Andersson","Daniel Bautista","Vishal Gajjar","Matthew Lebofsky","David H. E. MacMahon","Caleb Painter","Andrew P. V. Siemion"],"pdf_url":"https://arxiv.org/pdf/2411.16556v1.pdf","comment":"20 pages, 14 figures, submitted to AJ"},{"id":"http://arxiv.org/abs/2411.16554v1","updated":"2024-11-25T16:38:17Z","published":"2024-11-25T16:38:17Z","title":"Generating Out-Of-Distribution Scenarios Using Language Models","summary":"  The deployment of autonomous vehicles controlled by machine learning\ntechniques requires extensive testing in diverse real-world environments,\nrobust handling of edge cases and out-of-distribution scenarios, and\ncomprehensive safety validation to ensure that these systems can navigate\nsafely and effectively under unpredictable conditions. Addressing\nOut-Of-Distribution (OOD) driving scenarios is essential for enhancing safety,\nas OOD scenarios help validate the reliability of the models within the\nvehicle's autonomy stack. However, generating OOD scenarios is challenging due\nto their long-tailed distribution and rarity in urban driving dataset.\nRecently, Large Language Models (LLMs) have shown promise in autonomous\ndriving, particularly for their zero-shot generalization and common-sense\nreasoning capabilities. In this paper, we leverage these LLM strengths to\nintroduce a framework for generating diverse OOD driving scenarios. Our\napproach uses LLMs to construct a branching tree, where each branch represents\na unique OOD scenario. These scenarios are then simulated in the CARLA\nsimulator using an automated framework that aligns scene augmentation with the\ncorresponding textual descriptions. We evaluate our framework through extensive\nsimulations, and assess its performance via a diversity metric that measures\nthe richness of the scenarios. Additionally, we introduce a new \"OOD-ness\"\nmetric, which quantifies how much the generated scenarios deviate from typical\nurban driving conditions. Furthermore, we explore the capacity of modern\nVision-Language Models (VLMs) to interpret and safely navigate through the\nsimulated OOD scenarios. Our findings offer valuable insights into the\nreliability of language models in addressing OOD scenarios within the context\nof urban driving.\n","authors":["Erfan Aasi","Phat Nguyen","Shiva Sreeram","Guy Rosman","Sertac Karaman","Daniela Rus"],"pdf_url":"https://arxiv.org/pdf/2411.16554v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16550v1","updated":"2024-11-25T16:32:29Z","published":"2024-11-25T16:32:29Z","title":"Representation Collapsing Problems in Vector Quantization","summary":"  Vector quantization is a technique in machine learning that discretizes\ncontinuous representations into a set of discrete vectors. It is widely\nemployed in tokenizing data representations for large language models,\ndiffusion models, and other generative models. Despite its prevalence, the\ncharacteristics and behaviors of vector quantization in generative models\nremain largely underexplored. In this study, we investigate representation\ncollapse in vector quantization - a critical degradation where codebook tokens\nor latent embeddings lose their discriminative power by converging to a limited\nsubset of values. This collapse fundamentally compromises the model's ability\nto capture diverse data patterns. By leveraging both synthetic and real\ndatasets, we identify the severity of each type of collapses and triggering\nconditions. Our analysis reveals that restricted initialization and limited\nencoder capacity result in tokens collapse and embeddings collapse. Building on\nthese findings, we propose potential solutions aimed at mitigating each\ncollapse. To the best of our knowledge, this is the first comprehensive study\nexamining representation collapsing problems in vector quantization.\n","authors":["Wenhao Zhao","Qiran Zou","Rushi Shah","Dianbo Liu"],"pdf_url":"https://arxiv.org/pdf/2411.16550v1.pdf","comment":"13 pages, under review"},{"id":"http://arxiv.org/abs/2411.16549v1","updated":"2024-11-25T16:32:11Z","published":"2024-11-25T16:32:11Z","title":"Transformers are Deep Optimizers: Provable In-Context Learning for Deep\n  Model Training","summary":"  We investigate the transformer's capability for in-context learning (ICL) to\nsimulate the training process of deep models. Our key contribution is providing\na positive example of using a transformer to train a deep neural network by\ngradient descent in an implicit fashion via ICL. Specifically, we provide an\nexplicit construction of a $(2N+4)L$-layer transformer capable of simulating\n$L$ gradient descent steps of an $N$-layer ReLU network through ICL. We also\ngive the theoretical guarantees for the approximation within any given error\nand the convergence of the ICL gradient descent. Additionally, we extend our\nanalysis to the more practical setting using Softmax-based transformers. We\nvalidate our findings on synthetic datasets for 3-layer, 4-layer, and 6-layer\nneural networks. The results show that ICL performance matches that of direct\ntraining.\n","authors":["Weimin Wu","Maojiang Su","Jerry Yao-Chieh Hu","Zhao Song","Han Liu"],"pdf_url":"https://arxiv.org/pdf/2411.16549v1.pdf","comment":"66 pages, 3 figures"},{"id":"http://arxiv.org/abs/2311.17434v3","updated":"2024-11-25T16:23:35Z","published":"2023-11-29T08:26:18Z","title":"GSE: Group-wise Sparse and Explainable Adversarial Attacks","summary":"  Sparse adversarial attacks fool deep neural networks (DNNs) through minimal\npixel perturbations, often regularized by the $\\ell_0$ norm. Recent efforts\nhave replaced this norm with a structural sparsity regularizer, such as the\nnuclear group norm, to craft group-wise sparse adversarial attacks. The\nresulting perturbations are thus explainable and hold significant practical\nrelevance, shedding light on an even greater vulnerability of DNNs. However,\ncrafting such attacks poses an optimization challenge, as it involves computing\nnorms for groups of pixels within a non-convex objective. We address this by\npresenting a two-phase algorithm that generates group-wise sparse attacks\nwithin semantically meaningful areas of an image. Initially, we optimize a\nquasinorm adversarial loss using the $1/2-$quasinorm proximal operator tailored\nfor non-convex programming. Subsequently, the algorithm transitions to a\nprojected Nesterov's accelerated gradient descent with $2-$norm regularization\napplied to perturbation magnitudes. Rigorous evaluations on CIFAR-10 and\nImageNet datasets demonstrate a remarkable increase in group-wise sparsity,\ne.g., $50.9\\%$ on CIFAR-10 and $38.4\\%$ on ImageNet (average case, targeted\nattack). This performance improvement is accompanied by significantly faster\ncomputation times, improved explainability, and a $100\\%$ attack success rate.\n","authors":["Shpresim Sadiku","Moritz Wagner","Sebastian Pokutta"],"pdf_url":"https://arxiv.org/pdf/2311.17434v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03736v2","updated":"2024-11-25T16:21:05Z","published":"2024-09-30T21:18:05Z","title":"CliMB: An AI-enabled Partner for Clinical Predictive Modeling","summary":"  Despite its significant promise and continuous technical advances, real-world\napplications of artificial intelligence (AI) remain limited. We attribute this\nto the \"domain expert-AI-conundrum\": while domain experts, such as clinician\nscientists, should be able to build predictive models such as risk scores, they\nface substantial barriers in accessing state-of-the-art (SOTA) tools. While\nautomated machine learning (AutoML) has been proposed as a partner in clinical\npredictive modeling, many additional requirements need to be fulfilled to make\nmachine learning accessible for clinician scientists.\n  To address this gap, we introduce CliMB, a no-code AI-enabled partner\ndesigned to empower clinician scientists to create predictive models using\nnatural language. CliMB guides clinician scientists through the entire medical\ndata science pipeline, thus empowering them to create predictive models from\nreal-world data in just one conversation. CliMB also creates structured reports\nand interpretable visuals. In evaluations involving clinician scientists and\nsystematic comparisons against a baseline GPT-4, CliMB consistently\ndemonstrated superior performance in key areas such as planning, error\nprevention, code execution, and model performance. Moreover, in blinded\nassessments involving 45 clinicians from diverse specialties and career stages,\nmore than 80% preferred CliMB over GPT-4. Overall, by providing a no-code\ninterface with clear guidance and access to SOTA methods in the fields of\ndata-centric AI, AutoML, and interpretable ML, CliMB empowers clinician\nscientists to build robust predictive models.\n  The proof-of-concept version of CliMB is available as open-source software on\nGitHub: https://github.com/vanderschaarlab/climb.\n","authors":["Evgeny Saveliev","Tim Schubert","Thomas Pouplin","Vasilis Kosmoliaptsis","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2410.03736v2.pdf","comment":"* Evgeny Saveliev and Tim Schubert contributed equally to this work"},{"id":"http://arxiv.org/abs/2411.16532v1","updated":"2024-11-25T16:18:39Z","published":"2024-11-25T16:18:39Z","title":"Continual Deep Reinforcement Learning with Task-Agnostic Policy\n  Distillation","summary":"  Central to the development of universal learning systems is the ability to\nsolve multiple tasks without retraining from scratch when new data arrives.\nThis is crucial because each task requires significant training time.\nAddressing the problem of continual learning necessitates various methods due\nto the complexity of the problem space. This problem space includes: (1)\naddressing catastrophic forgetting to retain previously learned tasks, (2)\ndemonstrating positive forward transfer for faster learning, (3) ensuring\nscalability across numerous tasks, and (4) facilitating learning without\nrequiring task labels, even in the absence of clear task boundaries. In this\npaper, the Task-Agnostic Policy Distillation (TAPD) framework is introduced.\nThis framework alleviates problems (1)-(4) by incorporating a task-agnostic\nphase, where an agent explores its environment without any external goal and\nmaximizes only its intrinsic motivation. The knowledge gained during this phase\nis later distilled for further exploration. Therefore, the agent acts in a\nself-supervised manner by systematically seeking novel states. By utilizing\ntask-agnostic distilled knowledge, the agent can solve downstream tasks more\nefficiently, leading to improved sample efficiency. Our code is available at\nthe repository: https://github.com/wabbajack1/TAPD.\n","authors":["Muhammad Burhan Hafez","Kerim Erekmen"],"pdf_url":"https://arxiv.org/pdf/2411.16532v1.pdf","comment":"Accepted for publication in Scientific Reports"},{"id":"http://arxiv.org/abs/2411.16525v1","updated":"2024-11-25T16:12:17Z","published":"2024-11-25T16:12:17Z","title":"Fundamental Limits of Prompt Tuning Transformers: Universality, Capacity\n  and Efficiency","summary":"  We investigate the statistical and computational limits of prompt tuning for\ntransformer-based foundation models. Our key contributions are prompt tuning on\n\\textit{single-head} transformers with only a \\textit{single} self-attention\nlayer: (i) is universal, and (ii) supports efficient (even almost-linear time)\nalgorithms under the Strong Exponential Time Hypothesis (SETH). Statistically,\nwe prove that prompt tuning on such simplest possible transformers are\nuniversal approximators for sequence-to-sequence Lipschitz functions. In\naddition, we provide an exponential-in-$dL$ and -in-$(1/\\epsilon)$ lower bound\non the required soft-prompt tokens for prompt tuning to memorize any dataset\nwith 1-layer, 1-head transformers. Computationally, we identify a phase\ntransition in the efficiency of prompt tuning, determined by the norm of the\n\\textit{soft-prompt-induced} keys and queries, and provide an upper bound\ncriterion. Beyond this criterion, no sub-quadratic (efficient) algorithm for\nprompt tuning exists under SETH. Within this criterion, we showcase our theory\nby proving the existence of almost-linear time prompt tuning inference\nalgorithms. These fundamental limits provide important necessary conditions for\ndesigning expressive and efficient prompt tuning methods for practitioners.\n","authors":["Jerry Yao-Chieh Hu","Wei-Po Wang","Ammar Gilani","Chenyang Li","Zhao Song","Han Liu"],"pdf_url":"https://arxiv.org/pdf/2411.16525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01302v2","updated":"2024-11-25T15:47:57Z","published":"2024-02-02T10:44:42Z","title":"A Unified Framework for Center-based Clustering of Distributed Data","summary":"  We develop a family of distributed center-based clustering algorithms that\nwork over networks of users. In the proposed scenario, users contain a local\ndataset and communicate only with their immediate neighbours, with the aim of\nfinding a clustering of the full, joint data. The proposed family, termed\nDistributed Gradient Clustering (DGC-$\\mathcal{F}_\\rho$), is parametrized by\n$\\rho \\geq 1$, controling the proximity of users' center estimates, with\n$\\mathcal{F}$ determining the clustering loss. Our framework allows for a broad\nclass of smooth convex loss functions, including popular clustering losses like\n$K$-means and Huber loss. Specialized to popular clustering losses like\n$K$-means and Huber loss, DGC-$\\mathcal{F}_\\rho$ gives rise to novel\ndistributed clustering algorithms DGC-KM$_\\rho$ and DGC-HL$_\\rho$, while novel\nclustering losses based on Logistic and Fair functions lead to DGC-LL$_\\rho$\nand DGC-FL$_\\rho$. We provide a unified analysis and establish several strong\nresults, under mild assumptions. First, we show that the sequence of centers\ngenerated by the methods converges to a well-defined notion of fixed point,\nunder any center initialization and value of $\\rho$. Second, we prove that, as\n$\\rho$ increases, the family of fixed points produced by DGC-$\\mathcal{F}_\\rho$\nconverges to a notion of consensus fixed points. We show that consensus fixed\npoints of DGC-$\\mathcal{F}_{\\rho}$ are equivalent to fixed points of gradient\nclustering over the full data, guaranteeing a clustering of the full data is\nproduced. For the special case of Bregman losses, we show that our fixed points\nconverge to the set of Lloyd points. Extensive numerical experiments on\nsynthetic and real data confirm our theoretical findings, show strong\nperformance of our methods and demonstrate the usefulness and wide range of\npotential applications of our general framework, such as outlier detection.\n","authors":["Aleksandar Armacki","Dragana Bajović","Dušan Jakovetić","Soummya Kar"],"pdf_url":"https://arxiv.org/pdf/2402.01302v2.pdf","comment":"49 pages, 9 figures, 7 tables"},{"id":"http://arxiv.org/abs/2411.16509v1","updated":"2024-11-25T15:46:54Z","published":"2024-11-25T15:46:54Z","title":"Jaya R Package -- A Parameter-Free Solution for Advanced Single and\n  Multi-Objective Optimization","summary":"  The Jaya R package offers a robust and versatile implementation of the\nparameter-free Jaya optimization algorithm, suitable for solving both\nsingle-objective and multi-objective optimization problems. By integrating\nadvanced features such as constraint handling, adaptive population management,\nPareto front tracking for multi-objective trade-offs, and parallel processing\nfor computational efficiency, the package caters to a wide range of\noptimization challenges. Its intuitive design and flexibility allow users to\nsolve complex, real-world problems across various domains. To demonstrate its\npractical utility, a case study on energy modeling explores the optimization of\nrenewable energy shares, showcasing the package's ability to minimize carbon\nemissions and costs while enhancing system reliability. The Jaya R package is\nan invaluable tool for researchers and practitioners seeking efficient and\nadaptive optimization solutions.\n","authors":["Neeraj Dhanraj Bokde"],"pdf_url":"https://arxiv.org/pdf/2411.16509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16502v1","updated":"2024-11-25T15:37:27Z","published":"2024-11-25T15:37:27Z","title":"Interpreting Language Reward Models via Contrastive Explanations","summary":"  Reward models (RMs) are a crucial component in the alignment of large\nlanguage models' (LLMs) outputs with human values. RMs approximate human\npreferences over possible LLM responses to the same prompt by predicting and\ncomparing reward scores. However, as they are typically modified versions of\nLLMs with scalar output heads, RMs are large black boxes whose predictions are\nnot explainable. More transparent RMs would enable improved trust in the\nalignment of LLMs. In this work, we propose to use contrastive explanations to\nexplain any binary response comparison made by an RM. Specifically, we generate\na diverse set of new comparisons similar to the original one to characterise\nthe RM's local behaviour. The perturbed responses forming the new comparisons\nare generated to explicitly modify manually specified high-level evaluation\nattributes, on which analyses of RM behaviour are grounded. In quantitative\nexperiments, we validate the effectiveness of our method for finding\nhigh-quality contrastive explanations. We then showcase the qualitative\nusefulness of our method for investigating global sensitivity of RMs to each\nevaluation attribute, and demonstrate how representative examples can be\nautomatically extracted to explain and compare behaviours of different RMs. We\nsee our method as a flexible framework for RM explanation, providing a basis\nfor more interpretable and trustworthy LLM alignment.\n","authors":["Junqi Jiang","Tom Bewley","Saumitra Mishra","Freddy Lecue","Manuela Veloso"],"pdf_url":"https://arxiv.org/pdf/2411.16502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16498v1","updated":"2024-11-25T15:36:29Z","published":"2024-11-25T15:36:29Z","title":"Multi-Resolution Generative Modeling of Human Motion from Limited Data","summary":"  We present a generative model that learns to synthesize human motion from\nlimited training sequences. Our framework provides conditional generation and\nblending across multiple temporal resolutions. The model adeptly captures human\nmotion patterns by integrating skeletal convolution layers and a multi-scale\narchitecture. Our model contains a set of generative and adversarial networks,\nalong with embedding modules, each tailored for generating motions at specific\nframe rates while exerting control over their content and details. Notably, our\napproach also extends to the synthesis of co-speech gestures, demonstrating its\nability to generate synchronized gestures from speech inputs, even with limited\npaired data. Through direct synthesis of SMPL pose parameters, our approach\navoids test-time adjustments to fit human body meshes. Experimental results\nshowcase our model's ability to achieve extensive coverage of training\nexamples, while generating diverse motions, as indicated by local and global\ndiversity metrics.\n","authors":["David Eduardo Moreno-Villamarín","Anna Hilsmann","Peter Eisert"],"pdf_url":"https://arxiv.org/pdf/2411.16498v1.pdf","comment":"1O pages, 7 figures, published in European Conference on Visual Media\n  Production CVMP 24"},{"id":"http://arxiv.org/abs/2411.16483v1","updated":"2024-11-25T15:22:03Z","published":"2024-11-25T15:22:03Z","title":"Graph Transformer Networks for Accurate Band Structure Prediction: An\n  End-to-End Approach","summary":"  Predicting electronic band structures from crystal structures is crucial for\nunderstanding structure-property correlations in materials science.\nFirst-principles approaches are accurate but computationally intensive. Recent\nyears, machine learning (ML) has been extensively applied to this field, while\nexisting ML models predominantly focus on band gap predictions or indirect band\nstructure estimation via solving predicted Hamiltonians. An end-to-end model to\npredict band structure accurately and efficiently is still lacking. Here, we\nintroduce a graph Transformer-based end-to-end approach that directly predicts\nband structures from crystal structures with high accuracy. Our method\nleverages the continuity of the k-path and treat continuous bands as a\nsequence. We demonstrate that our model not only provides accurate band\nstructure predictions but also can derive other properties (such as band gap,\nband center, and band dispersion) with high accuracy. We verify the model\nperformance on large and diverse datasets.\n","authors":["Weiyi Gong","Tao Sun","Hexin Bai","Jeng-Yuan Tsai","Haibin Ling","Qimin Yan"],"pdf_url":"https://arxiv.org/pdf/2411.16483v1.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.16478v1","updated":"2024-11-25T15:20:40Z","published":"2024-11-25T15:20:40Z","title":"Distributed, communication-efficient, and differentially private\n  estimation of KL divergence","summary":"  A key task in managing distributed, sensitive data is to measure the extent\nto which a distribution changes. Understanding this drift can effectively\nsupport a variety of federated learning and analytics tasks. However, in many\npractical settings sharing such information can be undesirable (e.g., for\nprivacy concerns) or infeasible (e.g., for high communication costs). In this\nwork, we describe novel algorithmic approaches for estimating the KL divergence\nof data across federated models of computation, under differential privacy. We\nanalyze their theoretical properties and present an empirical study of their\nperformance. We explore parameter settings that optimize the accuracy of the\nalgorithm catering to each of the settings; these provide sub-variations that\nare applicable to real-world tasks, addressing different context- and\napplication-specific trust level requirements. Our experimental results confirm\nthat our private estimators achieve accuracy comparable to a baseline algorithm\nwithout differential privacy guarantees.\n","authors":["Mary Scott","Sayan Biswas","Graham Cormode","Carsten Maple"],"pdf_url":"https://arxiv.org/pdf/2411.16478v1.pdf","comment":"28 pages, 5 figures"},{"id":"http://arxiv.org/abs/2411.16477v1","updated":"2024-11-25T15:20:01Z","published":"2024-11-25T15:20:01Z","title":"Distributed Online Optimization with Stochastic Agent Availability","summary":"  Motivated by practical federated learning settings where clients may not be\nalways available, we investigate a variant of distributed online optimization\nwhere agents are active with a known probability $p$ at each time step, and\ncommunication between neighboring agents can only take place if they are both\nactive. We introduce a distributed variant of the FTRL algorithm and analyze\nits network regret, defined through the average of the instantaneous regret of\nthe active agents. Our analysis shows that, for any connected communication\ngraph $G$ over $N$ agents, the expected network regret of our FTRL variant\nafter $T$ steps is at most of order\n$(\\kappa/p^2)\\min\\big\\{\\sqrt{N},N^{1/4}/\\sqrt{p}\\big\\}\\sqrt{T}$, where $\\kappa$\nis the condition number of the Laplacian of $G$. We then show that similar\nregret bounds also hold with high probability. Moreover, we show that our\nnotion of regret (average-case over the agents) is essentially equivalent to\nthe standard notion of regret (worst-case over agents), implying that our\nbounds are not significantly improvable when $p=1$. Our theoretical results are\nsupported by experiments on synthetic datasets.\n","authors":["Juliette Achddou","Nicolò Cesa-Bianchi","Hao Qiu"],"pdf_url":"https://arxiv.org/pdf/2411.16477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16475v1","updated":"2024-11-25T15:19:19Z","published":"2024-11-25T15:19:19Z","title":"NonSysId: A nonlinear system identification package with improved model\n  term selection for NARMAX models","summary":"  System identification involves constructing mathematical models of dynamic\nsystems using input-output data, enabling analysis and prediction of system\nbehaviour in both time and frequency domains. This approach can model the\nentire system or capture specific dynamics within it. For meaningful analysis,\nit is essential for the model to accurately reflect the underlying system's\nbehaviour. This paper introduces NonSysId, an open-sourced MATLAB software\npackage designed for nonlinear system identification, specifically focusing on\nNARMAX models. The software incorporates an advanced term selection methodology\nthat prioritises on simulation (free-run) accuracy while preserving model\nparsimony. A key feature is the integration of iterative Orthogonal Forward\nRegression (iOFR) with Predicted Residual Sum of Squares (PRESS)\nstatistic-based term selection, facilitating robust model generalisation\nwithout the need for a separate validation dataset. Furthermore, techniques for\nreducing computational overheads are implemented. These features make NonSysId\nparticularly suitable for real-time applications such as structural health\nmonitoring, fault diagnosis, and biomedical signal processing, where it is a\nchallenge to capture the signals under consistent conditions, resulting in\nlimited or no validation data.\n","authors":["Rajintha Gunawardena","Zi-Qiang Lang","Fei He"],"pdf_url":"https://arxiv.org/pdf/2411.16475v1.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2411.06601v2","updated":"2024-11-25T15:17:30Z","published":"2024-11-10T21:26:17Z","title":"OffLight: An Offline Multi-Agent Reinforcement Learning Framework for\n  Traffic Signal Control","summary":"  Efficient traffic control (TSC) is essential for urban mobility, but\ntraditional systems struggle to handle the complexity of real-world traffic.\nMulti-agent Reinforcement Learning (MARL) offers adaptive solutions, but online\nMARL requires extensive interactions with the environment, making it costly and\nimpractical. Offline MARL mitigates these challenges by using historical\ntraffic data for training but faces significant difficulties with heterogeneous\nbehavior policies in real-world datasets, where mixed-quality data complicates\nlearning. We introduce OffLight, a novel offline MARL framework designed to\nhandle heterogeneous behavior policies in TSC datasets. To improve learning\nefficiency, OffLight incorporates Importance Sampling (IS) to correct for\ndistributional shifts and Return-Based Prioritized Sampling (RBPS) to focus on\nhigh-quality experiences. OffLight utilizes a Gaussian Mixture Variational\nGraph Autoencoder (GMM-VGAE) to capture the diverse distribution of behavior\npolicies from local observations. Extensive experiments across real-world urban\ntraffic scenarios show that OffLight outperforms existing offline RL methods,\nachieving up to a 7.8% reduction in average travel time and 11.2% decrease in\nqueue length. Ablation studies confirm the effectiveness of OffLight's\ncomponents in handling heterogeneous data and improving policy performance.\nThese results highlight OffLight's scalability and potential to improve urban\ntraffic management without the risks of online learning.\n","authors":["Rohit Bokade","Xiaoning Jin"],"pdf_url":"https://arxiv.org/pdf/2411.06601v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16466v1","updated":"2024-11-25T15:13:17Z","published":"2024-11-25T15:13:17Z","title":"No Identity, no problem: Motion through detection for people tracking","summary":"  Tracking-by-detection has become the de facto standard approach to people\ntracking. To increase robustness, some approaches incorporate re-identification\nusing appearance models and regressing motion offset, which requires costly\nidentity annotations. In this paper, we propose exploiting motion clues while\nproviding supervision only for the detections, which is much easier to do. Our\nalgorithm predicts detection heatmaps at two different times, along with a 2D\nmotion estimate between the two images. It then warps one heatmap using the\nmotion estimate and enforces consistency with the other one. This provides the\nrequired supervisory signal on the motion without the need for any motion\nannotations. In this manner, we couple the information obtained from different\nimages during training and increase accuracy, especially in crowded scenes and\nwhen using low frame-rate sequences. We show that our approach delivers\nstate-of-the-art results for single- and multi-view multi-target tracking on\nthe MOT17 and WILDTRACK datasets.\n","authors":["Martin Engilberge","F. Wilke Grosche","Pascal Fua"],"pdf_url":"https://arxiv.org/pdf/2411.16466v1.pdf","comment":"Accepted in TMLR November 2024"},{"id":"http://arxiv.org/abs/2411.16462v1","updated":"2024-11-25T15:08:24Z","published":"2024-11-25T15:08:24Z","title":"Lion Cub: Minimizing Communication Overhead in Distributed Lion","summary":"  Communication overhead is a key challenge in distributed deep learning,\nespecially on slower Ethernet interconnects, and given current hardware trends,\ncommunication is likely to become a major bottleneck. While gradient\ncompression techniques have been explored for SGD and Adam, the Lion optimizer\nhas the distinct advantage that its update vectors are the output of a sign\noperation, enabling straightforward quantization. However, simply compressing\nupdates for communication and using techniques like majority voting fails to\nlead to end-to-end speedups due to inefficient communication algorithms and\nreduced convergence. We analyze three factors critical to distributed learning\nwith Lion: optimizing communication methods, identifying effective quantization\nmethods, and assessing the necessity of momentum synchronization. Our findings\nshow that quantization techniques adapted to Lion and selective momentum\nsynchronization can significantly reduce communication costs while maintaining\nconvergence. We combine these into Lion Cub, which enables up to 5x speedups in\nend-to-end training compared to Lion. This highlights Lion's potential as a\ncommunication-efficient solution for distributed training.\n","authors":["Satoki Ishikawa","Tal Ben-Nun","Brian Van Essen","Rio Yokota","Nikoli Dryden"],"pdf_url":"https://arxiv.org/pdf/2411.16462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07082v2","updated":"2024-11-25T15:05:29Z","published":"2024-07-09T17:55:23Z","title":"Can Learned Optimization Make Reinforcement Learning Less Difficult?","summary":"  While reinforcement learning (RL) holds great potential for decision making\nin the real world, it suffers from a number of unique difficulties which often\nneed specific consideration. In particular: it is highly non-stationary;\nsuffers from high degrees of plasticity loss; and requires exploration to\nprevent premature convergence to local optima and maximize return. In this\npaper, we consider whether learned optimization can help overcome these\nproblems. Our method, Learned Optimization for Plasticity, Exploration and\nNon-stationarity (OPEN), meta-learns an update rule whose input features and\noutput structure are informed by previously proposed solutions to these\ndifficulties. We show that our parameterization is flexible enough to enable\nmeta-learning in diverse learning contexts, including the ability to use\nstochasticity for exploration. Our experiments demonstrate that when\nmeta-trained on single and small sets of environments, OPEN outperforms or\nequals traditionally used optimizers. Furthermore, OPEN shows strong\ngeneralization characteristics across a range of environments and agent\narchitectures.\n","authors":["Alexander David Goldie","Chris Lu","Matthew Thomas Jackson","Shimon Whiteson","Jakob Nicolaus Foerster"],"pdf_url":"https://arxiv.org/pdf/2407.07082v2.pdf","comment":"Neurips 2024"},{"id":"http://arxiv.org/abs/2411.16458v1","updated":"2024-11-25T15:05:00Z","published":"2024-11-25T15:05:00Z","title":"On the Reconstruction of Training Data from Group Invariant Networks","summary":"  Reconstructing training data from trained neural networks is an active area\nof research with significant implications for privacy and explainability.\nRecent advances have demonstrated the feasibility of this process for several\ndata types. However, reconstructing data from group-invariant neural networks\nposes distinct challenges that remain largely unexplored. This paper addresses\nthis gap by first formulating the problem and discussing some of its basic\nproperties. We then provide an experimental evaluation demonstrating that\nconventional reconstruction techniques are inadequate in this scenario.\nSpecifically, we observe that the resulting data reconstructions gravitate\ntoward symmetric inputs on which the group acts trivially, leading to\npoor-quality results. Finally, we propose two novel methods aiming to improve\nreconstruction in this setup and present promising preliminary experimental\nresults. Our work sheds light on the complexities of reconstructing data from\ngroup invariant neural networks and offers potential avenues for future\nresearch in this domain.\n","authors":["Ran Elbaz","Gilad Yehudai","Meirav Galun","Haggai Maron"],"pdf_url":"https://arxiv.org/pdf/2411.16458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10296v5","updated":"2024-11-25T15:00:32Z","published":"2024-04-16T05:40:30Z","title":"Interpolating neural network: A novel unification of machine learning\n  and interpolation theory","summary":"  Artificial intelligence (AI) has revolutionized software development,\nshifting from task-specific codes (Software 1.0) to neural network-based\napproaches (Software 2.0). However, applying this transition in engineering\nsoftware presents challenges, including low surrogate model accuracy, the curse\nof dimensionality in inverse design, and rising complexity in physical\nsimulations. We introduce an interpolating neural network (INN), grounded in\ninterpolation theory and tensor decomposition, to realize Engineering Software\n2.0 by advancing data training, partial differential equation solving, and\nparameter calibration. INN offers orders of magnitude fewer trainable/solvable\nparameters for comparable model accuracy than traditional multi-layer\nperceptron (MLP) or physics-informed neural networks (PINN). Demonstrated in\nmetal additive manufacturing, INN rapidly constructs an accurate surrogate\nmodel of Laser Powder Bed Fusion (L-PBF) heat transfer simulation, achieving\nsub-10-micrometer resolution for a 10 mm path in under 15 minutes on a single\nGPU. This makes a transformative step forward across all domains essential to\nengineering software.\n","authors":["Chanwook Park","Sourav Saha","Jiachen Guo","Hantao Zhang","Xiaoyu Xie","Miguel A. Bessa","Dong Qian","Wei Chen","Gregory J. Wagner","Jian Cao","Wing Kam Liu"],"pdf_url":"https://arxiv.org/pdf/2404.10296v5.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2411.16442v1","updated":"2024-11-25T14:44:26Z","published":"2024-11-25T14:44:26Z","title":"TIFeD: a Tiny Integer-based Federated learning algorithm with Direct\n  feedback alignment","summary":"  Training machine and deep learning models directly on extremely\nresource-constrained devices is the next challenge in the field of tiny machine\nlearning. The related literature in this field is very limited, since most of\nthe solutions focus only on on-device inference or model adaptation through\nonline learning, leaving the training to be carried out on external Cloud\nservices. An interesting technological perspective is to exploit Federated\nLearning (FL), which allows multiple devices to collaboratively train a shared\nmodel in a distributed way. However, the main drawback of state-of-the-art FL\nalgorithms is that they are not suitable for running on tiny devices. For the\nfirst time in the literature, in this paper we introduce TIFeD, a Tiny\nInteger-based Federated learning algorithm with Direct Feedback Alignment (DFA)\nentirely implemented by using an integer-only arithmetic and being specifically\ndesigned to operate on devices with limited resources in terms of memory,\ncomputation and energy. Besides the traditional full-network operating\nmodality, in which each device of the FL setting trains the entire neural\nnetwork on its own local data, we propose an innovative single-layer TIFeD\nimplementation, which enables each device to train only a portion of the neural\nnetwork model and opens the door to a new way of distributing the learning\nprocedure across multiple devices. The experimental results show the\nfeasibility and effectiveness of the proposed solution. The proposed TIFeD\nalgorithm, with its full-network and single-layer implementations, is made\navailable to the scientific community as a public repository.\n","authors":["Luca Colombo","Alessandro Falcetta","Manuel Roveri"],"pdf_url":"https://arxiv.org/pdf/2411.16442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16437v1","updated":"2024-11-25T14:39:18Z","published":"2024-11-25T14:39:18Z","title":"Privacy Protection in Personalized Diffusion Models via Targeted\n  Cross-Attention Adversarial Attack","summary":"  The growing demand for customized visual content has led to the rise of\npersonalized text-to-image (T2I) diffusion models. Despite their remarkable\npotential, they pose significant privacy risk when misused for malicious\npurposes. In this paper, we propose a novel and efficient adversarial attack\nmethod, Concept Protection by Selective Attention Manipulation (CoPSAM) which\ntargets only the cross-attention layers of a T2I diffusion model. For this\npurpose, we carefully construct an imperceptible noise to be added to clean\nsamples to get their adversarial counterparts. This is obtained during the\nfine-tuning process by maximizing the discrepancy between the corresponding\ncross-attention maps of the user-specific token and the class-specific token,\nrespectively. Experimental validation on a subset of CelebA-HQ face images\ndataset demonstrates that our approach outperforms existing methods. Besides\nthis, our method presents two important advantages derived from the qualitative\nevaluation: (i) we obtain better protection results for lower noise levels than\nour competitors; and (ii) we protect the content from unauthorized use thereby\nprotecting the individual's identity from potential misuse.\n","authors":["Xide Xu","Muhammad Atif Butt","Sandesh Kamath","Bogdan Raducanu"],"pdf_url":"https://arxiv.org/pdf/2411.16437v1.pdf","comment":"Accepted at Safe Generative AI Workshop (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2405.01125v2","updated":"2024-11-25T14:35:07Z","published":"2024-05-02T09:38:16Z","title":"Lipschitz constant estimation for general neural network architectures\n  using control tools","summary":"  This paper is devoted to the estimation of the Lipschitz constant of general\nneural network architectures using semidefinite programming. For this purpose,\nwe interpret neural networks as time-varying dynamical systems, where the\n$k$-th layer corresponds to the dynamics at time $k$. A key novelty with\nrespect to prior work is that we use this interpretation to exploit the series\ninterconnection structure of feedforward neural networks with a dynamic\nprogramming recursion. Nonlinearities, such as activation functions and\nnonlinear pooling layers, are handled with integral quadratic constraints. If\nthe neural network contains signal processing layers (convolutional or state\nspace model layers), we realize them as 1-D/2-D/N-D systems and exploit this\nstructure as well. We distinguish ourselves from related work on Lipschitz\nconstant estimation by more extensive structure exploitation (scalability) and\na generalization to a large class of common neural network architectures. To\nshow the versatility and computational advantages of our method, we apply it to\ndifferent neural network architectures trained on MNIST and CIFAR-10.\n","authors":["Patricia Pauli","Dennis Gramlich","Frank Allgöwer"],"pdf_url":"https://arxiv.org/pdf/2405.01125v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16427v1","updated":"2024-11-25T14:29:39Z","published":"2024-11-25T14:29:39Z","title":"Unsupervised Event Outlier Detection in Continuous Time","summary":"  Event sequence data record the occurrences of events in continuous time.\nEvent sequence forecasting based on temporal point processes (TPPs) has been\nextensively studied, but outlier or anomaly detection, especially without any\nsupervision from humans, is still underexplored. In this work, we develop, to\nthe best our knowledge, the first unsupervised outlier detection approach to\ndetecting abnormal events. Our novel unsupervised outlier detection framework\nis based on ideas from generative adversarial networks (GANs) and reinforcement\nlearning (RL). We train a 'generator' that corrects outliers in the data with a\n'discriminator' that learns to discriminate the corrected data from the real\ndata, which may contain outliers. A key insight is that if the generator made a\nmistake in the correction, it would generate anomalies that are different from\nthe anomalies in the real data, so it serves as data augmentation for the\ndiscriminator learning. Different from typical GAN-based outlier detection\napproaches, our method employs the generator to detect outliers in an online\nmanner. The experimental results show that our method can detect event outliers\nmore accurately than the state-of-the-art approaches.\n","authors":["Somjit Nath","Yik Chau Lui","Siqi Liu"],"pdf_url":"https://arxiv.org/pdf/2411.16427v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09721v5","updated":"2024-11-25T14:29:07Z","published":"2024-02-15T05:30:47Z","title":"Generalized Principal-Agent Problem with a Learning Agent","summary":"  Classic principal-agent problems such as Stackelberg games, contract design,\nand Bayesian persuasion, often assume that the agent is able to best respond to\nthe principal's committed strategy. We study repeated generalized\nprincipal-agent problems under the assumption that the principal does not have\ncommitment power and the agent uses algorithms to learn to respond to the\nprincipal. We reduce this problem to a one-shot generalized principal-agent\nproblem where the agent approximately best responds. Using this reduction, we\nshow that: (1) If the agent uses contextual no-regret learning algorithms with\nregret $\\mathrm{Reg}(T)$, then the principal can guarantee utility at least\n$U^* - \\Theta\\big(\\sqrt{\\tfrac{\\mathrm{Reg}(T)}{T}}\\big)$, where $U^*$ is the\nprincipal's optimal utility in the classic model with a best-responding agent.\n(2) If the agent uses contextual no-swap-regret learning algorithms with\nswap-regret $\\mathrm{SReg}(T)$, then the principal cannot obtain utility more\nthan $U^* + O(\\frac{\\mathrm{SReg(T)}}{T})$. But (3) if the agent uses\nmean-based learning algorithms (which can be no-regret but not no-swap-regret),\nthen the principal can sometimes do significantly better than $U^*$. These\nresults not only refine previous results in Stackelberg games and contract\ndesign, but also lead to new results for Bayesian persuasion with a learning\nagent and all generalized principal-agent problems where the agent does not\nhave private information.\n","authors":["Tao Lin","Yiling Chen"],"pdf_url":"https://arxiv.org/pdf/2402.09721v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16422v1","updated":"2024-11-25T14:27:07Z","published":"2024-11-25T14:27:07Z","title":"Turbofan Engine Remaining Useful Life (RUL) Prediction Based on\n  Bi-Directional Long Short-Term Memory (BLSTM)","summary":"  The aviation industry is rapidly evolving, driven by advancements in\ntechnology. Turbofan engines used in commercial aerospace are very complex\nsystems. The majority of turbofan engine components are susceptible to\ndegradation over the life of their operation. Turbofan engine degradation has\nan impact to engine performance, operability, and reliability. Predicting\naccurate remaining useful life (RUL) of a commercial turbofan engine based on a\nvariety of complex sensor data is of paramount importance for the safety of the\npassengers, safety of flight, and for cost effective operations. That is why it\nis essential for turbofan engines to be monitored, controlled, and maintained.\nRUL predictions can either come from model-based or data-based approaches. The\nmodel-based approach can be very expensive due to the complexity of the\nmathematical models and the deep expertise that is required in the domain of\nphysical systems. The data-based approach is more frequently used nowadays\nthanks to the high computational complexity of computers, the advancements in\nMachine Learning (ML) models, and advancements in sensors. This paper is going\nto be focused on Bi-Directional Long Short-Term Memory (BLSTM) models but will\nalso provide a benchmark of several RUL prediction databased models. The\nproposed RUL prediction models are going to be evaluated based on engine\nfailure prediction benchmark dataset Commercial Modular Aero-Propulsion System\nSimulation (CMAPSS). The CMAPSS dataset is from NASA which contains turbofan\nengine run to failure events.\n","authors":["Abedin Sherifi"],"pdf_url":"https://arxiv.org/pdf/2411.16422v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16421v1","updated":"2024-11-25T14:25:39Z","published":"2024-11-25T14:25:39Z","title":"Machine Learning for the Digital Typhoon Dataset: Extensions to Multiple\n  Basins and New Developments in Representations and Tasks","summary":"  This paper presents the Digital Typhoon Dataset V2, a new version of the\nlongest typhoon satellite image dataset for 40+ years aimed at benchmarking\nmachine learning models for long-term spatio-temporal data. The new addition in\nDataset V2 is tropical cyclone data from the southern hemisphere, in addition\nto the northern hemisphere data in Dataset V1. Having data from two hemispheres\nallows us to ask new research questions about regional differences across\nbasins and hemispheres. We also discuss new developments in representations and\ntasks of the dataset. We first introduce a self-supervised learning framework\nfor representation learning. Combined with the LSTM model, we discuss\nperformance on intensity forecasting and extra-tropical transition forecasting\ntasks. We then propose new tasks, such as the typhoon center estimation task.\nWe show that an object detection-based model performs better for stronger\ntyphoons. Finally, we study how machine learning models can generalize across\nbasins and hemispheres, by training the model on the northern hemisphere data\nand testing it on the southern hemisphere data. The dataset is publicly\navailable at \\url{http://agora.ex.nii.ac.jp/digital-typhoon/dataset/} and\n\\url{https://github.com/kitamoto-lab/digital-typhoon/}.\n","authors":["Asanobu Kitamoto","Erwan Dzik","Gaspar Faure"],"pdf_url":"https://arxiv.org/pdf/2411.16421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13951v2","updated":"2024-11-25T14:24:57Z","published":"2024-11-21T09:03:12Z","title":"A Dataset for Evaluating Online Anomaly Detection Approaches for\n  Discrete Multivariate Time Series","summary":"  Benchmarking anomaly detection approaches for multivariate time series is\nchallenging due to the lack of high-quality datasets. Current publicly\navailable datasets are too small, not diverse and feature trivial anomalies,\nwhich hinders measurable progress in this research area. We propose a solution:\na diverse, extensive, and non-trivial dataset generated via state-of-the-art\nsimulation tools that reflects realistic behaviour of an automotive powertrain,\nincluding its multivariate, dynamic and variable-state properties. To cater for\nboth unsupervised and semi-supervised anomaly detection settings, as well as\ntime series generation and forecasting, we make different versions of the\ndataset available, where training and test subsets are offered in contaminated\nand clean versions, depending on the task. We also provide baseline results\nfrom a small selection of approaches based on deterministic and variational\nautoencoders, as well as a non-parametric approach. As expected, the baseline\nexperimentation shows that the approaches trained on the semi-supervised\nversion of the dataset outperform their unsupervised counterparts, highlighting\na need for approaches more robust to contaminated training data.\n","authors":["Lucas Correia","Jan-Christoph Goos","Thomas Bäck","Anna V. Kononova"],"pdf_url":"https://arxiv.org/pdf/2411.13951v2.pdf","comment":"Submitted to the IEEE Transactions on Reliability journal"},{"id":"http://arxiv.org/abs/2411.16396v1","updated":"2024-11-25T14:01:55Z","published":"2024-11-25T14:01:55Z","title":"Statistical inference for quantum singular models","summary":"  Deep learning has seen substantial achievements, with numerical and\ntheoretical evidence suggesting that singularities of statistical models are\nconsidered a contributing factor to its performance. From this remarkable\nsuccess of classical statistical models, it is naturally expected that quantum\nsingular models will play a vital role in many quantum statistical tasks.\nHowever, while the theory of quantum statistical models in regular cases has\nbeen established, theoretical understanding of quantum singular models is still\nlimited. To investigate the statistical properties of quantum singular models,\nwe focus on two prominent tasks in quantum statistical inference: quantum state\nestimation and model selection. In particular, we base our study on classical\nsingular learning theory and seek to extend it within the framework of Bayesian\nquantum state estimation. To this end, we define quantum generalization and\ntraining loss functions and give their asymptotic expansions through algebraic\ngeometrical methods. The key idea of the proof is the introduction of a quantum\nanalog of the likelihood function using classical shadows. Consequently, we\nconstruct an asymptotically unbiased estimator of the quantum generalization\nloss, the quantum widely applicable information criterion (QWAIC), as a\ncomputable model selection metric from given measurement outcomes.\n","authors":["Hiroshi Yano","Yota Maeda","Naoki Yamamoto"],"pdf_url":"https://arxiv.org/pdf/2411.16396v1.pdf","comment":"57 pages, 8 figures"},{"id":"http://arxiv.org/abs/2306.01646v3","updated":"2024-11-25T13:59:11Z","published":"2023-06-02T16:15:24Z","title":"Auditing for Human Expertise","summary":"  High-stakes prediction tasks (e.g., patient diagnosis) are often handled by\ntrained human experts. A common source of concern about automation in these\nsettings is that experts may exercise intuition that is difficult to model\nand/or have access to information (e.g., conversations with a patient) that is\nsimply unavailable to a would-be algorithm. This raises a natural question\nwhether human experts add value which could not be captured by an algorithmic\npredictor. We develop a statistical framework under which we can pose this\nquestion as a natural hypothesis test. Indeed, as our framework highlights,\ndetecting human expertise is more subtle than simply comparing the accuracy of\nexpert predictions to those made by a particular learning algorithm. Instead,\nwe propose a simple procedure which tests whether expert predictions are\nstatistically independent from the outcomes of interest after conditioning on\nthe available inputs (`features'). A rejection of our test thus suggests that\nhuman experts may add value to any algorithm trained on the available data, and\nhas direct implications for whether human-AI `complementarity' is achievable in\na given prediction task. We highlight the utility of our procedure using\nadmissions data collected from the emergency department of a large academic\nhospital system, where we show that physicians' admit/discharge decisions for\npatients with acute gastrointestinal bleeding (AGIB) appear to be incorporating\ninformation that is not available to a standard algorithmic screening tool.\nThis is despite the fact that the screening tool is arguably more accurate than\nphysicians' discretionary decisions, highlighting that -- even absent normative\nconcerns about accountability or interpretability -- accuracy is insufficient\nto justify algorithmic automation.\n","authors":["Rohan Alur","Loren Laine","Darrick K. Li","Manish Raghavan","Devavrat Shah","Dennis Shung"],"pdf_url":"https://arxiv.org/pdf/2306.01646v3.pdf","comment":"30 pages, 10 figures. Appeared in the proceedings of the 37th\n  Conference on Neural Information Processing Systems (NeurIPS 2023). 11/2024\n  replacement fixes typo in the definition of $\\tau_k$, as pointed out by\n  Liuquan Nie"},{"id":"http://arxiv.org/abs/2409.15100v2","updated":"2024-11-25T13:51:04Z","published":"2024-09-23T15:11:40Z","title":"Robust Federated Learning Over the Air: Combating Heavy-Tailed Noise\n  with Median Anchored Clipping","summary":"  Leveraging over-the-air computations for model aggregation is an effective\napproach to cope with the communication bottleneck in federated edge learning.\nBy exploiting the superposition properties of multi-access channels, this\napproach facilitates an integrated design of communication and computation,\nthereby enhancing system privacy while reducing implementation costs. However,\nthe inherent electromagnetic interference in radio channels often exhibits\nheavy-tailed distributions, giving rise to exceptionally strong noise in\nglobally aggregated gradients that can significantly deteriorate the training\nperformance. To address this issue, we propose a novel gradient clipping\nmethod, termed Median Anchored Clipping (MAC), to combat the detrimental\neffects of heavy-tailed noise. We also derive analytical expressions for the\nconvergence rate of model training with analog over-the-air federated learning\nunder MAC, which quantitatively demonstrates the effect of MAC on training\nperformance. Extensive experimental results show that the proposed MAC\nalgorithm effectively mitigates the impact of heavy-tailed noise, hence\nsubstantially enhancing system robustness.\n","authors":["Jiaxing Li","Zihan Chen","Kai Fong Ernest Chong","Bikramjit Das","Tony Q. S. Quek","Howard H. Yang"],"pdf_url":"https://arxiv.org/pdf/2409.15100v2.pdf","comment":"This is the full version of the paper, and the appendix contains a\n  complete convergence analysis under non-convex conditions"},{"id":"http://arxiv.org/abs/2411.13615v2","updated":"2024-11-25T13:33:49Z","published":"2024-11-20T08:09:35Z","title":"A Deep Learning Approach to Predict the Fall [of Price] of\n  Cryptocurrency Long Before its Actual Fall","summary":"  In modern times, the cryptocurrency market is one of the world's most rapidly\nrising financial markets. The cryptocurrency market is regarded to be more\nvolatile and illiquid than traditional markets such as equities, foreign\nexchange, and commodities. The risk of this market creates an uncertain\ncondition among the investors. The purpose of this research is to predict the\nmagnitude of the risk factor of the cryptocurrency market. Risk factor is also\ncalled volatility. Our approach will assist people who invest in the\ncryptocurrency market by overcoming the problems and difficulties they\nexperience. Our approach starts with calculating the risk factor of the\ncryptocurrency market from the existing parameters. In twenty elements of the\ncryptocurrency market, the risk factor has been predicted using different\nmachine learning algorithms such as CNN, LSTM, BiLSTM, and GRU. All of the\nmodels have been applied to the calculated risk factor parameter. A new model\nhas been developed to predict better than the existing models. Our proposed\nmodel gives the highest RMSE value of 1.3229 and the lowest RMSE value of\n0.0089. Following our model, it will be easier for investors to trade in\ncomplicated and challenging financial assets like bitcoin, Ethereum, dogecoin,\netc. Where the other existing models, the highest RMSE was 14.5092, and the\nlower was 0.02769. So, the proposed model performs much better than models with\nproper generalization. Using our approach, it will be easier for investors to\ntrade in complicated and challenging financial assets like Bitcoin, Ethereum,\nand Dogecoin.\n","authors":["Anika Tahsin Meem"],"pdf_url":"https://arxiv.org/pdf/2411.13615v2.pdf","comment":"22 pages, 3 figures"},{"id":"http://arxiv.org/abs/2411.16370v1","updated":"2024-11-25T13:26:09Z","published":"2024-11-25T13:26:09Z","title":"A Review of Bayesian Uncertainty Quantification in Deep Probabilistic\n  Image Segmentation","summary":"  Advancements in image segmentation play an integral role within the greater\nscope of Deep Learning-based computer vision. Furthermore, their widespread\napplicability in critical real-world tasks has given rise to challenges related\nto the reliability of such algorithms. Hence, uncertainty quantification has\nbeen extensively studied within this context, enabling expression of model\nignorance (epistemic uncertainty) or data ambiguity (aleatoric uncertainty) to\nprevent uninformed decision making. Due to the rapid adoption of Convolutional\nNeural Network (CNN)-based segmentation models in high-stake applications, a\nsubstantial body of research has been published on this very topic, causing its\nswift expansion into a distinct field. This work provides a comprehensive\noverview of probabilistic segmentation by discussing fundamental concepts in\nuncertainty that govern advancements in the field as well as the application to\nvarious tasks. We identify that quantifying aleatoric and epistemic uncertainty\napproximates Bayesian inference w.r.t. to either latent variables or model\nparameters, respectively. Moreover, literature on both uncertainties trace back\nto four key applications; (1) to quantify statistical inconsistencies in the\nannotation process due ambiguous images, (2) correlating prediction error with\nuncertainty, (3) expanding the model hypothesis space for better\ngeneralization, and (4) active learning. Then, a discussion follows that\nincludes an overview of utilized datasets for each of the applications and\ncomparison of the available methods. We also highlight challenges related to\narchitectures, uncertainty-based active learning, standardization and\nbenchmarking, and recommendations for future work such as methods based on\nsingle forward passes and models that appropriately leverage volumetric data.\n","authors":["M. M. A. Valiuddin","R. J. G. van Sloun","C. G. A. Viviers","P. H. N. de With","F. van der Sommen"],"pdf_url":"https://arxiv.org/pdf/2411.16370v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2405.16623v2","updated":"2024-11-25T13:25:16Z","published":"2024-05-26T16:39:19Z","title":"Graph neural networks with configuration cross-attention for tensor\n  compilers","summary":"  With the recent popularity of neural networks comes the need for efficient\nserving of inference workloads. A neural network inference workload can be\nrepresented as a computational graph with nodes as operators transforming\nmultidimensional tensors. The tensors can be transposed and/or tiled in a\ncombinatorially large number of ways, some configurations leading to\naccelerated inference. We propose TGraph, a neural graph architecture that\nallows screening for fast configurations of the target computational graph,\nthus representing an artificial intelligence (AI) tensor compiler in contrast\nto the traditional heuristics-based compilers. The proposed solution improves\nmean Kendall's $\\tau$ across layout collections of TpuGraphs from 29.8% of the\nreliable baseline to 67.4% of TGraph. We estimate the potential CO$_2$ emission\nreduction associated with our work to be equivalent to over 50% of the total\nhousehold emissions in the areas hosting AI-oriented data centers.\n","authors":["Dmitrii Khizbullin","Eduardo Rocha de Andrade","Thanh Hau Nguyen","Matheus Pedroza Ferreira","David R. Pugh"],"pdf_url":"https://arxiv.org/pdf/2405.16623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16349v1","updated":"2024-11-25T12:58:00Z","published":"2024-11-25T12:58:00Z","title":"Machine learning for cerebral blood vessels' malformations","summary":"  Cerebral aneurysms and arteriovenous malformations are life-threatening\nhemodynamic pathologies of the brain. While surgical intervention is often\nessential to prevent fatal outcomes, it carries significant risks both during\nthe procedure and in the postoperative period, making the management of these\nconditions highly challenging. Parameters of cerebral blood flow, routinely\nmonitored during medical interventions, could potentially be utilized in\nmachine learning-assisted protocols for risk assessment and therapeutic\nprognosis. To this end, we developed a linear oscillatory model of blood\nvelocity and pressure for clinical data acquired from neurosurgical operations.\nUsing the method of Sparse Identification of Nonlinear Dynamics (SINDy), the\nparameters of our model can be reconstructed online within milliseconds from a\nshort time series of the hemodynamic variables. The identified parameter values\nenable automated classification of the blood-flow pathologies by means of\nlogistic regression, achieving an accuracy of 73 %. Our results demonstrate the\npotential of this model for both diagnostic and prognostic applications,\nproviding a robust and interpretable framework for assessing cerebral blood\nvessel conditions.\n","authors":["Irem Topal","Alexander Cherevko","Yuri Bugay","Maxim Shishlenin","Jean Barbier","Deniz Eroglu","Édgar Roldán","Roman Belousov"],"pdf_url":"https://arxiv.org/pdf/2411.16349v1.pdf","comment":"14 pages, 6 main figures, 5 supplementary figures, 2 supplementary\n  tables"},{"id":"http://arxiv.org/abs/2411.16346v1","updated":"2024-11-25T12:49:55Z","published":"2024-11-25T12:49:55Z","title":"Towards Foundation Models for Critical Care Time Series","summary":"  Notable progress has been made in generalist medical large language models\nacross various healthcare areas. However, large-scale modeling of in-hospital\ntime series data - such as vital signs, lab results, and treatments in critical\ncare - remains underexplored. Existing datasets are relatively small, but\ncombining them can enhance patient diversity and improve model robustness. To\neffectively utilize these combined datasets for large-scale modeling, it is\nessential to address the distribution shifts caused by varying treatment\npolicies, necessitating the harmonization of treatment variables across the\ndifferent datasets. This work aims to establish a foundation for training\nlarge-scale multi-variate time series models on critical care data and to\nprovide a benchmark for machine learning models in transfer learning across\nhospitals to study and address distribution shift challenges. We introduce a\nharmonized dataset for sequence modeling and transfer learning research,\nrepresenting the first large-scale collection to include core treatment\nvariables. Future plans involve expanding this dataset to support further\nadvancements in transfer learning and the development of scalable,\ngeneralizable models for critical healthcare applications.\n","authors":["Manuel Burger","Fedor Sergeev","Malte Londschien","Daphné Chopard","Hugo Yèche","Eike Gerdes","Polina Leshetkina","Alexander Morgenroth","Zeynep Babür","Jasmina Bogojeska","Martin Faltys","Rita Kuznetsova","Gunnar Rätsch"],"pdf_url":"https://arxiv.org/pdf/2411.16346v1.pdf","comment":"Accepted for Oral Presentation at AIM-FM Workshop at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2405.12085v2","updated":"2024-11-25T12:42:15Z","published":"2024-05-20T14:55:20Z","title":"Noise-tolerant learnability of shallow quantum circuits from statistics\n  and the cost of quantum pseudorandomness","summary":"  This work studies the learnability of quantum circuits in the near term. We\nshow the natural robustness of quantum statistical queries for learning quantum\nprocesses and provide an efficient way to benchmark global depolarizing noise\nfrom statistics, which gives us a powerful framework for developing\nnoise-tolerant algorithms. We adapt a learning algorithm for constant-depth\nquantum circuits to the quantum statistical query setting with a small overhead\nin the query complexity. We prove average-case lower bounds for learning random\nquantum circuits of logarithmic and higher depths within diamond distance with\nstatistical queries. Finally, we prove that pseudorandom unitaries (PRUs)\ncannot be constructed using circuits of constant depth by constructing an\nefficient distinguisher and proving a new variation of the quantum no-free\nlunch theorem.\n","authors":["Chirag Wadhwa","Mina Doosti"],"pdf_url":"https://arxiv.org/pdf/2405.12085v2.pdf","comment":"21+7 pages, 1 figure, 1 table"},{"id":"http://arxiv.org/abs/2411.10096v2","updated":"2024-11-25T12:41:46Z","published":"2024-11-15T10:44:29Z","title":"Neural Port-Hamiltonian Models for Nonlinear Distributed Control: An\n  Unconstrained Parametrization Approach","summary":"  The control of large-scale cyber-physical systems requires optimal\ndistributed policies relying solely on limited communication with neighboring\nagents. However, computing stabilizing controllers for nonlinear systems while\noptimizing complex costs remains a significant challenge. Neural Networks\n(NNs), known for their expressivity, can be leveraged to parametrize control\npolicies that yield good performance. However, NNs' sensitivity to small input\nchanges poses a risk of destabilizing the closed-loop system. Many existing\napproaches enforce constraints on the controllers' parameter space to guarantee\nclosed-loop stability, leading to computationally expensive optimization\nprocedures. To address these problems, we leverage the framework of\nport-Hamiltonian systems to design continuous-time distributed control policies\nfor nonlinear systems that guarantee closed-loop stability and finite\n$\\mathcal{L}_2$ or incremental $\\mathcal{L}_2$ gains, independent of the\noptimzation parameters of the controllers. This eliminates the need to\nconstrain parameters during optimization, allowing the use of standard\ntechniques such as gradient-based methods. Additionally, we discuss\ndiscretization schemes that preserve the dissipation properties of these\ncontrollers for implementation on embedded systems. The effectiveness of the\nproposed distributed controllers is demonstrated through consensus control of\nnon-holonomic mobile robots subject to collision avoidance and averaged voltage\nregulation with weighted power sharing in DC microgrids.\n","authors":["Muhammad Zakwan","Giancarlo Ferrari-Trecate"],"pdf_url":"https://arxiv.org/pdf/2411.10096v2.pdf","comment":"The paper has 15 pages, and has been submitted for a possible\n  publication. arXiv admin note: text overlap with arXiv:2403.17785"},{"id":"http://arxiv.org/abs/2411.16342v1","updated":"2024-11-25T12:38:59Z","published":"2024-11-25T12:38:59Z","title":"A Data-Driven Approach to Dataflow-Aware Online Scheduling for Graph\n  Neural Network Inference","summary":"  Graph Neural Networks (GNNs) have shown significant promise in various\ndomains, such as recommendation systems, bioinformatics, and network analysis.\nHowever, the irregularity of graph data poses unique challenges for efficient\ncomputation, leading to the development of specialized GNN accelerator\narchitectures that surpass traditional CPU and GPU performance. Despite this,\nthe structural diversity of input graphs results in varying performance across\ndifferent GNN accelerators, depending on their dataflows. This variability in\nperformance due to differing dataflows and graph properties remains largely\nunexplored, limiting the adaptability of GNN accelerators. To address this, we\npropose a data-driven framework for dataflow-aware latency prediction in GNN\ninference. Our approach involves training regressors to predict the latency of\nexecuting specific graphs on particular dataflows, using simulations on\nsynthetic graphs. Experimental results indicate that our regressors can predict\nthe optimal dataflow for a given graph with up to 91.28% accuracy and a Mean\nAbsolute Percentage Error (MAPE) of 3.78%. Additionally, we introduce an online\nscheduling algorithm that uses these regressors to enhance scheduling\ndecisions. Our experiments demonstrate that this algorithm achieves up to\n$3.17\\times$ speedup in mean completion time and $6.26\\times$ speedup in mean\nexecution time compared to the best feasible baseline across all datasets.\n","authors":["Pol Puigdemont","Enrico Russo","Axel Wassington","Abhijit Das","Sergi Abadal","Maurizio Palesi"],"pdf_url":"https://arxiv.org/pdf/2411.16342v1.pdf","comment":"Accepted for ASP-DAC 2025"},{"id":"http://arxiv.org/abs/2411.16339v1","updated":"2024-11-25T12:36:15Z","published":"2024-11-25T12:36:15Z","title":"Solaris: A Foundation Model of the Sun","summary":"  Foundation models have demonstrated remarkable success across various\nscientific domains, motivating our exploration of their potential in solar\nphysics. In this paper, we present Solaris, the first foundation model for\nforecasting the Sun's atmosphere. We leverage 13 years of full-disk,\nmulti-wavelength solar imagery from the Solar Dynamics Observatory, spanning a\ncomplete solar cycle, to pre-train Solaris for 12-hour interval forecasting.\nSolaris is built on a large-scale 3D Swin Transformer architecture with 109\nmillion parameters. We demonstrate Solaris' ability to generalize by\nfine-tuning on a low-data regime using a single wavelength (1700 {\\AA}), that\nwas not included in pre-training, outperforming models trained from scratch on\nthis specific wavelength. Our results indicate that Solaris can effectively\ncapture the complex dynamics of the solar atmosphere and transform solar\nforecasting.\n","authors":["Harris Abdul Majid","Pietro Sittoni","Francesco Tudisco"],"pdf_url":"https://arxiv.org/pdf/2411.16339v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04398v3","updated":"2024-11-25T12:26:11Z","published":"2023-12-07T16:10:10Z","title":"Intelligent Anomaly Detection for Lane Rendering Using Transformer with\n  Self-Supervised Pre-Training and Customized Fine-Tuning","summary":"  The burgeoning navigation services using digital maps provide great\nconvenience to drivers. Nevertheless, the presence of anomalies in lane\nrendering map images occasionally introduces potential hazards, as such\nanomalies can be misleading to human drivers and consequently contribute to\nunsafe driving conditions. In response to this concern and to accurately and\neffectively detect the anomalies, this paper transforms lane rendering image\nanomaly detection into a classification problem and proposes a four-phase\npipeline consisting of data pre-processing, self-supervised pre-training with\nthe masked image modeling (MiM) method, customized fine-tuning using\ncross-entropy based loss with label smoothing, and post-processing to tackle it\nleveraging state-of-the-art deep learning techniques, especially those\ninvolving Transformer models. Various experiments verify the effectiveness of\nthe proposed pipeline. Results indicate that the proposed pipeline exhibits\nsuperior performance in lane rendering image anomaly detection, and notably,\nthe self-supervised pre-training with MiM can greatly enhance the detection\naccuracy while significantly reducing the total training time. For instance,\nemploying the Swin Transformer with Uniform Masking as self-supervised\npretraining (Swin-Trans-UM) yielded a heightened accuracy at 94.77% and an\nimproved Area Under The Curve (AUC) score of 0.9743 compared with the pure Swin\nTransformer without pre-training (Swin-Trans) with an accuracy of 94.01% and an\nAUC of 0.9498. The fine-tuning epochs were dramatically reduced to 41 from the\noriginal 280. In conclusion, the proposed pipeline, with its incorporation of\nself-supervised pre-training using MiM and other advanced deep learning\ntechniques, emerges as a robust solution for enhancing the accuracy and\nefficiency of lane rendering image anomaly detection in digital navigation\nsystems.\n","authors":["Yongqi Dong","Xingmin Lu","Ruohan Li","Wei Song","Bart van Arem","Haneen Farah"],"pdf_url":"https://arxiv.org/pdf/2312.04398v3.pdf","comment":"25 pages, 7 figures, accepted by the 103rd Transportation Research\n  Board (TRB) Annual Meeting, under review by Transportation Research Record:\n  Journal of the Transportation Research Board"},{"id":"http://arxiv.org/abs/2411.08460v2","updated":"2024-11-25T12:10:05Z","published":"2024-11-13T09:31:06Z","title":"Trap-MID: Trapdoor-based Defense against Model Inversion Attacks","summary":"  Model Inversion (MI) attacks pose a significant threat to the privacy of Deep\nNeural Networks by recovering training data distribution from well-trained\nmodels. While existing defenses often rely on regularization techniques to\nreduce information leakage, they remain vulnerable to recent attacks. In this\npaper, we propose the Trapdoor-based Model Inversion Defense (Trap-MID) to\nmislead MI attacks. A trapdoor is integrated into the model to predict a\nspecific label when the input is injected with the corresponding trigger.\nConsequently, this trapdoor information serves as the \"shortcut\" for MI\nattacks, leading them to extract trapdoor triggers rather than private data. We\nprovide theoretical insights into the impacts of trapdoor's effectiveness and\nnaturalness on deceiving MI attacks. In addition, empirical experiments\ndemonstrate the state-of-the-art defense performance of Trap-MID against\nvarious MI attacks without the requirements for extra data or large\ncomputational overhead. Our source code is publicly available at\nhttps://github.com/ntuaislab/Trap-MID.\n","authors":["Zhen-Ting Liu","Shang-Tse Chen"],"pdf_url":"https://arxiv.org/pdf/2411.08460v2.pdf","comment":"Accepted by Neural Information Processing Systems (NeurIPS) 2024"},{"id":"http://arxiv.org/abs/2411.16315v1","updated":"2024-11-25T12:08:54Z","published":"2024-11-25T12:08:54Z","title":"Local Learning for Covariate Selection in Nonparametric Causal Effect\n  Estimation with Latent Variables","summary":"  Estimating causal effects from nonexperimental data is a fundamental problem\nin many fields of science. A key component of this task is selecting an\nappropriate set of covariates for confounding adjustment to avoid bias. Most\nexisting methods for covariate selection often assume the absence of latent\nvariables and rely on learning the global network structure among variables.\nHowever, identifying the global structure can be unnecessary and inefficient,\nespecially when our primary interest lies in estimating the effect of a\ntreatment variable on an outcome variable. To address this limitation, we\npropose a novel local learning approach for covariate selection in\nnonparametric causal effect estimation, which accounts for the presence of\nlatent variables. Our approach leverages testable independence and dependence\nrelationships among observed variables to identify a valid adjustment set for a\ntarget causal relationship, ensuring both soundness and completeness under\nstandard assumptions. We validate the effectiveness of our algorithm through\nextensive experiments on both synthetic and real-world data.\n","authors":["Zheng Li","Feng Xie","Yan Zeng","Zhi Geng"],"pdf_url":"https://arxiv.org/pdf/2411.16315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16313v1","updated":"2024-11-25T12:05:49Z","published":"2024-11-25T12:05:49Z","title":"CATP-LLM: Empowering Large Language Models for Cost-Aware Tool Planning","summary":"  Utilizing large language models (LLMs) for tool planning has emerged as a\npromising avenue for developing general AI systems, where LLMs automatically\nschedule external tools (e.g. vision models) to tackle complex tasks based on\ntask descriptions. To push this paradigm toward practical applications, it is\ncrucial for LLMs to consider tool execution costs (e.g. execution time) for\ntool planning. Unfortunately, prior studies overlook the tool execution costs,\nleading to the generation of expensive plans of which the costs outweigh task\nperformance. To fill this gap, we propose the Cost-Aware Tool Planning with\nLLMs (CATP-LLM) framework, which for the first time provides a coherent design\nto empower LLMs for cost-aware tool planning. Specifically, CATP-LLM\nincorporates a tool planning language to enhance the LLM to generate\nnon-sequential plans of multiple branches for efficient concurrent tool\nexecution and cost reduction. Moreover, it further designs a cost-aware offline\nreinforcement learning algorithm to fine-tune the LLM to optimize the\nperformance-cost trade-off in tool planning. In lack of public cost-related\ndatasets, we further present OpenCATP, the first platform for cost-aware\nplanning evaluation. Experiments on OpenCATP show that CATP-LLM outperforms\nGPT-4 even when using Llama2-7B as its backbone, with the average improvement\nof 28.2%-30.2% higher plan performance and 24.7%-45.8% lower costs even on the\nchallenging planning tasks. The codes of CATP-LLM and OpenCATP will be publicly\navailable.\n","authors":["Duo Wu","Jinghe Wang","Yuan Meng","Yanning Zhang","Le Sun","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2411.16313v1.pdf","comment":"In submission"},{"id":"http://arxiv.org/abs/2405.18979v3","updated":"2024-11-25T12:00:29Z","published":"2024-05-29T10:45:06Z","title":"MANO: Exploiting Matrix Norm for Unsupervised Accuracy Estimation Under\n  Distribution Shifts","summary":"  Leveraging the models' outputs, specifically the logits, is a common approach\nto estimating the test accuracy of a pre-trained neural network on\nout-of-distribution (OOD) samples without requiring access to the corresponding\nground truth labels. Despite their ease of implementation and computational\nefficiency, current logit-based methods are vulnerable to overconfidence\nissues, leading to prediction bias, especially under the natural shift. In this\nwork, we first study the relationship between logits and generalization\nperformance from the view of low-density separation assumption. Our findings\nmotivate our proposed method MaNo which (1) applies a data-dependent\nnormalization on the logits to reduce prediction bias, and (2) takes the $L_p$\nnorm of the matrix of normalized logits as the estimation score. Our\ntheoretical analysis highlights the connection between the provided score and\nthe model's uncertainty. We conduct an extensive empirical study on common\nunsupervised accuracy estimation benchmarks and demonstrate that MaNo achieves\nstate-of-the-art performance across various architectures in the presence of\nsynthetic, natural, or subpopulation shifts. The code is available at\n\\url{https://github.com/Renchunzi-Xie/MaNo}.\n","authors":["Renchunzi Xie","Ambroise Odonnat","Vasilii Feofanov","Weijian Deng","Jianfeng Zhang","Bo An"],"pdf_url":"https://arxiv.org/pdf/2405.18979v3.pdf","comment":"The three first authors contributed equally"},{"id":"http://arxiv.org/abs/2411.16305v1","updated":"2024-11-25T11:47:31Z","published":"2024-11-25T11:47:31Z","title":"Learning from Relevant Subgoals in Successful Dialogs using Iterative\n  Training for Task-oriented Dialog Systems","summary":"  Task-oriented Dialog (ToD) systems have to solve multiple subgoals to\naccomplish user goals, whereas feedback is often obtained only at the end of\nthe dialog. In this work, we propose SUIT (SUbgoal-aware ITerative Training),\nan iterative training approach for improving ToD systems. We sample dialogs\nfrom the model we aim to improve and determine subgoals that contribute to\ndialog success using distant supervision to obtain high quality training\nsamples. We show how this data improves supervised fine-tuning or,\nalternatively, preference learning results. SUIT is able to iteratively\ngenerate more data instead of relying on fixed static sets. SUIT reaches new\nstate-of-the-art performance on a popular ToD benchmark.\n","authors":["Magdalena Kaiser","Patrick Ernst","György Szarvas"],"pdf_url":"https://arxiv.org/pdf/2411.16305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16303v1","updated":"2024-11-25T11:43:22Z","published":"2024-11-25T11:43:22Z","title":"Understanding Generalization of Federated Learning: the Trade-off\n  between Model Stability and Optimization","summary":"  Federated Learning (FL) is a distributed learning approach that trains neural\nnetworks across multiple devices while keeping their local data private.\nHowever, FL often faces challenges due to data heterogeneity, leading to\ninconsistent local optima among clients. These inconsistencies can cause\nunfavorable convergence behavior and generalization performance degradation.\nExisting studies mainly describe this issue through \\textit{convergence\nanalysis}, focusing on how well a model fits training data, or through\n\\textit{algorithmic stability}, which examines the generalization gap. However,\nneither approach precisely captures the generalization performance of FL\nalgorithms, especially for neural networks. In this paper, we introduce the\nfirst generalization dynamics analysis framework in federated optimization,\nhighlighting the trade-offs between model stability and optimization. Through\nthis framework, we show how the generalization of FL algorithms is affected by\nthe interplay of algorithmic stability and optimization. This framework applies\nto standard federated optimization and its advanced versions, like server\nmomentum. We find that fast convergence from large local steps or accelerated\nmomentum enlarges stability but obtains better generalization performance. Our\ninsights into these trade-offs can guide the practice of future algorithms for\nbetter generalization.\n","authors":["Dun Zeng","Zheshun Wu","Shiyu Liu","Yu Pan","Xiaoying Tang","Zenglin Xu"],"pdf_url":"https://arxiv.org/pdf/2411.16303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16301v1","updated":"2024-11-25T11:36:34Z","published":"2024-11-25T11:36:34Z","title":"DiffDesign: Controllable Diffusion with Meta Prior for Efficient\n  Interior Design Generation","summary":"  Interior design is a complex and creative discipline involving aesthetics,\nfunctionality, ergonomics, and materials science. Effective solutions must meet\ndiverse requirements, typically producing multiple deliverables such as\nrenderings and design drawings from various perspectives. Consequently,\ninterior design processes are often inefficient and demand significant\ncreativity. With advances in machine learning, generative models have emerged\nas a promising means of improving efficiency by creating designs from text\ndescriptions or sketches. However, few generative works focus on interior\ndesign, leading to substantial discrepancies between outputs and practical\nneeds, such as differences in size, spatial scope, and the lack of controllable\ngeneration quality. To address these challenges, we propose DiffDesign, a\ncontrollable diffusion model with meta priors for efficient interior design\ngeneration. Specifically, we utilize the generative priors of a 2D diffusion\nmodel pre-trained on a large image dataset as our rendering backbone. We\nfurther guide the denoising process by disentangling cross-attention control\nover design attributes, such as appearance, pose, and size, and introduce an\noptimal transfer-based alignment module to enforce view consistency.\nSimultaneously, we construct an interior design-specific dataset, DesignHelper,\nconsisting of over 400 solutions across more than 15 spatial types and 15\ndesign styles. This dataset helps fine-tune DiffDesign. Extensive experiments\nconducted on various benchmark datasets demonstrate the effectiveness and\nrobustness of DiffDesign.\n","authors":["Yuxuan Yang","Jingyao Wang","Tao Geng","Wenwen Qiang","Changwen Zheng","Fuchun Sun"],"pdf_url":"https://arxiv.org/pdf/2411.16301v1.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2411.16298v1","updated":"2024-11-25T11:31:53Z","published":"2024-11-25T11:31:53Z","title":"Evaluating Rank-N-Contrast: Continuous and Robust Representations for\n  Regression","summary":"  This document is a replication of the original \"Rank-N-Contrast\"\n(arXiv:2210.01189v2) paper published in 2023. This evaluation is done for\nacademic purposes. Deep regression models often fail to capture the continuous\nnature of sample orders, creating fragmented representations and suboptimal\nperformance. To address this, we reproduced the Rank-N-Contrast (RNC)\nframework, which learns continuous representations by contrasting samples by\ntheir rankings in the target space. Our study validates RNC's theoretical and\nempirical benefits, including improved performance and robustness. We extended\nthe evaluation to an additional regression dataset and conducted robustness\ntests using a holdout method, where a specific range of continuous data was\nexcluded from the training set. This approach assessed the model's ability to\ngeneralise to unseen data and achieve state-of-the-art performance. This\nreplication study validates the original findings and broadens the\nunderstanding of RNC's applicability and robustness.\n","authors":["Six Valentin","Chidiac Alexandre","Worlikar Arkin"],"pdf_url":"https://arxiv.org/pdf/2411.16298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16285v1","updated":"2024-11-25T11:10:16Z","published":"2024-11-25T11:10:16Z","title":"A Graph Neural Architecture Search Approach for Identifying Bots in\n  Social Media","summary":"  Social media platforms, including X, Facebook, and Instagram, host millions\nof daily users, giving rise to bots-automated programs disseminating\nmisinformation and ideologies with tangible real-world consequences. While bot\ndetection in platform X has been the area of many deep learning models with\nadequate results, most approaches neglect the graph structure of social media\nrelationships and often rely on hand-engineered architectures. Our work\nintroduces the implementation of a Neural Architecture Search (NAS) technique,\nnamely Deep and Flexible Graph Neural Architecture Search (DFG-NAS), tailored\nto Relational Graph Convolutional Neural Networks (RGCNs) in the task of bot\ndetection in platform X. Our model constructs a graph that incorporates both\nthe user relationships and their metadata. Then, DFG-NAS is adapted to\nautomatically search for the optimal configuration of Propagation and\nTransformation functions in the RGCNs. Our experiments are conducted on the\nTwiBot-20 dataset, constructing a graph with 229,580 nodes and 227,979 edges.\nWe study the five architectures with the highest performance during the search\nand achieve an accuracy of 85.7%, surpassing state-of-the-art models. Our\napproach not only addresses the bot detection challenge but also advocates for\nthe broader implementation of NAS models in neural network design automation.\n","authors":["Georgios Tzoumanekas","Michail Chatzianastasis","Loukas Ilias","George Kiokes","John Psarras","Dimitris Askounis"],"pdf_url":"https://arxiv.org/pdf/2411.16285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16278v1","updated":"2024-11-25T10:59:25Z","published":"2024-11-25T10:59:25Z","title":"Even Sparser Graph Transformers","summary":"  Graph Transformers excel in long-range dependency modeling, but generally\nrequire quadratic memory complexity in the number of nodes in an input graph,\nand hence have trouble scaling to large graphs. Sparse attention variants such\nas Exphormer can help, but may require high-degree augmentations to the input\ngraph for good performance, and do not attempt to sparsify an already-dense\ninput graph. As the learned attention mechanisms tend to use few of these\nedges, such high-degree connections may be unnecessary. We show (empirically\nand with theoretical backing) that attention scores on graphs are usually quite\nconsistent across network widths, and use this observation to propose a\ntwo-stage procedure, which we call Spexphormer: first, train a narrow network\non the full augmented graph. Next, use only the active connections to train a\nwider network on a much sparser graph. We establish theoretical conditions when\na narrow network's attention scores can match those of a wide network, and show\nthat Spexphormer achieves good performance with drastically reduced memory\nrequirements on various graph datasets.\n","authors":["Hamed Shirzad","Honghao Lin","Balaji Venkatachalam","Ameya Velingker","David Woodruff","Danica Sutherland"],"pdf_url":"https://arxiv.org/pdf/2411.16278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15402v3","updated":"2024-11-25T10:54:48Z","published":"2024-02-23T16:05:51Z","title":"Grasp, See and Place: Efficient Unknown Object Rearrangement with Policy\n  Structure Prior","summary":"  We focus on the task of unknown object rearrangement, where a robot is\nsupposed to re-configure the objects into a desired goal configuration\nspecified by an RGB-D image. Recent works explore unknown object rearrangement\nsystems by incorporating learning-based perception modules. However, they are\nsensitive to perception error, and pay less attention to task-level\nperformance. In this paper, we aim to develop an effective system for unknown\nobject rearrangement amidst perception noise. We theoretically reveal that the\nnoisy perception impacts grasp and place in a decoupled way, and show such a\ndecoupled structure is valuable to improve task optimality. We propose GSP, a\ndual-loop system with the decoupled structure as prior. For the inner loop, we\nlearn a see policy for self-confident in-hand object matching. For the outer\nloop, we learn a grasp policy aware of object matching and grasp capability\nguided by task-level rewards. We leverage the foundation model CLIP for object\nmatching, policy learning and self-termination. A series of experiments\nindicate that GSP can conduct unknown object rearrangement with higher\ncompletion rates and fewer steps.\n","authors":["Kechun Xu","Zhongxiang Zhou","Jun Wu","Haojian Lu","Rong Xiong","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2402.15402v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16273v1","updated":"2024-11-25T10:51:40Z","published":"2024-11-25T10:51:40Z","title":"Deep Learning for Motion Classification in Ankle Exoskeletons Using\n  Surface EMG and IMU Signals","summary":"  Ankle exoskeletons have garnered considerable interest for their potential to\nenhance mobility and reduce fall risks, particularly among the aging\npopulation. The efficacy of these devices relies on accurate real-time\nprediction of the user's intended movements through sensor-based inputs. This\npaper presents a novel motion prediction framework that integrates three\nInertial Measurement Units (IMUs) and eight surface Electromyography (sEMG)\nsensors to capture both kinematic and muscular activity data. A comprehensive\nset of activities, representative of everyday movements in barrier-free\nenvironments, was recorded for the purpose. Our findings reveal that\nConvolutional Neural Networks (CNNs) slightly outperform Long Short-Term Memory\n(LSTM) networks on a dataset of five motion tasks, achieving classification\naccuracies of $96.5 \\pm 0.8 \\%$ and $87.5 \\pm 2.9 \\%$, respectively.\nFurthermore, we demonstrate the system's proficiency in transfer learning,\nenabling accurate motion classification for new subjects using just ten samples\nper class for finetuning. The robustness of the model is demonstrated by its\nresilience to sensor failures resulting in absent signals, maintaining reliable\nperformance in real-world scenarios. These results underscore the potential of\ndeep learning algorithms to enhance the functionality and safety of ankle\nexoskeletons, ultimately improving their usability in daily life.\n","authors":["Silas Ruhrberg Estévez","Josée Mallah","Dominika Kazieczko","Chenyu Tang","Luigi G. Occhipinti"],"pdf_url":"https://arxiv.org/pdf/2411.16273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00486v2","updated":"2024-11-25T10:50:55Z","published":"2023-12-01T10:34:22Z","title":"REDUCR: Robust Data Downsampling Using Class Priority Reweighting","summary":"  Modern machine learning models are becoming increasingly expensive to train\nfor real-world image and text classification tasks, where massive web-scale\ndata is collected in a streaming fashion. To reduce the training cost, online\nbatch selection techniques have been developed to choose the most informative\ndatapoints. However, these techniques can suffer from poor worst-class\ngeneralization performance due to class imbalance and distributional shifts.\nThis work introduces REDUCR, a robust and efficient data downsampling method\nthat uses class priority reweighting. REDUCR reduces the training data while\npreserving worst-class generalization performance. REDUCR assigns priority\nweights to datapoints in a class-aware manner using an online learning\nalgorithm. We demonstrate the data efficiency and robust performance of REDUCR\non vision and text classification tasks. On web-scraped datasets with\nimbalanced class distributions, REDUCR significantly improves worst-class test\naccuracy (and average accuracy), surpassing state-of-the-art methods by around\n15%.\n","authors":["William Bankes","George Hughes","Ilija Bogunovic","Zi Wang"],"pdf_url":"https://arxiv.org/pdf/2312.00486v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2205.12751v3","updated":"2024-11-25T10:47:56Z","published":"2022-05-25T13:01:09Z","title":"Fast Stochastic Composite Minimization and an Accelerated Frank-Wolfe\n  Algorithm under Parallelization","summary":"  We consider the problem of minimizing the sum of two convex functions. One of\nthose functions has Lipschitz-continuous gradients, and can be accessed via\nstochastic oracles, whereas the other is \"simple\". We provide a Bregman-type\nalgorithm with accelerated convergence in function values to a ball containing\nthe minimum. The radius of this ball depends on problem-dependent constants,\nincluding the variance of the stochastic oracle. We further show that this\nalgorithmic setup naturally leads to a variant of Frank-Wolfe achieving\nacceleration under parallelization. More precisely, when minimizing a smooth\nconvex function on a bounded domain, we show that one can achieve an $\\epsilon$\nprimal-dual gap (in expectation) in $\\tilde{O}(1/ \\sqrt{\\epsilon})$ iterations,\nby only accessing gradients of the original function and a linear maximization\noracle with $O(1/\\sqrt{\\epsilon})$ computing units in parallel. We illustrate\nthis fast convergence on synthetic numerical experiments.\n","authors":["Benjamin Dubois-Taine","Francis Bach","Quentin Berthet","Adrien Taylor"],"pdf_url":"https://arxiv.org/pdf/2205.12751v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09852v3","updated":"2024-11-25T10:47:11Z","published":"2023-12-15T14:58:34Z","title":"Learning Distributions on Manifolds with Free-Form Flows","summary":"  We propose Manifold Free-Form Flows (M-FFF), a simple new generative model\nfor data on manifolds. The existing approaches to learning a distribution on\narbitrary manifolds are expensive at inference time, since sampling requires\nsolving a differential equation. Our method overcomes this limitation by\nsampling in a single function evaluation. The key innovation is to optimize a\nneural network via maximum likelihood on the manifold, possible by adapting the\nfree-form flow framework to Riemannian manifolds. M-FFF is straightforwardly\nadapted to any manifold with a known projection. It consistently matches or\noutperforms previous single-step methods specialized to specific manifolds. It\nis typically two orders of magnitude faster than multi-step methods based on\ndiffusion or flow matching, achieving better likelihoods in several\nexperiments. We provide our code at https://github.com/vislearn/FFF.\n","authors":["Peter Sorrenson","Felix Draxler","Armand Rousselot","Sander Hummerich","Ullrich Köthe"],"pdf_url":"https://arxiv.org/pdf/2312.09852v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.16267v1","updated":"2024-11-25T10:37:48Z","published":"2024-11-25T10:37:48Z","title":"Local Bayesian Optimization for Controller Tuning with Crash Constraints","summary":"  Controller tuning is crucial for closed-loop performance but often involves\nmanual adjustments. Although Bayesian optimization (BO) has been established as\na data-efficient method for automated tuning, applying it to large and\nhigh-dimensional search spaces remains challenging. We extend a recently\nproposed local variant of BO to include crash constraints, where the controller\ncan only be successfully evaluated in an a-priori unknown feasible region. We\ndemonstrate the efficiency of the proposed method through simulations and\nhardware experiments. Our findings showcase the potential of local BO to\nenhance controller performance and reduce the time and resources necessary for\ntuning.\n","authors":["Alexander von Rohr","David Stenger","Dominik Scheurenberg","Sebastian Trimpe"],"pdf_url":"https://arxiv.org/pdf/2411.16267v1.pdf","comment":"Published in at-Automatisierungstechnik"},{"id":"http://arxiv.org/abs/2411.12070v2","updated":"2024-11-25T10:36:37Z","published":"2024-11-18T21:29:50Z","title":"Autoassociative Learning of Structural Representations for Modeling and\n  Classification in Medical Imaging","summary":"  Deep learning architectures based on convolutional neural networks tend to\nrely on continuous, smooth features. While this characteristics provides\nsignificant robustness and proves useful in many real-world tasks, it is\nstrikingly incompatible with the physical characteristic of the world, which,\nat the scale in which humans operate, comprises crisp objects, typically\nrepresenting well-defined categories. This study proposes a class of\nneurosymbolic systems that learn by reconstructing the observed images in terms\nof visual primitives and are thus forced to form high-level, structural\nexplanations of them. When applied to the task of diagnosing abnormalities in\nhistological imaging, the method proved superior to a conventional deep\nlearning architecture in terms of classification accuracy, while being more\ntransparent.\n","authors":["Zuzanna Buchnajzer","Kacper Dobek","Stanisław Hapke","Daniel Jankowski","Krzysztof Krawiec"],"pdf_url":"https://arxiv.org/pdf/2411.12070v2.pdf","comment":"16 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.15714v2","updated":"2024-11-25T10:36:34Z","published":"2024-10-21T07:33:42Z","title":"Offline reinforcement learning for job-shop scheduling problems","summary":"  Recent advances in deep learning have shown significant potential for solving\ncombinatorial optimization problems in real-time. Unlike traditional methods,\ndeep learning can generate high-quality solutions efficiently, which is crucial\nfor applications like routing and scheduling. However, existing approaches like\ndeep reinforcement learning (RL) and behavioral cloning have notable\nlimitations, with deep RL suffering from slow learning and behavioral cloning\nrelying solely on expert actions, which can lead to generalization issues and\nneglect of the optimization objective. This paper introduces a novel offline RL\nmethod designed for combinatorial optimization problems with complex\nconstraints, where the state is represented as a heterogeneous graph and the\naction space is variable. Our approach encodes actions in edge attributes and\nbalances expected rewards with the imitation of expert solutions. We\ndemonstrate the effectiveness of this method on job-shop scheduling and\nflexible job-shop scheduling benchmarks, achieving superior performance\ncompared to state-of-the-art techniques.\n","authors":["Imanol Echeverria","Maialen Murua","Roberto Santana"],"pdf_url":"https://arxiv.org/pdf/2410.15714v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02429v2","updated":"2024-11-25T10:32:49Z","published":"2024-02-04T09:58:42Z","title":"Towards an Information Theoretic Framework of Context-Based Offline\n  Meta-Reinforcement Learning","summary":"  As a marriage between offline RL and meta-RL, the advent of offline\nmeta-reinforcement learning (OMRL) has shown great promise in enabling RL\nagents to multi-task and quickly adapt while acquiring knowledge safely. Among\nwhich, context-based OMRL (COMRL) as a popular paradigm, aims to learn a\nuniversal policy conditioned on effective task representations. In this work,\nby examining several key milestones in the field of COMRL, we propose to\nintegrate these seemingly independent methodologies into a unified framework.\nMost importantly, we show that the pre-existing COMRL algorithms are\nessentially optimizing the same mutual information objective between the task\nvariable $M$ and its latent representation $Z$ by implementing various\napproximate bounds. Such theoretical insight offers ample design freedom for\nnovel algorithms. As demonstrations, we propose a supervised and a\nself-supervised implementation of $I(Z; M)$, and empirically show that the\ncorresponding optimization algorithms exhibit remarkable generalization across\na broad spectrum of RL benchmarks, context shift scenarios, data qualities and\ndeep learning architectures. This work lays the information theoretic\nfoundation for COMRL methods, leading to a better understanding of task\nrepresentation learning in the context of reinforcement learning.\n","authors":["Lanqing Li","Hai Zhang","Xinyu Zhang","Shatong Zhu","Yang Yu","Junqiao Zhao","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2402.02429v2.pdf","comment":"25 pages, 8 figures, 7 tables. TLDR: We propose a novel information\n  theoretic framework of the context-based offline meta-RL paradigm, which\n  unifies several mainstream methods and leads to two robust algorithm\n  implementations"},{"id":"http://arxiv.org/abs/2410.07838v2","updated":"2024-11-25T10:30:36Z","published":"2024-10-10T11:56:09Z","title":"Minority-Focused Text-to-Image Generation via Prompt Optimization","summary":"  We investigate the generation of minority samples using pretrained\ntext-to-image (T2I) latent diffusion models. Minority instances, in the context\nof T2I generation, can be defined as ones living on low-density regions of\ntext-conditional data distributions. They are valuable for various applications\nof modern T2I generators, such as data augmentation and creative AI.\nUnfortunately, existing pretrained T2I diffusion models primarily focus on\nhigh-density regions, largely due to the influence of guided samplers (like\nCFG) that are essential for producing high-quality generations. To address\nthis, we present a novel framework to counter the high-density-focus of T2I\ndiffusion models. Specifically, we first develop an online prompt optimization\nframework that can encourage the emergence of desired properties during\ninference while preserving semantic contents of user-provided prompts. We\nsubsequently tailor this generic prompt optimizer into a specialized solver\nthat promotes the generation of minority features by incorporating a\ncarefully-crafted likelihood objective. Our comprehensive experiments,\nconducted across various types of T2I models, demonstrate that our approach\nsignificantly enhances the capability to produce high-quality minority\ninstances compared to existing samplers.\n","authors":["Soobin Um","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2410.07838v2.pdf","comment":"20 pages, 9 figures"},{"id":"http://arxiv.org/abs/2411.16260v1","updated":"2024-11-25T10:23:11Z","published":"2024-11-25T10:23:11Z","title":"Unraveling Arithmetic in Large Language Models: The Role of Algebraic\n  Structures","summary":"  Large language models (LLMs) have demonstrated remarkable mathematical\ncapabilities, largely driven by chain-of-thought (CoT) prompting, which\ndecomposes complex reasoning into step-by-step solutions. This approach has\nenabled significant advancements, as evidenced by performance on benchmarks\nlike GSM8K and MATH. However, the mechanisms underlying LLMs' ability to\nperform arithmetic in a single step of CoT remain poorly understood. Existing\nstudies debate whether LLMs encode numerical values or rely on symbolic\nreasoning, while others explore attention and multi-layered processing in\narithmetic tasks. In this work, we propose that LLMs learn arithmetic by\ncapturing algebraic structures, such as \\emph{Commutativity} and\n\\emph{Identity} properties. Since these structures are observable through\ninput-output relationships, they can generalize to unseen data. We empirically\ndemonstrate that LLMs can learn algebraic structures using a custom dataset of\narithmetic problems. Our findings indicate that leveraging algebraic structures\ncan enhance the LLMs' arithmetic capabilities, offering insights into improving\ntheir arithmetic performance.\n","authors":["Fu-Chieh Chang","Pei-Yuan Wu"],"pdf_url":"https://arxiv.org/pdf/2411.16260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12901v2","updated":"2024-11-25T10:16:37Z","published":"2024-06-09T18:17:08Z","title":"Interpretable machine learning approach for electron antineutrino\n  selection in a large liquid scintillator detector","summary":"  Several neutrino detectors, KamLAND, Daya Bay, Double Chooz, RENO, and the\nforthcoming large-scale JUNO, rely on liquid scintillator to detect reactor\nantineutrino interactions. In this context, inverse beta decay represents the\ngolden channel for antineutrino detection, providing a pair of correlated\nevents, thus a strong experimental signature to distinguish the signal from a\nvariety of backgrounds. However, given the low cross-section of antineutrino\ninteractions, the development of a powerful event selection algorithm becomes\nimperative to achieve effective discrimination between signal and backgrounds.\nIn this study, we introduce a machine learning (ML) model to achieve this goal:\na fully connected neural network as a powerful signal-background discriminator\nfor a large liquid scintillator detector. We demonstrate, using the JUNO\ndetector as an example, that, despite the already high efficiency of a\ncut-based approach, the presented ML model can further improve the overall\nevent selection efficiency. Moreover, it allows for the retention of signal\nevents at the detector edges that would otherwise be rejected because of the\noverwhelming amount of background events in that region. We also present the\nfirst interpretable analysis of the ML approach for event selection in reactor\nneutrino experiments. This method provides insights into the decision-making\nprocess of the model and offers valuable information for improving and updating\ntraditional event selection approaches.\n","authors":["A. Gavrikov","V. Cerrone","A. Serafini","R. Brugnera","A. Garfagnini","M. Grassi","B. Jelmini","L. Lastrucci","S. Aiello","G. Andronico","V. Antonelli","A. Barresi","D. Basilico","M. Beretta","A. Bergnoli","M. Borghesi","A. Brigatti","R. Bruno","A. Budano","B. Caccianiga","A. Cammi","R. Caruso","D. Chiesa","C. Clementi","S. Dusini","A. Fabbri","G. Felici","F. Ferraro","M. G. Giammarchi","N. Giudice","R. M. Guizzetti","N. Guardone","C. Landini","I. Lippi","S. Loffredo","L. Loi","P. Lombardi","C. Lombardo","F. Mantovani","S. M. Mari","A. Martini","L. Miramonti","M. Montuschi","M. Nastasi","D. Orestano","F. Ortica","A. Paoloni","E. Percalli","F. Petrucci","E. Previtali","G. Ranucci","A. C. Re","M. Redchuck","B. Ricci","A. Romani","P. Saggese","G. Sava","C. Sirignano","M. Sisti","L. Stanco","E. Stanescu Farilla","V. Strati","M. D. C. Torri","A. Triossi","C. Tuvé","C. Venettacci","G. Verde","L. Votano"],"pdf_url":"https://arxiv.org/pdf/2406.12901v2.pdf","comment":"This is a post-peer-review, pre-copyedit version of an article\n  published in Phys. Lett. B. The final published version is available online:\n  https://www.sciencedirect.com/science/article/pii/S0370269324006993"},{"id":"http://arxiv.org/abs/2411.16251v1","updated":"2024-11-25T10:10:09Z","published":"2024-11-25T10:10:09Z","title":"Transparent Neighborhood Approximation for Text Classifier Explanation","summary":"  Recent literature highlights the critical role of neighborhood construction\nin deriving model-agnostic explanations, with a growing trend toward deploying\ngenerative models to improve synthetic instance quality, especially for\nexplaining text classifiers. These approaches overcome the challenges in\nneighborhood construction posed by the unstructured nature of texts, thereby\nimproving the quality of explanations. However, the deployed generators are\nusually implemented via neural networks and lack inherent explainability,\nsparking arguments over the transparency of the explanation process itself. To\naddress this limitation while preserving neighborhood quality, this paper\nintroduces a probability-based editing method as an alternative to black-box\ntext generators. This approach generates neighboring texts by implementing\nmanipulations based on in-text contexts. Substituting the generator-based\nconstruction process with recursive probability-based editing, the resultant\nexplanation method, XPROB (explainer with probability-based editing), exhibits\ncompetitive performance according to the evaluation conducted on two real-world\ndatasets. Additionally, XPROB's fully transparent and more controllable\nconstruction process leads to superior stability compared to the\ngenerator-based explainers.\n","authors":["Yi Cai","Arthur Zimek","Eirini Ntoutsi","Gerhard Wunder"],"pdf_url":"https://arxiv.org/pdf/2411.16251v1.pdf","comment":"IEEE DSAA'24"},{"id":"http://arxiv.org/abs/2411.16246v1","updated":"2024-11-25T10:04:37Z","published":"2024-11-25T10:04:37Z","title":"Efficient pooling of predictions via kernel embeddings","summary":"  Probabilistic predictions are probability distributions over the set of\npossible outcomes. Such predictions quantify the uncertainty in the outcome,\nmaking them essential for effective decision making. By combining multiple\npredictions, the information sources used to generate the predictions are\npooled, often resulting in a more informative forecast. Probabilistic\npredictions are typically combined by linearly pooling the individual\npredictive distributions; this encompasses several ensemble learning\ntechniques, for example. The weights assigned to each prediction can be\nestimated based on their past performance, allowing more accurate predictions\nto receive a higher weight. This can be achieved by finding the weights that\noptimise a proper scoring rule over some training data. By embedding\npredictions into a Reproducing Kernel Hilbert Space (RKHS), we illustrate that\nestimating the linear pool weights that optimise kernel-based scoring rules is\na convex quadratic optimisation problem. This permits an efficient\nimplementation of the linear pool when optimally combining predictions on\narbitrary outcome domains. This result also holds for other combination\nstrategies, and we additionally study a flexible generalisation of the linear\npool that overcomes some of its theoretical limitations, whilst allowing an\nefficient implementation within the RKHS framework. These approaches are\ncompared in an application to operational wind speed forecasts, where this\ngeneralisation is found to offer substantial improvements upon the traditional\nlinear pool.\n","authors":["Sam Allen","David Ginsbourger","Johanna Ziegel"],"pdf_url":"https://arxiv.org/pdf/2411.16246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16234v1","updated":"2024-11-25T09:48:11Z","published":"2024-11-25T09:48:11Z","title":"Flow Annealed Importance Sampling Bootstrap meets Differentiable\n  Particle Physics","summary":"  High-energy physics requires the generation of large numbers of simulated\ndata samples from complex but analytically tractable distributions called\nmatrix elements. Surrogate models, such as normalizing flows, are gaining\npopularity for this task due to their computational efficiency. We adopt an\napproach based on Flow Annealed importance sampling Bootstrap (FAB) that\nevaluates the differentiable target density during training and helps avoid the\ncostly generation of training data in advance. We show that FAB reaches higher\nsampling efficiency with fewer target evaluations in high dimensions in\ncomparison to other methods.\n","authors":["Annalena Kofler","Vincent Stimper","Mikhail Mikhasenko","Michael Kagan","Lukas Heinrich"],"pdf_url":"https://arxiv.org/pdf/2411.16234v1.pdf","comment":"Accepted at the 'Machine Learning and the Physical Sciences 2024'\n  workshop at NeurIPS"},{"id":"http://arxiv.org/abs/2408.07435v2","updated":"2024-11-25T09:45:15Z","published":"2024-08-14T10:12:15Z","title":"Real-world validation of safe reinforcement learning, model predictive\n  control and decision tree-based home energy management systems","summary":"  Recent advancements in machine learning based energy management approaches,\nspecifically reinforcement learning with a safety layer (OptLayerPolicy) and a\nmetaheuristic algorithm generating a decision tree control policy (TreeC), have\nshown promise. However, their effectiveness has only been demonstrated in\ncomputer simulations. This paper presents the real-world validation of these\nmethods, comparing against model predictive control and simple rule-based\ncontrol benchmark. The experiments were conducted on the electrical\ninstallation of 4 reproductions of residential houses, which all have their own\nbattery, photovoltaic and dynamic load system emulating a non-controllable\nelectrical load and a controllable electric vehicle charger. The results show\nthat the simple rules, TreeC, and model predictive control-based methods\nachieved similar costs, with a difference of only 0.6%. The reinforcement\nlearning based method, still in its training phase, obtained a cost 25.5\\%\nhigher to the other methods. Additional simulations show that the costs can be\nfurther reduced by using a more representative training dataset for TreeC and\naddressing errors in the model predictive control implementation caused by its\nreliance on accurate data from various sources. The OptLayerPolicy safety layer\nallows safe online training of a reinforcement learning agent in the\nreal-world, given an accurate constraint function formulation. The proposed\nsafety layer method remains error-prone, nonetheless, it is found beneficial\nfor all investigated methods. The TreeC method, which does require building a\nrealistic simulation for training, exhibits the safest operational performance,\nexceeding the grid limit by only 27.1 Wh compared to 593.9 Wh for reinforcement\nlearning.\n","authors":["Julian Ruddick","Glenn Ceusters","Gilles Van Kriekinge","Evgenii Genov","Cedric De Cauwer","Thierry Coosemans","Maarten Messagie"],"pdf_url":"https://arxiv.org/pdf/2408.07435v2.pdf","comment":"Accepted version Energy and AI:\n  https://doi.org/10.1016/j.egyai.2024.100448"},{"id":"http://arxiv.org/abs/2411.16229v1","updated":"2024-11-25T09:42:42Z","published":"2024-11-25T09:42:42Z","title":"Effective Non-Random Extreme Learning Machine","summary":"  The Extreme Learning Machine (ELM) is a growing statistical technique widely\napplied to regression problems. In essence, ELMs are single-layer neural\nnetworks where the hidden layer weights are randomly sampled from a specific\ndistribution, while the output layer weights are learned from the data. Two of\nthe key challenges with this approach are the architecture design, specifically\ndetermining the optimal number of neurons in the hidden layer, and the method's\nsensitivity to the random initialization of hidden layer weights.\n  This paper introduces a new and enhanced learning algorithm for regression\ntasks, the Effective Non-Random ELM (ENR-ELM), which simplifies the\narchitecture design and eliminates the need for random hidden layer weight\nselection. The proposed method incorporates concepts from signal processing,\nsuch as basis functions and projections, into the ELM framework. We introduce\ntwo versions of the ENR-ELM: the approximated ENR-ELM and the incremental\nENR-ELM. Experimental results on both synthetic and real datasets demonstrate\nthat our method overcomes the problems of traditional ELM while maintaining\ncomparable predictive performance.\n","authors":["Daniela De Canditiis","Fabiano Veglianti"],"pdf_url":"https://arxiv.org/pdf/2411.16229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16227v1","updated":"2024-11-25T09:41:20Z","published":"2024-11-25T09:41:20Z","title":"EigenHearts: Cardiac Diseases Classification Using EigenFaces Approach","summary":"  In the realm of cardiovascular medicine, medical imaging plays a crucial role\nin accurately classifying cardiac diseases and making precise diagnoses.\nHowever, the field faces significant challenges when integrating data science\ntechniques, as a significant volume of images is required for these techniques.\nAs a consequence, it is necessary to investigate different avenues to overcome\nthis challenge. In this contribution, we offer an innovative tool to conquer\nthis limitation. In particular, we delve into the application of a well\nrecognized method known as the EigenFaces approach to classify cardiac\ndiseases. This approach was originally motivated for efficiently representing\npictures of faces using principal component analysis, which provides a set of\neigenvectors (aka eigenfaces), explaining the variation between face images. As\nthis approach proven to be efficient for face recognition, it motivated us to\nexplore its efficiency on more complicated data bases. In particular, we\nintegrate this approach, with convolutional neural networks (CNNs) to classify\nechocardiography images taken from mice in five distinct cardiac conditions\n(healthy, diabetic cardiomyopathy, myocardial infarction, obesity and TAC\nhypertension). Performing a preprocessing step inspired from the eigenfaces\napproach on the echocardiography datasets, yields sets of pod modes, which we\nwill call eigenhearts. To demonstrate the proposed approach, we compare two\ntestcases: (i) supplying the CNN with the original images directly, (ii)\nsupplying the CNN with images projected into the obtained pod modes. The\nresults show a substantial and noteworthy enhancement when employing SVD for\npre-processing, with classification accuracy increasing by approximately 50%.\n","authors":["Nourelhouda Groun","Maria Villalba-Orero","Lucia Casado-Martin","Enrique Lara-Pezzi","Eusebio Valero","Soledad Le Clainche","Jesus Garicano-Mena"],"pdf_url":"https://arxiv.org/pdf/2411.16227v1.pdf","comment":"16 pages, 9 figures, 3 tables"},{"id":"http://arxiv.org/abs/2409.17201v2","updated":"2024-11-25T09:32:53Z","published":"2024-09-25T15:04:42Z","title":"Immersion and Invariance-based Coding for Privacy-Preserving Federated\n  Learning","summary":"  Federated learning (FL) has emerged as a method to preserve privacy in\ncollaborative distributed learning. In FL, clients train AI models directly on\ntheir devices rather than sharing data with a centralized server, which can\npose privacy risks. However, it has been shown that despite FL's partial\nprotection of local data privacy, information about clients' data can still be\ninferred from shared model updates during training. In recent years, several\nprivacy-preserving approaches have been developed to mitigate this privacy\nleakage in FL, though they often provide privacy at the cost of model\nperformance or system efficiency. Balancing these trade-offs presents a\nsignificant challenge in implementing FL schemes. In this manuscript, we\nintroduce a privacy-preserving FL framework that combines differential privacy\nand system immersion tools from control theory. The core idea is to treat the\noptimization algorithms used in standard FL schemes (e.g., gradient-based\nalgorithms) as a dynamical system that we seek to immerse into a\nhigher-dimensional system (referred to as the target optimization algorithm).\nThe target algorithm's dynamics are designed such that, first, the model\nparameters of the original algorithm are immersed in its parameters; second, it\noperates on distorted parameters; and third, it converges to an encoded version\nof the true model parameters from the original algorithm. These encoded\nparameters can then be decoded at the server to retrieve the original model\nparameters. We demonstrate that the proposed privacy-preserving scheme can be\ntailored to offer any desired level of differential privacy for both local and\nglobal model parameters, while maintaining the same accuracy and convergence\nrate as standard FL algorithms.\n","authors":["Haleh Hayati","Carlos Murguia","Nathan van de Wouw"],"pdf_url":"https://arxiv.org/pdf/2409.17201v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.02968v3","updated":"2024-11-25T09:19:51Z","published":"2024-05-05T15:27:05Z","title":"CoverLib: Classifiers-equipped Experience Library by Iterative Problem\n  Distribution Coverage Maximization for Domain-tuned Motion Planning","summary":"  Library-based methods are known to be very effective for fast motion planning\nby adapting an experience retrieved from a precomputed library. This article\npresents CoverLib, a principled approach for constructing and utilizing such a\nlibrary. CoverLib iteratively adds an experience-classifier-pair to the\nlibrary, where each classifier corresponds to an adaptable region of the\nexperience within the problem space. This iterative process is an active\nprocedure, as it selects the next experience based on its ability to\neffectively cover the uncovered region. During the query phase, these\nclassifiers are utilized to select an experience that is expected to be\nadaptable for a given problem. Experimental results demonstrate that CoverLib\neffectively mitigates the trade-off between plannability and speed observed in\nglobal (e.g. sampling-based) and local (e.g. optimization-based) methods. As a\nresult, it achieves both fast planning and high success rates over the problem\ndomain. Moreover, due to its adaptation-algorithm-agnostic nature, CoverLib\nseamlessly integrates with various adaptation methods, including nonlinear\nprogramming-based and sampling-based algorithms.\n","authors":["Hirokazu Ishida","Naoki Hiraoka","Kei Okada","Masayuki Inaba"],"pdf_url":"https://arxiv.org/pdf/2405.02968v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16206v1","updated":"2024-11-25T09:14:09Z","published":"2024-11-25T09:14:09Z","title":"Batch Bayesian Optimization via Expected Subspace Improvement","summary":"  Extending Bayesian optimization to batch evaluation can enable the designer\nto make the most use of parallel computing technology. Most of current batch\napproaches use artificial functions to simulate the sequential Bayesian\noptimization algorithm's behavior to select a batch of points for parallel\nevaluation. However, as the batch size grows, the accumulated error introduced\nby these artificial functions increases rapidly, which dramatically decreases\nthe optimization efficiency of the algorithm. In this work, we propose a simple\nand efficient approach to extend Bayesian optimization to batch evaluation.\nDifferent from existing batch approaches, the idea of the new approach is to\ndraw a batch of subspaces of the original problem and select one acquisition\npoint from each subspace. To achieve this, we propose the expected subspace\nimprovement criterion to measure the amount of the improvement that a candidate\npoint can achieve within a certain subspace. By optimizing these expected\nsubspace improvement functions simultaneously, we can get a batch of query\npoints for expensive evaluation. Numerical experiments show that our proposed\napproach can achieve near-linear speedup when compared with the sequential\nBayesian optimization algorithm, and performs very competitively when compared\nwith eight state-of-the-art batch algorithms. This work provides a simple yet\nefficient approach for batch Bayesian optimization. A Matlab implementation of\nour approach is available at\nhttps://github.com/zhandawei/Expected_Subspace_Improvement_Batch_Bayesian_Optimization\n","authors":["Dawei Zhan","Zhaoxi Zeng","Shuoxiao Wei","Ping Wu"],"pdf_url":"https://arxiv.org/pdf/2411.16206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09603v3","updated":"2024-11-25T09:13:47Z","published":"2024-03-14T17:44:35Z","title":"Optimistic Verifiable Training by Controlling Hardware Nondeterminism","summary":"  The increasing compute demands of AI systems have led to the emergence of\nservices that train models on behalf of clients lacking necessary resources.\nHowever, ensuring correctness of training and guarding against potential\ntraining-time attacks, such as data poisoning and backdoors, poses challenges.\nExisting works on verifiable training largely fall into two classes:\nproof-based systems, which are difficult to scale, and ``optimistic'' methods\nthat consider a third-party auditor who can replicate the training process and\ncontest the trainer. A key challenge with the latter is that nondeterminism\nbetween GPU types during training prevents exact replication of the training\nprocess, resulting in schemes that are non-robust. We propose a method that\ncombines training in a higher precision than the target, rounding after\nintermediate computations, and sharing rounding decisions based on an adaptive\nthresholding procedure, to successfully control for nondeterminism. Across\nthree different NVIDIA GPUs (A40, Titan XP, RTX 2080 Ti), we achieve exact\ntraining replication at FP32 precision for both full-training and fine-tuning\nof ResNet-50 (23M) and GPT-2 (117M) models. Our verifiable training scheme\nsignificantly decreases the storage and time costs compared to proof-based\nsystems, and is publicly released at\nhttps://github.com/meghabyte/verifiable-training.\n","authors":["Megha Srivastava","Simran Arora","Dan Boneh"],"pdf_url":"https://arxiv.org/pdf/2403.09603v3.pdf","comment":"11 pages, 5 figures, Neural Information Processing Systems (NeurIPS)\n  2024,"},{"id":"http://arxiv.org/abs/2405.20114v2","updated":"2024-11-25T09:00:40Z","published":"2024-05-30T14:51:57Z","title":"Towards Faster Decentralized Stochastic Optimization with Communication\n  Compression","summary":"  Communication efficiency has garnered significant attention as it is\nconsidered the main bottleneck for large-scale decentralized Machine Learning\napplications in distributed and federated settings. In this regime, clients are\nrestricted to transmitting small amounts of quantized information to their\nneighbors over a communication graph. Numerous endeavors have been made to\naddress this challenging problem by developing algorithms with compressed\ncommunication for decentralized non-convex optimization problems. Despite\nconsiderable efforts, the current results suffer from various issues such as\nnon-scalability with the number of clients, requirements for large batches, or\nbounded gradient assumption. In this paper, we introduce MoTEF, a novel\napproach that integrates communication compression with Momentum Tracking and\nError Feedback. Our analysis demonstrates that MoTEF achieves most of the\ndesired properties, and significantly outperforms existing methods under\narbitrary data heterogeneity. We provide numerical experiments to validate our\ntheoretical findings and confirm the practical superiority of MoTEF.\n","authors":["Rustem Islamov","Yuan Gao","Sebastian U. Stich"],"pdf_url":"https://arxiv.org/pdf/2405.20114v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16201v1","updated":"2024-11-25T08:59:39Z","published":"2024-11-25T08:59:39Z","title":"Video-Text Dataset Construction from Multi-AI Feedback: Promoting\n  Weak-to-Strong Preference Learning for Video Large Language Models","summary":"  High-quality video-text preference data is crucial for Multimodal Large\nLanguage Models (MLLMs) alignment. However, existing preference data is very\nscarce. Obtaining VQA preference data for preference training is costly, and\nmanually annotating responses is highly unreliable, which could result in\nlow-quality pairs. Meanwhile, AI-generated responses controlled by temperature\nadjustment lack diversity. To address these issues, we propose a high-quality\nVQA preference dataset, called \\textit{\\textbf{M}ultiple \\textbf{M}ultimodal\n\\textbf{A}rtificial \\textbf{I}ntelligence \\textbf{P}reference Datasets in\n\\textbf{V}QA} (\\textbf{MMAIP-V}), which is constructed by sampling from the\nresponse distribution set and using an external scoring function for response\nevaluation. Furthermore, to fully leverage the preference knowledge in MMAIP-V\nand ensure sufficient optimization, we propose \\textit{\\textbf{Iter}ative\n\\textbf{W}eak-to-\\textbf{S}trong \\textbf{R}einforcement \\textbf{L}earning from\n\\textbf{AI} \\textbf{F}eedback for video MLLMs} (\\textbf{Iter-W2S-RLAIF}), a\nframework that gradually enhances MLLMs' alignment capabilities by iteratively\nupdating the reference model and performing parameter extrapolation. Finally,\nwe propose an unbiased and information-complete evaluation scheme in VQA\nevaluation. Experiments demonstrate that MMAIP-V is beneficial for MLLMs in\npreference learning and Iter-W2S-RLAIF fully exploits the alignment information\nin MMAIP-V. We believe that the proposed automatic VQA preference data\ngeneration pipeline based on AI feedback can greatly promote future work in the\nMLLMs alignment. \\textbf{Code and dataset are available}\n\\href{https://anonymous.4open.science/r/MMAIP-V_Iter-W2S-RLAIF-702F}{MMAIP-V\\_Iter-W2S-RLAIF-702F}.\n","authors":["Hao Yi","Qingyang Li","Yulan Hu","Fuzheng Zhang","Di Zhang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2411.16201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19198v2","updated":"2024-11-25T08:57:20Z","published":"2024-07-27T07:34:49Z","title":"Towards the Dynamics of a DNN Learning Symbolic Interactions","summary":"  This study proves the two-phase dynamics of a deep neural network (DNN)\nlearning interactions. Despite the long disappointing view of the faithfulness\nof post-hoc explanation of a DNN, a series of theorems have been proven in\nrecent years to show that for a given input sample, a small set of interactions\nbetween input variables can be considered as primitive inference patterns that\nfaithfully represent a DNN's detailed inference logic on that sample.\nParticularly, Zhang et al. have observed that various DNNs all learn\ninteractions of different complexities in two distinct phases, and this\ntwo-phase dynamics well explains how a DNN changes from under-fitting to\nover-fitting. Therefore, in this study, we mathematically prove the two-phase\ndynamics of interactions, providing a theoretical mechanism for how the\ngeneralization power of a DNN changes during the training process. Experiments\nshow that our theory well predicts the real dynamics of interactions on\ndifferent DNNs trained for various tasks.\n","authors":["Qihan Ren","Junpeng Zhang","Yang Xu","Yue Xin","Dongrui Liu","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.19198v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16200v1","updated":"2024-11-25T08:57:05Z","published":"2024-11-25T08:57:05Z","title":"Neural Network-based High-index Saddle Dynamics Method for Searching\n  Saddle Points and Solution Landscape","summary":"  The high-index saddle dynamics (HiSD) method is a powerful approach for\ncomputing saddle points and solution landscape. However, its practical\napplicability is constrained by the need for the explicit energy function\nexpression. To overcome this challenge, we propose a neural network-based\nhigh-index saddle dynamics (NN-HiSD) method. It utilizes neural network-based\nsurrogate model to approximates the energy function, allowing the use of the\nHiSD method in the cases where the energy function is either unavailable or\ncomputationally expensive. We further enhance the efficiency of the NN-HiSD\nmethod by incorporating momentum acceleration techniques, specifically\nNesterov's acceleration and the heavy-ball method. We also provide a rigorous\nconvergence analysis of the NN-HiSD method. We conduct numerical experiments on\nsystems with and without explicit energy functions, specifically including the\nalanine dipeptide model and bacterial ribosomal assembly intermediates for the\nlatter, demonstrating the effectiveness and reliability of the proposed method.\n","authors":["Yuankai Liu","Lei Zhang","Jin Zhao"],"pdf_url":"https://arxiv.org/pdf/2411.16200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16196v1","updated":"2024-11-25T08:52:46Z","published":"2024-11-25T08:52:46Z","title":"Learn from Foundation Model: Fruit Detection Model without Manual\n  Annotation","summary":"  Recent breakthroughs in large foundation models have enabled the possibility\nof transferring knowledge pre-trained on vast datasets to domains with limited\ndata availability. Agriculture is one of the domains that lacks sufficient\ndata. This study proposes a framework to train effective, domain-specific,\nsmall models from foundation models without manual annotation. Our approach\nbegins with SDM (Segmentation-Description-Matching), a stage that leverages two\nfoundation models: SAM2 (Segment Anything in Images and Videos) for\nsegmentation and OpenCLIP (Open Contrastive Language-Image Pretraining) for\nzero-shot open-vocabulary classification. In the second stage, a novel\nknowledge distillation mechanism is utilized to distill compact,\nedge-deployable models from SDM, enhancing both inference speed and perception\naccuracy. The complete method, termed SDM-D\n(Segmentation-Description-Matching-Distilling), demonstrates strong performance\nacross various fruit detection tasks object detection, semantic segmentation,\nand instance segmentation) without manual annotation. It nearly matches the\nperformance of models trained with abundant labels. Notably, SDM-D outperforms\nopen-set detection methods such as Grounding SAM and YOLO-World on all tested\nfruit detection datasets. Additionally, we introduce MegaFruits, a\ncomprehensive fruit segmentation dataset encompassing over 25,000 images, and\nall code and datasets are made publicly available at\nhttps://github.com/AgRoboticsResearch/SDM-D.git.\n","authors":["Yanan Wang","Zhenghao Fei","Ruichen Li","Yibin Ying"],"pdf_url":"https://arxiv.org/pdf/2411.16196v1.pdf","comment":"17 pages, 12 figures, conference or other essential info"},{"id":"http://arxiv.org/abs/2411.16195v1","updated":"2024-11-25T08:49:33Z","published":"2024-11-25T08:49:33Z","title":"On the Robustness of the Successive Projection Algorithm","summary":"  The successive projection algorithm (SPA) is a workhorse algorithm to learn\nthe $r$ vertices of the convex hull of a set of $(r-1)$-dimensional data\npoints, a.k.a. a latent simplex, which has numerous applications in data\nscience. In this paper, we revisit the robustness to noise of SPA and several\nof its variants. In particular, when $r \\geq 3$, we prove the tightness of the\nexisting error bounds for SPA and for two more robust preconditioned variants\nof SPA. We also provide significantly improved error bounds for SPA, by a\nfactor proportional to the conditioning of the $r$ vertices, in two special\ncases: for the first extracted vertex, and when $r \\leq 2$. We then provide\nfurther improvements for the error bounds of a translated version of SPA\nproposed by Arora et al. (''A practical algorithm for topic modeling with\nprovable guarantees'', ICML, 2013) in two special cases: for the first two\nextracted vertices, and when $r \\leq 3$. Finally, we propose a new more robust\nvariant of SPA that first shifts and lifts the data points in order to minimize\nthe conditioning of the problem. We illustrate our results on synthetic data.\n","authors":["Giovanni Barbarino","Nicolas Gillis"],"pdf_url":"https://arxiv.org/pdf/2411.16195v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2312.06254v2","updated":"2024-11-25T08:46:53Z","published":"2023-12-11T09:50:52Z","title":"Modyn: Data-Centric Machine Learning Pipeline Orchestration","summary":"  In real-world machine learning (ML) pipelines, datasets are continuously\ngrowing. Models must incorporate this new training data to improve\ngeneralization and adapt to potential distribution shifts. The cost of model\nretraining is proportional to how frequently the model is retrained and how\nmuch data it is trained on, which makes the naive approach of retraining from\nscratch each time impractical.\n  We present Modyn, a data-centric end-to-end machine learning platform.\nModyn's ML pipeline abstraction enables users to declaratively describe\npolicies for continuously training a model on a growing dataset. Modyn\npipelines allow users to apply data selection policies (to reduce the number of\ndata points) and triggering policies (to reduce the number of trainings). Modyn\nexecutes and orchestrates these continuous ML training pipelines. The system is\nopen-source and comes with an ecosystem of benchmark datasets, models, and\ntooling. We formally discuss how to measure the performance of ML pipelines by\nintroducing the concept of composite models, enabling fair comparison of\npipelines with different data selection and triggering policies. We empirically\nanalyze how various data selection and triggering policies impact model\naccuracy, and also show that Modyn enables high throughput training with\nsample-level data selection.\n","authors":["Maximilian Böther","Ties Robroek","Viktor Gsteiger","Robin Holzinger","Xianzhe Ma","Pınar Tözün","Ana Klimovic"],"pdf_url":"https://arxiv.org/pdf/2312.06254v2.pdf","comment":"accepted at SIGMOD'25; 30 pages"},{"id":"http://arxiv.org/abs/2410.01405v3","updated":"2024-11-25T08:17:14Z","published":"2024-10-02T10:31:17Z","title":"On Expressive Power of Looped Transformers: Theoretical Analysis and\n  Enhancement via Timestep Encoding","summary":"  Looped Transformers offer advantages in parameter efficiency and Turing\ncompleteness. However, their expressive power for function approximation and\napproximation rate remains underexplored. In this paper, we establish\napproximation rates of Looped Transformers by defining the concept of the\nmodulus of continuity for sequence-to-sequence functions. This reveals a\nlimitation specific to the looped architecture. That is, the analysis prompts\nus to incorporate scaling parameters for each loop, conditioned on timestep\nencoding. Experimental results demonstrate that increasing the number of loops\nenhances performance, with further gains achieved through the timestep encoding\narchitecture.\n","authors":["Kevin Xu","Issei Sato"],"pdf_url":"https://arxiv.org/pdf/2410.01405v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15851v2","updated":"2024-11-25T08:01:15Z","published":"2024-10-21T10:27:57Z","title":"R2I-rPPG: A Robust Region of Interest Selection Method for Remote\n  Photoplethysmography to Extract Heart Rate","summary":"  The COVID-19 pandemic has underscored the need for low-cost, scalable\napproaches to measuring contactless vital signs, either during initial triage\nat a healthcare facility or virtual telemedicine visits. Remote\nphotoplethysmography (rPPG) can accurately estimate heart rate (HR) when\napplied to close-up videos of healthy volunteers in well-lit laboratory\nsettings. However, results from such highly optimized laboratory studies may\nnot be readily translated to healthcare settings. One significant barrier to\nthe practical application of rPPG in health care is the accurate localization\nof the region of interest (ROI). Clinical or telemedicine visits may involve\nsub-optimal lighting, movement artifacts, variable camera angle, and subject\ndistance. This paper presents an rPPG ROI selection method based on 3D facial\nlandmarks and patient head yaw angle. We then demonstrate the robustness of\nthis ROI selection method when coupled to the Plane-Orthogonal-to-Skin (POS)\nrPPG method when applied to videos of patients presenting to an Emergency\nDepartment for respiratory complaints. Our results demonstrate the\neffectiveness of our proposed approach in improving the accuracy and robustness\nof rPPG in a challenging clinical environment.\n","authors":["Sandeep Nagar","Mark Hasegawa-Johnson","David G. Beiser","Narendra Ahuja"],"pdf_url":"https://arxiv.org/pdf/2410.15851v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2411.16167v1","updated":"2024-11-25T07:46:57Z","published":"2024-11-25T07:46:57Z","title":"BadSFL: Backdoor Attack against Scaffold Federated Learning","summary":"  Federated learning (FL) enables the training of deep learning models on\ndistributed clients to preserve data privacy. However, this learning paradigm\nis vulnerable to backdoor attacks, where malicious clients can upload poisoned\nlocal models to embed backdoors into the global model, leading to\nattacker-desired predictions. Existing backdoor attacks mainly focus on FL with\nindependently and identically distributed (IID) scenarios, while real-world FL\ntraining data are typically non-IID. Current strategies for non-IID backdoor\nattacks suffer from limitations in maintaining effectiveness and durability. To\naddress these challenges, we propose a novel backdoor attack method, \\name,\nspecifically designed for the FL framework using the scaffold aggregation\nalgorithm in non-IID settings. \\name leverages a Generative Adversarial Network\n(GAN) based on the global model to complement the training set, achieving high\naccuracy on both backdoor and benign samples. It utilizes a specific feature as\nthe backdoor trigger to ensure stealthiness, and exploits the Scaffold's\ncontrol variate to predict the global model's convergence direction, ensuring\nthe backdoor's persistence. Extensive experiments on three benchmark datasets\ndemonstrate the high effectiveness, stealthiness, and durability of \\name.\nNotably, our attack remains effective over 60 rounds in the global model and up\nto 3 times longer than existing baseline attacks after stopping the injection\nof malicious updates.\n","authors":["Xingshuo Han","Xiang Lan","Haozhao Wang","Shengmin Xu","Shen Ren","Jason Zeng","Ming Wu","Michael Heinrich","Tianwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.16167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07404v2","updated":"2024-11-25T07:42:16Z","published":"2024-10-09T20:05:45Z","title":"Fostering Intrinsic Motivation in Reinforcement Learning with Pretrained\n  Foundation Models","summary":"  Exploration remains a significant challenge in reinforcement learning,\nespecially in environments where extrinsic rewards are sparse or non-existent.\nThe recent rise of foundation models, such as CLIP, offers an opportunity to\nleverage pretrained, semantically rich embeddings that encapsulate broad and\nreusable knowledge. In this work we explore the potential of these foundation\nmodels not just to drive exploration, but also to analyze the critical role of\nthe episodic novelty term in enhancing exploration effectiveness of the agent.\nWe also investigate whether providing the intrinsic module with complete state\ninformation -- rather than just partial observations -- can improve\nexploration, despite the difficulties in handling small variations within large\nstate spaces. Our experiments in the MiniGrid domain reveal that intrinsic\nmodules can effectively utilize full state information, significantly\nincreasing sample efficiency while learning an optimal policy. Moreover, we\nshow that the embeddings provided by foundation models are sometimes even\nbetter than those constructed by the agent during training, further\naccelerating the learning process, especially when coupled with the episodic\nnovelty term to enhance exploration.\n","authors":["Alain Andres","Javier Del Ser"],"pdf_url":"https://arxiv.org/pdf/2410.07404v2.pdf","comment":"Accepted at the Intrinsically Motivated Open-ended Learning workshop\n  at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.16162v1","updated":"2024-11-25T07:36:46Z","published":"2024-11-25T07:36:46Z","title":"Sparse patches adversarial attacks via extrapolating point-wise\n  information","summary":"  Sparse and patch adversarial attacks were previously shown to be applicable\nin realistic settings and are considered a security risk to autonomous systems.\nSparse adversarial perturbations constitute a setting in which the adversarial\nperturbations are limited to affecting a relatively small number of points in\nthe input. Patch adversarial attacks denote the setting where the sparse\nattacks are limited to a given structure, i.e., sparse patches with a given\nshape and number. However, previous patch adversarial attacks do not\nsimultaneously optimize multiple patches' locations and perturbations. This\nwork suggests a novel approach for sparse patches adversarial attacks via\npoint-wise trimming dense adversarial perturbations. Our approach enables\nsimultaneous optimization of multiple sparse patches' locations and\nperturbations for any given number and shape. Moreover, our approach is also\napplicable for standard sparse adversarial attacks, where we show that it\nsignificantly improves the state-of-the-art over multiple extensive settings. A\nreference implementation of the proposed method and the reported experiments is\nprovided at \\url{https://github.com/yanemcovsky/SparsePatches.git}\n","authors":["Yaniv Nemcovsky","Avi Mendelson","Chaim Baskin"],"pdf_url":"https://arxiv.org/pdf/2411.16162v1.pdf","comment":"AdvML-Frontiers 24: The 3nd Workshop on New Frontiers in Adversarial\n  Machine Learning, NeurIPS 24"},{"id":"http://arxiv.org/abs/2411.16158v1","updated":"2024-11-25T07:34:53Z","published":"2024-11-25T07:34:53Z","title":"MixPE: Quantization and Hardware Co-design for Efficient LLM Inference","summary":"  Transformer-based large language models (LLMs) have achieved remarkable\nsuccess as model sizes continue to grow, yet their deployment remains\nchallenging due to significant computational and memory demands. Quantization\nhas emerged as a promising solution, and state-of-the-art quantization\nalgorithms for LLMs introduce the need for mixed-precision matrix\nmultiplication (mpGEMM), where lower-precision weights are multiplied with\nhigher-precision activations. Despite its benefits, current hardware\naccelerators such as GPUs and TPUs lack native support for efficient mpGEMM,\nleading to inefficient dequantization operations in the main sequential loop.\nTo address this limitation, we introduce MixPE, a specialized mixed-precision\nprocessing element designed for efficient low-bit quantization in LLM\ninference. MixPE leverages two key innovations to minimize dequantization\noverhead and unlock the full potential of low-bit quantization. First,\nrecognizing that scale and zero point are shared within each quantization\ngroup, we propose performing dequantization after per-group mpGEMM,\nsignificantly reducing dequantization overhead. Second, instead of relying on\nconventional multipliers, MixPE utilizes efficient shift\\&add operations for\nmultiplication, optimizing both computation and energy efficiency. Our\nexperimental results demonstrate that MixPE surpasses the state-of-the-art\nquantization accelerators by $2.6\\times$ speedup and $1.4\\times$ energy\nreduction.\n","authors":["Yu Zhang","Mingzi Wang","Lancheng Zou","Wulong Liu","Hui-Ling Zhen","Mingxuan Yuan","Bei Yu"],"pdf_url":"https://arxiv.org/pdf/2411.16158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16156v1","updated":"2024-11-25T07:32:02Z","published":"2024-11-25T07:32:02Z","title":"VideoOrion: Tokenizing Object Dynamics in Videos","summary":"  We present VideoOrion, a Video Large Language Model (Video-LLM) that\nexplicitly captures the key semantic information in videos--the\nspatial-temporal dynamics of objects throughout the videos. VideoOrion employs\nexpert vision models to extract object dynamics through a detect-segment-track\npipeline, encoding them into a set of object tokens by aggregating\nspatial-temporal object features. Our method addresses the persistent challenge\nin Video-LLMs of efficiently compressing high-dimensional video data into\nsemantic tokens that are comprehensible to LLMs. Compared to prior methods\nwhich resort to downsampling the original video or aggregating visual tokens\nusing resamplers, leading to information loss and entangled semantics,\nVideoOrion not only offers a more natural and efficient way to derive compact,\ndisentangled semantic representations but also enables explicit object modeling\nof video content with minimal computational cost. Moreover, the introduced\nobject tokens naturally allow VideoOrion to accomplish video-based referring\ntasks. Experimental results show that VideoOrion can learn to make good use of\nthe object tokens, and achieves competitive results on both general video\nquestion answering and video-based referring benchmarks.\n","authors":["Yicheng Feng","Yijiang Li","Wanpeng Zhang","Sipeng Zheng","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2411.16156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16155v1","updated":"2024-11-25T07:30:52Z","published":"2024-11-25T07:30:52Z","title":"Graph Adapter of EEG Foundation Models for Parameter Efficient Fine\n  Tuning","summary":"  In diagnosing mental diseases from electroencephalography (EEG) data, neural\nnetwork models such as Transformers have been employed to capture temporal\ndynamics. Additionally, it is crucial to learn the spatial relationships\nbetween EEG sensors, for which Graph Neural Networks (GNNs) are commonly used.\nHowever, fine-tuning large-scale complex neural network models simultaneously\nto capture both temporal and spatial features increases computational costs due\nto the more significant number of trainable parameters. It causes the limited\navailability of EEG datasets for downstream tasks, making it challenging to\nfine-tune large models effectively. We propose EEG-GraphAdapter (EGA), a\nparameter-efficient fine-tuning (PEFT) approach to address these challenges.\nEGA is integrated into pre-trained temporal backbone models as a GNN-based\nmodule and fine-tuned itself alone while keeping the backbone model parameters\nfrozen. This enables the acquisition of spatial representations of EEG signals\nfor downstream tasks, significantly reducing computational overhead and data\nrequirements. Experimental evaluations on healthcare-related downstream tasks\nof Major Depressive Disorder and Abnormality Detection demonstrate that our EGA\nimproves performance by up to 16.1% in the F1-score compared with the backbone\nBENDR model.\n","authors":["Toyotaro Suzumura","Hiroki Kanezashi","Shotaro Akahori"],"pdf_url":"https://arxiv.org/pdf/2411.16155v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2411.16154v1","updated":"2024-11-25T07:26:22Z","published":"2024-11-25T07:26:22Z","title":"DeDe: Detecting Backdoor Samples for SSL Encoders via Decoders","summary":"  Self-supervised learning (SSL) is pervasively exploited in training\nhigh-quality upstream encoders with a large amount of unlabeled data. However,\nit is found to be susceptible to backdoor attacks merely via polluting a small\nportion of training data. The victim encoders mismatch triggered inputs with\ntarget embeddings, e.g., match the triggered cat input to an airplane\nembedding, such that the downstream tasks are affected to misbehave when the\ntrigger is activated. Emerging backdoor attacks have shown great threats in\ndifferent SSL paradigms such as contrastive learning and CLIP, while few\nresearch is devoted to defending against such attacks. Besides, the existing\nones fall short in detecting advanced stealthy backdoors. To address the\nlimitations, we propose a novel detection mechanism, DeDe, which detects the\nactivation of the backdoor mapping with the cooccurrence of victim encoder and\ntrigger inputs. Specifically, DeDe trains a decoder for the SSL encoder on an\nauxiliary dataset (can be out-of-distribution or even slightly poisoned), such\nthat for any triggered input that misleads to the target embedding, the decoder\noutputs an image significantly different from the input. We empirically\nevaluate DeDe on both contrastive learning and CLIP models against various\ntypes of backdoor attacks, and demonstrate its superior performance over SOTA\ndetection methods in both upstream detection performance and ability of\npreventing backdoors in downstream tasks.\n","authors":["Sizai Hou","Songze Li","Duanyi Yao"],"pdf_url":"https://arxiv.org/pdf/2411.16154v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2411.04389v2","updated":"2024-11-25T07:20:43Z","published":"2024-11-07T03:04:58Z","title":"Approximate FW Algorithm with a novel DMO method over Graph-structured\n  Support Set","summary":"  In this project, we reviewed a paper that deals graph-structured convex\noptimization (GSCO) problem with the approximate Frank-Wolfe (FW) algorithm. We\nanalyzed and implemented the original algorithm and introduced some extensions\nbased on that. Then we conducted experiments to compare the results and\nconcluded that our backtracking line-search method effectively reduced the\nnumber of iterations, while our new DMO method (Top-g+ optimal visiting) did\nnot make satisfying enough improvements.\n","authors":["Yijian Pan","Hongjiao Qiang"],"pdf_url":"https://arxiv.org/pdf/2411.04389v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16145v1","updated":"2024-11-25T07:11:45Z","published":"2024-11-25T07:11:45Z","title":"Local Intrinsic Dimensionality for Dynamic Graph Embeddings","summary":"  The notion of local intrinsic dimensionality (LID) has important theoretical\nimplications and practical applications in the fields of data mining and\nmachine learning. Recent research efforts indicate that LID measures defined\nfor graphs can improve graph representational learning methods based on random\nwalks. In this paper, we discuss how NC-LID, a LID measure designed for static\ngraphs, can be adapted for dynamic networks. Focusing on dynnode2vec as the\nmost representative dynamic graph embedding method based on random walks, we\nexamine correlations between NC-LID and the intrinsic quality of 10 real-world\ndynamic network embeddings. The obtained results show that NC-LID can be used\nas a good indicator of nodes whose embedding vectors do not tend to preserve\ntemporal graph structure well. Thus, our empirical findings constitute the\nfirst step towards LID-aware dynamic graph embedding methods.\n","authors":["Dušica Knežević","Miloš Savić","Miloš Radovanović"],"pdf_url":"https://arxiv.org/pdf/2411.16145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16142v1","updated":"2024-11-25T07:05:27Z","published":"2024-11-25T07:05:27Z","title":"Causal Adjacency Learning for Spatiotemporal Prediction Over Graphs","summary":"  Spatiotemporal prediction over graphs (STPG) is crucial for transportation\nsystems. In existing STPG models, an adjacency matrix is an important component\nthat captures the relations among nodes over graphs. However, most studies\ncalculate the adjacency matrix by directly memorizing the data, such as\ndistance- and correlation-based matrices. These adjacency matrices do not\nconsider potential pattern shift for the test data, and may result in\nsuboptimal performance if the test data has a different distribution from the\ntraining one. This issue is known as the Out-of-Distribution generalization\nproblem. To address this issue, in this paper we propose a Causal Adjacency\nLearning (CAL) method to discover causal relations over graphs. The learned\ncausal adjacency matrix is evaluated on a downstream spatiotemporal prediction\ntask using real-world graph data. Results demonstrate that our proposed\nadjacency matrix can capture the causal relations, and using our learned\nadjacency matrix can enhance prediction performance on the OOD test data, even\nthough causal learning is not conducted in the downstream task.\n","authors":["Zhaobin Mo","Qingyuan Liu","Baohua Yan","Longxiang Zhang","Xuan Di"],"pdf_url":"https://arxiv.org/pdf/2411.16142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16139v1","updated":"2024-11-25T06:59:16Z","published":"2024-11-25T06:59:16Z","title":"Beyond Task Vectors: Selective Task Arithmetic Based on Importance\n  Metrics","summary":"  Pretrained models have revolutionized deep learning by enabling significant\nperformance improvements across a wide range of tasks, leveraging large-scale,\npre-learned knowledge representations. However, deploying these models in\nreal-world multi-task learning (MTL) scenarios poses substantial challenges,\nprimarily due to high computational costs and inefficiencies in inference.\nTraditional approaches such as pruning, quantization, and knowledge\ndistillation have been explored to mitigate these issues, but they often fall\nshort in fully addressing the complexities of multi-task environments. This\npaper introduces \\textbf{\\underline{S}}elective \\textbf{\\underline{T}}ask\n\\textbf{\\underline{A}}rithmetic \\underline{\\textbf{(STA)}}, a training-free\nframework designed to enhance multi-task performance through task-specific\nparameter fusion. STA addresses three key challenges: (i) \\textbf{Parameter\nimportance diversity: } Recognizing that different tasks relie on distinct\nparameters, STA employs a loss-sensitive parameter importance metric derived\nfrom a first-order Taylor expansion to accurately measure the importance of\nparameters for each task. (ii) \\textbf{Over-reliance on hyperparameter tuning:\n}By enhancing the sparsity of task vectors through parameter importance\nmetrics, STA reduces the need for extensive hyperparameter tuning, thereby\nimproving the generalization and robustness of the model. (iii) \\textbf{Neglect\nof other abilities in task arithmetic: } Previous works have largely overlooked\nthe potential for more precise task forgetting. STA leverages its parameter\nimportance metric to achieve more controlled and effective task forgetting,\nminimizing the impact of noisy elements that can degrade model performance.\nExperimental results demonstrate that STA achieves superior multi-task\nperformance across benchmarks and excellent performance in task forgetting.\n","authors":["Tian Bowen","Lai Songning","Wu Jiemin","Shuai Zhihao","Ge Shiming","Yue Yutao"],"pdf_url":"https://arxiv.org/pdf/2411.16139v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2411.14449v2","updated":"2024-11-25T06:51:39Z","published":"2024-11-10T07:01:53Z","title":"Unlearn to Relearn Backdoors: Deferred Backdoor Functionality Attacks on\n  Deep Learning Models","summary":"  Deep learning models are vulnerable to backdoor attacks, where adversaries\ninject malicious functionality during training that activates on trigger inputs\nat inference time. Extensive research has focused on developing stealthy\nbackdoor attacks to evade detection and defense mechanisms. However, these\napproaches still have limitations that leave the door open for detection and\nmitigation due to their inherent design to cause malicious behavior in the\npresence of a trigger. To address this limitation, we introduce Deferred\nActivated Backdoor Functionality (DABF), a new paradigm in backdoor attacks.\nUnlike conventional attacks, DABF initially conceals its backdoor, producing\nbenign outputs even when triggered. This stealthy behavior allows DABF to\nbypass multiple detection and defense methods, remaining undetected during\ninitial inspections. The backdoor functionality is strategically activated only\nafter the model undergoes subsequent updates, such as retraining on benign\ndata. DABF attacks exploit the common practice in the life cycle of machine\nlearning models to perform model updates and fine-tuning after initial\ndeployment. To implement DABF attacks, we approach the problem by making the\nunlearning of the backdoor fragile, allowing it to be easily cancelled and\nsubsequently reactivate the backdoor functionality. To achieve this, we propose\na novel two-stage training scheme, called DeferBad. Our extensive experiments\nacross various fine-tuning scenarios, backdoor attack types, datasets, and\nmodel architectures demonstrate the effectiveness and stealthiness of DeferBad.\n","authors":["Jeongjin Shin","Sangdon Park"],"pdf_url":"https://arxiv.org/pdf/2411.14449v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16133v1","updated":"2024-11-25T06:48:38Z","published":"2024-11-25T06:48:38Z","title":"Context Awareness Gate For Retrieval Augmented Generation","summary":"  Retrieval Augmented Generation (RAG) has emerged as a widely adopted approach\nto mitigate the limitations of large language models (LLMs) in answering\ndomain-specific questions. Previous research has predominantly focused on\nimproving the accuracy and quality of retrieved data chunks to enhance the\noverall performance of the generation pipeline. However, despite ongoing\nadvancements, the critical issue of retrieving irrelevant information -- which\ncan impair the ability of the model to utilize its internal knowledge\neffectively -- has received minimal attention. In this work, we investigate the\nimpact of retrieving irrelevant information in open-domain question answering,\nhighlighting its significant detrimental effect on the quality of LLM outputs.\nTo address this challenge, we propose the Context Awareness Gate (CAG)\narchitecture, a novel mechanism that dynamically adjusts the LLMs' input prompt\nbased on whether the user query necessitates external context retrieval.\nAdditionally, we introduce the Vector Candidates method, a core mathematical\ncomponent of CAG that is statistical, LLM-independent, and highly scalable. We\nfurther examine the distributions of relationships between contexts and\nquestions, presenting a statistical analysis of these distributions. This\nanalysis can be leveraged to enhance the context retrieval process in Retrieval\nAugmented Generation (RAG) systems.\n","authors":["Mohammad Hassan Heydari","Arshia Hemmat","Erfan Naman","Afsaneh Fatemi"],"pdf_url":"https://arxiv.org/pdf/2411.16133v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16127v1","updated":"2024-11-25T06:26:58Z","published":"2024-11-25T06:26:58Z","title":"DF-GNN: Dynamic Fusion Framework for Attention Graph Neural Networks on\n  GPUs","summary":"  Attention Graph Neural Networks (AT-GNNs), such as GAT and Graph Transformer,\nhave demonstrated superior performance compared to other GNNs. However,\nexisting GNN systems struggle to efficiently train AT-GNNs on GPUs due to their\nintricate computation patterns. The execution of AT-GNN operations without\nkernel fusion results in heavy data movement and significant kernel launch\noverhead, while fixed thread scheduling in existing GNN kernel fusion\nstrategies leads to sub-optimal performance, redundant computation and\nunbalanced workload. To address these challenges, we propose a dynamic kernel\nfusion framework, DF-GNN, for the AT-GNN family. DF-GNN introduces a dynamic\nbi-level thread scheduling strategy, enabling flexible adjustments to thread\nscheduling while retaining the benefits of shared memory within the fused\nkernel. DF-GNN tailors specific thread scheduling for operations in AT-GNNs and\nconsiders the performance bottleneck shift caused by the presence of super\nnodes. Additionally, DF-GNN is integrated with the PyTorch framework for high\nprogrammability. Evaluations across diverse GNN models and multiple datasets\nreveal that DF-GNN surpasses existing GNN kernel optimization works like\ncuGraph and dgNN, with speedups up to $7.0\\times$ over the state-of-the-art\nnon-fusion DGL sparse library. Moreover, it achieves an average speedup of\n$2.16\\times$ in end-to-end training compared to the popular GNN computing\nframework DGL.\n","authors":["Jiahui Liu","Zhenkun Cai","Zhiyong Chen","Minjie Wang"],"pdf_url":"https://arxiv.org/pdf/2411.16127v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16121v1","updated":"2024-11-25T06:14:06Z","published":"2024-11-25T06:14:06Z","title":"DP-CDA: An Algorithm for Enhanced Privacy Preservation in Dataset\n  Synthesis Through Randomized Mixing","summary":"  In recent years, the growth of data across various sectors, including\nhealthcare, security, finance, and education, has created significant\nopportunities for analysis and informed decision-making. However, these\ndatasets often contain sensitive and personal information, which raises serious\nprivacy concerns. Protecting individual privacy is crucial, yet many existing\nmachine learning and data publishing algorithms struggle with high-dimensional\ndata, facing challenges related to computational efficiency and privacy\npreservation. To address these challenges, we introduce an effective data\npublishing algorithm \\emph{DP-CDA}. Our proposed algorithm generates synthetic\ndatasets by randomly mixing data in a class-specific manner, and inducing\ncarefully-tuned randomness to ensure formal privacy guarantees. Our\ncomprehensive privacy accounting shows that DP-CDA provides a stronger privacy\nguarantee compared to existing methods, allowing for better utility while\nmaintaining strict level of privacy. To evaluate the effectiveness of DP-CDA,\nwe examine the accuracy of predictive models trained on the synthetic data,\nwhich serves as a measure of dataset utility. Importantly, we identify an\noptimal order of mixing that balances privacy guarantee with predictive\naccuracy. Our results indicate that synthetic datasets produced using the\nDP-CDA can achieve superior utility compared to those generated by traditional\ndata publishing algorithms, even when subject to the same privacy requirements.\n","authors":["Utsab Saha","Tanvir Muntakim Tonoy","Hafiz Imtiaz"],"pdf_url":"https://arxiv.org/pdf/2411.16121v1.pdf","comment":"Under review in Elsevier Array"},{"id":"http://arxiv.org/abs/2411.16120v1","updated":"2024-11-25T06:11:46Z","published":"2024-11-25T06:11:46Z","title":"Why the Agent Made that Decision: Explaining Deep Reinforcement Learning\n  with Vision Masks","summary":"  Due to the inherent lack of transparency in deep neural networks, it is\nchallenging for deep reinforcement learning (DRL) agents to gain trust and\nacceptance from users, especially in safety-critical applications such as\nmedical diagnosis and military operations. Existing methods for explaining an\nagent's decision either require to retrain the agent using models that support\nexplanation generation or rely on perturbation-based techniques to reveal the\nsignificance of different input features in the decision making process.\nHowever, retraining the agent may compromise its integrity and performance,\nwhile perturbation-based methods have limited performance and lack knowledge\naccumulation or learning capabilities. Moreover, since each perturbation is\nperformed independently, the joint state of the perturbed inputs may not be\nphysically meaningful. To address these challenges, we introduce\n$\\textbf{VisionMask}$, a standalone explanation model trained end-to-end to\nidentify the most critical regions in the agent's visual input that can explain\nits actions. VisionMask is trained in a self-supervised manner without relying\non human-generated labels. Importantly, its training does not alter the agent\nmodel, hence preserving the agent's performance and integrity. We evaluate\nVisionMask on Super Mario Bros (SMB) and three Atari games. Compared to\nexisting methods, VisionMask achieves a 14.9% higher insertion accuracy and a\n30.08% higher F1-Score in reproducing original actions from the selected visual\nexplanations. We also present examples illustrating how VisionMask can be used\nfor counterfactual analysis.\n","authors":["Rui Zuo","Zifan Wang","Simon Khan","Garrett Ethan Katz","Qinru Qiu"],"pdf_url":"https://arxiv.org/pdf/2411.16120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19458v2","updated":"2024-11-25T06:09:45Z","published":"2024-09-28T21:26:50Z","title":"Scalable Fine-tuning from Multiple Data Sources: A First-Order\n  Approximation Approach","summary":"  We study the problem of fine-tuning a language model (LM) for a target task\nby optimally using the information from $n$ auxiliary tasks. This problem has\nbroad applications in NLP, such as targeted instruction tuning and data\nselection in chain-of-thought fine-tuning. The key challenge of this problem is\nthat not all auxiliary tasks are useful to improve the performance of the\ntarget task. Thus, choosing the right subset of auxiliary tasks is crucial.\nConventional subset selection methods, such as forward and backward stepwise\nselection, are unsuitable for LM fine-tuning because they require repeated\ntraining on subsets of auxiliary tasks. This paper introduces a new algorithm\nto estimate model fine-tuning performances without repeated training. Our\nalgorithm first performs multitask training using the data of all the tasks to\nobtain a meta initialization. Then, we approximate the model fine-tuning loss\nof a subset using functional values and gradients from the meta initialization.\nEmpirically, we find that this gradient-based approximation holds with\nremarkable accuracy for twelve transformer-based LMs. Thus, we can now estimate\nfine-tuning performances on CPUs within a few seconds. Finally, we fine-tune\nthe pretrained base model for once on the selected subset of tasks. We conduct\nextensive experiments to validate this approach, delivering a speedup of\n$30\\times$ over conventional subset selection while incurring only $1\\%$ error\nof the true fine-tuning performances. In downstream evaluations involving both\ninstruction tuning and chain-of-thought fine-tuning, this loss-based selection\napproach improves over prior gradient or representation similarity-based\nmethods for subset selection by up to $3.8\\%$.\n","authors":["Dongyue Li","Ziniu Zhang","Lu Wang","Hongyang R. Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.19458v2.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2405.19644v2","updated":"2024-11-25T06:03:02Z","published":"2024-05-30T02:53:19Z","title":"EgoSurgery-Phase: A Dataset of Surgical Phase Recognition from\n  Egocentric Open Surgery Videos","summary":"  Surgical phase recognition has gained significant attention due to its\npotential to offer solutions to numerous demands of the modern operating room.\nHowever, most existing methods concentrate on minimally invasive surgery (MIS),\nleaving surgical phase recognition for open surgery understudied. This\ndiscrepancy is primarily attributed to the scarcity of publicly available open\nsurgery video datasets for surgical phase recognition. To address this issue,\nwe introduce a new egocentric open surgery video dataset for phase recognition,\nnamed EgoSurgery-Phase. This dataset comprises 15 hours of real open surgery\nvideos spanning 9 distinct surgical phases all captured using an egocentric\ncamera attached to the surgeon's head. In addition to video, the\nEgoSurgery-Phase offers eye gaze. As far as we know, it is the first real open\nsurgery video dataset for surgical phase recognition publicly available.\nFurthermore, inspired by the notable success of masked autoencoders (MAEs) in\nvideo understanding tasks (e.g., action recognition), we propose a gaze-guided\nmasked autoencoder (GGMAE). Considering the regions where surgeons' gaze\nfocuses are often critical for surgical phase recognition (e.g., surgical\nfield), in our GGMAE, the gaze information acts as an empirical semantic\nrichness prior to guiding the masking process, promoting better attention to\nsemantically rich spatial regions. GGMAE significantly improves the previous\nstate-of-the-art recognition method (6.4% in Jaccard) and the masked\nautoencoder-based method (3.1% in Jaccard) on EgoSurgery-Phase.\n","authors":["Ryo Fujii","Masashi Hatano","Hideo Saito","Hiroki Kajita"],"pdf_url":"https://arxiv.org/pdf/2405.19644v2.pdf","comment":"Early accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.03095v3","updated":"2024-11-25T05:58:10Z","published":"2024-06-05T09:36:15Z","title":"EgoSurgery-Tool: A Dataset of Surgical Tool and Hand Detection from\n  Egocentric Open Surgery Videos","summary":"  Surgical tool detection is a fundamental task for understanding egocentric\nopen surgery videos. However, detecting surgical tools presents significant\nchallenges due to their highly imbalanced class distribution, similar shapes\nand similar textures, and heavy occlusion. The lack of a comprehensive\nlarge-scale dataset compounds these challenges. In this paper, we introduce\nEgoSurgery-Tool, an extension of the existing EgoSurgery-Phase dataset, which\ncontains real open surgery videos captured using an egocentric camera attached\nto the surgeon's head, along with phase annotations. EgoSurgery-Tool has been\ndensely annotated with surgical tools and comprises over 49K surgical tool\nbounding boxes across 15 categories, constituting a large-scale surgical tool\ndetection dataset. EgoSurgery-Tool also provides annotations for hand detection\nwith over 46K hand-bounding boxes, capturing hand-object interactions that are\ncrucial for understanding activities in egocentric open surgery.\nEgoSurgery-Tool is superior to existing datasets due to its larger scale,\ngreater variety of surgical tools, more annotations, and denser scenes. We\nconduct a comprehensive analysis of EgoSurgery-Tool using nine popular object\ndetectors to assess their effectiveness in both surgical tool and hand\ndetection.\n","authors":["Ryo Fujii","Hideo Saito","Hiroki Kajita"],"pdf_url":"https://arxiv.org/pdf/2406.03095v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16110v1","updated":"2024-11-25T05:51:38Z","published":"2024-11-25T05:51:38Z","title":"FUN-AD: Fully Unsupervised Learning for Anomaly Detection with Noisy\n  Training Data","summary":"  While the mainstream research in anomaly detection has mainly followed the\none-class classification, practical industrial environments often incur noisy\ntraining data due to annotation errors or lack of labels for new or refurbished\nproducts. To address these issues, we propose a novel learning-based approach\nfor fully unsupervised anomaly detection with unlabeled and potentially\ncontaminated training data. Our method is motivated by two observations, that\ni) the pairwise feature distances between the normal samples are on average\nlikely to be smaller than those between the anomaly samples or heterogeneous\nsamples and ii) pairs of features mutually closest to each other are likely to\nbe homogeneous pairs, which hold if the normal data has smaller variance than\nthe anomaly data. Building on the first observation that nearest-neighbor\ndistances can distinguish between confident normal samples and anomalies, we\npropose a pseudo-labeling strategy using an iteratively reconstructed memory\nbank (IRMB). The second observation is utilized as a new loss function to\npromote class-homogeneity between mutually closest pairs thereby reducing the\nill-posedness of the task. Experimental results on two public industrial\nanomaly benchmarks and semantic anomaly examples validate the effectiveness of\nFUN-AD across different scenarios and anomaly-to-normal ratios. Our code is\navailable at https://github.com/HY-Vision-Lab/FUNAD.\n","authors":["Jiin Im","Yongho Son","Je Hyeong Hong"],"pdf_url":"https://arxiv.org/pdf/2411.16110v1.pdf","comment":"Accepted at WACV 2025. Supplementary material included after\n  references. 17 pages, 7 figures, 14 tables"},{"id":"http://arxiv.org/abs/2408.05160v2","updated":"2024-11-25T05:43:10Z","published":"2024-08-09T16:31:41Z","title":"Federated Hypergraph Learning: Hyperedge Completion with Local\n  Differential Privacy","summary":"  As the volume and complexity increase, graph-structured data commonly need to\nbe split and stored across distributed systems. To enable data mining on\nsubgraphs within these distributed systems, federated graph learning has been\nproposed, allowing collaborative training of Graph Neural Networks (GNNs)\nacross clients without sharing raw node features. However, when dealing with\ngraph structures that involve high-order relationships between nodes, known as\nhypergraphs, existing federated graph learning methods are less effective. In\nthis study, we introduce FedHGL, an innovative federated hypergraph learning\nalgorithm. FedHGL is designed to collaboratively train a comprehensive\nhypergraph neural network across multiple clients, facilitating mining tasks on\nsubgraphs of a hypergraph where relationships are not merely pairwise. To\naddress the high-order information loss between subgraphs caused by distributed\nstorage, we introduce a pre-propagation hyperedge completion operation before\nthe federated training process. In this pre-propagation step, cross-client\nfeature aggregation is performed and distributed at the central server to\nensure that this information can be utilized by the clients. Furthermore, by\nincorporating local differential privacy (LDP) mechanisms, we ensure that the\noriginal node features are not disclosed during this aggregation process.\nExperimental results on seven real-world datasets confirm the effectiveness of\nour approach and demonstrate its performance advantages over traditional\nfederated graph learning methods.\n","authors":["Linfeng Luo","Fengxiao Tang","Xiyu Liu","Zhiqi Guo","Zihao Qiu","Ming Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.05160v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16105v1","updated":"2024-11-25T05:32:34Z","published":"2024-11-25T05:32:34Z","title":"Adaptive Circuit Behavior and Generalization in Mechanistic\n  Interpretability","summary":"  Mechanistic interpretability aims to understand the inner workings of large\nneural networks by identifying circuits, or minimal subgraphs within the model\nthat implement algorithms responsible for performing specific tasks. These\ncircuits are typically discovered and analyzed using a narrowly defined prompt\nformat. However, given the abilities of large language models (LLMs) to\ngeneralize across various prompt formats for the same task, it remains unclear\nhow well these circuits generalize. For instance, it is unclear whether the\nmodels generalization results from reusing the same circuit components, the\ncomponents behaving differently, or the use of entirely different components.\nIn this paper, we investigate the generality of the indirect object\nidentification (IOI) circuit in GPT-2 small, which is well-studied and believed\nto implement a simple, interpretable algorithm. We evaluate its performance on\nprompt variants that challenge the assumptions of this algorithm. Our findings\nreveal that the circuit generalizes surprisingly well, reusing all of its\ncomponents and mechanisms while only adding additional input edges. Notably,\nthe circuit generalizes even to prompt variants where the original algorithm\nshould fail; we discover a mechanism that explains this which we term S2\nHacking. Our findings indicate that circuits within LLMs may be more flexible\nand general than previously recognized, underscoring the importance of studying\ncircuit generalization to better understand the broader capabilities of these\nmodels.\n","authors":["Jatin Nainani","Sankaran Vaidyanathan","AJ Yeung","Kartik Gupta","David Jensen"],"pdf_url":"https://arxiv.org/pdf/2411.16105v1.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2404.16789v3","updated":"2024-11-25T05:27:13Z","published":"2024-04-25T17:38:57Z","title":"Continual Learning of Large Language Models: A Comprehensive Survey","summary":"  The recent success of large language models (LLMs) trained on static,\npre-collected, general datasets has sparked numerous research directions and\napplications. One such direction addresses the non-trivial challenge of\nintegrating pre-trained LLMs into dynamic data distributions, task structures,\nand user preferences. Pre-trained LLMs, when tailored for specific needs, often\nexperience significant performance degradation in previous knowledge domains --\na phenomenon known as \"catastrophic forgetting\". While extensively studied in\nthe continual learning (CL) community, it presents new manifestations in the\nrealm of LLMs. In this survey, we provide a comprehensive overview of the\ncurrent research progress on LLMs within the context of CL. This survey is\nstructured into four main sections: we first describe an overview of\ncontinually learning LLMs, consisting of two directions of continuity: vertical\ncontinuity (or vertical continual learning), i.e., continual adaptation from\ngeneral to specific capabilities, and horizontal continuity (or horizontal\ncontinual learning), i.e., continual adaptation across time and domains\n(Section 3). We then summarize three stages of learning LLMs in the context of\nmodern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP),\nand Continual Fine-Tuning (CFT) (Section 4). Then we provide an overview of\nevaluation protocols for continual learning with LLMs, along with the current\navailable data sources (Section 5). Finally, we discuss intriguing questions\npertaining to continual learning for LLMs (Section 6). The full list of papers\nexamined in this survey is available at\nhttps://github.com/Wang-ML-Lab/llm-continual-learning-survey.\n","authors":["Haizhou Shi","Zihao Xu","Hengyi Wang","Weiyi Qin","Wenyuan Wang","Yibin Wang","Zifeng Wang","Sayna Ebrahimi","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2404.16789v3.pdf","comment":"44 pages, 2 figures, 4 tables; Work in progress"},{"id":"http://arxiv.org/abs/2411.16102v1","updated":"2024-11-25T05:24:53Z","published":"2024-11-25T05:24:53Z","title":"BlendServe: Optimizing Offline Inference for Auto-regressive Large\n  Models with Resource-aware Batching","summary":"  Offline batch inference, which leverages the flexibility of request batching\nto achieve higher throughput and lower costs, is becoming more popular for\nlatency-insensitive applications. Meanwhile, recent progress in model\ncapability and modality makes requests more diverse in compute and memory\ndemands, creating unique opportunities for throughput improvement by resource\noverlapping. However, a request schedule that maximizes resource overlapping\ncan conflict with the schedule that maximizes prefix sharing, a widely-used\nperformance optimization, causing sub-optimal inference throughput. We present\nBlendServe, a system that maximizes resource utilization of offline batch\ninference by combining the benefits of resource overlapping and prefix sharing\nusing a resource-aware prefix tree. BlendServe exploits the relaxed latency\nrequirements in offline batch inference to reorder and overlap requests with\nvaried resource demands while ensuring high prefix sharing. We evaluate\nBlendServe on a variety of synthetic multi-modal workloads and show that it\nprovides up to $1.44\\times$ throughput boost compared to widely-used industry\nstandards, vLLM and SGLang.\n","authors":["Yilong Zhao","Shuo Yang","Kan Zhu","Lianmin Zheng","Baris Kasikci","Yang Zhou","Jiarong Xing","Ion Stoica"],"pdf_url":"https://arxiv.org/pdf/2411.16102v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16095v1","updated":"2024-11-25T05:07:00Z","published":"2024-11-25T05:07:00Z","title":"LDACP: Long-Delayed Ad Conversions Prediction Model for Bidding Strategy","summary":"  In online advertising, once an ad campaign is deployed, the automated bidding\nsystem dynamically adjusts the bidding strategy to optimize Cost Per Action\n(CPA) based on the number of ad conversions. For ads with a long conversion\ndelay, relying solely on the real-time tracked conversion number as a signal\nfor bidding strategy can significantly overestimate the current CPA, leading to\nconservative bidding strategies. Therefore, it is crucial to predict the number\nof long-delayed conversions. Nonetheless, it is challenging to predict ad\nconversion numbers through traditional regression methods due to the wide range\nof ad conversion numbers. Previous regression works have addressed this\nchallenge by transforming regression problems into bucket classification\nproblems, achieving success in various scenarios. However, specific challenges\narise when predicting the number of ad conversions: 1) The integer nature of ad\nconversion numbers exacerbates the discontinuity issue in one-hot hard labels;\n2) The long-tail distribution of ad conversion numbers complicates tail data\nprediction. In this paper, we propose the Long-Delayed Ad Conversions\nPrediction model for bidding strategy (LDACP), which consists of two\nsub-modules. To alleviate the issue of discontinuity in one-hot hard labels,\nthe Bucket Classification Module with label Smoothing method (BCMS) converts\none-hot hard labels into non-normalized soft labels, then fits these soft\nlabels by minimizing classification loss and regression loss. To address the\nchallenge of predicting tail data, the Value Regression Module with Proxy\nlabels (VRMP) uses the prediction bias of aggregated pCTCVR as proxy labels.\nFinally, a Mixture of Experts (MoE) structure integrates the predictions from\nBCMS and VRMP to obtain the final predicted ad conversion number.\n","authors":["Peng Cui","Yiming Yang","Fusheng Jin","Siyuan Tang","Yunli Wang","Fukang Yang","Yalong Jia","Qingpeng Cai","Fei Pan","Changcheng Li","Peng Jiang"],"pdf_url":"https://arxiv.org/pdf/2411.16095v1.pdf","comment":"10 pages, 8 figures, 6 tables"},{"id":"http://arxiv.org/abs/2411.16094v1","updated":"2024-11-25T05:02:35Z","published":"2024-11-25T05:02:35Z","title":"Very Basics of Tensors with Graphical Notations: Unfolding,\n  Calculations, and Decompositions","summary":"  Tensor network diagram (graphical notation) is a useful tool that graphically\nrepresents multiplications between multiple tensors using nodes and edges.\nUsing the graphical notation, complex multiplications between tensors can be\ndescribed simply and intuitively, and it also helps to understand the essence\nof tensor products. In fact, most of matrix/tensor products including inner\nproduct, outer product, Hadamard product, Kronecker product, and Khatri-Rao\nproduct can be written in graphical notation. These matrix/tensor operations\nare essential building blocks for the use of matrix/tensor decompositions in\nsignal processing and machine learning. The purpose of this lecture note is to\nlearn the very basics of tensors and how to represent them in mathematical\nsymbols and graphical notation. Many papers using tensors omit these detailed\ndefinitions and explanations, which can be difficult for the reader. I hope\nthis note will be of help to such readers.\n","authors":["Tatsuya Yokota"],"pdf_url":"https://arxiv.org/pdf/2411.16094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16086v1","updated":"2024-11-25T04:40:42Z","published":"2024-11-25T04:40:42Z","title":"HiDP: Hierarchical DNN Partitioning for Distributed Inference on\n  Heterogeneous Edge Platforms","summary":"  Edge inference techniques partition and distribute Deep Neural Network (DNN)\ninference tasks among multiple edge nodes for low latency inference, without\nconsidering the core-level heterogeneity of edge nodes. Further, default DNN\ninference frameworks also do not fully utilize the resources of heterogeneous\nedge nodes, resulting in higher inference latency. In this work, we propose a\nhierarchical DNN partitioning strategy (HiDP) for distributed inference on\nheterogeneous edge nodes. Our strategy hierarchically partitions DNN workloads\nat both global and local levels by considering the core-level heterogeneity of\nedge nodes. We evaluated our proposed HiDP strategy against relevant\ndistributed inference techniques over widely used DNN models on commercial edge\ndevices. On average our strategy achieved 38% lower latency, 46% lower energy,\nand 56% higher throughput in comparison with other relevant approaches.\n","authors":["Zain Taufique","Aman Vyas","Antonio Miele","Pasi Liljeberg","Anil Kanduri"],"pdf_url":"https://arxiv.org/pdf/2411.16086v1.pdf","comment":"7 pages, 8 figures, 1 table, and 1 algorithm. The manuscript is\n  accepted to be published in 28th Design, Automation and Test in Europe\n  Conference (IEEE DATE, 2025)"},{"id":"http://arxiv.org/abs/2411.16085v1","updated":"2024-11-25T04:36:01Z","published":"2024-11-25T04:36:01Z","title":"Cautious Optimizers: Improving Training with One Line of Code","summary":"  AdamW has been the default optimizer for transformer pretraining. For many\nyears, our community searches for faster and more stable optimizers with only\nconstraint positive outcomes. In this work, we propose a \\textbf{single-line\nmodification in Pytorch} to any momentum-based optimizer, which we rename\nCautious Optimizer, e.g. C-AdamW and C-Lion. Our theoretical result shows that\nthis modification preserves Adam's Hamiltonian function and it does not break\nthe convergence guarantee under the Lyapunov analysis. In addition, a whole new\nfamily of optimizers is revealed by our theoretical insight. Among them, we\npick the simplest one for empirical experiments, showing speed-up on Llama and\nMAE pretraining up to $1.47\\times$. Code is available at\nhttps://github.com/kyleliang919/C-Optim\n","authors":["Kaizhao Liang","Lizhang Chen","Bo Liu","Qiang Liu"],"pdf_url":"https://arxiv.org/pdf/2411.16085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16081v1","updated":"2024-11-25T04:22:17Z","published":"2024-11-25T04:22:17Z","title":"Exploring the Generalization Capabilities of AID-based Bi-level\n  Optimization","summary":"  Bi-level optimization has achieved considerable success in contemporary\nmachine learning applications, especially for given proper hyperparameters.\nHowever, due to the two-level optimization structure, commonly, researchers\nfocus on two types of bi-level optimization methods: approximate implicit\ndifferentiation (AID)-based and iterative differentiation (ITD)-based\napproaches. ITD-based methods can be readily transformed into single-level\noptimization problems, facilitating the study of their generalization\ncapabilities. In contrast, AID-based methods cannot be easily transformed\nsimilarly but must stay in the two-level structure, leaving their\ngeneralization properties enigmatic. In this paper, although the outer-level\nfunction is nonconvex, we ascertain the uniform stability of AID-based methods,\nwhich achieves similar results to a single-level nonconvex problem. We conduct\na convergence analysis for a carefully chosen step size to maintain stability.\nCombining the convergence and stability results, we give the generalization\nability of AID-based bi-level optimization methods. Furthermore, we carry out\nan ablation study of the parameters and assess the performance of these methods\non real-world tasks. Our experimental results corroborate the theoretical\nfindings, demonstrating the effectiveness and potential applications of these\nmethods.\n","authors":["Congliang Chen","Li Shen","Zhiqiang Xu","Wei Liu","Zhi-Quan Luo","Peilin Zhao"],"pdf_url":"https://arxiv.org/pdf/2411.16081v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16080v1","updated":"2024-11-25T04:20:52Z","published":"2024-11-25T04:20:52Z","title":"Boosting 3D Object Generation through PBR Materials","summary":"  Automatic 3D content creation has gained increasing attention recently, due\nto its potential in various applications such as video games, film industry,\nand AR/VR. Recent advancements in diffusion models and multimodal models have\nnotably improved the quality and efficiency of 3D object generation given a\nsingle RGB image. However, 3D objects generated even by state-of-the-art\nmethods are still unsatisfactory compared to human-created assets. Considering\nonly textures instead of materials makes these methods encounter challenges in\nphoto-realistic rendering, relighting, and flexible appearance editing. And\nthey also suffer from severe misalignment between geometry and high-frequency\ntexture details. In this work, we propose a novel approach to boost the quality\nof generated 3D objects from the perspective of Physics-Based Rendering (PBR)\nmaterials. By analyzing the components of PBR materials, we choose to consider\nalbedo, roughness, metalness, and bump maps. For albedo and bump maps, we\nleverage Stable Diffusion fine-tuned on synthetic data to extract these values,\nwith novel usages of these fine-tuned models to obtain 3D consistent albedo UV\nand bump UV for generated objects. In terms of roughness and metalness maps, we\nadopt a semi-automatic process to provide room for interactive adjustment,\nwhich we believe is more practical. Extensive experiments demonstrate that our\nmodel is generally beneficial for various state-of-the-art generation methods,\nsignificantly boosting the quality and realism of their generated 3D objects,\nwith natural relighting effects and substantially improved geometry.\n","authors":["Yitong Wang","Xudong Xu","Li Ma","Haoran Wang","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2411.16080v1.pdf","comment":"Accepted to SIGGRAPH Asia 2024 Conference Papers"},{"id":"http://arxiv.org/abs/2411.16073v1","updated":"2024-11-25T03:52:47Z","published":"2024-11-25T03:52:47Z","title":"Soft-TransFormers for Continual Learning","summary":"  Inspired by Well-initialized Lottery Ticket Hypothesis (WLTH), which provides\nsuboptimal fine-tuning solutions, we propose a novel fully fine-tuned continual\nlearning (CL) method referred to as Soft-TransFormers (Soft-TF). Soft-TF\nsequentially learns and selects an optimal soft-network or subnetwork for each\ntask. During sequential training in CL, Soft-TF jointly optimizes the weights\nof sparse layers to obtain task-adaptive soft (real-valued) networks or\nsubnetworks (binary masks), while keeping the well-pre-trained layer parameters\nfrozen. In inference, the identified task-adaptive network of Soft-TF masks the\nparameters of the pre-trained network, mapping to an optimal solution for each\ntask and minimizing Catastrophic Forgetting (CF) - the soft-masking preserves\nthe knowledge of the pre-trained network. Extensive experiments on Vision\nTransformer (ViT) and CLIP demonstrate the effectiveness of Soft-TF, achieving\nstate-of-the-art performance across various CL scenarios, including\nClass-Incremental Learning (CIL) and Task-Incremental Learning (TIL), supported\nby convergence theory.\n","authors":["Haeyong Kang","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2411.16073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08633v2","updated":"2024-11-25T03:39:51Z","published":"2024-10-11T08:55:17Z","title":"Transformers Provably Solve Parity Efficiently with Chain of Thought","summary":"  This work provides the first theoretical analysis of training transformers to\nsolve complex problems by recursively generating intermediate states, analogous\nto fine-tuning for chain-of-thought (CoT) reasoning. We consider training a\none-layer transformer to solve the fundamental $k$-parity problem, extending\nthe work on RNNs by Wies et al. (2023). We establish three key results: (1) any\nfinite-precision gradient-based algorithm, without intermediate supervision,\nrequires substantial iterations to solve parity with finite samples. (2) In\ncontrast, when intermediate parities are incorporated into the loss function,\nour model can learn parity in one gradient update when aided by \\emph{teacher\nforcing}, where ground-truth labels of the reasoning chain are provided at each\ngeneration step. (3) Even without teacher forcing, where the model must\ngenerate CoT chains end-to-end, parity can be learned efficiently if augmented\ndata is employed to internally verify the soundness of intermediate steps. Our\nfindings, supported by numerical experiments, show that task decomposition and\nstepwise reasoning naturally arise from optimizing transformers with CoT;\nmoreover, self-consistency checking can improve multi-step reasoning ability,\naligning with empirical studies of CoT.\n","authors":["Juno Kim","Taiji Suzuki"],"pdf_url":"https://arxiv.org/pdf/2410.08633v2.pdf","comment":"NeurIPS 2024 M3L Workshop"},{"id":"http://arxiv.org/abs/2410.20483v2","updated":"2024-11-25T03:28:36Z","published":"2024-10-27T15:39:52Z","title":"Improving Decision Sparsity","summary":"  Sparsity is a central aspect of interpretability in machine learning.\nTypically, sparsity is measured in terms of the size of a model globally, such\nas the number of variables it uses. However, this notion of sparsity is not\nparticularly relevant for decision-making; someone subjected to a decision does\nnot care about variables that do not contribute to the decision. In this work,\nwe dramatically expand a notion of decision sparsity called the Sparse\nExplanation Value(SEV) so that its explanations are more meaningful. SEV\nconsiders movement along a hypercube towards a reference point. By allowing\nflexibility in that reference and by considering how distances along the\nhypercube translate to distances in feature space, we can derive sparser and\nmore meaningful explanations for various types of function classes. We present\ncluster-based SEV and its variant tree-based SEV, introduce a method that\nimproves credibility of explanations, and propose algorithms that optimize\ndecision sparsity in machine learning models.\n","authors":["Yiyang Sun","Tong Wang","Cynthia Rudin"],"pdf_url":"https://arxiv.org/pdf/2410.20483v2.pdf","comment":"Accepted to 38th Conference on Neural Information Processing Systems\n  (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2411.16063v1","updated":"2024-11-25T03:25:17Z","published":"2024-11-25T03:25:17Z","title":"VICON: Vision In-Context Operator Networks for Multi-Physics Fluid\n  Dynamics Prediction","summary":"  In-Context Operator Networks (ICONs) are models that learn operators across\ndifferent types of PDEs using a few-shot, in-context approach. Although they\nshow successful generalization to various PDEs, existing methods treat each\ndata point as a single token, and suffer from computational inefficiency when\nprocessing dense data, limiting their application in higher spatial dimensions.\nIn this work, we propose Vision In-Context Operator Networks (VICON),\nincorporating a vision transformer architecture that efficiently processes 2D\nfunctions through patch-wise operations. We evaluated our method on three fluid\ndynamics datasets, demonstrating both superior performance (reducing scaled\n$L^2$ error by $40\\%$ and $61.6\\%$ for two benchmark datasets for compressible\nflows, respectively) and computational efficiency (requiring only one-third of\nthe inference time per frame) in long-term rollout predictions compared to the\ncurrent state-of-the-art sequence-to-sequence model with fixed timestep\nprediction: Multiple Physics Pretraining (MPP). Compared to MPP, our method\npreserves the benefits of in-context operator learning, enabling flexible\ncontext formation when dealing with insufficient frame counts or varying\ntimestep values.\n","authors":["Yadi Cao","Yuxuan Liu","Liu Yang","Rose Yu","Hayden Schaeffer","Stanley Osher"],"pdf_url":"https://arxiv.org/pdf/2411.16063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13565v3","updated":"2024-11-25T03:05:52Z","published":"2024-03-20T12:58:46Z","title":"AdaTrans: Feature-wise and Sample-wise Adaptive Transfer Learning for\n  High-dimensional Regression","summary":"  We consider the transfer learning problem in the high dimensional linear\nregression setting, where the feature dimension is larger than the sample size.\nTo learn transferable information, which may vary across features or the source\nsamples, we propose an adaptive transfer learning method that can detect and\naggregate the feature-wise (F-AdaTrans) or sample-wise (S-AdaTrans)\ntransferable structures. We achieve this by employing a fused-penalty, coupled\nwith weights that can adapt according to the transferable structure. To choose\nthe weight, we propose a theoretically informed, data-driven procedure,\nenabling F-AdaTrans to selectively fuse the transferable signals with the\ntarget while filtering out non-transferable signals, and S-AdaTrans to obtain\nthe optimal combination of information transferred from each source sample. We\nshow that, with appropriately chosen weights, F-AdaTrans achieves a convergence\nrate close to that of an oracle estimator with a known transferable structure,\nand S-AdaTrans recovers existing near-minimax optimal rates as a special case.\nThe effectiveness of the proposed method is validated using both simulation and\nreal data, demonstrating favorable performance compared to the existing\nmethods.\n","authors":["Zelin He","Ying Sun","Jingyuan Liu","Runze Li"],"pdf_url":"https://arxiv.org/pdf/2403.13565v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16052v1","updated":"2024-11-25T02:42:33Z","published":"2024-11-25T02:42:33Z","title":"Machine-learning emergent spacetime from linear response in future\n  tabletop quantum gravity experiments","summary":"  We introduce a novel interpretable Neural Network (NN) model designed to\nperform precision bulk reconstruction under the AdS/CFT correspondence.\nAccording to the correspondence, a specific condensed matter system on a ring\nis holographically equivalent to a gravitational system on a bulk disk, through\nwhich tabletop quantum gravity experiments may be possible as reported in\narXiv:2211.13863. The purpose of this paper is to reconstruct a\nhigher-dimensional gravity metric from the condensed matter system data via\nmachine learning using the NN. Our machine reads spatially and temporarily\ninhomogeneous linear response data of the condensed matter system, and\nincorporates a novel layer that implements the Runge-Kutta method to achieve\nbetter numerical control. We confirm that our machine can let a\nhigher-dimensional gravity metric be automatically emergent as its\ninterpretable weights, using a linear response of the condensed matter system\nas data, through supervised machine learning. The developed method could serve\nas a foundation for generic bulk reconstruction, i.e., a practical solution to\nthe AdS/CFT correspondence, and would be implemented in future tabletop quantum\ngravity experiments.\n","authors":["Koji Hashimoto","Koshiro Matsuo","Masaki Murata","Gakuto Ogiwara","Daichi Takeda"],"pdf_url":"https://arxiv.org/pdf/2411.16052v1.pdf","comment":"24 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.06209v7","updated":"2024-11-25T02:39:03Z","published":"2024-10-08T17:11:24Z","title":"LeanAgent: Lifelong Learning for Formal Theorem Proving","summary":"  Large Language Models (LLMs) have been successful in mathematical reasoning\ntasks such as formal theorem proving when integrated with interactive proof\nassistants like Lean. Existing approaches involve training or fine-tuning an\nLLM on a specific dataset to perform well on particular domains, such as\nundergraduate-level mathematics. These methods struggle with generalizability\nto advanced mathematics. A fundamental limitation is that these approaches\noperate on static domains, failing to capture how mathematicians often work\nacross multiple domains and projects simultaneously or cyclically. We present\nLeanAgent, a novel lifelong learning framework for formal theorem proving that\ncontinuously generalizes to and improves on ever-expanding mathematical\nknowledge without forgetting previously learned knowledge. LeanAgent introduces\nseveral key innovations, including a curriculum learning strategy that\noptimizes the learning trajectory in terms of mathematical difficulty, a\ndynamic database for efficient management of evolving mathematical knowledge,\nand progressive training to balance stability and plasticity. LeanAgent\nsuccessfully proves 155 theorems previously unproved formally by humans across\n23 diverse Lean repositories, many from advanced mathematics. It performs\nsignificantly better than the static LLM baseline, proving challenging theorems\nin domains like abstract algebra and algebraic topology while showcasing a\nclear progression of learning from basic concepts to advanced topics. In\naddition, we analyze LeanAgent's superior performance on key lifelong learning\nmetrics. LeanAgent achieves exceptional scores in stability and backward\ntransfer, where learning new tasks improves performance on previously learned\ntasks. This emphasizes LeanAgent's continuous generalizability and improvement,\nexplaining its superior theorem-proving performance.\n","authors":["Adarsh Kumarappan","Mo Tiwari","Peiyang Song","Robert Joseph George","Chaowei Xiao","Anima Anandkumar"],"pdf_url":"https://arxiv.org/pdf/2410.06209v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01140v2","updated":"2024-11-25T02:23:02Z","published":"2024-11-02T05:00:44Z","title":"Privacy-Preserving Federated Learning with Differentially Private\n  Hyperdimensional Computing","summary":"  Federated Learning (FL) is essential for efficient data exchange in Internet\nof Things (IoT) environments, as it trains Machine Learning (ML) models locally\nand shares only model updates. However, FL is vulnerable to privacy threats\nlike model inversion and membership inference attacks, which can expose\nsensitive training data. To address these privacy concerns, Differential\nPrivacy (DP) mechanisms are often applied. Yet, adding DP noise to black-box ML\nmodels degrades performance, especially in dynamic IoT systems where\ncontinuous, lifelong FL learning accumulates excessive noise over time. To\nmitigate this issue, we introduce Federated HyperDimensional computing with\nPrivacy-preserving (FedHDPrivacy), an eXplainable Artificial Intelligence (XAI)\nframework that combines the neuro-symbolic paradigm with DP. FedHDPrivacy\ncarefully manages the balance between privacy and performance by theoretically\ntracking cumulative noise from previous rounds and adding only the necessary\nincremental noise to meet privacy requirements. In a real-world case study\ninvolving in-process monitoring of manufacturing machining operations,\nFedHDPrivacy demonstrates robust performance, outperforming standard FL\nframeworks-including Federated Averaging (FedAvg), Federated Stochastic\nGradient Descent (FedSGD), Federated Proximal (FedProx), Federated Normalized\nAveraging (FedNova), and Federated Adam (FedAdam)-by up to 38%. FedHDPrivacy\nalso shows potential for future enhancements, such as multimodal data fusion.\n","authors":["Fardin Jalil Piran","Zhiling Chen","Mohsen Imani","Farhad Imani"],"pdf_url":"https://arxiv.org/pdf/2411.01140v2.pdf","comment":"28 Pages, 10 Figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2309.05058v3","updated":"2024-11-25T15:19:42Z","published":"2023-09-10T15:52:56Z","title":"Multimodal Fish Feeding Intensity Assessment in Aquaculture","summary":"  Fish feeding intensity assessment (FFIA) aims to evaluate fish appetite\nchanges during feeding, which is crucial in industrial aquaculture\napplications. Existing FFIA methods are limited by their robustness to noise,\ncomputational complexity, and the lack of public datasets for developing the\nmodels. To address these issues, we first introduce AV-FFIA, a new dataset\ncontaining 27,000 labeled audio and video clips that capture different levels\nof fish feeding intensity. Then, we introduce multi-modal approaches for FFIA\nby leveraging the models pre-trained on individual modalities and fused with\ndata fusion methods. We perform benchmark studies of these methods on AV-FFIA,\nand demonstrate the advantages of the multi-modal approach over the\nsingle-modality based approach, especially in noisy environments. However,\ncompared to the methods developed for individual modalities, the multimodal\napproaches may involve higher computational costs due to the need for\nindependent encoders for each modality. To overcome this issue, we further\npresent a novel unified mixed-modality based method for FFIA, termed as U-FFIA.\nU-FFIA is a single model capable of processing audio, visual, or audio-visual\nmodalities, by leveraging modality dropout during training and knowledge\ndistillation using the models pre-trained with data from single modality. We\ndemonstrate that U-FFIA can achieve performance better than or on par with the\nstate-of-the-art modality-specific FFIA models, with significantly lower\ncomputational overhead, enabling robust and efficient FFIA for improved\naquaculture management.\n","authors":["Meng Cui","Xubo Liu","Haohe Liu","Zhuangzhuang Du","Tao Chen","Guoping Lian","Daoliang Li","Wenwu Wang"],"pdf_url":"https://arxiv.org/pdf/2309.05058v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16331v1","updated":"2024-11-25T12:24:52Z","published":"2024-11-25T12:24:52Z","title":"Sonic: Shifting Focus to Global Audio Perception in Portrait Animation","summary":"  The study of talking face generation mainly explores the intricacies of\nsynchronizing facial movements and crafting visually appealing,\ntemporally-coherent animations. However, due to the limited exploration of\nglobal audio perception, current approaches predominantly employ auxiliary\nvisual and spatial knowledge to stabilize the movements, which often results in\nthe deterioration of the naturalness and temporal inconsistencies.Considering\nthe essence of audio-driven animation, the audio signal serves as the ideal and\nunique priors to adjust facial expressions and lip movements, without resorting\nto interference of any visual signals. Based on this motivation, we propose a\nnovel paradigm, dubbed as Sonic, to {s}hift f{o}cus on the exploration of\nglobal audio per{c}ept{i}o{n}.To effectively leverage global audio knowledge,\nwe disentangle it into intra- and inter-clip audio perception and collaborate\nwith both aspects to enhance overall perception.For the intra-clip audio\nperception, 1). \\textbf{Context-enhanced audio learning}, in which long-range\nintra-clip temporal audio knowledge is extracted to provide facial expression\nand lip motion priors implicitly expressed as the tone and speed of speech. 2).\n\\textbf{Motion-decoupled controller}, in which the motion of the head and\nexpression movement are disentangled and independently controlled by\nintra-audio clips. Most importantly, for inter-clip audio perception, as a\nbridge to connect the intra-clips to achieve the global perception,\n\\textbf{Time-aware position shift fusion}, in which the global inter-clip audio\ninformation is considered and fused for long-audio inference via through\nconsecutively time-aware shifted windows. Extensive experiments demonstrate\nthat the novel audio-driven paradigm outperform existing SOTA methodologies in\nterms of video quality, temporally consistency, lip synchronization precision,\nand motion diversity.\n","authors":["Xiaozhong Ji","Xiaobin Hu","Zhihong Xu","Junwei Zhu","Chuming Lin","Qingdong He","Jiangning Zhang","Donghao Luo","Yi Chen","Qin Lin","Qinglin Lu","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2411.16331v1.pdf","comment":"refer to our main-page \\url{https://jixiaozhong.github.io/Sonic/}"},{"id":"http://arxiv.org/abs/2310.16334v4","updated":"2024-11-25T05:19:58Z","published":"2023-10-25T03:30:37Z","title":"Structured Multi-Track Accompaniment Arrangement via Style Prior\n  Modelling","summary":"  In the realm of music AI, arranging rich and structured multi-track\naccompaniments from a simple lead sheet presents significant challenges. Such\nchallenges include maintaining track cohesion, ensuring long-term coherence,\nand optimizing computational efficiency. In this paper, we introduce a novel\nsystem that leverages prior modelling over disentangled style factors to\naddress these challenges. Our method presents a two-stage process: initially, a\npiano arrangement is derived from the lead sheet by retrieving piano texture\nstyles; subsequently, a multi-track orchestration is generated by infusing\norchestral function styles into the piano arrangement. Our key design is the\nuse of vector quantization and a unique multi-stream Transformer to model the\nlong-term flow of the orchestration style, which enables flexible,\ncontrollable, and structured music generation. Experiments show that by\nfactorizing the arrangement task into interpretable sub-stages, our approach\nenhances generative capacity while improving efficiency. Additionally, our\nsystem supports a variety of music genres and provides style control at\ndifferent composition hierarchies. We further show that our system achieves\nsuperior coherence, structure, and overall arrangement quality compared to\nexisting baselines.\n","authors":["Jingwei Zhao","Gus Xia","Ziyu Wang","Ye Wang"],"pdf_url":"https://arxiv.org/pdf/2310.16334v4.pdf","comment":"Accepted by NeurIPS 2024; typos addressed"},{"id":"http://arxiv.org/abs/2411.16096v1","updated":"2024-11-25T05:15:38Z","published":"2024-11-25T05:15:38Z","title":"ENCLIP: Ensembling and Clustering-Based Contrastive Language-Image\n  Pretraining for Fashion Multimodal Search with Limited Data and Low-Quality\n  Images","summary":"  Multimodal search has revolutionized the fashion industry, providing a\nseamless and intuitive way for users to discover and explore fashion items.\nBased on their preferences, style, or specific attributes, users can search for\nproducts by combining text and image information. Text-to-image searches enable\nusers to find visually similar items or describe products using natural\nlanguage. This paper presents an innovative approach called ENCLIP, for\nenhancing the performance of the Contrastive Language-Image Pretraining (CLIP)\nmodel, specifically in Multimodal Search targeted towards the domain of fashion\nintelligence. This method focuses on addressing the challenges posed by limited\ndata availability and low-quality images. This paper proposes an algorithm that\ninvolves training and ensembling multiple instances of the CLIP model, and\nleveraging clustering techniques to group similar images together. The\nexperimental findings presented in this study provide evidence of the\neffectiveness of the methodology. This approach unlocks the potential of CLIP\nin the domain of fashion intelligence, where data scarcity and image quality\nissues are prevalent. Overall, the ENCLIP method represents a valuable\ncontribution to the field of fashion intelligence and provides a practical\nsolution for optimizing the CLIP model in scenarios with limited data and\nlow-quality images.\n","authors":["Prithviraj Purushottam Naik","Rohit Agarwal"],"pdf_url":"https://arxiv.org/pdf/2411.16096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13811v2","updated":"2024-11-25T00:21:53Z","published":"2024-11-21T03:21:42Z","title":"X-CrossNet: A complex spectral mapping approach to target speaker\n  extraction with cross attention speaker embedding fusion","summary":"  Target speaker extraction (TSE) is a technique for isolating a target\nspeaker's voice from mixed speech using auxiliary features associated with the\ntarget speaker. It is another attempt at addressing the cocktail party problem\nand is generally considered to have more practical application prospects than\ntraditional speech separation methods. Although academic research in this area\nhas achieved high performance and evaluation scores on public datasets, most\nmodels exhibit significantly reduced performance in real-world noisy or\nreverberant conditions. To address this limitation, we propose a novel TSE\nmodel, X-CrossNet, which leverages CrossNet as its backbone. CrossNet is a\nspeech separation network specifically optimized for challenging noisy and\nreverberant environments, achieving state-of-the-art performance in tasks such\nas speaker separation under these conditions. Additionally, to enhance the\nnetwork's ability to capture and utilize auxiliary features of the target\nspeaker, we integrate a Cross-Attention mechanism into the global multi-head\nself-attention (GMHSA) module within each CrossNet block. This facilitates more\neffective integration of target speaker features with mixed speech features.\nExperimental results show that our method performs superior separation on the\nWSJ0-2mix and WHAMR! datasets, demonstrating strong robustness and stability.\n","authors":["Chang Sun","Bo Qin"],"pdf_url":"https://arxiv.org/pdf/2411.13811v2.pdf","comment":null}]},"2024-11-24T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2409.20302v2","updated":"2024-11-24T23:38:31Z","published":"2024-09-30T14:00:04Z","title":"OM4OV: Leveraging Ontology Matching for Ontology Versioning","summary":"  Due to the dynamic nature of the semantic web, ontology version control is\nrequired to capture time-varying information, most importantly for widely-used\nontologies. Despite the long-standing recognition of ontology versioning (OV)\nas a crucial component for efficient ontology management, the growing size of\nontologies and accumulating errors caused by manual labour overwhelm current OV\napproaches. In this paper, we propose yet another approach to performing OV\nusing existing ontology matching (OM) techniques and systems. We introduce a\nunified OM4OV pipeline. From an OM perspective, we reconstruct a new task\nformulation, measurement, and testbed for OV tasks. Reusing the prior\nalignment(s) from OM, we propose a pipeline optimisation method called\ncross-reference (CR) mechanism to improve overall OV performance. We\nexperimentally validate the OM4OV pipeline and the cross-reference mechanism in\nmodified Ontology Alignment Evaluation Initiative (OAEI) datasets. We also\ndiscuss the insights on OM used for OV tasks, where some false mappings\ndetected by OV systems are not actually false.\n","authors":["Zhangcheng Qiang","Kerry Taylor","Weiqing Wang"],"pdf_url":"https://arxiv.org/pdf/2409.20302v2.pdf","comment":"9 pages, 6 figures, 1 table"},{"id":"http://arxiv.org/abs/2407.14985v3","updated":"2024-11-24T23:25:33Z","published":"2024-07-20T21:24:40Z","title":"Generalization v.s. Memorization: Tracing Language Models' Capabilities\n  Back to Pretraining Data","summary":"  The impressive capabilities of large language models (LLMs) have sparked\ndebate over whether these models genuinely generalize to unseen tasks or\npredominantly rely on memorizing vast amounts of pretraining data. To explore\nthis issue, we introduce an extended concept of memorization, distributional\nmemorization, which measures the correlation between the LLM output\nprobabilities and the pretraining data frequency. To effectively capture\ntask-specific pretraining data frequency, we propose a novel task-gram language\nmodel, which is built by counting the co-occurrence of semantically related\n$n$-gram pairs from task inputs and outputs in the pretraining corpus. Using\nthe Pythia models trained on the Pile dataset, we evaluate four distinct tasks:\nmachine translation, factual question answering, world knowledge understanding,\nand math reasoning. Our findings reveal varying levels of memorization, with\nthe strongest effect observed in factual question answering. Furthermore, while\nmodel performance improves across all tasks as LLM size increases, only factual\nquestion answering shows an increase in memorization, whereas machine\ntranslation and reasoning tasks exhibit greater generalization, producing more\nnovel outputs. This study demonstrates that memorization plays a larger role in\nsimpler, knowledge-intensive tasks, while generalization is the key for harder,\nreasoning-based tasks, providing a scalable method for analyzing large\npretraining corpora in greater depth.\n","authors":["Xinyi Wang","Antonis Antoniades","Yanai Elazar","Alfonso Amayuelas","Alon Albalak","Kexun Zhang","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2407.14985v3.pdf","comment":"updated 10-page version"},{"id":"http://arxiv.org/abs/2411.16002v1","updated":"2024-11-24T22:48:44Z","published":"2024-11-24T22:48:44Z","title":"Exploring Performance Contrasts in TableQA: Step-by-Step Reasoning\n  Boosts Bigger Language Models, Limits Smaller Language Models","summary":"  This paper proposes a detailed prompting flow, termed Table-Logic, to\ninvestigate the performance contrasts between bigger and smaller language\nmodels (LMs) utilizing step-by-step reasoning methods in the TableQA task. The\nmethod processes tasks by sequentially identifying critical columns and rows\ngiven question and table with its structure, determining necessary\naggregations, calculations, or comparisons, and finally inferring the results\nto generate a precise prediction. By deploying this method, we observe a 7.8%\naccuracy improvement in bigger LMs like Llama-3-70B compared to the vanilla on\nHybridQA, while smaller LMs like Llama-2-7B shows an 11% performance decline.\nWe empirically investigate the potential causes of performance contrasts by\nexploring the capabilities of bigger and smaller LMs from various dimensions in\nTableQA task. Our findings highlight the limitations of the step-by-step\nreasoning method in small models and provide potential insights for making\nimprovements.\n","authors":["Haoyan Yang","Yixuan Wang","Keyue Tong","Hongjin Zhu","Yuanxin Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.16002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15999v1","updated":"2024-11-24T22:37:59Z","published":"2024-11-24T22:37:59Z","title":"Multi-ToM: Evaluating Multilingual Theory of Mind Capabilities in Large\n  Language Models","summary":"  Theory of Mind (ToM) refers to the cognitive ability to infer and attribute\nmental states to oneself and others. As large language models (LLMs) are\nincreasingly evaluated for social and cognitive capabilities, it remains\nunclear to what extent these models demonstrate ToM across diverse languages\nand cultural contexts. In this paper, we introduce a comprehensive study of\nmultilingual ToM capabilities aimed at addressing this gap. Our approach\nincludes two key components: (1) We translate existing ToM datasets into\nmultiple languages, effectively creating a multilingual ToM dataset and (2) We\nenrich these translations with culturally specific elements to reflect the\nsocial and cognitive scenarios relevant to diverse populations. We conduct\nextensive evaluations of six state-of-the-art LLMs to measure their ToM\nperformance across both the translated and culturally adapted datasets. The\nresults highlight the influence of linguistic and cultural diversity on the\nmodels' ability to exhibit ToM, and questions their social reasoning\ncapabilities. This work lays the groundwork for future research into enhancing\nLLMs' cross-cultural social cognition and contributes to the development of\nmore culturally aware and socially intelligent AI systems. All our data and\ncode are publicly available.\n","authors":["Jayanta Sadhu","Ayan Antik Khan","Noshin Nawal","Sanju Basak","Abhik Bhattacharjee","Rifat Shahriyar"],"pdf_url":"https://arxiv.org/pdf/2411.15999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15993v1","updated":"2024-11-24T22:06:26Z","published":"2024-11-24T22:06:26Z","title":"Investigating Factuality in Long-Form Text Generation: The Roles of\n  Self-Known and Self-Unknown","summary":"  Large language models (LLMs) have demonstrated strong capabilities in text\nunderstanding and generation. However, they often lack factuality, producing a\nmixture of true and false information, especially in long-form generation. In\nthis work, we investigates the factuality of long-form text generation across\nvarious large language models (LLMs), including GPT-4, Gemini-1.5-Pro,\nClaude-3-Opus, Llama-3-70B, and Mistral. Our analysis reveals that factuality\nscores tend to decline in later sentences of the generated text, accompanied by\na rise in the number of unsupported claims. Furthermore, we explore the\neffectiveness of different evaluation settings to assess whether LLMs can\naccurately judge the correctness of their own outputs: Self-Known (the\npercentage of supported atomic claims, decomposed from LLM outputs, that the\ncorresponding LLMs judge as correct) and Self-Unknown (the percentage of\nunsupported atomic claims that the corresponding LLMs judge as incorrect). The\nresults indicate that even advanced models like GPT-4 and Gemini-1.5-Pro fail\nto achieve perfect Self-Known scores, while their Self-Unknown scores remain\nnotably above zero, reflecting ongoing uncertainty in their self-assessments.\nMoreover, we find a correlation between higher Self-Known scores and improved\nfactuality, while higher Self-Unknown scores are associated with lower\nfactuality. Interestingly, even without significant changes in the models'\nself-judgment (Self-Known and Self-Unknown), the number of unsupported claims\ncan increases, likely as an artifact of long-form generation. These findings\nshow the limitations of current LLMs in long-form generation, and provide\nvaluable insights for improving factuality in long-form text generation.\n","authors":["Lifu Tu","Rui Meng","Shafiq Joty","Yingbo Zhou","Semih Yavuz"],"pdf_url":"https://arxiv.org/pdf/2411.15993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17011v3","updated":"2024-11-24T21:49:20Z","published":"2024-09-25T15:15:57Z","title":"AutoLLM-CARD: Towards a Description and Landscape of Large Language\n  Models","summary":"  With the rapid growth of the Natural Language Processing (NLP) field, a vast\nvariety of Large Language Models (LLMs) continue to emerge for diverse NLP\ntasks. As more papers are published, researchers and developers face the\nchallenge of information overload. Thus, developing a system that can\nautomatically extract and organise key information about LLMs from academic\npapers is particularly important. The standard format for documenting\ninformation about LLMs is the LLM model card (\\textbf{LLM-Card}). We propose a\nmethod for automatically generating LLM model cards from scientific\npublications. We use Named Entity Recognition (\\textbf{NER}) and Relation\nExtraction (\\textbf{RE}) methods that automatically extract key information\nabout LLMs from the papers, helping researchers to access information about\nLLMs efficiently. These features include model \\textit{licence}, model\n\\textit{name}, and model \\textit{application}. With these features, we can form\na model card for each paper. We processed 106 academic papers by defining three\ndictionaries -- LLM's name, licence, and application. 11,051 sentences were\nextracted through dictionary lookup, and the dataset was constructed through\nmanual review of the final selection of 129 sentences with a link between the\nname and the \\textit{licence}, and 106 sentences with a link between the model\nname and the \\textit{application}. The resulting resource is relevant for LLM\ncard illustrations using relational knowledge graphs. Our code and findings can\ncontribute to automatic LLM card generation. Data and code in\n\\textsc{autoLLM-Card} will be shared and freely available at\n\\url{https://github.com/shengwei-tian/dependency-parser-visualization}\n","authors":["Shengwei Tian","Lifeng Han","Goran Nenadic"],"pdf_url":"https://arxiv.org/pdf/2409.17011v3.pdf","comment":"ongoing work, technical report"},{"id":"http://arxiv.org/abs/2411.15979v1","updated":"2024-11-24T20:44:27Z","published":"2024-11-24T20:44:27Z","title":"Kleene algebra with commutativity conditions is undecidable","summary":"  We prove that the equational theory of Kleene algebra with commutativity\n  conditions on primitives (or atomic terms) is undecidable, thereby settling a\n  longstanding open question in the theory of Kleene algebra. While this\n  question has also been recently solved independently by Kuznetsov, our\nresults\n  hold even for weaker theories that do not support the induction axioms\n  of Kleene algebra.\n","authors":["Arthur Azevedo de Amorim","Cheng Zhang","Marco Gaboardi"],"pdf_url":"https://arxiv.org/pdf/2411.15979v1.pdf","comment":"Published at CSL 2025"},{"id":"http://arxiv.org/abs/2402.01109v6","updated":"2024-11-24T20:09:55Z","published":"2024-02-02T02:56:50Z","title":"Vaccine: Perturbation-aware Alignment for Large Language Models against\n  Harmful Fine-tuning Attack","summary":"  The new paradigm of finetuning-as-a-service introduces a new attack surface\nfor Large Language Models (LLMs): a few harmful data uploaded by users can\neasily trick the finetuning to produce an alignment-broken model. We conduct an\nempirical analysis and uncover a \\textit{harmful embedding drift} phenomenon,\nshowing a probable cause of the alignment-broken effect. Inspired by our\nfindings, we propose Vaccine, a perturbation-aware alignment technique to\nmitigate the security risk of users finetuning. The core idea of Vaccine is to\nproduce invariant hidden embeddings by progressively adding crafted\nperturbation to them in the alignment phase. This enables the embeddings to\nwithstand harmful perturbation from un-sanitized user data in the finetuning\nphase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna)\ndemonstrate that Vaccine can boost the robustness of alignment against harmful\nprompts induced embedding drift while reserving reasoning ability towards\nbenign prompts. Our code is available at\n\\url{https://github.com/git-disl/Vaccine}.\n","authors":["Tiansheng Huang","Sihao Hu","Ling Liu"],"pdf_url":"https://arxiv.org/pdf/2402.01109v6.pdf","comment":"Rejected by ICML2024. Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2405.10313v2","updated":"2024-11-24T18:44:27Z","published":"2024-05-16T17:59:02Z","title":"How Far Are We From AGI: Are LLMs All We Need?","summary":"  The evolution of artificial intelligence (AI) has profoundly impacted human\nsociety, driving significant advancements in multiple sectors. AGI,\ndistinguished by its ability to execute diverse real-world tasks with\nefficiency and effectiveness comparable to human intelligence, reflects a\nparamount milestone in AI evolution. While existing studies have reviewed\nspecific advancements in AI and proposed potential paths to AGI, such as large\nlanguage models (LLMs), they fall short of providing a thorough exploration of\nAGI's definitions, objectives, and developmental trajectories. Unlike previous\nsurvey papers, this work goes beyond summarizing LLMs by addressing key\nquestions about our progress toward AGI and outlining the strategies essential\nfor its realization through comprehensive analysis, in-depth discussions, and\nnovel insights. We start by articulating the requisite capability frameworks\nfor AGI, integrating the internal, interface, and system dimensions. As the\nrealization of AGI requires more advanced capabilities and adherence to\nstringent constraints, we further discuss necessary AGI alignment technologies\nto harmonize these factors. Notably, we emphasize the importance of approaching\nAGI responsibly by first defining the key levels of AGI progression, followed\nby the evaluation framework that situates the status quo, and finally giving\nour roadmap of how to reach the pinnacle of AGI. Moreover, to give tangible\ninsights into the ubiquitous impact of the integration of AI, we outline\nexisting challenges and potential pathways toward AGI in multiple domains. In\nsum, serving as a pioneering exploration into the current state and future\ntrajectory of AGI, this paper aims to foster a collective comprehension and\ncatalyze broader public discussions among researchers and practitioners on AGI.\n","authors":["Tao Feng","Chuanyang Jin","Jingyu Liu","Kunlun Zhu","Haoqin Tu","Zirui Cheng","Guanyu Lin","Jiaxuan You"],"pdf_url":"https://arxiv.org/pdf/2405.10313v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12815v4","updated":"2024-11-24T18:14:20Z","published":"2023-10-19T15:12:09Z","title":"Formalizing and Benchmarking Prompt Injection Attacks and Defenses","summary":"  A prompt injection attack aims to inject malicious instruction/data into the\ninput of an LLM-Integrated Application such that it produces results as an\nattacker desires. Existing works are limited to case studies. As a result, the\nliterature lacks a systematic understanding of prompt injection attacks and\ntheir defenses. We aim to bridge the gap in this work. In particular, we\npropose a framework to formalize prompt injection attacks. Existing attacks are\nspecial cases in our framework. Moreover, based on our framework, we design a\nnew attack by combining existing ones. Using our framework, we conduct a\nsystematic evaluation on 5 prompt injection attacks and 10 defenses with 10\nLLMs and 7 tasks. Our work provides a common benchmark for quantitatively\nevaluating future prompt injection attacks and defenses. To facilitate research\non this topic, we make our platform public at\nhttps://github.com/liu00222/Open-Prompt-Injection.\n","authors":["Yupei Liu","Yuqi Jia","Runpeng Geng","Jinyuan Jia","Neil Zhenqiang Gong"],"pdf_url":"https://arxiv.org/pdf/2310.12815v4.pdf","comment":"Published in USENIX Security Symposium 2024; the model sizes for\n  closed-source models are from blog posts"},{"id":"http://arxiv.org/abs/2404.07921v3","updated":"2024-11-24T18:03:43Z","published":"2024-04-11T17:05:50Z","title":"AmpleGCG: Learning a Universal and Transferable Generative Model of\n  Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs","summary":"  As large language models (LLMs) become increasingly prevalent and integrated\ninto autonomous systems, ensuring their safety is imperative. Despite\nsignificant strides toward safety alignment, recent work\nGCG~\\citep{zou2023universal} proposes a discrete token optimization algorithm\nand selects the single suffix with the lowest loss to successfully jailbreak\naligned LLMs. In this work, we first discuss the drawbacks of solely picking\nthe suffix with the lowest loss during GCG optimization for jailbreaking and\nuncover the missed successful suffixes during the intermediate steps. Moreover,\nwe utilize those successful suffixes as training data to learn a generative\nmodel, named AmpleGCG, which captures the distribution of adversarial suffixes\ngiven a harmful query and enables the rapid generation of hundreds of suffixes\nfor any harmful queries in seconds. AmpleGCG achieves near 100\\% attack success\nrate (ASR) on two aligned LLMs (Llama-2-7B-chat and Vicuna-7B), surpassing two\nstrongest attack baselines. More interestingly, AmpleGCG also transfers\nseamlessly to attack different models, including closed-source LLMs, achieving\na 99\\% ASR on the latest GPT-3.5. To summarize, our work amplifies the impact\nof GCG by training a generative model of adversarial suffixes that is universal\nto any harmful queries and transferable from attacking open-source LLMs to\nclosed-source LLMs. In addition, it can generate 200 adversarial suffixes for\none harmful query in only 4 seconds, rendering it more challenging to defend.\n","authors":["Zeyi Liao","Huan Sun"],"pdf_url":"https://arxiv.org/pdf/2404.07921v3.pdf","comment":"Published as a conference paper at COLM 2024\n  (https://colmweb.org/index.html)"},{"id":"http://arxiv.org/abs/2411.15927v1","updated":"2024-11-24T17:32:20Z","published":"2024-11-24T17:32:20Z","title":"Generative Context Distillation","summary":"  Prompts used in recent large language model based applications are often\nfixed and lengthy, leading to significant computational overhead. To address\nthis challenge, we propose Generative Context Distillation (GCD), a lightweight\nprompt internalization method that employs a joint training approach. This\nmethod not only replicates the behavior of models with prompt inputs but also\ngenerates the content of the prompt along with reasons for why the model's\nbehavior should change accordingly. We demonstrate that our approach\neffectively internalizes complex prompts across various agent-based application\nscenarios. For effective training without interactions with the dedicated\nenvironments, we introduce a data synthesis technique that autonomously\ncollects conversational datasets by swapping the roles of the agent and\nenvironment. This method is especially useful in scenarios where only a\npredefined prompt is available without a corresponding training dataset. By\ninternalizing complex prompts, Generative Context Distillation enables\nhigh-performance and efficient inference without the need for explicit prompts.\n","authors":["Haebin Shin","Lei Ji","Yeyun Gong","Sungdong Kim","Eunbi Choi","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2411.15927v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13591v2","updated":"2024-11-24T16:39:08Z","published":"2024-11-18T05:47:12Z","title":"Improved GUI Grounding via Iterative Narrowing","summary":"  Graphical User Interface (GUI) grounding plays a crucial role in enhancing\nthe capabilities of Vision-Language Model (VLM) agents. While general VLMs,\nsuch as GPT-4V, demonstrate strong performance across various tasks, their\nproficiency in GUI grounding remains suboptimal. Recent studies have focused on\nfine-tuning these models specifically for one-shot GUI grounding, yielding\nsignificant improvements over baseline performance. We introduce a visual\nprompting framework that employs an iterative narrowing mechanism to improve\nthe performance of both general and fine-tuned models in GUI grounding by up to\n61%. For evaluation, we tested our method on a comprehensive benchmark\ncomprising various UI platforms and provided the code to reproduce our results.\n","authors":["Anthony Nguyen"],"pdf_url":"https://arxiv.org/pdf/2411.13591v2.pdf","comment":"Code available at\n  https://github.com/ant-8/GUI-Grounding-via-Iterative-Narrowing"},{"id":"http://arxiv.org/abs/2411.15888v1","updated":"2024-11-24T15:51:56Z","published":"2024-11-24T15:51:56Z","title":"Evaluating Large Language Models for Causal Modeling","summary":"  In this paper, we consider the process of transforming causal domain\nknowledge into a representation that aligns more closely with guidelines from\ncausal data science. To this end, we introduce two novel tasks related to\ndistilling causal domain knowledge into causal variables and detecting\ninteraction entities using LLMs. We have determined that contemporary LLMs are\nhelpful tools for conducting causal modeling tasks in collaboration with human\nexperts, as they can provide a wider perspective. Specifically, LLMs, such as\nGPT-4-turbo and Llama3-70b, perform better in distilling causal domain\nknowledge into causal variables compared to sparse expert models, such as\nMixtral-8x22b. On the contrary, sparse expert models such as Mixtral-8x22b\nstand out as the most effective in identifying interaction entities. Finally,\nwe highlight the dependency between the domain where the entities are generated\nand the performance of the chosen LLM for causal modeling.\n","authors":["Houssam Razouk","Leonie Benischke","Georg Niess","Roman Kern"],"pdf_url":"https://arxiv.org/pdf/2411.15888v1.pdf","comment":"13 pages, 6 figutrd, 4 tabels"},{"id":"http://arxiv.org/abs/2411.15862v1","updated":"2024-11-24T14:38:59Z","published":"2024-11-24T14:38:59Z","title":"LLMs Do Not Think Step-by-step In Implicit Reasoning","summary":"  It has been well-known that Chain-of-Thought can remarkably enhance LLMs'\nperformance on complex tasks. However, because it also introduces slower\ninference speeds and higher computational costs, many researches have attempted\nto use implicit CoT, which does not need LLMs to explicitly generate the\nintermediate steps. But there is still gap between their efficacy and typical\nexplicit CoT methods. This leaves us a doubt that, does implicit CoT really\nequal to explicit CoT? Therefore, in this study, we address this question\nthrough experiments. We probe the information of intermediate steps from the\nmodel's hidden states when it is performing implicit CoT. The results\nsurprisingly indicate that LLMs hardly think about intermediate steps,\nsuggesting they may just rely on experience rather than strict step-by-step\nreasoning. Moreover, we find LLMs' implicit reasoning capabilities are\nsusceptible and unstable, reaffirming the necessity of explicit CoT to\neffectively support complex tasks.\n","authors":["Yijiong Yu"],"pdf_url":"https://arxiv.org/pdf/2411.15862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16283v2","updated":"2024-11-24T14:36:20Z","published":"2024-10-05T00:13:33Z","title":"Understanding the Effect of Algorithm Transparency of Model Explanations\n  in Text-to-SQL Semantic Parsing","summary":"  Explaining the decisions of AI has become vital for fostering appropriate\nuser trust in these systems. This paper investigates explanations for a\nstructured prediction task called ``text-to-SQL Semantic Parsing'', which\ntranslates a natural language question into a structured query language (SQL)\nprogram. In this task setting, we designed three levels of model explanation,\neach exposing a different amount of the model's decision-making details (called\n``algorithm transparency''), and investigated how different model explanations\ncould potentially yield different impacts on the user experience. Our study\nwith $\\sim$100 participants shows that (1) the low-/high-transparency\nexplanations often lead to less/more user reliance on the model decisions,\nwhereas the medium-transparency explanations strike a good balance. We also\nshow that (2) only the medium-transparency participant group was able to engage\nfurther in the interaction and exhibit increasing performance over time, and\nthat (3) they showed the least changes in trust before and after the study.\n","authors":["Daking Rai","Rydia R. Weiland","Kayla Margaret Gabriella Herrera","Tyler H. Shaw","Ziyu Yao"],"pdf_url":"https://arxiv.org/pdf/2410.16283v2.pdf","comment":"15 pages, 18 figure, Preprint"},{"id":"http://arxiv.org/abs/2403.13802v3","updated":"2024-11-24T14:25:05Z","published":"2024-03-20T17:59:14Z","title":"ZigMa: A DiT-style Zigzag Mamba Diffusion Model","summary":"  The diffusion model has long been plagued by scalability and quadratic\ncomplexity issues, especially within transformer-based structures. In this\nstudy, we aim to leverage the long sequence modeling capability of a\nState-Space Model called Mamba to extend its applicability to visual data\ngeneration. Firstly, we identify a critical oversight in most current\nMamba-based vision methods, namely the lack of consideration for spatial\ncontinuity in the scan scheme of Mamba. Secondly, building upon this insight,\nwe introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba,\nwhich outperforms Mamba-based baselines and demonstrates improved speed and\nmemory utilization compared to transformer-based baselines. Lastly, we\nintegrate Zigzag Mamba with the Stochastic Interpolant framework to investigate\nthe scalability of the model on large-resolution visual datasets, such as\nFacesHQ $1024\\times 1024$ and UCF101, MultiModal-CelebA-HQ, and MS COCO\n$256\\times 256$ . Code will be released at https://taohu.me/zigma/\n","authors":["Vincent Tao Hu","Stefan Andreas Baumann","Ming Gui","Olga Grebenkova","Pingchuan Ma","Johannes Schusterbauer","Björn Ommer"],"pdf_url":"https://arxiv.org/pdf/2403.13802v3.pdf","comment":"ECCV 2024 Project Page: https://taohu.me/zigma/"},{"id":"http://arxiv.org/abs/2407.00487v3","updated":"2024-11-24T14:11:56Z","published":"2024-06-29T16:34:23Z","title":"It's Morphing Time: Unleashing the Potential of Multiple LLMs via\n  Multi-objective Optimization","summary":"  In this paper, we introduce a novel approach for addressing the\nmulti-objective optimization problem in large language model merging via\nblack-box multi-objective optimization algorithms. The goal of model merging is\nto combine multiple models, each excelling in different tasks, into a single\nmodel that outperforms any of the individual source models. However, model\nmerging faces two significant challenges: First, existing methods rely heavily\non human knowledge or intuition. Second, it's difficult to obtain the great\nmodel merging configuration in limited evaluations. To address these\nchallenges, we formalize model merging as a multi-objective optimization\nproblem and propose an automated optimization approach named MM-MO. This method\nleverages multi-objective optimization algorithms to autonomously search for\noptimal merging configurations across various tasks, alleviating the need for\nhuman intervention. In MM-MO, a weak-to-strong method is employed to enhance\nthe acquisition function, allowing previously evaluated superior configurations\nto guide the search for new ones. Meanwhile, Fisher information is applied to\nscreen these configurations, increasing the possibility of identifying\nhigh-quality merging configuration. Additionally, we designed a sparsity metric\nas an additional optimization objective to enhance the model's generalization\nperformance across different tasks. We conducted comprehensive experiments with\nother mainstream model merging methods, demonstrating that the proposed MM-MO\nalgorithm is competitive and effective in achieving high-quality model merging.\n","authors":["Bingdong Li","Zixiang Di","Yanting Yang","Hong Qian","Peng Yang","Hao Hao","Ke Tang","Aimin Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.00487v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06965v2","updated":"2024-11-24T13:43:37Z","published":"2024-10-09T15:02:34Z","title":"Uncovering Factor Level Preferences to Improve Human-Model Alignment","summary":"  Despite advancements in Large Language Model (LLM) alignment, understanding\nthe reasons behind LLM preferences remains crucial for bridging the gap between\ndesired and actual behavior. LLMs often exhibit biases or tendencies that\ndiverge from human preferences, such as favoring certain writing styles or\nproducing overly verbose outputs. However, current methods for evaluating\npreference alignment often lack explainability, relying on coarse-grained\ncomparisons. To address this, we introduce PROFILE (PRObing Factors of\nInfLuence for Explainability), a novel framework that uncovers and quantifies\nthe influence of specific factors driving preferences. PROFILE's factor level\nanalysis explains the 'why' behind human-model alignment and misalignment,\noffering insights into the direction of model improvement. We apply PROFILE to\nanalyze human and LLM preferences across three tasks: summarization, helpful\nresponse generation, and document-based question-answering. Our factor level\nanalysis reveals a substantial discrepancy between human and LLM preferences in\ngeneration tasks, whereas LLMs show strong alignment with human preferences in\nevaluation tasks. We demonstrate how leveraging factor level insights,\nincluding addressing misaligned factors or exploiting the generation-evaluation\ngap, can improve alignment with human preferences. This work underscores the\nimportance of explainable preference analysis and highlights PROFILE's\npotential to provide valuable training signals, driving further improvements in\nhuman-model alignment.\n","authors":["Juhyun Oh","Eunsu Kim","Jiseon Kim","Wenda Xu","Inha Cha","William Yang Wang","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2410.06965v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11910v2","updated":"2024-11-24T12:59:44Z","published":"2024-11-17T13:40:35Z","title":"AIGS: Generating Science from AI-Powered Automated Falsification","summary":"  Rapid development of artificial intelligence has drastically accelerated the\ndevelopment of scientific discovery. Trained with large-scale observation data,\ndeep neural networks extract the underlying patterns in an end-to-end manner\nand assist human researchers with highly-precised predictions in unseen\nscenarios. The recent rise of Large Language Models (LLMs) and the empowered\nautonomous agents enable scientists to gain help through interaction in\ndifferent stages of their research, including but not limited to literature\nreview, research ideation, idea implementation, and academic writing. However,\nAI researchers instantiated by foundation model empowered agents with\nfull-process autonomy are still in their infancy. In this paper, we study\n$\\textbf{AI-Generated Science}$ (AIGS), where agents independently and\nautonomously complete the entire research process and discover scientific laws.\nBy revisiting the definition of scientific research, we argue that\n$\\textit{falsification}$ is the essence of both human research process and the\ndesign of an AIGS system. Through the lens of falsification, prior systems\nattempting towards AI-Generated Science either lack the part in their design,\nor rely heavily on existing verification engines that narrow the use in\nspecialized domains. In this work, we propose Baby-AIGS as a baby-step\ndemonstration of a full-process AIGS system, which is a multi-agent system with\nagents in roles representing key research process. By introducing\nFalsificationAgent, which identify and then verify possible scientific\ndiscoveries, we empower the system with explicit falsification. Experiments on\nthree tasks preliminarily show that Baby-AIGS could produce meaningful\nscientific discoveries, though not on par with experienced human researchers.\nFinally, we discuss on the limitations of current Baby-AIGS, actionable\ninsights, and related ethical issues in detail.\n","authors":["Zijun Liu","Kaiming Liu","Yiqi Zhu","Xuanyu Lei","Zonghan Yang","Zhenhe Zhang","Peng Li","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2411.11910v2.pdf","comment":"Pre-print. 35 pages. Official website:\n  https://agent-force.github.io/AIGS/"},{"id":"http://arxiv.org/abs/2411.15821v1","updated":"2024-11-24T12:51:50Z","published":"2024-11-24T12:51:50Z","title":"Is Training Data Quality or Quantity More Impactful to Small Language\n  Model Performance?","summary":"  This study investigates the relative impact of training data quality versus\nquantity on the performance of small language models (SLMs), utilizing the\nTinyStories dataset for empirical analysis. Analysis of dataset variations with\nrespect to size (25% and 50% of the original size) and duplication (controlled\nrates of 25%, 50%, 75%, and 100%) were performed. Model performance was\nevaluated based on the validation loss, accuracy, and perplexity metrics.\nResults indicate training data quality plays a more significant role in the\noverall performance of SLMs, especially given scale of this experiment. Minimal\nduplication positively impacted model accuracy (+0.87% increase in accuracy at\n25% duplication) without significantly increasing perplexity (+0.52% increase\ngoing from 0% to 25% duplication) but excessive duplication led to pronounced\nperformance degradation (-40% drop in accuracy at 100% duplication). The\nimplications of this exploration extend beyond just model performance; training\nlarge-scale models imposes significant financial and computational burdens,\nwhich can be prohibitive for organizations, individuals, and the public at\nlarge, especially in developing countries. Additionally, the energy consumption\nassociated with large-scale training raises environmental concerns.\nUnderstanding the relative importance of data quality versus quantity could\ndemocratize AI technology, making advanced models more accessible and\nsustainable for all.\n","authors":["Aryan Sajith","Krishna Chaitanya Rao Kathala"],"pdf_url":"https://arxiv.org/pdf/2411.15821v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2411.15804v1","updated":"2024-11-24T12:21:14Z","published":"2024-11-24T12:21:14Z","title":"LoRA-Mini : Adaptation Matrices Decomposition and Selective Training","summary":"  The rapid advancements in large language models (LLMs) have revolutionized\nnatural language processing, creating an increased need for efficient,\ntask-specific fine-tuning methods. Traditional fine-tuning of LLMs involves\nupdating a large number of parameters, which is computationally expensive and\nmemory-intensive. Low-Rank Adaptation (LoRA) has emerged as a promising\nsolution, enabling parameter-efficient fine-tuning by reducing the number of\ntrainable parameters. However, while LoRA reduces the number of trainable\nparameters, LoRA modules still create significant storage challenges. We\npropose LoRA-Mini, an optimized adaptation of LoRA that improves parameter\nefficiency by splitting low-rank matrices into four parts, with only the two\ninner matrices being trainable. This approach achieves upto a 20x reduction\ncompared to standard LoRA in the number of trainable parameters while\npreserving performance levels comparable to standard LoRA, addressing both\ncomputational and storage efficiency in LLM fine-tuning.\n","authors":["Ayush Singh","Rajdeep Aher","Shivank Garg"],"pdf_url":"https://arxiv.org/pdf/2411.15804v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2411.12493v2","updated":"2024-11-24T12:12:08Z","published":"2024-11-19T13:23:53Z","title":"Bias-Free Sentiment Analysis through Semantic Blinding and Graph Neural\n  Networks","summary":"  This paper introduces the Semantic Propagation Graph Neural Network (SProp\nGNN), a machine learning sentiment analysis (SA) architecture that relies\nexclusively on syntactic structures and word-level emotional cues to predict\nemotions in text. By semantically blinding the model to information about\nspecific words, it is robust to biases such as political or gender bias that\nhave been plaguing previous machine learning-based SA systems. The SProp GNN\nshows performance superior to lexicon-based alternatives such as VADER and\nEmoAtlas on two different prediction tasks, and across two languages.\nAdditionally, it approaches the accuracy of transformer-based models while\nsignificantly reducing bias in emotion prediction tasks. By offering improved\nexplainability and reducing bias, the SProp GNN bridges the methodological gap\nbetween interpretable lexicon approaches and powerful, yet often opaque, deep\nlearning models, offering a robust tool for fair and effective emotion analysis\nin understanding human behavior through text.\n","authors":["Hubert Plisiecki"],"pdf_url":"https://arxiv.org/pdf/2411.12493v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15785v1","updated":"2024-11-24T11:30:00Z","published":"2024-11-24T11:30:00Z","title":"A Method for Building Large Language Models with Predefined KV Cache\n  Capacity","summary":"  This paper proposes a method for building large language models with\npredefined Key-Value (KV) cache capacity, particularly suitable for the\nattention layers in Transformer decode-only architectures. This method\nintroduces fixed-length KV caches to address the issue of excessive memory\nconsumption in traditional KV caches when handling infinite contexts. By\ndynamically updating the key-value vector sequences, it achieves efficient\ninference within limited cache capacity, significantly reducing memory usage\nwhile maintaining model performance and system throughput. Experimental results\nshow that this method significantly reduces memory usage while maintaining the\nmodel's inference quality.\n","authors":["Zhonghua Yi","Ge Niu","Lei Wang","Wei Tang","Liqiu Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.15785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01469v2","updated":"2024-11-24T09:59:23Z","published":"2023-10-30T21:41:49Z","title":"Leveraging Language Models to Detect Greenwashing","summary":"  In recent years, climate change repercussions have increasingly captured\npublic interest. Consequently, corporations are emphasizing their environmental\nefforts in sustainability reports to bolster their public image. Yet, the\nabsence of stringent regulations in review of such reports allows potential\ngreenwashing. In this study, we introduce a novel preliminary methodology to\ntrain a language model on generated labels for greenwashing risk. Our primary\ncontributions encompass: developing a preliminary mathematical formulation to\nquantify greenwashing risk, a fine-tuned ClimateBERT model for this problem,\nand a comparative analysis of results. On a test set comprising of\nsustainability reports, our best model achieved an average accuracy score of\n86.34% and F1 score of 0.67, demonstrating that our proof-of-concept\nmethodology shows a promising direction of exploration for this task.\n","authors":["Avalon Vinella","Margaret Capetz","Rebecca Pattichis","Christina Chance","Reshmi Ghosh","Kai-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2311.01469v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04541v2","updated":"2024-11-24T09:40:34Z","published":"2024-07-05T14:28:12Z","title":"PoPreRo: A New Dataset for Popularity Prediction of Romanian Reddit\n  Posts","summary":"  We introduce PoPreRo, the first dataset for Popularity Prediction of Romanian\nposts collected from Reddit. The PoPreRo dataset includes a varied compilation\nof post samples from five distinct subreddits of Romania, totaling 28,107 data\nsamples. Along with our novel dataset, we introduce a set of competitive models\nto be used as baselines for future research. Interestingly, the top-scoring\nmodel achieves an accuracy of 61.35% and a macro F1 score of 60.60% on the test\nset, indicating that the popularity prediction task on PoPreRo is very\nchallenging. Further investigations based on few-shot prompting the Falcon-7B\nLarge Language Model also point in the same direction. We thus believe that\nPoPreRo is a valuable resource that can be used to evaluate models on\npredicting the popularity of social media posts in Romanian. We release our\ndataset at https://github.com/ana-rogoz/PoPreRo.\n","authors":["Ana-Cristina Rogoz","Maria Ilinca Nechita","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2407.04541v2.pdf","comment":"Accepted at ICPR 2024"},{"id":"http://arxiv.org/abs/2411.15768v1","updated":"2024-11-24T09:31:38Z","published":"2024-11-24T09:31:38Z","title":"Detecting Turkish Synonyms Used in Different Time Periods","summary":"  Dynamic structure of languages poses significant challenges in applying\nnatural language processing models on historical texts, causing decreased\nperformance in various downstream tasks. Turkish is a prominent example of\nrapid linguistic transformation due to the language reform in the 20th century.\nIn this paper, we propose two methods for detecting synonyms used in different\ntime periods, focusing on Turkish. In our first method, we use Orthogonal\nProcrustes method to align the embedding spaces created using documents written\nin the corresponding time periods. In our second method, we extend the first\none by incorporating Spearman's correlation between frequencies of words\nthroughout the years. In our experiments, we show that our proposed methods\noutperform the baseline method. Furthermore, we observe that the efficacy of\nour methods remains consistent when the target time period shifts from the\n1960s to the 1980s. However, their performance slightly decreases for\nsubsequent time periods.\n","authors":["Umur Togay Yazar","Mucahid Kutlu"],"pdf_url":"https://arxiv.org/pdf/2411.15768v1.pdf","comment":"published at Innovations in Intelligent Systems and Applications\n  Conference (Ak{\\i}ll{\\i} Sistemlerde Yenilikler ve Uygulamalar{\\i}\n  Konferans{\\i} - ASYU) 2024"},{"id":"http://arxiv.org/abs/2405.11143v4","updated":"2024-11-24T08:34:48Z","published":"2024-05-20T01:04:40Z","title":"OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework","summary":"  As large language models (LLMs) continue to grow by scaling laws,\nreinforcement learning from human feedback (RLHF) has gained significant\nattention due to its outstanding performance. However, unlike pretraining or\nfine-tuning a single model, scaling reinforcement learning from human feedback\n(RLHF) for training large language models poses coordination challenges across\nfour models. We present OpenRLHF, an open-source framework enabling efficient\nRLHF scaling. Unlike existing RLHF frameworks that co-locate four models on the\nsame GPUs, OpenRLHF re-designs scheduling for the models beyond 70B parameters\nusing Ray, vLLM, and DeepSpeed, leveraging improved resource utilization and\ndiverse training approaches. Integrating seamlessly with Hugging Face, OpenRLHF\nprovides an out-of-the-box solution with optimized algorithms and launch\nscripts, which ensures user-friendliness. OpenRLHF implements RLHF, DPO,\nrejection sampling, and other alignment techniques. Empowering state-of-the-art\nLLM development, OpenRLHF's code is available at\n\\url{https://github.com/OpenRLHF/OpenRLHF}.\n","authors":["Jian Hu","Xibin Wu","Zilin Zhu"," Xianyu","Weixun Wang","Dehao Zhang","Yu Cao"],"pdf_url":"https://arxiv.org/pdf/2405.11143v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11266v2","updated":"2024-11-24T08:29:00Z","published":"2024-11-18T03:45:34Z","title":"VersaTune: Harnessing Vertical Domain Insights for Multi-Ability LLM\n  Supervised Fine-Tuning","summary":"  Large Language Models (LLMs) exhibit remarkable capabilities in handling\nmultiple tasks across domains due to their emergent properties. These\ncapabilities are further augmented during the Supervised Fine-Tuning (SFT)\nphase. Despite their potential, existing work mainly focuses on domain-specific\nenhancements during fine-tuning, the challenge of which lies in catastrophic\nforgetting of knowledge across other domains. In this study, we introduce\nVersaTune, a novel data composition framework designed for enhancing LLMs'\noverall multi-ability performances during fine-tuning. We categorize knowledge\ninto distinct domains including law, medicine, finance, science, code. We begin\nwith detecting the distribution of domain-specific knowledge within the base\nmodel, followed by the composition of training data that aligns with the\nmodel's existing knowledge distribution. During the fine-tuning process,\nweights of different domains are dynamically adjusted based on their learnable\npotential and forgetting degree. Experimental results demonstrate that\nVersaTune achieves significant improvements in multi-domain performance, with a\n35.21% enhancement in comprehensive multi-domain tasks. Additionally, in\nscenarios where specific domain optimization is required, VersaTune reduces the\ndegradation of performance in other domains by 38.77%, without compromising the\ntarget domain's training efficacy.\n","authors":["Keer Lu","Keshi Zhao","Zheng Liang","Da Pan","Shusen Zhang","Xin Wu","Weipeng Chen","Zenan Zhou","Guosheng Dong","Bin Cui","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.11266v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15737v1","updated":"2024-11-24T07:02:32Z","published":"2024-11-24T07:02:32Z","title":"TableTime: Reformulating Time Series Classification as Zero-Shot Table\n  Understanding via Large Language Models","summary":"  Large language models (LLMs) have demonstrated their effectiveness in\nmultivariate time series classification (MTSC). Effective adaptation of LLMs\nfor MTSC necessitates informative data representations. Existing LLM-based\nmethods directly encode embeddings for time series within the latent space of\nLLMs from scratch to align with semantic space of LLMs. Despite their\neffectiveness, we reveal that these methods conceal three inherent bottlenecks:\n(1) they struggle to encode temporal and channel-specific information in a\nlossless manner, both of which are critical components of multivariate time\nseries; (2) it is much difficult to align the learned representation space with\nthe semantic space of the LLMs; (3) they require task-specific retraining,\nwhich is both computationally expensive and labor-intensive. To bridge these\ngaps, we propose TableTime, which reformulates MTSC as a table understanding\ntask. Specifically, TableTime introduces the following strategies: (1) convert\nmultivariate time series into a tabular form, thus minimizing information loss\nto the greatest extent; (2) represent tabular time series in text format to\nachieve natural alignment with the semantic space of LLMs; (3) design a\nreasoning framework that integrates contextual text information, neighborhood\nassistance, multi-path inference and problem decomposition to enhance the\nreasoning ability of LLMs and realize zero-shot classification. Extensive\nexperiments performed on 10 publicly representative datasets from UEA archive\nverify the superiorities of the TableTime.\n","authors":["Jiahao Wang","Mingyue Cheng","Qingyang Mao","Qi Liu","Feiyang Xu","Xin Li","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2411.15737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.03581v2","updated":"2024-11-24T06:55:18Z","published":"2023-08-07T13:37:05Z","title":"Towards Controllable Natural Language Inference through Lexical\n  Inference Types","summary":"  Explainable natural language inference aims to provide a mechanism to produce\nexplanatory (abductive) inference chains which ground claims to their\nsupporting premises. A recent corpus called EntailmentBank strives to advance\nthis task by explaining the answer to a question using an entailment tree\n\\cite{dalvi2021explaining}. They employ the T5 model to directly generate the\ntree, which can explain how the answer is inferred. However, it lacks the\nability to explain and control the generation of intermediate steps, which is\ncrucial for the multi-hop inference process. % One recent corpus,\nEntailmentBank, aims to push this task forward by explaining an answer to a\nquestion according to an entailment tree \\cite{dalvi2021explaining}. They\nemploy T5 to generate the tree directly, which can explain how the answer is\ninferred but cannot explain how the intermediate is generated, which is\nessential to the multi-hop inference process. In this work, we focus on\nproposing a controlled natural language inference architecture for\nmulti-premise explanatory inference. To improve control and enable explanatory\nanalysis over the generation, we define lexical inference types based on\nAbstract Meaning Representation (AMR) graph and modify the architecture of T5\nto learn a latent sentence representation (T5 bottleneck) conditioned on said\ntype information. We also deliver a dataset of approximately 5000 annotated\nexplanatory inference steps, with well-grounded lexical-symbolic operations.\nExperimental results indicate that the inference typing induced at the T5\nbottleneck can help T5 to generate a conclusion under explicit control.\n","authors":["Yingji Zhang","Danilo S. Carvalho","Ian Pratt-Hartmann","Andre Freitas"],"pdf_url":"https://arxiv.org/pdf/2308.03581v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.13065v2","updated":"2024-11-24T06:39:21Z","published":"2023-09-15T03:09:43Z","title":"Personality Profiling: How informative are social media profiles in\n  predicting personal information?","summary":"  Personality profiling has been utilised by companies for targeted\nadvertising, political campaigns and public health campaigns. However, the\naccuracy and versatility of such models remains relatively unknown. Here we\nexplore the extent to which peoples' online digital footprints can be used to\nprofile their Myers-Briggs personality type. We analyse and compare four\nmodels: logistic regression, naive Bayes, support vector machines (SVMs) and\nrandom forests. We discover that a SVM model achieves the best accuracy of\n20.95% for predicting a complete personality type. However, logistic regression\nmodels perform only marginally worse and are significantly faster to train and\nperform predictions. Moreover, we develop a statistical framework for assessing\nthe importance of different sets of features in our models. We discover some\nfeatures to be more informative than others in the Intuitive/Sensory (p =\n0.032) and Thinking/Feeling (p = 0.019) models. Many labelled datasets present\nsubstantial class imbalances of personal characteristics on social media,\nincluding our own. We therefore highlight the need for attentive consideration\nwhen reporting model performance on such datasets and compare a number of\nmethods to fix class-imbalance problems.\n","authors":["Joshua Watt","Lewis Mitchell","Jonathan Tuke"],"pdf_url":"https://arxiv.org/pdf/2309.13065v2.pdf","comment":"11 pages, 6 figures. Dataset available at\n  https://figshare.com/articles/dataset/Self-Reported_Myers-Briggs_Personality_Types_on_Twitter/23620554"},{"id":"http://arxiv.org/abs/2410.17196v2","updated":"2024-11-24T06:38:29Z","published":"2024-10-22T17:15:20Z","title":"VoiceBench: Benchmarking LLM-Based Voice Assistants","summary":"  Building on the success of large language models (LLMs), recent advancements\nsuch as GPT-4o have enabled real-time speech interactions through LLM-based\nvoice assistants, offering a significantly improved user experience compared to\ntraditional text-based interactions. However, the absence of benchmarks\ndesigned to evaluate these speech interaction capabilities has hindered\nprogress of LLM-based voice assistants development. Current evaluations focus\nprimarily on automatic speech recognition (ASR) or general knowledge evaluation\nwith clean speeches, neglecting the more intricate, real-world scenarios that\ninvolve diverse speaker characteristics, environmental and content factors. To\naddress this, we introduce VoiceBench, the first benchmark designed to provide\na multi-faceted evaluation of LLM-based voice assistants. VoiceBench also\nincludes both real and synthetic spoken instructions that incorporate the above\nthree key real-world variations. Extensive experiments reveal the limitations\nof current LLM-based voice assistant models and offer valuable insights for\nfuture research and development in this field.\n","authors":["Yiming Chen","Xianghu Yue","Chen Zhang","Xiaoxue Gao","Robby T. Tan","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2410.17196v2.pdf","comment":"Work in progress. Data is available at\n  https://github.com/MatthewCYM/VoiceBench"},{"id":"http://arxiv.org/abs/2411.15734v1","updated":"2024-11-24T06:38:24Z","published":"2024-11-24T06:38:24Z","title":"Development of Pre-Trained Transformer-based Models for the Nepali\n  Language","summary":"  Transformer-based pre-trained language models have dominated the field of\nNatural Language Processing (NLP) for quite some time now. However, the Nepali\nlanguage, spoken by approximately 32 million people worldwide, remains\nsignificantly underrepresented in this domain. This underrepresentation is\nprimarily attributed to the scarcity of monolingual data corpora and limited\navailable resources for the Nepali language. While existing efforts have\npredominantly concentrated on basic encoder-based models, there is a notable\ngap in the exploration of decoder-based architectures. To address this gap, we\nhave collected 27.5 GB of Nepali text data, approximately 2.4x larger than any\npreviously available Nepali language corpus. Leveraging this data, we\npre-trained three different models i.e., BERT, RoBERTa, and GPT-2, exclusively\nfor the Nepali Language. Furthermore, we performed instruction tuning and\nexplored its potential for monolingual Nepali data, providing a foundation for\nfuture research. Our models outperformed the existing best model by 2 points on\nNep-gLUE benchmark, scoring 95.60 and also outperformed existing models on text\ngeneration tasks, demonstrating improvements in both understanding and\ngenerating Nepali text.\n","authors":["Prajwal Thapa","Jinu Nyachhyon","Mridul Sharma","Bal Krishna Bal"],"pdf_url":"https://arxiv.org/pdf/2411.15734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06927v4","updated":"2024-11-24T06:31:59Z","published":"2024-09-11T00:56:02Z","title":"Representation Tuning","summary":"  Activation engineering is becoming increasingly popular as a means of online\ncontrol of large language models (LLMs). In this work, we extend the idea of\ninference-time steering with vectors that represent a behavioral direction of\ninterest to tuning those vectors directly into the model, obviating the need\nfor online control. First, we identify activation vectors related to honesty in\nan open-source LLM (Llama-2-13b-chat). Next, we demonstrate that model output\ncan be made more or less honest by adding positive or negative multiples of\nthese vectors to residual stream activations during generation. Then, we show\nthat a similar effect can be achieved by fine-tuning the vectors directly into\nthe model, by use of a dual loss function based on the cosine similarity of\nresidual stream activations to the vectors combined with a standard token-based\nloss (\"representation tuning\"). Finally, we compare the generations in response\nto honesty-probing prompts from the resulting models to those from models\nfine-tuned with a token-based loss alone, and to those from the untuned model\nsubjected to online steering. Overall, fine-tuning the vectors into the models\nusing the cosine similarity plus token loss showed a stronger effect than\nonline steering, and generalized better than using the standard loss,\nsuggesting the potential utility of this approach as a safety measure. Code and\ndata are available at https://github.com/cma1114/representation_tuning. Tuned\nmodels are available at\nhttps://huggingface.co/collections/cackerman/representation-tuning-66da1e5ab41cd1b824687d9f.\n","authors":["Christopher M. Ackerman"],"pdf_url":"https://arxiv.org/pdf/2409.06927v4.pdf","comment":"10 pages, 7 figures, 6 tables"},{"id":"http://arxiv.org/abs/2404.03027v4","updated":"2024-11-24T06:22:37Z","published":"2024-04-03T19:23:18Z","title":"JailBreakV: A Benchmark for Assessing the Robustness of MultiModal Large\n  Language Models against Jailbreak Attacks","summary":"  With the rapid advancements in Multimodal Large Language Models (MLLMs),\nsecuring these models against malicious inputs while aligning them with human\nvalues has emerged as a critical challenge. In this paper, we investigate an\nimportant and unexplored question of whether techniques that successfully\njailbreak Large Language Models (LLMs) can be equally effective in jailbreaking\nMLLMs. To explore this issue, we introduce JailBreakV-28K, a pioneering\nbenchmark designed to assess the transferability of LLM jailbreak techniques to\nMLLMs, thereby evaluating the robustness of MLLMs against diverse jailbreak\nattacks. Utilizing a dataset of 2, 000 malicious queries that is also proposed\nin this paper, we generate 20, 000 text-based jailbreak prompts using advanced\njailbreak attacks on LLMs, alongside 8, 000 image-based jailbreak inputs from\nrecent MLLMs jailbreak attacks, our comprehensive dataset includes 28, 000 test\ncases across a spectrum of adversarial scenarios. Our evaluation of 10\nopen-source MLLMs reveals a notably high Attack Success Rate (ASR) for attacks\ntransferred from LLMs, highlighting a critical vulnerability in MLLMs that\nstems from their text-processing capabilities. Our findings underscore the\nurgent need for future research to address alignment vulnerabilities in MLLMs\nfrom both textual and visual inputs.\n","authors":["Weidi Luo","Siyuan Ma","Xiaogeng Liu","Xiaoyu Guo","Chaowei Xiao"],"pdf_url":"https://arxiv.org/pdf/2404.03027v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22770v2","updated":"2024-11-24T05:31:53Z","published":"2024-10-30T07:39:42Z","title":"InjecGuard: Benchmarking and Mitigating Over-defense in Prompt Injection\n  Guardrail Models","summary":"  Prompt injection attacks pose a critical threat to large language models\n(LLMs), enabling goal hijacking and data leakage. Prompt guard models, though\neffective in defense, suffer from over-defense -- falsely flagging benign\ninputs as malicious due to trigger word bias. To address this issue, we\nintroduce NotInject, an evaluation dataset that systematically measures\nover-defense across various prompt guard models. NotInject contains 339 benign\nsamples enriched with trigger words common in prompt injection attacks,\nenabling fine-grained evaluation. Our results show that state-of-the-art models\nsuffer from over-defense issues, with accuracy dropping close to random\nguessing levels (60%). To mitigate this, we propose InjecGuard, a novel prompt\nguard model that incorporates a new training strategy, Mitigating Over-defense\nfor Free (MOF), which significantly reduces the bias on trigger words.\nInjecGuard demonstrates state-of-the-art performance on diverse benchmarks\nincluding NotInject, surpassing the existing best model by 30.8%, offering a\nrobust and open-source solution for detecting prompt injection attacks. The\ncode and datasets are released at https://github.com/SaFoLab-WISC/InjecGuard.\n","authors":["Hao Li","Xiaogeng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.22770v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15708v1","updated":"2024-11-24T04:26:04Z","published":"2024-11-24T04:26:04Z","title":"LLaMA-MoE v2: Exploring Sparsity of LLaMA from Perspective of\n  Mixture-of-Experts with Post-Training","summary":"  Recently, inspired by the concept of sparsity, Mixture-of-Experts (MoE)\nmodels have gained increasing popularity for scaling model size while keeping\nthe number of activated parameters constant. In this study, we thoroughly\ninvestigate the sparsity of the dense LLaMA model by constructing MoE for both\nthe attention (i.e., Attention MoE) and MLP (i.e., MLP MoE) modules in the\ntransformer blocks. Specifically, we investigate different expert construction\nmethods and granularities under the same activation conditions to analyze the\nimpact of sparsifying the model. Additionally, to comprehensively evaluate the\nmodel's capabilities across various domains (e.g., conversation, code, math)\nafter sparsification, we apply sparsity to the instructed large language models\n(LLMs) and construct instructed MoE models. To counteract the performance\ndegradation resulting from increased sparsity, we design a two-stage\npost-training strategy to enhance model performance. Experiments on the LLaMA3\nmodel demonstrate the potential effectiveness of this approach for future\ndevelopments of instructed MoE models. The source codes and models are\navailable at: \\url{https://github.com/OpenSparseLLMs/LLaMA-MoE-v2}.\n","authors":["Xiaoye Qu","Daize Dong","Xuyang Hu","Tong Zhu","Weigao Sun","Yu Cheng"],"pdf_url":"https://arxiv.org/pdf/2411.15708v1.pdf","comment":"Technical report,13 pages"},{"id":"http://arxiv.org/abs/2411.15700v1","updated":"2024-11-24T03:56:43Z","published":"2024-11-24T03:56:43Z","title":"RAMIE: Retrieval-Augmented Multi-task Information Extraction with Large\n  Language Models on Dietary Supplements","summary":"  \\textbf{Objective:} We aimed to develop an advanced multi-task large language\nmodel (LLM) framework to extract multiple types of information about dietary\nsupplements (DS) from clinical records.\n  \\textbf{Methods:} We used four core DS information extraction tasks - namely,\nnamed entity recognition (NER: 2,949 clinical sentences), relation extraction\n(RE: 4,892 sentences), triple extraction (TE: 2,949 sentences), and usage\nclassification (UC: 2,460 sentences) as our multitasks. We introduced a novel\nRetrieval-Augmented Multi-task Information Extraction (RAMIE) Framework,\nincluding: 1) employed instruction fine-tuning techniques with task-specific\nprompts, 2) trained LLMs for multiple tasks with improved storage efficiency\nand lower training costs, and 3) incorporated retrieval augmentation generation\n(RAG) techniques by retrieving similar examples from the training set. We\ncompared RAMIE's performance to LLMs with instruction fine-tuning alone and\nconducted an ablation study to assess the contributions of multi-task learning\nand RAG to improved multitasking performance.\n  \\textbf{Results:} With the aid of the RAMIE framework, Llama2-13B achieved an\nF1 score of 87.39 (3.51\\% improvement) on the NER task and demonstrated\noutstanding performance on the RE task with an F1 score of 93.74 (1.15\\%\nimprovement). For the TE task, Llama2-7B scored 79.45 (14.26\\% improvement),\nand MedAlpaca-7B achieved the highest F1 score of 93.45 (0.94\\% improvement) on\nthe UC task. The ablation study revealed that while MTL increased efficiency\nwith a slight trade-off in performance, RAG significantly boosted overall\naccuracy.\n  \\textbf{Conclusion:} This study presents a novel RAMIE framework that\ndemonstrates substantial improvements in multi-task information extraction for\nDS-related data from clinical records. Our framework can potentially be applied\nto other domains.\n","authors":["Zaifu Zhan","Shuang Zhou","Mingchen Li","Rui Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.15700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19716v2","updated":"2024-11-24T03:47:38Z","published":"2024-05-30T05:53:49Z","title":"Enhancing Large Vision Language Models with Self-Training on Image\n  Comprehension","summary":"  Large vision language models (LVLMs) integrate large language models (LLMs)\nwith pre-trained vision encoders, thereby activating the perception capability\nof the model to understand image inputs for different queries and conduct\nsubsequent reasoning. Improving this capability requires high-quality\nvision-language data, which is costly and labor-intensive to acquire.\nSelf-training approaches have been effective in single-modal settings to\nalleviate the need for labeled data by leveraging model's own generation.\nHowever, effective self-training remains a challenge regarding the unique\nvisual perception and reasoning capability of LVLMs. To address this, we\nintroduce Self-Training on Image Comprehension (STIC), which emphasizes a\nself-training approach specifically for image comprehension. First, the model\nself-constructs a preference dataset for image descriptions using unlabeled\nimages. Preferred responses are generated through a step-by-step prompt, while\ndis-preferred responses are generated from either corrupted images or\nmisleading prompts. To further self-improve reasoning on the extracted visual\ninformation, we let the model reuse a small portion of existing\ninstruction-tuning data and append its self-generated image descriptions to the\nprompts. We validate the effectiveness of STIC across seven different\nbenchmarks, demonstrating substantial performance gains of 4.0% on average\nwhile using 70% less supervised fine-tuning data than the current method.\nFurther studies investigate various components of STIC and highlight its\npotential to leverage vast quantities of unlabeled images for self-training.\nCode and data are made publicly available.\n","authors":["Yihe Deng","Pan Lu","Fan Yin","Ziniu Hu","Sheng Shen","Quanquan Gu","James Zou","Kai-Wei Chang","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2405.19716v2.pdf","comment":"22 pages, 14 figures, 9 tables"},{"id":"http://arxiv.org/abs/2403.09613v2","updated":"2024-11-24T03:37:38Z","published":"2024-03-14T17:51:54Z","title":"Reawakening knowledge: Anticipatory recovery from catastrophic\n  interference via structured training","summary":"  We explore the training dynamics of neural networks in a structured non-IID\nsetting where documents are presented cyclically in a fixed, repeated sequence.\nTypically, networks suffer from catastrophic interference when training on a\nsequence of documents; however, we discover a curious and remarkable property\nof LLMs finetuned sequentially in this setting: they exhibit anticipatory\nbehavior, recovering from the forgetting on documents before encountering them\nagain. This behavior occurs even though the documents are never presented in\ncontext together. The behavior emerges and becomes more robust as the\narchitecture scales up its number of parameters. Through comprehensive\nexperiments and visualizations, we demonstrate a new mechanism by which\nover-parametrized neural networks can recover from catastrophic interference\nand uncover new insights into training over-parameterized networks in\ncyclically structured environments.\n","authors":["Yanlai Yang","Matt Jones","Michael C. Mozer","Mengye Ren"],"pdf_url":"https://arxiv.org/pdf/2403.09613v2.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024), Vancouver"},{"id":"http://arxiv.org/abs/2411.15694v1","updated":"2024-11-24T03:17:37Z","published":"2024-11-24T03:17:37Z","title":"Deep Sparse Latent Feature Models for Knowledge Graph Completion","summary":"  Recent progress in knowledge graph completion (KGC) has focused on text-based\napproaches to address the challenges of large-scale knowledge graphs (KGs).\nDespite their achievements, these methods often overlook the intricate\ninterconnections between entities, a key aspect of the underlying topological\nstructure of a KG. Stochastic blockmodels (SBMs), particularly the latent\nfeature relational model (LFRM), offer robust probabilistic frameworks that can\ndynamically capture latent community structures and enhance link prediction. In\nthis paper, we introduce a novel framework of sparse latent feature models for\nKGC, optimized through a deep variational autoencoder (VAE). Our approach not\nonly effectively completes missing triples but also provides clear\ninterpretability of the latent structures, leveraging textual information.\nComprehensive experiments on the WN18RR, FB15k-237, and Wikidata5M datasets\nshow that our method significantly improves performance by revealing latent\ncommunities and producing interpretable representations.\n","authors":["Haotian Li","Rui Zhang","Lingzhi Wang","Bin Yu","Youwei Wang","Yuliang Wei","Kai Wang","Richard Yi Da Xu","Bailing Wang"],"pdf_url":"https://arxiv.org/pdf/2411.15694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14629v3","updated":"2024-11-24T02:24:29Z","published":"2024-06-20T18:00:17Z","title":"Can LLMs Learn by Teaching for Better Reasoning? A Preliminary Study","summary":"  Teaching to improve student models (e.g., knowledge distillation) is an\nextensively studied methodology in LLMs. However, for humans, teaching improves\nnot only students but also teachers, by fostering more rigorous and clear\nreasoning as well as knowledge building. We ask: Can LLMs also learn by\nteaching (LbT) for better reasoning? If the answer is yes, we can potentially\nunlock the possibility of continuously advancing the models without solely\nrelying on human-produced data or stronger models. In this paper, we provide a\npreliminary exploration on this question. We show that LbT ideas can be\nincorporated into existing LLM training/prompting pipelines and bring\nimprovements. Specifically, we design three methods, each mimicking one of the\nthree levels of LbT: observing students' feedback, learning from the feedback,\nand learning iteratively, with the goals of improving answer accuracy without\ntraining or improving models' inherent capability with fine-tuning. We reveal\nsome findings: (1) Teaching materials that make it easier for students to learn\nhave clearer and more accurate logic when using in-context learning as the\nstudent's \"learning\" method; (2) Weak-to-strong generalization: LbT might help\nimprove strong models by teaching weak models; (3) Diversity in students might\nhelp: teaching multiple students could be better than teaching one student or\nthe teacher itself. We hope that our exploration can inspire future research on\nLbT and more broadly adopting the advanced techniques in education to improve\nLLMs. The code and website are at https://github.com/imagination-research/lbt\nand https://sites.google.com/view/llm-learning-by-teaching.\n","authors":["Xuefei Ning","Zifu Wang","Shiyao Li","Zinan Lin","Peiran Yao","Tianyu Fu","Matthew B. Blaschko","Guohao Dai","Huazhong Yang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2406.14629v3.pdf","comment":"NeurIPS 2024"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.20302v2","updated":"2024-11-24T23:38:31Z","published":"2024-09-30T14:00:04Z","title":"OM4OV: Leveraging Ontology Matching for Ontology Versioning","summary":"  Due to the dynamic nature of the semantic web, ontology version control is\nrequired to capture time-varying information, most importantly for widely-used\nontologies. Despite the long-standing recognition of ontology versioning (OV)\nas a crucial component for efficient ontology management, the growing size of\nontologies and accumulating errors caused by manual labour overwhelm current OV\napproaches. In this paper, we propose yet another approach to performing OV\nusing existing ontology matching (OM) techniques and systems. We introduce a\nunified OM4OV pipeline. From an OM perspective, we reconstruct a new task\nformulation, measurement, and testbed for OV tasks. Reusing the prior\nalignment(s) from OM, we propose a pipeline optimisation method called\ncross-reference (CR) mechanism to improve overall OV performance. We\nexperimentally validate the OM4OV pipeline and the cross-reference mechanism in\nmodified Ontology Alignment Evaluation Initiative (OAEI) datasets. We also\ndiscuss the insights on OM used for OV tasks, where some false mappings\ndetected by OV systems are not actually false.\n","authors":["Zhangcheng Qiang","Kerry Taylor","Weiqing Wang"],"pdf_url":"https://arxiv.org/pdf/2409.20302v2.pdf","comment":"9 pages, 6 figures, 1 table"},{"id":"http://arxiv.org/abs/2410.16283v2","updated":"2024-11-24T14:36:20Z","published":"2024-10-05T00:13:33Z","title":"Understanding the Effect of Algorithm Transparency of Model Explanations\n  in Text-to-SQL Semantic Parsing","summary":"  Explaining the decisions of AI has become vital for fostering appropriate\nuser trust in these systems. This paper investigates explanations for a\nstructured prediction task called ``text-to-SQL Semantic Parsing'', which\ntranslates a natural language question into a structured query language (SQL)\nprogram. In this task setting, we designed three levels of model explanation,\neach exposing a different amount of the model's decision-making details (called\n``algorithm transparency''), and investigated how different model explanations\ncould potentially yield different impacts on the user experience. Our study\nwith $\\sim$100 participants shows that (1) the low-/high-transparency\nexplanations often lead to less/more user reliance on the model decisions,\nwhereas the medium-transparency explanations strike a good balance. We also\nshow that (2) only the medium-transparency participant group was able to engage\nfurther in the interaction and exhibit increasing performance over time, and\nthat (3) they showed the least changes in trust before and after the study.\n","authors":["Daking Rai","Rydia R. Weiland","Kayla Margaret Gabriella Herrera","Tyler H. Shaw","Ziyu Yao"],"pdf_url":"https://arxiv.org/pdf/2410.16283v2.pdf","comment":"15 pages, 18 figure, Preprint"},{"id":"http://arxiv.org/abs/2411.15766v1","updated":"2024-11-24T09:27:43Z","published":"2024-11-24T09:27:43Z","title":"ScalingNote: Scaling up Retrievers with Large Language Models for\n  Real-World Dense Retrieval","summary":"  Dense retrieval in most industries employs dual-tower architectures to\nretrieve query-relevant documents. Due to online deployment requirements,\nexisting real-world dense retrieval systems mainly enhance performance by\ndesigning negative sampling strategies, overlooking the advantages of scaling\nup. Recently, Large Language Models (LLMs) have exhibited superior performance\nthat can be leveraged for scaling up dense retrieval. However, scaling up\nretrieval models significantly increases online query latency. To address this\nchallenge, we propose ScalingNote, a two-stage method to exploit the scaling\npotential of LLMs for retrieval while maintaining online query latency. The\nfirst stage is training dual towers, both initialized from the same LLM, to\nunlock the potential of LLMs for dense retrieval. Then, we distill only the\nquery tower using mean squared error loss and cosine similarity to reduce\nonline costs. Through theoretical analysis and comprehensive offline and online\nexperiments, we show the effectiveness and efficiency of ScalingNote. Our\ntwo-stage scaling method outperforms end-to-end models and verifies the scaling\nlaw of dense retrieval with LLMs in industrial scenarios, enabling\ncost-effective scaling of dense retrieval systems. Our online method\nincorporating ScalingNote significantly enhances the relevance between\nretrieved documents and queries.\n","authors":["Suyuan Huang","Chao Zhang","Yuanyuan Wu","Haoxin Zhang","Yuan Wang","Maolin Wang","Shaosheng Cao","Tong Xu","Xiangyu Zhao","Zengchang Qin","Yan Gao","Yunhan Bai","Jun Fan","Yao Hu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2411.15766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15741v1","updated":"2024-11-24T07:30:29Z","published":"2024-11-24T07:30:29Z","title":"Proceedings of the 6th International Workshop on Reading Music Systems","summary":"  The International Workshop on Reading Music Systems (WoRMS) is a workshop\nthat tries to connect researchers who develop systems for reading music, such\nas in the field of Optical Music Recognition, with other researchers and\npractitioners that could benefit from such systems, like librarians or\nmusicologists. The relevant topics of interest for the workshop include, but\nare not limited to: Music reading systems; Optical music recognition; Datasets\nand performance evaluation; Image processing on music scores; Writer\nidentification; Authoring, editing, storing and presentation systems for music\nscores; Multi-modal systems; Novel input-methods for music to produce written\nmusic; Web-based Music Information Retrieval services; Applications and\nprojects; Use-cases related to written music.\n  These are the proceedings of the 6th International Workshop on Reading Music\nSystems, held Online on November 22nd 2024.\n","authors":["Jorge Calvo-Zaragoza","Alexander Pacha","Elona Shatri"],"pdf_url":"https://arxiv.org/pdf/2411.15741v1.pdf","comment":"Proceedings edited by Jorge Calvo-Zaragoza, Alexander Pacha and Elona\n  Shatri"},{"id":"http://arxiv.org/abs/2402.19101v2","updated":"2024-11-24T06:58:16Z","published":"2024-02-29T12:29:58Z","title":"Effective Two-Stage Knowledge Transfer for Multi-Entity Cross-Domain\n  Recommendation","summary":"  In recent years, the recommendation content on e-commerce platforms has\nbecome increasingly rich -- a single user feed may contain multiple entities,\nsuch as selling products, short videos, and content posts. To deal with the\nmulti-entity recommendation problem, an intuitive solution is to adopt the\nshared-network-based architecture for joint training. The idea is to transfer\nthe extracted knowledge from one type of entity (source entity) to another\n(target entity). However, different from the conventional same-entity\ncross-domain recommendation, multi-entity knowledge transfer encounters several\nimportant issues: (1) data distributions of the source entity and target entity\nare naturally different, making the shared-network-based joint training\nsusceptible to the negative transfer issue, (2) more importantly, the\ncorresponding feature schema of each entity is not exactly aligned (e.g., price\nis an essential feature for selling product while missing for content posts),\nmaking the existing methods no longer appropriate. Recent researchers have also\nexperimented with the pre-training and fine-tuning paradigm. Again, they only\nconsider the scenarios with the same entity type and feature systems, which is\ninappropriate in our case. To this end, we design a pre-training & fine-tuning\nbased Multi-entity Knowledge Transfer framework called MKT. MKT utilizes a\nmulti-entity pre-training module to extract transferable knowledge across\ndifferent entities. In particular, a feature alignment module is first applied\nto scale and align different feature schemas. Afterward, a couple of knowledge\nextractors are employed to extract the common and entity-specific knowledge. In\nthe end, the extracted common knowledge is adopted for target entity model\ntraining. Through extensive offline and online experiments, we demonstrated the\nsuperiority of MKT over multiple State-Of-The-Art methods.\n","authors":["Jianyu Guan","Zongming Yin","Tianyi Zhang","Leihui Chen","Yin Zhang","Fei Huang","Jufeng Chen","Shuguang Han"],"pdf_url":"https://arxiv.org/pdf/2402.19101v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15731v1","updated":"2024-11-24T06:21:59Z","published":"2024-11-24T06:21:59Z","title":"Fusion Matters: Learning Fusion in Deep Click-through Rate Prediction\n  Models","summary":"  The evolution of previous Click-Through Rate (CTR) models has mainly been\ndriven by proposing complex components, whether shallow or deep, that are adept\nat modeling feature interactions. However, there has been less focus on\nimproving fusion design. Instead, two naive solutions, stacked and parallel\nfusion, are commonly used. Both solutions rely on pre-determined fusion\nconnections and fixed fusion operations. It has been repetitively observed that\nchanges in fusion design may result in different performances, highlighting the\ncritical role that fusion plays in CTR models. While there have been attempts\nto refine these basic fusion strategies, these efforts have often been\nconstrained to specific settings or dependent on specific components. Neural\narchitecture search has also been introduced to partially deal with fusion\ndesign, but it comes with limitations. The complexity of the search space can\nlead to inefficient and ineffective results. To bridge this gap, we introduce\nOptFusion, a method that automates the learning of fusion, encompassing both\nthe connection learning and the operation selection. We have proposed a\none-shot learning algorithm tackling these tasks concurrently. Our experiments\nare conducted over three large-scale datasets. Extensive experiments prove both\nthe effectiveness and efficiency of OptFusion in improving CTR model\nperformance. Our code implementation is available\nhere\\url{https://github.com/kexin-kxzhang/OptFusion}.\n","authors":["Kexin Zhang","Fuyuan Lyu","Xing Tang","Dugang Liu","Chen Ma","Kaize Ding","Xiuqiang He","Xue Liu"],"pdf_url":"https://arxiv.org/pdf/2411.15731v1.pdf","comment":"Accepted by WSDM 2025"},{"id":"http://arxiv.org/abs/2411.15716v1","updated":"2024-11-24T04:56:45Z","published":"2024-11-24T04:56:45Z","title":"Tackling Data Heterogeneity in Federated Time Series Forecasting","summary":"  Time series forecasting plays a critical role in various real-world\napplications, including energy consumption prediction, disease transmission\nmonitoring, and weather forecasting. Although substantial progress has been\nmade in time series forecasting, most existing methods rely on a centralized\ntraining paradigm, where large amounts of data are collected from distributed\ndevices (e.g., sensors, wearables) to a central cloud server. However, this\nparadigm has overloaded communication networks and raised privacy concerns.\nFederated learning, a popular privacy-preserving technique, enables\ncollaborative model training across distributed data sources. However, directly\napplying federated learning to time series forecasting often yields suboptimal\nresults, as time series data generated by different devices are inherently\nheterogeneous. In this paper, we propose a novel framework, Fed-TREND, to\naddress data heterogeneity by generating informative synthetic data as\nauxiliary knowledge carriers. Specifically, Fed-TREND generates two types of\nsynthetic data. The first type of synthetic data captures the representative\ndistribution information from clients' uploaded model updates and enhances\nclients' local training consensus. The second kind of synthetic data extracts\nlong-term influence insights from global model update trajectories and is used\nto refine the global model after aggregation. Fed-TREND is compatible with most\ntime series forecasting models and can be seamlessly integrated into existing\nfederated learning frameworks to improve prediction performance. Extensive\nexperiments on eight datasets, using several federated learning baselines and\nfour popular time series forecasting models, demonstrate the effectiveness and\ngeneralizability of Fed-TREND.\n","authors":["Wei Yuan","Guanhua Ye","Xiangyu Zhao","Quoc Viet Hung Nguyen","Yang Cao","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2411.15716v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2411.15801v1","updated":"2024-11-24T12:07:47Z","published":"2024-11-24T12:07:47Z","title":"A review on Machine Learning based User-Centric Multimedia Streaming\n  Techniques","summary":"  The multimedia content and streaming are a major means of information\nexchange in the modern era and there is an increasing demand for such services.\nThis coupled with the advancement of future wireless networks B5G/6G and the\nproliferation of intelligent handheld mobile devices, has facilitated the\navailability of multimedia content to heterogeneous mobile users. Apart from\nthe conventional video, the 360$^o$ videos have gained popularity with the\nemerging virtual reality applications. All formats of videos (conventional and\n360$^o$) undergo processing, compression, and transmission across dynamic\nwireless channels with restricted bandwidth to facilitate the streaming\nservices. This causes video impairments, leading to quality degradation and\nposes challenges in delivering good Quality-of-Experience (QoE) to the viewers.\nThe QoE is a prominent subjective quality measure to assess multimedia\nservices. This requires end-to-end QoE evaluation. Efficient multimedia\nstreaming techniques can improve the service quality while dealing with dynamic\nnetwork and end-user challenges. A paradigm shift in user-centric multimedia\nservices is envisioned with a focus on Machine Learning (ML) based QoE modeling\nand streaming strategies. This survey paper presents a comprehensive overview\nof the overall and continuous, time varying QoE modeling for the purpose of QoE\nmanagement in multimedia services. It also examines the recent research on\nintelligent and adaptive multimedia streaming strategies, with a special\nemphasis on ML based techniques for video (conventional and 360$^o$) streaming.\nThis paper discusses the overall and continuous QoE modeling to optimize the\nend-user viewing experience, efficient video streaming with a focus on\nuser-centric strategies, associated datasets for modeling and streaming, along\nwith existing shortcoming and open challenges.\n","authors":["Monalisa Ghosh","Chetna Singhal"],"pdf_url":"https://arxiv.org/pdf/2411.15801v1.pdf","comment":"Computer Communications"},{"id":"http://arxiv.org/abs/2411.15759v1","updated":"2024-11-24T08:47:00Z","published":"2024-11-24T08:47:00Z","title":"Advanced Learning-Based Inter Prediction for Future Video Coding","summary":"  In the fourth generation Audio Video coding Standard (AVS4), the Inter\nPrediction Filter (INTERPF) reduces discontinuities between prediction and\nadjacent reconstructed pixels in inter prediction. The paper proposes a low\ncomplexity learning-based inter prediction (LLIP) method to replace the\ntraditional INTERPF. LLIP enhances the filtering process by leveraging a\nlightweight neural network model, where parameters can be exported for\nefficient inference. Specifically, we extract pixels and coordinates utilized\nby the traditional INTERPF to form the training dataset. Subsequently, we\nexport the weights and biases of the trained neural network model and implement\nthe inference process without any third-party dependency, enabling seamless\nintegration into video codec without relying on Libtorch, thus achieving faster\ninference speed. Ultimately, we replace the traditional handcraft filtering\nparameters in INTERPF with the learned optimal filtering parameters. This\npractical solution makes the combination of deep learning encoding tools with\ntraditional video encoding schemes more efficient. Experimental results show\nthat our approach achieves 0.01%, 0.31%, and 0.25% coding gain for the Y, U,\nand V components under the random access (RA) configuration on average.\n","authors":["Yanchen Zhao","Wenhong Duan","Chuanmin Jia","Shanshe Wang","Siwei Ma"],"pdf_url":"https://arxiv.org/pdf/2411.15759v1.pdf","comment":null}]},"2024-11-23T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2407.21783v3","updated":"2024-11-23T23:27:33Z","published":"2024-07-31T17:54:27Z","title":"The Llama 3 Herd of Models","summary":"  Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment.\n","authors":["Aaron Grattafiori","Abhimanyu Dubey","Abhinav Jauhri","Abhinav Pandey","Abhishek Kadian","Ahmad Al-Dahle","Aiesha Letman","Akhil Mathur","Alan Schelten","Alex Vaughan","Amy Yang","Angela Fan","Anirudh Goyal","Anthony Hartshorn","Aobo Yang","Archi Mitra","Archie Sravankumar","Artem Korenev","Arthur Hinsvark","Arun Rao","Aston Zhang","Aurelien Rodriguez","Austen Gregerson","Ava Spataru","Baptiste Roziere","Bethany Biron","Binh Tang","Bobbie Chern","Charlotte Caucheteux","Chaya Nayak","Chloe Bi","Chris Marra","Chris McConnell","Christian Keller","Christophe Touret","Chunyang Wu","Corinne Wong","Cristian Canton Ferrer","Cyrus Nikolaidis","Damien Allonsius","Daniel Song","Danielle Pintz","Danny Livshits","Danny Wyatt","David Esiobu","Dhruv Choudhary","Dhruv Mahajan","Diego Garcia-Olano","Diego Perino","Dieuwke Hupkes","Egor Lakomkin","Ehab AlBadawy","Elina Lobanova","Emily Dinan","Eric Michael Smith","Filip Radenovic","Francisco Guzmán","Frank Zhang","Gabriel Synnaeve","Gabrielle Lee","Georgia Lewis Anderson","Govind Thattai","Graeme Nail","Gregoire Mialon","Guan Pang","Guillem Cucurell","Hailey Nguyen","Hannah Korevaar","Hu Xu","Hugo Touvron","Iliyan Zarov","Imanol Arrieta Ibarra","Isabel Kloumann","Ishan Misra","Ivan Evtimov","Jack Zhang","Jade Copet","Jaewon Lee","Jan Geffert","Jana Vranes","Jason Park","Jay Mahadeokar","Jeet Shah","Jelmer van der Linde","Jennifer Billock","Jenny Hong","Jenya Lee","Jeremy Fu","Jianfeng Chi","Jianyu Huang","Jiawen Liu","Jie Wang","Jiecao Yu","Joanna Bitton","Joe Spisak","Jongsoo Park","Joseph Rocca","Joshua Johnstun","Joshua Saxe","Junteng Jia","Kalyan Vasuden Alwala","Karthik Prasad","Kartikeya Upasani","Kate Plawiak","Ke Li","Kenneth Heafield","Kevin Stone","Khalid El-Arini","Krithika Iyer","Kshitiz Malik","Kuenley Chiu","Kunal Bhalla","Kushal Lakhotia","Lauren Rantala-Yeary","Laurens van der Maaten","Lawrence Chen","Liang Tan","Liz Jenkins","Louis Martin","Lovish Madaan","Lubo Malo","Lukas Blecher","Lukas Landzaat","Luke de Oliveira","Madeline Muzzi","Mahesh Pasupuleti","Mannat Singh","Manohar Paluri","Marcin Kardas","Maria Tsimpoukelli","Mathew Oldham","Mathieu Rita","Maya Pavlova","Melanie Kambadur","Mike Lewis","Min Si","Mitesh Kumar Singh","Mona Hassan","Naman Goyal","Narjes Torabi","Nikolay Bashlykov","Nikolay Bogoychev","Niladri Chatterji","Ning Zhang","Olivier Duchenne","Onur Çelebi","Patrick Alrassy","Pengchuan Zhang","Pengwei Li","Petar Vasic","Peter Weng","Prajjwal Bhargava","Pratik Dubal","Praveen Krishnan","Punit Singh Koura","Puxin Xu","Qing He","Qingxiao Dong","Ragavan Srinivasan","Raj Ganapathy","Ramon Calderer","Ricardo Silveira Cabral","Robert Stojnic","Roberta Raileanu","Rohan Maheswari","Rohit Girdhar","Rohit Patel","Romain Sauvestre","Ronnie Polidoro","Roshan Sumbaly","Ross Taylor","Ruan Silva","Rui Hou","Rui Wang","Saghar Hosseini","Sahana Chennabasappa","Sanjay Singh","Sean Bell","Seohyun Sonia Kim","Sergey Edunov","Shaoliang Nie","Sharan Narang","Sharath Raparthy","Sheng Shen","Shengye Wan","Shruti Bhosale","Shun Zhang","Simon Vandenhende","Soumya Batra","Spencer Whitman","Sten Sootla","Stephane Collot","Suchin Gururangan","Sydney Borodinsky","Tamar Herman","Tara Fowler","Tarek Sheasha","Thomas Georgiou","Thomas Scialom","Tobias Speckbacher","Todor Mihaylov","Tong Xiao","Ujjwal Karn","Vedanuj Goswami","Vibhor Gupta","Vignesh Ramanathan","Viktor Kerkez","Vincent Gonguet","Virginie Do","Vish Vogeti","Vítor Albiero","Vladan Petrovic","Weiwei Chu","Wenhan Xiong","Wenyin Fu","Whitney Meers","Xavier Martinet","Xiaodong Wang","Xiaofang Wang","Xiaoqing Ellen Tan","Xide Xia","Xinfeng Xie","Xuchao Jia","Xuewei Wang","Yaelle Goldschlag","Yashesh Gaur","Yasmine Babaei","Yi Wen","Yiwen Song","Yuchen Zhang","Yue Li","Yuning Mao","Zacharie Delpierre Coudert","Zheng Yan","Zhengxing Chen","Zoe Papakipos","Aaditya Singh","Aayushi Srivastava","Abha Jain","Adam Kelsey","Adam Shajnfeld","Adithya Gangidi","Adolfo Victoria","Ahuva Goldstand","Ajay Menon","Ajay Sharma","Alex Boesenberg","Alexei Baevski","Allie Feinstein","Amanda Kallet","Amit Sangani","Amos Teo","Anam Yunus","Andrei Lupu","Andres Alvarado","Andrew Caples","Andrew Gu","Andrew Ho","Andrew Poulton","Andrew Ryan","Ankit Ramchandani","Annie Dong","Annie Franco","Anuj Goyal","Aparajita Saraf","Arkabandhu Chowdhury","Ashley Gabriel","Ashwin Bharambe","Assaf Eisenman","Azadeh Yazdan","Beau James","Ben Maurer","Benjamin Leonhardi","Bernie Huang","Beth Loyd","Beto De Paola","Bhargavi Paranjape","Bing Liu","Bo Wu","Boyu Ni","Braden Hancock","Bram Wasti","Brandon Spence","Brani Stojkovic","Brian Gamido","Britt Montalvo","Carl Parker","Carly Burton","Catalina Mejia","Ce Liu","Changhan Wang","Changkyu Kim","Chao Zhou","Chester Hu","Ching-Hsiang Chu","Chris Cai","Chris Tindal","Christoph Feichtenhofer","Cynthia Gao","Damon Civin","Dana Beaty","Daniel Kreymer","Daniel Li","David Adkins","David Xu","Davide Testuggine","Delia David","Devi Parikh","Diana Liskovich","Didem Foss","Dingkang Wang","Duc Le","Dustin Holland","Edward Dowling","Eissa Jamil","Elaine Montgomery","Eleonora Presani","Emily Hahn","Emily Wood","Eric-Tuan Le","Erik Brinkman","Esteban Arcaute","Evan Dunbar","Evan Smothers","Fei Sun","Felix Kreuk","Feng Tian","Filippos Kokkinos","Firat Ozgenel","Francesco Caggioni","Frank Kanayet","Frank Seide","Gabriela Medina Florez","Gabriella Schwarz","Gada Badeer","Georgia Swee","Gil Halpern","Grant Herman","Grigory Sizov"," Guangyi"," Zhang","Guna Lakshminarayanan","Hakan Inan","Hamid Shojanazeri","Han Zou","Hannah Wang","Hanwen Zha","Haroun Habeeb","Harrison Rudolph","Helen Suk","Henry Aspegren","Hunter Goldman","Hongyuan Zhan","Ibrahim Damlaj","Igor Molybog","Igor Tufanov","Ilias Leontiadis","Irina-Elena Veliche","Itai Gat","Jake Weissman","James Geboski","James Kohli","Janice Lam","Japhet Asher","Jean-Baptiste Gaya","Jeff Marcus","Jeff Tang","Jennifer Chan","Jenny Zhen","Jeremy Reizenstein","Jeremy Teboul","Jessica Zhong","Jian Jin","Jingyi Yang","Joe Cummings","Jon Carvill","Jon Shepard","Jonathan McPhie","Jonathan Torres","Josh Ginsburg","Junjie Wang","Kai Wu","Kam Hou U","Karan Saxena","Kartikay Khandelwal","Katayoun Zand","Kathy Matosich","Kaushik Veeraraghavan","Kelly Michelena","Keqian Li","Kiran Jagadeesh","Kun Huang","Kunal Chawla","Kyle Huang","Lailin Chen","Lakshya Garg","Lavender A","Leandro Silva","Lee Bell","Lei Zhang","Liangpeng Guo","Licheng Yu","Liron Moshkovich","Luca Wehrstedt","Madian Khabsa","Manav Avalani","Manish Bhatt","Martynas Mankus","Matan Hasson","Matthew Lennie","Matthias Reso","Maxim Groshev","Maxim Naumov","Maya Lathi","Meghan Keneally","Miao Liu","Michael L. Seltzer","Michal Valko","Michelle Restrepo","Mihir Patel","Mik Vyatskov","Mikayel Samvelyan","Mike Clark","Mike Macey","Mike Wang","Miquel Jubert Hermoso","Mo Metanat","Mohammad Rastegari","Munish Bansal","Nandhini Santhanam","Natascha Parks","Natasha White","Navyata Bawa","Nayan Singhal","Nick Egebo","Nicolas Usunier","Nikhil Mehta","Nikolay Pavlovich Laptev","Ning Dong","Norman Cheng","Oleg Chernoguz","Olivia Hart","Omkar Salpekar","Ozlem Kalinli","Parkin Kent","Parth Parekh","Paul Saab","Pavan Balaji","Pedro Rittner","Philip Bontrager","Pierre Roux","Piotr Dollar","Polina Zvyagina","Prashant Ratanchandani","Pritish Yuvraj","Qian Liang","Rachad Alao","Rachel Rodriguez","Rafi Ayub","Raghotham Murthy","Raghu Nayani","Rahul Mitra","Rangaprabhu Parthasarathy","Raymond Li","Rebekkah Hogan","Robin Battey","Rocky Wang","Russ Howes","Ruty Rinott","Sachin Mehta","Sachin Siby","Sai Jayesh Bondu","Samyak Datta","Sara Chugh","Sara Hunt","Sargun Dhillon","Sasha Sidorov","Satadru Pan","Saurabh Mahajan","Saurabh Verma","Seiji Yamamoto","Sharadh Ramaswamy","Shaun Lindsay","Shaun Lindsay","Sheng Feng","Shenghao Lin","Shengxin Cindy Zha","Shishir Patil","Shiva Shankar","Shuqiang Zhang","Shuqiang Zhang","Sinong Wang","Sneha Agarwal","Soji Sajuyigbe","Soumith Chintala","Stephanie Max","Stephen Chen","Steve Kehoe","Steve Satterfield","Sudarshan Govindaprasad","Sumit Gupta","Summer Deng","Sungmin Cho","Sunny Virk","Suraj Subramanian","Sy Choudhury","Sydney Goldman","Tal Remez","Tamar Glaser","Tamara Best","Thilo Koehler","Thomas Robinson","Tianhe Li","Tianjun Zhang","Tim Matthews","Timothy Chou","Tzook Shaked","Varun Vontimitta","Victoria Ajayi","Victoria Montanez","Vijai Mohan","Vinay Satish Kumar","Vishal Mangla","Vlad Ionescu","Vlad Poenaru","Vlad Tiberiu Mihailescu","Vladimir Ivanov","Wei Li","Wenchen Wang","Wenwen Jiang","Wes Bouaziz","Will Constable","Xiaocheng Tang","Xiaojian Wu","Xiaolan Wang","Xilun Wu","Xinbo Gao","Yaniv Kleinman","Yanjun Chen","Ye Hu","Ye Jia","Ye Qi","Yenda Li","Yilin Zhang","Ying Zhang","Yossi Adi","Youngjin Nam"," Yu"," Wang","Yu Zhao","Yuchen Hao","Yundi Qian","Yunlu Li","Yuzi He","Zach Rait","Zachary DeVito","Zef Rosnbrick","Zhaoduo Wen","Zhenyu Yang","Zhiwei Zhao","Zhiyu Ma"],"pdf_url":"https://arxiv.org/pdf/2407.21783v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15666v1","updated":"2024-11-23T23:05:48Z","published":"2024-11-23T23:05:48Z","title":"Ontology-Constrained Generation of Domain-Specific Clinical Summaries","summary":"  Large Language Models (LLMs) offer promising solutions for text\nsummarization. However, some domains require specific information to be\navailable in the summaries. Generating these domain-adapted summaries is still\nan open challenge. Similarly, hallucinations in generated content is a major\ndrawback of current approaches, preventing their deployment. This study\nproposes a novel approach that leverages ontologies to create domain-adapted\nsummaries both structured and unstructured. We employ an ontology-guided\nconstrained decoding process to reduce hallucinations while improving\nrelevance. When applied to the medical domain, our method shows potential in\nsummarizing Electronic Health Records (EHRs) across different specialties,\nallowing doctors to focus on the most relevant information to their domain.\nEvaluation on the MIMIC-III dataset demonstrates improvements in generating\ndomain-adapted summaries of clinical notes and hallucination reduction.\n","authors":["Gaya Mehenni","Amal Zouaq"],"pdf_url":"https://arxiv.org/pdf/2411.15666v1.pdf","comment":"24th International Conference on Knowledge Engineering and Knowledge\n  Management (EKAW 2024), November 26-28, 2024, Amsterdam, The Netherlands"},{"id":"http://arxiv.org/abs/2411.09688v2","updated":"2024-11-23T22:11:42Z","published":"2024-11-14T18:54:19Z","title":"Squeezed Attention: Accelerating Long Context Length LLM Inference","summary":"  Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models.\n","authors":["Coleman Hooper","Sehoon Kim","Hiva Mohammadzadeh","Monishwaran Maheswaran","June Paik","Michael W. Mahoney","Kurt Keutzer","Amir Gholami"],"pdf_url":"https://arxiv.org/pdf/2411.09688v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15661v1","updated":"2024-11-23T22:09:58Z","published":"2024-11-23T22:09:58Z","title":"Improving Next Tokens via Second-Last Predictions with Generate and\n  Refine","summary":"  Autoregressive language models like GPT aim at predicting next tokens, while\nautoencoding models such as BERT are trained on tasks such as predicting masked\ntokens. We train a decoder only architecture for predicting the second last\ntoken for a sequence of tokens. Our approach yields higher computational\ntraining efficiency than BERT-style models by employing a structured\ndeterministic approach towards masking tokens. We use our model to improve the\nnext token predictions of a standard GPT by combining both predictions in a\n``generate-then-refine'' approach. We show on different variants of GPT-2 and\ndifferent datasets that (not unexpectedly) second last token predictions are\nmuch more accurate, i.e., more than 15\\% higher accuracy than ordinary next\ntoken predictors. The ``generate-then-refine'' approach also demonstrates\nnotable improvements in next-token predictions, yielding smaller yet consistent\nand significant gains.\n","authors":["Johannes Schneider"],"pdf_url":"https://arxiv.org/pdf/2411.15661v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14337v2","updated":"2024-11-23T21:16:55Z","published":"2024-02-22T07:12:34Z","title":"How Ambiguous are the Rationales for Natural Language Reasoning? A\n  Simple Approach to Handling Rationale Uncertainty","summary":"  Rationales behind answers not only explain model decisions but boost language\nmodels to reason well on complex reasoning tasks. However, obtaining impeccable\nrationales is often impossible. Besides, it is non-trivial to estimate the\ndegree to which the rationales are faithful enough to encourage model\nperformance. Thus, such reasoning tasks often compel models to output correct\nanswers under undesirable rationales and are sub-optimal compared to what the\nmodels are fully capable of. In this work, we propose how to deal with\nimperfect rationales causing aleatoric uncertainty. We first define the\nambiguous rationales with entropy scores of given rationales, using model prior\nbeliefs as informativeness. We then guide models to select one of two different\nreasoning models according to the ambiguity of rationales. We empirically argue\nthat our proposed method produces robust performance superiority against the\nadversarial quality of rationales and low-resource settings.\n","authors":["Hazel Kim"],"pdf_url":"https://arxiv.org/pdf/2402.14337v2.pdf","comment":"Coling2025"},{"id":"http://arxiv.org/abs/2409.00099v2","updated":"2024-11-23T20:55:13Z","published":"2024-08-27T03:44:57Z","title":"Query-by-Example Keyword Spotting Using Spectral-Temporal Graph\n  Attentive Pooling and Multi-Task Learning","summary":"  Existing keyword spotting (KWS) systems primarily rely on predefined keyword\nphrases. However, the ability to recognize customized keywords is crucial for\ntailoring interactions with intelligent devices. In this paper, we present a\nnovel Query-by-Example (QbyE) KWS system that employs spectral-temporal graph\nattentive pooling and multi-task learning. This framework aims to effectively\nlearn speaker-invariant and linguistic-informative embeddings for QbyE KWS\ntasks. Within this framework, we investigate three distinct network\narchitectures for encoder modeling: LiCoNet, Conformer and ECAPA_TDNN. The\nexperimental results on a substantial internal dataset of $629$ speakers have\ndemonstrated the effectiveness of the proposed QbyE framework in maximizing the\npotential of simpler models such as LiCoNet. Particularly, LiCoNet, which is\n13x more efficient, achieves comparable performance to the computationally\nintensive Conformer model (1.98% vs. 1.63\\% FRR at 0.3 FAs/Hr).\n","authors":["Zhenyu Wang","Shuyu Kong","Li Wan","Biqiao Zhang","Yiteng Huang","Mumin Jin","Ming Sun","Xin Lei","Zhaojun Yang"],"pdf_url":"https://arxiv.org/pdf/2409.00099v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15640v1","updated":"2024-11-23T19:43:02Z","published":"2024-11-23T19:43:02Z","title":"AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering\n  Benchmark Dataset","summary":"  Recent advancements in large language model(LLM) performance on medical\nmultiple choice question (MCQ) benchmarks have stimulated interest from\nhealthcare providers and patients globally. Particularly in low-and\nmiddle-income countries (LMICs) facing acute physician shortages and lack of\nspecialists, LLMs offer a potentially scalable pathway to enhance healthcare\naccess and reduce costs. However, their effectiveness in the Global South,\nespecially across the African continent, remains to be established. In this\nwork, we introduce AfriMed-QA, the first large scale Pan-African English\nmulti-specialty medical Question-Answering (QA) dataset, 15,000 questions (open\nand closed-ended) sourced from over 60 medical schools across 16 countries,\ncovering 32 medical specialties. We further evaluate 30 LLMs across multiple\naxes including correctness and demographic bias. Our findings show significant\nperformance variation across specialties and geographies, MCQ performance\nclearly lags USMLE (MedQA). We find that biomedical LLMs underperform general\nmodels and smaller edge-friendly LLMs struggle to achieve a passing score.\nInterestingly, human evaluations show a consistent consumer preference for LLM\nanswers and explanations when compared with clinician answers.\n","authors":["Tobi Olatunji","Charles Nimo","Abraham Owodunni","Tassallah Abdullahi","Emmanuel Ayodele","Mardhiyah Sanni","Chinemelu Aka","Folafunmi Omofoye","Foutse Yuehgoh","Timothy Faniran","Bonaventure F. P. Dossou","Moshood Yekini","Jonas Kemp","Katherine Heller","Jude Chidubem Omeke","Chidi Asuzu MD","Naome A. Etori","Aimérou Ndiaye","Ifeoma Okoh","Evans Doe Ocansey","Wendy Kinara","Michael Best","Irfan Essa","Stephen Edward Moore","Chris Fourie","Mercy Nyamewaa Asiedu"],"pdf_url":"https://arxiv.org/pdf/2411.15640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15634v1","updated":"2024-11-23T19:18:08Z","published":"2024-11-23T19:18:08Z","title":"\"All that Glitters\": Approaches to Evaluations with Unreliable Model and\n  Human Annotations","summary":"  \"Gold\" and \"ground truth\" human-mediated labels have error. The effects of\nthis error can escape commonly reported metrics of label quality or obscure\nquestions of accuracy, bias, fairness, and usefulness during model evaluation.\nThis study demonstrates methods for answering such questions even in the\ncontext of very low reliabilities from expert humans. We analyze human labels,\nGPT model ratings, and transformer encoder model annotations describing the\nquality of classroom teaching, an important, expensive, and currently only\nhuman task. We answer the question of whether such a task can be automated\nusing two Large Language Model (LLM) architecture families--encoders and GPT\ndecoders, using novel approaches to evaluating label quality across six\ndimensions: Concordance, Confidence, Validity, Bias, Fairness, and Helpfulness.\nFirst, we demonstrate that using standard metrics in the presence of poor\nlabels can mask both label and model quality: the encoder family of models\nachieve state-of-the-art, even \"super-human\", results across all classroom\nannotation tasks. But not all these positive results remain after using more\nrigorous evaluation measures which reveal spurious correlations and nonrandom\nracial biases across models and humans. This study then expands these methods\nto estimate how model use would change to human label quality if models were\nused in a human-in-the-loop context, finding that the variance captured in GPT\nmodel labels would worsen reliabilities for humans influenced by these models.\nWe identify areas where some LLMs, within the generalizability of the current\ndata, could improve the quality of expensive human ratings of classroom\ninstruction.\n","authors":["Michael Hardy"],"pdf_url":"https://arxiv.org/pdf/2411.15634v1.pdf","comment":"20 pages, 15 figures, 58 pages with references and appendices"},{"id":"http://arxiv.org/abs/2406.14829v3","updated":"2024-11-23T19:08:45Z","published":"2024-06-21T02:18:03Z","title":"Is This a Bad Table? A Closer Look at the Evaluation of Table Generation\n  from Text","summary":"  Understanding whether a generated table is of good quality is important to be\nable to use it in creating or editing documents using automatic methods. In\nthis work, we underline that existing measures for table quality evaluation\nfail to capture the overall semantics of the tables, and sometimes unfairly\npenalize good tables and reward bad ones. We propose TabEval, a novel table\nevaluation strategy that captures table semantics by first breaking down a\ntable into a list of natural language atomic statements and then compares them\nwith ground truth statements using entailment-based measures. To validate our\napproach, we curate a dataset comprising of text descriptions for 1,250 diverse\nWikipedia tables, covering a range of topics and structures, in contrast to the\nlimited scope of existing datasets. We compare TabEval with existing metrics\nusing unsupervised and supervised text-to-table generation methods,\ndemonstrating its stronger correlation with human judgments of table quality\nacross four datasets.\n","authors":["Pritika Ramu","Aparna Garimella","Sambaran Bandyopadhyay"],"pdf_url":"https://arxiv.org/pdf/2406.14829v3.pdf","comment":"EMNLP 2024 (short)"},{"id":"http://arxiv.org/abs/2409.17073v4","updated":"2024-11-23T19:07:10Z","published":"2024-09-25T16:32:35Z","title":"Enhancing Post-Hoc Attributions in Long Document Comprehension via\n  Coarse Grained Answer Decomposition","summary":"  Accurately attributing answer text to its source document is crucial for\ndeveloping a reliable question-answering system. However, attribution for long\ndocuments remains largely unexplored. Post-hoc attribution systems are designed\nto map answer text back to the source document, yet the granularity of this\nmapping has not been addressed. Furthermore, a critical question arises: What\nexactly should be attributed? This involves identifying the specific\ninformation units within an answer that require grounding. In this paper, we\npropose and investigate a novel approach to the factual decomposition of\ngenerated answers for attribution, employing template-based in-context\nlearning. To accomplish this, we utilize the question and integrate negative\nsampling during few-shot in-context learning for decomposition. This approach\nenhances the semantic understanding of both abstractive and extractive answers.\nWe examine the impact of answer decomposition by providing a thorough\nexamination of various attribution approaches, ranging from retrieval-based\ntechniques to LLM-based attributors.\n","authors":["Pritika Ramu","Koustava Goswami","Apoorv Saxena","Balaji Vasan Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2409.17073v4.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2411.15623v1","updated":"2024-11-23T18:27:35Z","published":"2024-11-23T18:27:35Z","title":"Multi-label Sequential Sentence Classification via Large Language Model","summary":"  Sequential sentence classification (SSC) in scientific publications is\ncrucial for supporting downstream tasks such as fine-grained information\nretrieval and extractive summarization. However, current SSC methods are\nconstrained by model size, sequence length, and single-label setting. To\naddress these limitations, this paper proposes LLM-SSC, a large language model\n(LLM)-based framework for both single- and multi-label SSC tasks. Unlike\nprevious approaches that employ small- or medium-sized language models, the\nproposed framework utilizes LLMs to generate SSC labels through designed\nprompts, which enhance task understanding by incorporating demonstrations and a\nquery to describe the prediction target. We also present a multi-label\ncontrastive learning loss with auto-weighting scheme, enabling the multi-label\nclassification task. To support our multi-label SSC analysis, we introduce and\nrelease a new dataset, biorc800, which mainly contains unstructured abstracts\nin the biomedical domain with manual annotations. Experiments demonstrate\nLLM-SSC's strong performance in SSC under both in-context learning and\ntask-specific tuning settings. We release biorc800 and our code at:\nhttps://github.com/ScienceNLP-Lab/LLM-SSC.\n","authors":["Mengfei Lan","Lecheng Zheng","Shufan Ming","Halil Kilicoglu"],"pdf_url":"https://arxiv.org/pdf/2411.15623v1.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2404.15269v3","updated":"2024-11-23T16:19:03Z","published":"2024-04-23T17:57:47Z","title":"Aligning LLM Agents by Learning Latent Preference from User Edits","summary":"  We study interactive learning of LLM-based language agents based on user\nedits made to the agent's output. In a typical setting such as writing\nassistants, the user interacts with a language agent to generate a response\ngiven a context, and may optionally edit the agent response to personalize it\nbased on their latent preference, in addition to improving the correctness. The\nedit feedback is naturally generated, making it a suitable candidate for\nimproving the agent's alignment with the user's preference, and for reducing\nthe cost of user edits over time. We propose a learning framework, PRELUDE that\ninfers a description of the user's latent preference based on historic edit\ndata. The inferred user preference descriptions are used to define prompts for\ngenerating responses in the future. This avoids fine-tuning the agent, which is\ncostly, challenging to scale with the number of users, and may even degrade its\nperformance on other tasks. Furthermore, learning descriptive preference\nimproves interpretability, allowing the user to view and modify the learned\npreference. However, user preference can be complex, subtle, and vary based on\ncontext, making it challenging to learn. To address this, we propose a simple\nyet effective algorithm named CIPHER that leverages the LLM to infer the user\npreference for a given context based on user edits. In the future, CIPHER\nretrieves inferred preferences from the k-closest contexts in the history, and\nforms an aggregate preference for response generation. We introduce two\ninteractive environments -- summarization and email writing, and use a GPT-4\nsimulated user for evaluation. On both tasks, CIPHER outperforms several\nbaselines by achieving the lowest edit distance cost while only having a small\noverhead in LLM query cost. Our analysis reports that user preferences learned\nby CIPHER show significant similarity to the ground truth latent preferences.\n","authors":["Ge Gao","Alexey Taymanov","Eduardo Salinas","Paul Mineiro","Dipendra Misra"],"pdf_url":"https://arxiv.org/pdf/2404.15269v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15594v1","updated":"2024-11-23T16:03:35Z","published":"2024-11-23T16:03:35Z","title":"A Survey on LLM-as-a-Judge","summary":"  Accurate and consistent evaluation is crucial for decision-making across\nnumerous fields, yet it remains a challenging task due to inherent\nsubjectivity, variability, and scale. Large Language Models (LLMs) have\nachieved remarkable success across diverse domains, leading to the emergence of\n\"LLM-as-a-Judge,\" where LLMs are employed as evaluators for complex tasks. With\ntheir ability to process diverse data types and provide scalable,\ncost-effective, and consistent assessments, LLMs present a compelling\nalternative to traditional expert-driven evaluations. However, ensuring the\nreliability of LLM-as-a-Judge systems remains a significant challenge that\nrequires careful design and standardization. This paper provides a\ncomprehensive survey of LLM-as-a-Judge, addressing the core question: How can\nreliable LLM-as-a-Judge systems be built? We explore strategies to enhance\nreliability, including improving consistency, mitigating biases, and adapting\nto diverse assessment scenarios. Additionally, we propose methodologies for\nevaluating the reliability of LLM-as-a-Judge systems, supported by a novel\nbenchmark designed for this purpose. To advance the development and real-world\ndeployment of LLM-as-a-Judge systems, we also discussed practical applications,\nchallenges, and future directions. This survey serves as a foundational\nreference for researchers and practitioners in this rapidly evolving field.\n","authors":["Jiawei Gu","Xuhui Jiang","Zhichao Shi","Hexiang Tan","Xuehao Zhai","Chengjin Xu","Wei Li","Yinghan Shen","Shengjie Ma","Honghao Liu","Yuanzhuo Wang","Jian Guo"],"pdf_url":"https://arxiv.org/pdf/2411.15594v1.pdf","comment":"33 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:2310.05470 by other authors"},{"id":"http://arxiv.org/abs/2310.04407v2","updated":"2024-11-23T16:00:51Z","published":"2023-10-06T17:55:23Z","title":"Policy-Gradient Training of Language Models for Ranking","summary":"  Text retrieval plays a crucial role in incorporating factual knowledge for\ndecision making into language processing pipelines, ranging from chat-based web\nsearch to question answering systems. Current state-of-the-art text retrieval\nmodels leverage pre-trained large language models (LLMs) to achieve competitive\nperformance, but training LLM-based retrievers via typical contrastive losses\nrequires intricate heuristics, including selecting hard negatives and using\nadditional supervision as learning signals. This reliance on heuristics stems\nfrom the fact that the contrastive loss itself is heuristic and does not\ndirectly optimize the downstream metrics of decision quality at the end of the\nprocessing pipeline. To address this issue, we introduce Neural PG-RANK, a\nnovel training algorithm that learns to rank by instantiating a LLM as a\nPlackett-Luce ranking policy. Neural PG-RANK provides a principled method for\nend-to-end training of retrieval models as part of larger decision systems via\npolicy gradient, with little reliance on complex heuristics, and it effectively\nunifies the training objective with downstream decision-making quality. We\nconduct extensive experiments on various text retrieval benchmarks. The results\ndemonstrate that when the training objective aligns with the evaluation setup,\nNeural PG-RANK yields remarkable in-domain performance improvement, with\nsubstantial out-of-domain generalization to some critical datasets employed in\ndownstream question answering tasks.\n","authors":["Ge Gao","Jonathan D. Chang","Claire Cardie","Kianté Brantley","Thorsten Joachim"],"pdf_url":"https://arxiv.org/pdf/2310.04407v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15586v1","updated":"2024-11-23T15:26:01Z","published":"2024-11-23T15:26:01Z","title":"Transparent but Powerful: Explainability, Accuracy, and Generalizability\n  in ADHD Detection from Social Media Data","summary":"  Attention-deficit/hyperactivity disorder (ADHD) is a prevalent mental health\ncondition affecting both children and adults, yet it remains severely\nunderdiagnosed. Recent advances in artificial intelligence, particularly in\nNatural Language Processing (NLP) and Machine Learning (ML), offer promising\nsolutions for scalable and non-invasive ADHD screening methods using social\nmedia data. This paper presents a comprehensive study on ADHD detection,\nleveraging both shallow machine learning models and deep learning approaches,\nincluding BiLSTM and transformer-based models, to analyze linguistic patterns\nin ADHD-related social media text. Our results highlight the trade-offs between\ninterpretability and performance across different models, with BiLSTM offering\na balance of transparency and accuracy. Additionally, we assess the\ngeneralizability of these models using cross-platform data from Reddit and\nTwitter, uncovering key linguistic features associated with ADHD that could\ncontribute to more effective digital screening tools.\n","authors":["D. Wiechmann","E. Kempa","E. Kerz","Y. Qiao"],"pdf_url":"https://arxiv.org/pdf/2411.15586v1.pdf","comment":"12 pages (including references and appendix)"},{"id":"http://arxiv.org/abs/2407.01834v2","updated":"2024-11-23T15:01:52Z","published":"2024-07-01T22:17:17Z","title":"A Study of Nationality Bias in Names and Perplexity using Off-the-Shelf\n  Affect-related Tweet Classifiers","summary":"  In this paper, we apply a method to quantify biases associated with named\nentities from various countries. We create counterfactual examples with small\nperturbations on target-domain data instead of relying on templates or specific\ndatasets for bias detection. On widely used classifiers for subjectivity\nanalysis, including sentiment, emotion, hate speech, and offensive text using\nTwitter data, our results demonstrate positive biases related to the language\nspoken in a country across all classifiers studied. Notably, the presence of\ncertain country names in a sentence can strongly influence predictions, up to a\n23\\% change in hate speech detection and up to a 60\\% change in the prediction\nof negative emotions such as anger. We hypothesize that these biases stem from\nthe training data of pre-trained language models (PLMs) and find correlations\nbetween affect predictions and PLMs likelihood in English and unknown languages\nlike Basque and Maori, revealing distinct patterns with exacerbate\ncorrelations. Further, we followed these correlations in-between counterfactual\nexamples from a same sentence to remove the syntactical component, uncovering\ninteresting results suggesting the impact of the pre-training data was more\nimportant for English-speaking-country names. Our anonymized code is\n[https://anonymous.4open.science/r/biases_ppl-576B/README.md](available here).\n","authors":["Valentin Barriere","Sebastian Cifuentes"],"pdf_url":"https://arxiv.org/pdf/2407.01834v2.pdf","comment":"updated EMNLP camera ready version"},{"id":"http://arxiv.org/abs/2411.15577v1","updated":"2024-11-23T14:47:10Z","published":"2024-11-23T14:47:10Z","title":"From MTEB to MTOB: Retrieval-Augmented Classification for Descriptive\n  Grammars","summary":"  Recent advances in language modeling have demonstrated significant\nimprovements in zero-shot capabilities, including in-context learning,\ninstruction following, and machine translation for extremely under-resourced\nlanguages (Tanzer et al., 2024). However, many languages with limited written\nresources rely primarily on formal descriptions of grammar and vocabulary.\n  In this paper, we introduce a set of benchmarks to evaluate how well models\ncan extract and classify information from the complex descriptions found in\nlinguistic grammars. We present a Retrieval-Augmented Generation (RAG)-based\napproach that leverages these descriptions for downstream tasks such as machine\ntranslation. Our benchmarks encompass linguistic descriptions for 248 languages\nacross 142 language families, focusing on typological features from WALS and\nGrambank.\n  This set of benchmarks offers the first comprehensive evaluation of language\nmodels' in-context ability to accurately interpret and extract linguistic\nfeatures, providing a critical resource for scaling NLP to low-resource\nlanguages. The code and data are publicly available at\n\\url{https://github.com/al-the-eigenvalue/RAG-on-grammars}.\n","authors":["Albert Kornilov","Tatiana Shavrina"],"pdf_url":"https://arxiv.org/pdf/2411.15577v1.pdf","comment":"submitted to COLING 2025"},{"id":"http://arxiv.org/abs/2405.14129v2","updated":"2024-11-23T14:38:05Z","published":"2024-05-23T03:07:56Z","title":"AlignGPT: Multi-modal Large Language Models with Adaptive Alignment\n  Capability","summary":"  Multimodal Large Language Models (MLLMs) are widely regarded as crucial in\nthe exploration of Artificial General Intelligence (AGI). The core of MLLMs\nlies in their capability to achieve cross-modal alignment. To attain this goal,\ncurrent MLLMs typically follow a two-phase training paradigm: the pre-training\nphase and the instruction-tuning phase. Despite their success, there are\nshortcomings in the modeling of alignment capabilities within these models.\nFirstly, during the pre-training phase, the model usually assumes that all\nimage-text pairs are uniformly aligned, but in fact the degree of alignment\nbetween different image-text pairs is inconsistent. Secondly, the instructions\ncurrently used for finetuning incorporate a variety of tasks and different\ntasks usually require different levels of alignment capabilities, but previous\nMLLMs overlook these differentiated alignment needs. To tackle these issues, we\npropose a new multimodal large language model AlignGPT. In the pre-training\nstage, instead of treating all image-text pairs equally, we divide them into\ndifferent groups according to the degrees of alignment of them. Then, the model\nis trained to learn the representations of different alignment levels. In the\ninstruction-tuning phase, we adaptively combine these representations of\nalignment levels to meet the dynamic alignment needs of different tasks.\nExtensive experimental results show that our model achieves competitive\nperformance on 12 benchmarks.\n","authors":["Fei Zhao","Taotian Pang","Chunhui Li","Zhen Wu","Junjie Guo","Shangyu Xing","Xinyu Dai"],"pdf_url":"https://arxiv.org/pdf/2405.14129v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04742v3","updated":"2024-11-23T13:37:21Z","published":"2023-11-08T15:11:57Z","title":"Large-scale study of human memory for meaningful narratives","summary":"  The statistical study of human memory requires large-scale experiments,\ninvolving many stimuli conditions and test subjects. While this approach has\nproven to be quite fruitful for meaningless material such as random lists of\nwords, naturalistic stimuli, like narratives, have until now resisted such a\nlarge-scale study, due to the quantity of manual labor required to design and\nanalyze such experiments. In this work, we develop a pipeline that uses large\nlanguage models (LLMs) both to design naturalistic narrative stimuli for\nlarge-scale recall and recognition memory experiments, as well as to analyze\nthe results. We performed online memory experiments with a large number of\nparticipants and collected recognition and recall data for narratives of\ndifferent sizes. We found that both recall and recognition performance scale\nlinearly with narrative length; however, for longer narratives people tend to\nsummarize the content rather than recalling precise details. To investigate the\nrole of narrative comprehension in memory, we repeated these experiments using\nscrambled versions of the narratives. Although recall performance declined\nsignificantly, recognition remained largely unaffected. Recalls in this\ncondition seem to follow the original narrative order rather than the actual\nscrambled presentation, pointing to a contextual reconstruction of the story in\nmemory. Finally, using LLM text embeddings, we construct a simple measure for\neach clause based on semantic similarity to the whole narrative, that shows a\nstrong correlation with recall probability. Overall, our work demonstrates the\npower of LLMs in accessing new regimes in the study of human memory, as well as\nsuggesting novel psychologically informed benchmarks for LLM performance.\n","authors":["Antonios Georgiou","Tankut Can","Mikhail Katkov","Misha Tsodyks"],"pdf_url":"https://arxiv.org/pdf/2311.04742v3.pdf","comment":"45 pages, significant revision"},{"id":"http://arxiv.org/abs/2411.15560v1","updated":"2024-11-23T13:34:50Z","published":"2024-11-23T13:34:50Z","title":"Do LLMs Agree on the Creativity Evaluation of Alternative Uses?","summary":"  This paper investigates whether large language models (LLMs) show agreement\nin assessing creativity in responses to the Alternative Uses Test (AUT). While\nLLMs are increasingly used to evaluate creative content, previous studies have\nprimarily focused on a single model assessing responses generated by the same\nmodel or humans. This paper explores whether LLMs can impartially and\naccurately evaluate creativity in outputs generated by both themselves and\nother models. Using an oracle benchmark set of AUT responses, categorized by\ncreativity level (common, creative, and highly creative), we experiment with\nfour state-of-the-art LLMs evaluating these outputs. We test both scoring and\nranking methods and employ two evaluation settings (comprehensive and\nsegmented) to examine if LLMs agree on the creativity evaluation of alternative\nuses. Results reveal high inter-model agreement, with Spearman correlations\naveraging above 0.7 across models and reaching over 0.77 with respect to the\noracle, indicating a high level of agreement and validating the reliability of\nLLMs in creativity assessment of alternative uses. Notably, models do not\nfavour their own responses, instead they provide similar creativity assessment\nscores or rankings for alternative uses generated by other models. These\nfindings suggest that LLMs exhibit impartiality and high alignment in\ncreativity evaluation, offering promising implications for their use in\nautomated creativity assessment.\n","authors":["Abdullah Al Rabeyah","Fabrício Góes","Marco Volpe","Talles Medeiros"],"pdf_url":"https://arxiv.org/pdf/2411.15560v1.pdf","comment":"19 pages, 7 figures, 15 tables"},{"id":"http://arxiv.org/abs/2411.12865v2","updated":"2024-11-23T12:37:54Z","published":"2024-11-19T21:15:47Z","title":"AzSLD: Azerbaijani Sign Language Dataset for Fingerspelling, Word, and\n  Sentence Translation with Baseline Software","summary":"  Sign language processing technology development relies on extensive and\nreliable datasets, instructions, and ethical guidelines. We present a\ncomprehensive Azerbaijani Sign Language Dataset (AzSLD) collected from diverse\nsign language users and linguistic parameters to facilitate advancements in\nsign recognition and translation systems and support the local sign language\ncommunity. The dataset was created within the framework of a vision-based AzSL\ntranslation project. This study introduces the dataset as a summary of the\nfingerspelling alphabet and sentence- and word-level sign language datasets.\nThe dataset was collected from signers of different ages, genders, and signing\nstyles, with videos recorded from two camera angles to capture each sign in\nfull detail. This approach ensures robust training and evaluation of gesture\nrecognition models. AzSLD contains 30,000 videos, each carefully annotated with\naccurate sign labels and corresponding linguistic translations. The dataset is\naccompanied by technical documentation and source code to facilitate its use in\ntraining and testing. This dataset offers a valuable resource of labeled data\nfor researchers and developers working on sign language recognition,\ntranslation, or synthesis. Ethical guidelines were strictly followed throughout\nthe project, with all participants providing informed consent for collecting,\npublishing, and using the data.\n","authors":["Nigar Alishzade","Jamaladdin Hasanov"],"pdf_url":"https://arxiv.org/pdf/2411.12865v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05006v2","updated":"2024-11-23T12:34:59Z","published":"2024-07-06T08:58:26Z","title":"Recent Advancements and Challenges of Turkic Central Asian Language\n  Processing","summary":"  Research in NLP for Central Asian Turkic languages - Kazakh, Uzbek, Kyrgyz,\nand Turkmen - faces typical low-resource language challenges like data\nscarcity, limited linguistic resources and technology development. However,\nrecent advancements have included the collection of language-specific datasets\nand the development of models for downstream tasks. Thus, this paper aims to\nsummarize recent progress and identify future research directions. It provides\na high-level overview of each language's linguistic features, the current\ntechnology landscape, the application of transfer learning from higher-resource\nlanguages, and the availability of labeled and unlabeled data. By outlining the\ncurrent state, we hope to inspire and facilitate future research.\n","authors":["Yana Veitsman","Mareike Hartmann"],"pdf_url":"https://arxiv.org/pdf/2407.05006v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08074v2","updated":"2024-11-23T12:27:07Z","published":"2024-06-12T10:48:53Z","title":"A Concept-Based Explainability Framework for Large Multimodal Models","summary":"  Large multimodal models (LMMs) combine unimodal encoders and large language\nmodels (LLMs) to perform multimodal tasks. Despite recent advancements towards\nthe interpretability of these models, understanding internal representations of\nLMMs remains largely a mystery. In this paper, we present a novel framework for\nthe interpretation of LMMs. We propose a dictionary learning based approach,\napplied to the representation of tokens. The elements of the learned dictionary\ncorrespond to our proposed concepts. We show that these concepts are well\nsemantically grounded in both vision and text. Thus we refer to these as\n``multi-modal concepts''. We qualitatively and quantitatively evaluate the\nresults of the learnt concepts. We show that the extracted multimodal concepts\nare useful to interpret representations of test samples. Finally, we evaluate\nthe disentanglement between different concepts and the quality of grounding\nconcepts visually and textually. Our implementation is publicly available.\n","authors":["Jayneel Parekh","Pegah Khayatan","Mustafa Shukor","Alasdair Newson","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2406.08074v2.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.11053v4","updated":"2024-11-23T12:25:17Z","published":"2024-11-17T12:31:04Z","title":"SRA-MCTS: Self-driven Reasoning Augmentation with Monte Carlo Tree\n  Search for Code Generation","summary":"  Large language models demonstrate exceptional performance in simple code\ngeneration tasks but still face challenges in tackling complex problems. These\nchallenges may stem from insufficient reasoning and problem decomposition\ncapabilities. To address this issue, we propose a reasoning-augmented data\ngeneration process, SRA-MCTS, which guides the model to autonomously generate\nhigh-quality intermediate reasoning paths. This creates a positive feedback\nloop, enabling continuous improvement. Our method operates entirely through the\nmodel itself without requiring additional supervision. By synthesizing natural\nlanguage reasoning paths and translating them into executable code, the\napproach ensures analytical accuracy and enhances the success rate in solving\ncomplex tasks. Experimental results show that, even without additional\nsupervisory signals, our method achieves performance improvements across\ndifferent model scales, demonstrating the significant potential of\nself-improvement in small models. Furthermore, the method remains robust when\ntraditional Chain-of-Thought (CoT) approaches exhibit performance degradation,\nwith notable improvements observed in diversity metrics such as pass@10. We\nencourage further exploration of reasoning processes within training data to\nenhance the ability of language models to address complex problems. Our code\nand data are public at https://github.com/DIRECT-BIT/SRA-MCTS.\n","authors":["Bin Xu","Yiguan Lin","Yinghao Li","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2411.11053v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15530v1","updated":"2024-11-23T11:47:03Z","published":"2024-11-23T11:47:03Z","title":"QEQR: An Exploration of Query Expansion Methods for Question Retrieval\n  in CQA Services","summary":"  CQA services are valuable sources of knowledge that can be used to find\nanswers to users' information needs. In these services, question retrieval aims\nto help users with their information needs by finding similar questions to\ntheirs. However, finding similar questions is obstructed by the lexical gap\nthat exists between relevant questions. In this work, we target this problem by\nusing query expansion methods. We use word-similarity-based methods, propose a\nquestion-similarity-based method and selective expansion of these methods to\nexpand a question that's been submitted and mitigate the lexical gap problem.\nOur best method achieves a significant relative improvement of 1.8\\% compared\nto the best-performing baseline without query expansion.\n","authors":["Yasin Ghafourian","Sajad Movahedi","Azadeh Shakery"],"pdf_url":"https://arxiv.org/pdf/2411.15530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.12091v7","updated":"2024-11-23T11:06:05Z","published":"2022-01-28T13:00:17Z","title":"Linear Adversarial Concept Erasure","summary":"  Modern neural models trained on textual data rely on pre-trained\nrepresentations that emerge without direct supervision. As these\nrepresentations are increasingly being used in real-world applications, the\ninability to \\emph{control} their content becomes an increasingly important\nproblem. We formulate the problem of identifying and erasing a linear subspace\nthat corresponds to a given concept, in order to prevent linear predictors from\nrecovering the concept. We model this problem as a constrained, linear maximin\ngame, and show that existing solutions are generally not optimal for this task.\nWe derive a closed-form solution for certain objectives, and propose a convex\nrelaxation, \\method, that works well for others. When evaluated in the context\nof binary gender removal, the method recovers a low-dimensional subspace whose\nremoval mitigates bias by intrinsic and extrinsic evaluation. We show that the\nmethod is highly expressive, effectively mitigating bias in deep nonlinear\nclassifiers while maintaining tractability and interpretability.\n","authors":["Shauli Ravfogel","Michael Twiton","Yoav Goldberg","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2201.12091v7.pdf","comment":"Accepted in ICML 2022; a revised version"},{"id":"http://arxiv.org/abs/2411.07180v2","updated":"2024-11-23T11:00:34Z","published":"2024-11-11T17:57:30Z","title":"Counterfactual Generation from Language Models","summary":"  Understanding and manipulating the causal generation mechanisms in language\nmodels is essential for controlling their behavior. Previous work has primarily\nrelied on techniques such as representation surgery -- e.g., model ablations or\nmanipulation of linear subspaces tied to specific concepts -- to intervene on\nthese models. To understand the impact of interventions precisely, it is useful\nto examine counterfactuals -- e.g., how a given sentence would have appeared\nhad it been generated by the model following a specific intervention. We\nhighlight that counterfactual reasoning is conceptually distinct from\ninterventions, as articulated in Pearl's causal hierarchy. Based on this\nobservation, we propose a framework for generating true string counterfactuals\nby reformulating language models as Generalized Structural-equation. Models\nusing the Gumbel-max trick. This allows us to model the joint distribution over\noriginal strings and their counterfactuals resulting from the same\ninstantiation of the sampling noise. We develop an algorithm based on hindsight\nGumbel sampling that allows us to infer the latent noise variables and generate\ncounterfactuals of observed strings. Our experiments demonstrate that the\napproach produces meaningful counterfactuals while at the same time showing\nthat commonly used intervention techniques have considerable undesired side\neffects.\n","authors":["Shauli Ravfogel","Anej Svete","Vésteinn Snæbjarnarson","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2411.07180v2.pdf","comment":"A preprint"},{"id":"http://arxiv.org/abs/2411.15523v1","updated":"2024-11-23T10:57:41Z","published":"2024-11-23T10:57:41Z","title":"Enhancing Grammatical Error Detection using BERT with Cleaned Lang-8\n  Dataset","summary":"  This paper presents an improved LLM based model for Grammatical Error\nDetection (GED), which is a very challenging and equally important problem for\nmany applications. The traditional approach to GED involved hand-designed\nfeatures, but recently, Neural Networks (NN) have automated the discovery of\nthese features, improving performance in GED. Traditional rule-based systems\nhave an F1 score of 0.50-0.60 and earlier machine learning models give an F1\nscore of 0.65-0.75, including decision trees and simple neural networks.\nPrevious deep learning models, for example, Bi-LSTM, have reported F1 scores\nwithin the range from 0.80 to 0.90. In our study, we have fine-tuned various\ntransformer models using the Lang8 dataset rigorously cleaned by us. In our\nexperiments, the BERT-base-uncased model gave an impressive performance with an\nF1 score of 0.91 and accuracy of 98.49% on training data and 90.53% on testing\ndata, also showcasing the importance of data cleaning. Increasing model size\nusing BERT-large-uncased or RoBERTa-large did not give any noticeable\nimprovements in performance or advantage for this task, underscoring that\nlarger models are not always better. Our results clearly show how far rigorous\ndata cleaning and simple transformer-based models can go toward significantly\nimproving the quality of GED.\n","authors":["Rahul Nihalani","Kushal Shah"],"pdf_url":"https://arxiv.org/pdf/2411.15523v1.pdf","comment":"10 pages, 6 tables, 20 references"},{"id":"http://arxiv.org/abs/2411.03962v3","updated":"2024-11-23T10:18:33Z","published":"2024-11-06T14:51:02Z","title":"How Does A Text Preprocessing Pipeline Affect Ontology Syntactic\n  Matching?","summary":"  The generic text preprocessing pipeline, comprising Tokenisation,\nNormalisation, Stop Words Removal, and Stemming/Lemmatisation, has been\nimplemented in many ontology matching (OM) systems. However, the lack of\nstandardisation in text preprocessing creates diversity in mapping results. In\nthis paper, we investigate the effect of the text preprocessing pipeline on OM\ntasks at syntactic levels. Our experiments on 8 Ontology Alignment Evaluation\nInitiative (OAEI) track repositories with 49 distinct alignments indicate: (1)\nTokenisation and Normalisation are currently more effective than Stop Words\nRemoval and Stemming/Lemmatisation; and (2) The selection of Lemmatisation and\nStemming is task-specific. We recommend standalone Lemmatisation or Stemming\nwith post-hoc corrections. We find that (3) Porter Stemmer and Snowball Stemmer\nperform better than Lancaster Stemmer; and that (4) Part-of-Speech (POS)\nTagging does not help Lemmatisation. To repair less effective Stop Words\nRemoval and Stemming/Lemmatisation used in OM tasks, we propose a novel\ncontext-based pipeline repair approach that significantly improves matching\ncorrectness and overall matching performance. We also discuss the use of text\npreprocessing pipeline in the new era of large language models (LLMs).\n","authors":["Zhangcheng Qiang","Kerry Taylor","Weiqing Wang"],"pdf_url":"https://arxiv.org/pdf/2411.03962v3.pdf","comment":"13 pages, 26 figures, 4 tables"},{"id":"http://arxiv.org/abs/2411.15500v1","updated":"2024-11-23T09:27:38Z","published":"2024-11-23T09:27:38Z","title":"MolMetaLM: a Physicochemical Knowledge-Guided Molecular Meta Language\n  Model","summary":"  Most current molecular language models transfer the masked language model or\nimage-text generation model from natural language processing to molecular\nfield. However, molecules are not solely characterized by atom/bond symbols;\nthey encapsulate important physical/chemical properties. Moreover, normal\nlanguage models bring grammar rules that are irrelevant for understanding\nmolecules. In this study, we propose a novel physicochemical knowledge-guided\nmolecular meta language framework MolMetaLM. We design a molecule-specialized\nmeta language paradigm, formatted as multiple <S,P,O> (subject, predicate,\nobject) knowledge triples sharing the same S (i.e., molecule) to enhance\nlearning the semantic relationships between physicochemical knowledge and\nmolecules. By introducing different molecular knowledge and noises, the meta\nlanguage paradigm generates tens of thousands of pretraining tasks. By\nrecovering the token/sequence/order-level noises, MolMetaLM exhibits\nproficiency in large-scale benchmark evaluations involving property prediction,\nmolecule generation, conformation inference, and molecular optimization.\nThrough MolMetaLM, we offer a new insight for designing language models.\n","authors":["Yifan Wu","Min Zeng","Yang Li","Yang Zhang","Min Li"],"pdf_url":"https://arxiv.org/pdf/2411.15500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15491v1","updated":"2024-11-23T08:24:15Z","published":"2024-11-23T08:24:15Z","title":"Traditional Chinese Medicine Case Analysis System for High-Level\n  Semantic Abstraction: Optimized with Prompt and RAG","summary":"  This paper details a technical plan for building a clinical case database for\nTraditional Chinese Medicine (TCM) using web scraping. Leveraging multiple\nplatforms, including 360doc, we gathered over 5,000 TCM clinical cases,\nperformed data cleaning, and structured the dataset with crucial fields such as\npatient details, pathogenesis, syndromes, and annotations. Using the\n$Baidu\\_ERNIE\\_Speed\\_128K$ API, we removed redundant information and generated\nthe final answers through the $DeepSeekv2$ API, outputting results in standard\nJSON format. We optimized data recall with RAG and rerank techniques during\nretrieval and developed a hybrid matching scheme. By combining two-stage\nretrieval method with keyword matching via Jieba, we significantly enhanced the\naccuracy of model outputs.\n","authors":["Peng Xu","Hongjin Wu","Jinle Wang","Rongjia Lin","Liwei Tan"],"pdf_url":"https://arxiv.org/pdf/2411.15491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19346v2","updated":"2024-11-23T08:23:27Z","published":"2024-10-25T07:04:16Z","title":"AgentSense: Benchmarking Social Intelligence of Language Agents through\n  Interactive Scenarios","summary":"  Large language models (LLMs) are increasingly leveraged to empower autonomous\nagents to simulate human beings in various fields of behavioral research.\nHowever, evaluating their capacity to navigate complex social interactions\nremains a challenge. Previous studies face limitations due to insufficient\nscenario diversity, complexity, and a single-perspective focus. To this end, we\nintroduce AgentSense: Benchmarking Social Intelligence of Language Agents\nthrough Interactive Scenarios. Drawing on Dramaturgical Theory, AgentSense\nemploys a bottom-up approach to create 1,225 diverse social scenarios\nconstructed from extensive scripts. We evaluate LLM-driven agents through\nmulti-turn interactions, emphasizing both goal completion and implicit\nreasoning. We analyze goals using ERG theory and conduct comprehensive\nexperiments. Our findings highlight that LLMs struggle with goals in complex\nsocial scenarios, especially high-level growth needs, and even GPT-4o requires\nimprovement in private information reasoning. Code and data are available at\n\\url{https://github.com/ljcleo/agent_sense}.\n","authors":["Xinyi Mou","Jingcong Liang","Jiayu Lin","Xinnong Zhang","Xiawei Liu","Shiyue Yang","Rong Ye","Lei Chen","Haoyu Kuang","Xuanjing Huang","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2410.19346v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15488v1","updated":"2024-11-23T08:06:06Z","published":"2024-11-23T08:06:06Z","title":"Automatic Evaluation for Text-to-image Generation: Task-decomposed\n  Framework, Distilled Training, and Meta-evaluation Benchmark","summary":"  Driven by the remarkable progress in diffusion models, text-to-image\ngeneration has made significant strides, creating a pressing demand for\nautomatic quality evaluation of generated images. Current state-of-the-art\nautomatic evaluation methods heavily rely on Multi-modal Large Language Models\n(MLLMs), particularly powerful commercial models like GPT-4o. While these\nmodels are highly effective, their substantial costs limit scalability in\nlarge-scale evaluations. Adopting open-source MLLMs is an alternative; however,\ntheir performance falls short due to significant limitations in processing\nmulti-modal data compared to commercial MLLMs. To tackle these problems, we\nfirst propose a task decomposition evaluation framework based on GPT-4o to\nautomatically construct a new training dataset, where the complex evaluation\ntask is decoupled into simpler sub-tasks, effectively reducing the learning\ncomplexity. Based on this dataset, we design innovative training strategies to\neffectively distill GPT-4o's evaluation capabilities into a 7B open-source\nMLLM, MiniCPM-V-2.6. Furthermore, to reliably and comprehensively assess prior\nworks and our proposed model, we manually annotate a meta-evaluation benchmark\nthat includes chain-of-thought explanations alongside quality scores for\ngenerated images. Experimental results demonstrate that our distilled\nopen-source MLLM significantly outperforms the current state-of-the-art\nGPT-4o-base baseline, VIEScore, with over 4.6\\% improvement in Spearman and\nKendall correlations with human judgments.\n","authors":["Rong-Cheng Tu","Zi-Ao Ma","Tian Lan","Yuehao Zhao","Heyan Huang","Xian-Ling Mao"],"pdf_url":"https://arxiv.org/pdf/2411.15488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15486v1","updated":"2024-11-23T07:54:15Z","published":"2024-11-23T07:54:15Z","title":"Transition Network Analysis: A Novel Framework for Modeling,\n  Visualizing, and Identifying the Temporal Patterns of Learners and Learning\n  Processes","summary":"  This paper proposes a novel analytical framework: Transition Network Analysis\n(TNA), an approach that integrates Stochastic Process Mining and probabilistic\ngraph representation to model, visualize, and identify transition patterns in\nthe learning process data. Combining the relational and temporal aspects into a\nsingle lens offers capabilities beyond either framework, including centralities\nto capture important learning events, community finding to identify patterns of\nbehavior, and clustering to reveal temporal patterns. This paper introduces the\ntheoretical and mathematical foundations of TNA. To demonstrate the\nfunctionalities of TNA, we present a case study with students (n=191) engaged\nin small-group collaboration to map patterns of group dynamics using the\ntheories of co-regulation and socially-shared regulated learning. The analysis\nrevealed that TNA could reveal the regulatory processes and identify important\nevents, temporal patterns and clusters. Bootstrap validation established the\nsignificant transitions and eliminated spurious transitions. In doing so, we\nshowcase TNA's utility to capture learning dynamics and provide a robust\nframework for investigating the temporal evolution of learning processes.\nFuture directions include advancing estimation methods, expanding reliability\nassessment, exploring longitudinal TNA, and comparing TNA networks using\npermutation tests.\n","authors":["Mohammed Saqr","Sonsoles López-Pernas","Tiina Törmänen","Rogers Kaliisa","Kamila Misiejuk","Santtu Tikka"],"pdf_url":"https://arxiv.org/pdf/2411.15486v1.pdf","comment":"Accepted at Learning Analytics & Knowledge (LAK '25)"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2404.15269v3","updated":"2024-11-23T16:19:03Z","published":"2024-04-23T17:57:47Z","title":"Aligning LLM Agents by Learning Latent Preference from User Edits","summary":"  We study interactive learning of LLM-based language agents based on user\nedits made to the agent's output. In a typical setting such as writing\nassistants, the user interacts with a language agent to generate a response\ngiven a context, and may optionally edit the agent response to personalize it\nbased on their latent preference, in addition to improving the correctness. The\nedit feedback is naturally generated, making it a suitable candidate for\nimproving the agent's alignment with the user's preference, and for reducing\nthe cost of user edits over time. We propose a learning framework, PRELUDE that\ninfers a description of the user's latent preference based on historic edit\ndata. The inferred user preference descriptions are used to define prompts for\ngenerating responses in the future. This avoids fine-tuning the agent, which is\ncostly, challenging to scale with the number of users, and may even degrade its\nperformance on other tasks. Furthermore, learning descriptive preference\nimproves interpretability, allowing the user to view and modify the learned\npreference. However, user preference can be complex, subtle, and vary based on\ncontext, making it challenging to learn. To address this, we propose a simple\nyet effective algorithm named CIPHER that leverages the LLM to infer the user\npreference for a given context based on user edits. In the future, CIPHER\nretrieves inferred preferences from the k-closest contexts in the history, and\nforms an aggregate preference for response generation. We introduce two\ninteractive environments -- summarization and email writing, and use a GPT-4\nsimulated user for evaluation. On both tasks, CIPHER outperforms several\nbaselines by achieving the lowest edit distance cost while only having a small\noverhead in LLM query cost. Our analysis reports that user preferences learned\nby CIPHER show significant similarity to the ground truth latent preferences.\n","authors":["Ge Gao","Alexey Taymanov","Eduardo Salinas","Paul Mineiro","Dipendra Misra"],"pdf_url":"https://arxiv.org/pdf/2404.15269v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04407v2","updated":"2024-11-23T16:00:51Z","published":"2023-10-06T17:55:23Z","title":"Policy-Gradient Training of Language Models for Ranking","summary":"  Text retrieval plays a crucial role in incorporating factual knowledge for\ndecision making into language processing pipelines, ranging from chat-based web\nsearch to question answering systems. Current state-of-the-art text retrieval\nmodels leverage pre-trained large language models (LLMs) to achieve competitive\nperformance, but training LLM-based retrievers via typical contrastive losses\nrequires intricate heuristics, including selecting hard negatives and using\nadditional supervision as learning signals. This reliance on heuristics stems\nfrom the fact that the contrastive loss itself is heuristic and does not\ndirectly optimize the downstream metrics of decision quality at the end of the\nprocessing pipeline. To address this issue, we introduce Neural PG-RANK, a\nnovel training algorithm that learns to rank by instantiating a LLM as a\nPlackett-Luce ranking policy. Neural PG-RANK provides a principled method for\nend-to-end training of retrieval models as part of larger decision systems via\npolicy gradient, with little reliance on complex heuristics, and it effectively\nunifies the training objective with downstream decision-making quality. We\nconduct extensive experiments on various text retrieval benchmarks. The results\ndemonstrate that when the training objective aligns with the evaluation setup,\nNeural PG-RANK yields remarkable in-domain performance improvement, with\nsubstantial out-of-domain generalization to some critical datasets employed in\ndownstream question answering tasks.\n","authors":["Ge Gao","Jonathan D. Chang","Claire Cardie","Kianté Brantley","Thorsten Joachim"],"pdf_url":"https://arxiv.org/pdf/2310.04407v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15550v1","updated":"2024-11-23T13:15:13Z","published":"2024-11-23T13:15:13Z","title":"Class Order Disorder in Wikidata and First Fixes","summary":"  Wikidata has a large ontology with classes at several orders. The Wikidata\nontology has long been known to have violations of class order and information\nrelated to class order that appears suspect. SPARQL queries were evaluated\nagainst Wikidata to determine the prevalence of several kinds of violations and\nsuspect information and the results analyzed. Some changes were manually made\nto Wikidata to remove some of these results and the queries rerun, showing the\neffect of the changes. Suggestions are provided on how the problems uncovered\nmight be addressed, either though better tooling or involvement of the Wikidata\ncommunity.\n","authors":["Peter F. Patel-Schneider","Ege Atacan Doğan"],"pdf_url":"https://arxiv.org/pdf/2411.15550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15530v1","updated":"2024-11-23T11:47:03Z","published":"2024-11-23T11:47:03Z","title":"QEQR: An Exploration of Query Expansion Methods for Question Retrieval\n  in CQA Services","summary":"  CQA services are valuable sources of knowledge that can be used to find\nanswers to users' information needs. In these services, question retrieval aims\nto help users with their information needs by finding similar questions to\ntheirs. However, finding similar questions is obstructed by the lexical gap\nthat exists between relevant questions. In this work, we target this problem by\nusing query expansion methods. We use word-similarity-based methods, propose a\nquestion-similarity-based method and selective expansion of these methods to\nexpand a question that's been submitted and mitigate the lexical gap problem.\nOur best method achieves a significant relative improvement of 1.8\\% compared\nto the best-performing baseline without query expansion.\n","authors":["Yasin Ghafourian","Sajad Movahedi","Azadeh Shakery"],"pdf_url":"https://arxiv.org/pdf/2411.15530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15451v1","updated":"2024-11-23T04:45:18Z","published":"2024-11-23T04:45:18Z","title":"Quantitative Analysis of IITs' Research Growth and SDG Contributions","summary":"  The Indian Institutes of Technology (IITs) are vital to India's research\necosystem, advancing technology and engineering for industrial and societal\nbenefits. This study reviews the research performance of top IITs-Bombay,\nDelhi, Madras, Kharagpur, and Kanpur based on Scopus-indexed publications\n(1952-2024). Research output has grown exponentially, supported by increased\nfunding and collaborations. IIT-Kanpur excels in research impact, while\nIIT-Bombay and IIT-Madras are highly productive but show slightly lower\nper-paper impact. Internationally, IITs collaborate robustly with the USA,\nGermany, and the UK, alongside Asian nations like Japan and South Korea, with\nIIT-Madras leading inter-IIT partnerships. Research priorities align with SDG 3\n(Health), SDG 7 (Clean Energy), and SDG 11 (Sustainable Cities). Despite\nstrengths in fields like energy, fluid dynamics, and materials science,\nchallenges persist, including limited collaboration with newer IITs and gaps in\nemerging fields. Strengthening specialization and partnerships is crucial for\naddressing global challenges and advancing sustainable development.\n","authors":["Kiran Sharma","Akshat Nagori"," Manya","Mehul Dubey","Parul Khurana"],"pdf_url":"https://arxiv.org/pdf/2411.15451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15430v1","updated":"2024-11-23T03:15:31Z","published":"2024-11-23T03:15:31Z","title":"The Landscape of Data Reuse in Interactive Information Retrieval:\n  Motivations, Sources, and Evaluation of Reusability","summary":"  Sharing and reusing research data can effectively reduce redundant efforts in\ndata collection and curation, especially for small labs and research teams\nconducting human-centered system research, and enhance the replicability of\nevaluation experiments. Building a sustainable data reuse process and culture\nrelies on frameworks that encompass policies, standards, roles, and\nresponsibilities, all of which must address the diverse needs of data\nproviders, curators, and reusers.\n  To advance the knowledge and accumulate empirical understandings on data\nreuse, this study investigated the data reuse practices of experienced\nresearchers from the area of Interactive Information Retrieval (IIR) studies,\nwhere data reuse has been strongly advocated but still remains a challenge. To\nenhance the knowledge on data reuse behavior and reusability assessment\nstrategies within IIR community, we conducted 21 semi-structured in-depth\ninterviews with IIR researchers from varying demographic backgrounds,\ninstitutions, and stages of careers on their motivations, experiences, and\nconcerns over data reuse. We uncovered the reasons, strategies of reusability\nassessments, and challenges faced by data reusers within the field of IIR as\nthey attempt to reuse researcher data in their studies. The empirical finding\nimproves our understanding of researchers' motivations for reusing data, their\napproaches to discovering reusable research data, as well as their concerns and\ncriteria for assessing data reusability, and also enriches the on-going\ndiscussions on evaluating user-generated data and research resources and\npromoting community-level data reuse culture and standards.\n","authors":["Tianji Jiang","Wenqi Li","Jiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2411.15430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15396v1","updated":"2024-11-23T00:43:27Z","published":"2024-11-23T00:43:27Z","title":"The Decoy Dilemma in Online Medical Information Evaluation: A\n  Comparative Study of Credibility Assessments by LLM and Human Judges","summary":"  Can AI be cognitively biased in automated information judgment tasks? Despite\nrecent progresses in measuring and mitigating social and algorithmic biases in\nAI and large language models (LLMs), it is not clear to what extent LLMs behave\n\"rationally\", or if they are also vulnerable to human cognitive bias triggers.\nTo address this open problem, our study, consisting of a crowdsourcing user\nexperiment and a LLM-enabled simulation experiment, compared the credibility\nassessments by LLM and human judges under potential decoy effects in an\ninformation retrieval (IR) setting, and empirically examined the extent to\nwhich LLMs are cognitively biased in COVID-19 medical (mis)information\nassessment tasks compared to traditional human assessors as a baseline. The\nresults, collected from a between-subject user experiment and a LLM-enabled\nreplicate experiment, demonstrate that 1) Larger and more recent LLMs tend to\nshow a higher level of consistency and accuracy in distinguishing credible\ninformation from misinformation. However, they are more likely to give higher\nratings for misinformation due to the presence of a more salient, decoy\nmisinformation result; 2) While decoy effect occurred in both human and LLM\nassessments, the effect is more prevalent across different conditions and\ntopics in LLM judgments compared to human credibility ratings. In contrast to\nthe generally assumed \"rationality\" of AI tools, our study empirically confirms\nthe cognitive bias risks embedded in LLM agents, evaluates the decoy impact on\nLLMs against human credibility assessments, and thereby highlights the\ncomplexity and importance of debiasing AI agents and developing\npsychology-informed AI audit techniques and policies for automated judgment\ntasks and beyond.\n","authors":["Jiqun Liu","Jiangen He"],"pdf_url":"https://arxiv.org/pdf/2411.15396v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2411.15457v1","updated":"2024-11-23T05:18:43Z","published":"2024-11-23T05:18:43Z","title":"Hindi audio-video-Deepfake (HAV-DF): A Hindi language-based Audio-video\n  Deepfake Dataset","summary":"  Deepfakes offer great potential for innovation and creativity, but they also\npose significant risks to privacy, trust, and security. With a vast\nHindi-speaking population, India is particularly vulnerable to deepfake-driven\nmisinformation campaigns. Fake videos or speeches in Hindi can have an enormous\nimpact on rural and semi-urban communities, where digital literacy tends to be\nlower and people are more inclined to trust video content. The development of\neffective frameworks and detection tools to combat deepfake misuse requires\nhigh-quality, diverse, and extensive datasets. The existing popular datasets\nlike FF-DF (FaceForensics++), and DFDC (DeepFake Detection Challenge) are based\non English language.. Hence, this paper aims to create a first novel Hindi deep\nfake dataset, named ``Hindi audio-video-Deepfake'' (HAV-DF). The dataset has\nbeen generated using the faceswap, lipsyn and voice cloning methods. This\nmulti-step process allows us to create a rich, varied dataset that captures the\nnuances of Hindi speech and facial expressions, providing a robust foundation\nfor training and evaluating deepfake detection models in a Hindi language\ncontext. It is unique of its kind as all of the previous datasets contain\neither deepfake videos or synthesized audio. This type of deepfake dataset can\nbe used for training a detector for both deepfake video and audio datasets.\nNotably, the newly introduced HAV-DF dataset demonstrates lower detection\naccuracy's across existing detection methods like Headpose, Xception-c40, etc.\nCompared to other well-known datasets FF-DF, and DFDC. This trend suggests that\nthe HAV-DF dataset presents deeper challenges to detect, possibly due to its\nfocus on Hindi language content and diverse manipulation techniques. The HAV-DF\ndataset fills the gap in Hindi-specific deepfake datasets, aiding multilingual\ndeepfake detection development.\n","authors":["Sukhandeep Kaur","Mubashir Buhari","Naman Khandelwal","Priyansh Tyagi","Kiran Sharma"],"pdf_url":"https://arxiv.org/pdf/2411.15457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15455v1","updated":"2024-11-23T05:13:27Z","published":"2024-11-23T05:13:27Z","title":"MUFM: A Mamba-Enhanced Feedback Model for Micro Video Popularity\n  Prediction","summary":"  The surge in micro-videos is transforming the concept of popularity. As\nresearchers delve into vast multi-modal datasets, there is a growing interest\nin understanding the origins of this popularity and the forces driving its\nrapid expansion. Recent studies suggest that the virality of short videos is\nnot only tied to their inherent multi-modal content but is also heavily\ninfluenced by the strength of platform recommendations driven by audience\nfeedback. In this paper, we introduce a framework for capturing long-term\ndependencies in user feedback and dynamic event interactions, based on the\nMamba Hawkes process. Our experiments on the large-scale open-source\nmulti-modal dataset show that our model significantly outperforms\nstate-of-the-art approaches across various metrics by 23.2%. We believe our\nmodel's capability to map the relationships within user feedback behavior\nsequences will not only contribute to the evolution of next-generation\nrecommendation algorithms and platform applications but also enhance our\nunderstanding of micro video dissemination and its broader societal impact.\n","authors":["Jiacheng Lu","Mingyuan Xiao","Weijian Wang","Yuxin Du","Yi Cui","Jingnan Zhao","Cheng Hua"],"pdf_url":"https://arxiv.org/pdf/2411.15455v1.pdf","comment":"14 pages,9 figures"},{"id":"http://arxiv.org/abs/2411.15447v1","updated":"2024-11-23T04:27:19Z","published":"2024-11-23T04:27:19Z","title":"Gotta Hear Them All: Sound Source Aware Vision to Audio Generation","summary":"  Vision-to-audio (V2A) synthesis has broad applications in multimedia. Recent\nadvancements of V2A methods have made it possible to generate relevant audios\nfrom inputs of videos or still images. However, the immersiveness and\nexpressiveness of the generation are limited. One possible problem is that\nexisting methods solely rely on the global scene and overlook details of local\nsounding objects (i.e., sound sources). To address this issue, we propose a\nSound Source-Aware V2A (SSV2A) generator. SSV2A is able to locally perceive\nmultimodal sound sources from a scene with visual detection and cross-modality\ntranslation. It then contrastively learns a Cross-Modal Sound Source (CMSS)\nManifold to semantically disambiguate each source. Finally, we attentively mix\ntheir CMSS semantics into a rich audio representation, from which a pretrained\naudio generator outputs the sound. To model the CMSS manifold, we curate a\nnovel single-sound-source visual-audio dataset VGGS3 from VGGSound. We also\ndesign a Sound Source Matching Score to measure localized audio relevance. This\nis to our knowledge the first work to address V2A generation at the\nsound-source level. Extensive experiments show that SSV2A surpasses\nstate-of-the-art methods in both generation fidelity and relevance. We further\ndemonstrate SSV2A's ability to achieve intuitive V2A control by compositing\nvision, text, and audio conditions. Our SSV2A generation can be tried and heard\nat https://ssv2a.github.io/SSV2A-demo .\n","authors":["Wei Guo","Heng Wang","Weidong Cai","Jianbo Ma"],"pdf_url":"https://arxiv.org/pdf/2411.15447v1.pdf","comment":"16 pages, 9 figures, source code released at\n  https://github.com/wguo86/SSV2A"}]}}